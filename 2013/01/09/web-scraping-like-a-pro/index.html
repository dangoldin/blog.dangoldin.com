<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Web scraping like a pro - Dan Goldin
</title><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=referrer content="no-referrer"><meta name=description content="Tips to help you write better web scrapers"><meta property="og:site_name" content="Dan Goldin"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:url" content="/2013/01/09/web-scraping-like-a-pro/"><meta property="og:title" content="Web scraping like a pro"><meta property="og:image" content="/image/photo.jpg"><meta property="og:description" content="Tips to help you write better web scrapers"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@dangoldin"><meta name=twitter:creator content="@dangoldin"><meta name=twitter:title content="Web scraping like a pro"><meta name=twitter:description content="Tips to help you write better web scrapers"><meta name=twitter:image content="/image/photo.jpg"><link rel=canonical href=/2013/01/09/web-scraping-like-a-pro/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.min.css integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin=anonymous><link rel=stylesheet href=https://nanx-assets.netlify.app/fonts.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github-gist.min.css integrity="sha512-od7JLoOTxM8w/HSKGzP9Kexc20K9p/M2zxSWsd7H1e4Ctf+8SQFtCWEZnW5u6ul5ehSECa5QmOk9ju2nQMmlVA==" crossorigin=anonymous><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36313127-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-36313127-1")</script></head><body><div class="my-4 my-lg-5 header"><div class=container><div class=row><div class="col-auto offset-lg-2 d-none d-lg-block"><a href=/><img class="ml-lg-4 logo img-fluid d-block rounded-circle" src=/image/photo.jpg alt=logo></a></div><div class="col-auto align-self-center mr-auto"><a href=/><h1 class=name>Dan Goldin</h1></a><ul class="nav nav-primary"><li class=nav-item><a class="nav-link text-home" href=/>Home</a></li><li class=nav-item><a class="nav-link text-about" href=/about/>About</a></li><li class=nav-item><a class="nav-link text-lore" href=/lore/>Lore</a></li></ul></div></div></div></div><div class=content><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><h1 class="mx-0 mx-md-4 blog-post-title">Web scraping like a pro</h1><div class="mb-md-4 meta"><span class="date middot" title='Wed Jan 9 2013 00:00:00 UTC'>2013-01-09
</span><span class="reading-time middot">5 min read</span><div class="d-none d-md-inline tags"><ul class="list-unstyled d-inline"><li class="d-inline middot"><a href=/tags/code>code</a></li></ul></div><div class="d-none d-md-inline tags"><ul class="list-unstyled d-inline"></ul></div></div><div class="markdown blog-post-content"><p>I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.</p><p>Hope they’re useful!</p><ul><li><b>Avoid it if possible</b><br>It is a bit odd that I’m starting off with this as the first tip but if there are alternatives definitely take a look at those; many sites come with an API and that may be a much better approach. Otherwise, every time there’s a change in the HTML structure you run a risk of breaking your scraper which will leave you scrambling to fix your code. It’s also a good idea to organize your code such that a change in the HTML for one of the scraped items does not break the others. For example, if you want to get the name and address of a restaurant from Yelp, have one method that will get the name and another that will get the address. This will most likely be less efficient so you’ll need to use your judgement to see whether the risk-speed tradeoff is worth it.</li><li><b>Use a library</b></br/>
Unless you’re doing a one off job, use a library. Every major language has one: Python has <a href=http://www.crummy.com/software/BeautifulSoup/>BeautifulSoup</a>, Perl has <a href=http://search.cpan.org/~cjm/HTML-Tree-5.03/lib/HTML/TreeBuilder.pm>HTML::TreeBuilder</a>, Javascript has <a href=https://npmjs.org/package/htmlparser>htmlparser</a>, and there’s no excuse to not use one. If you ever need to go back to make some changes (which you most likely will need to), you’ll be glad you did. You can also find libraries that let you simulate browser behavior by storing cookies and letting you submit forms. This gives you the ability to scrape sites that require a login. Some sites try to prevent scraping by obfuscating their HTML a bit in which case you’ll need to do either a string replacement or a basic regular expression to get it parsed by the library.</li><li><b>HTML/DOM inspectors are a must</b><br>Since scraping requires getting specific elements from a web page, we need to understand the HTML structure of that page. For me, doing this work within the browser works best since it gives you the ability to both see the HTML that’s responsible for a certain element and also gives you a console window which lets you test a scraping approach. The two browsers I’ve used successfully for this are <a href=https://www.google.com/intl/en/chrome/browser/>Google Chrome</a> and <a href=http://www.mozilla.org/en-US/firefox/new/>Firefox</a> with the <a href=http://getfirebug.com/>Firebug</a> plugin.</li><li><b>User agent spoofing</b><br>Every time your browser visits a website, it submits a request that contains information about the browser. This is why some sites show a different page when you’re using a phone versus a computer. Every once in awhile you will need to trick the site into sending back the proper page by “spoofing” the user agent. A simple way to check if you need to do this is to view the source of a page in a browser and compare it with what you’re retrieving in your code. If they’re different, try changing the user agent and see if that fixes it.</li><li><b>Be clever</b><br>Looking at the source of a page may be a bit overwhelming and there may be easier ways of getting at that information so be clever! An example of two approaches that I stumbled across were to spoof a mobile browser and to call the AJAX url directly. Spoofing a mobile browser tends to give you simpler and more lightweight HTML which is easier to parse. Loading the content via AJAX lets you get at the content quicker and usually in a more structured format, like JSON or XML. These approaches won’t work on every site so you need to do some research and experiment a little to understand how each site is setup. After that you can figure out the best approach for your scraper.</li><li><b>Be specific</b><br>When scraping, you want to make your scraping code rigorous enough to not fail if the page structure ever changes. A good rule of thumb is to be specific when you write your scraper. Use a specific id rather than a class since the id is guaranteed to be unique. Similarly, avoid an ordinal approach where you reference the 2nd or 3rd div. Sometimes this is unavoidable but try to see if there’s another approach. Another useful tidbit is to use the more content-descriptive identifier in the page. For example, if you see a div with the address you want to scrape and that div has two classes, “location-address” and “blue-highlight”, use the “location-address” one since that’s defining what the content is, not how it’s displayed.</li><li><b>Save the HTML of the retrieved pages</b><br>It’s helpful to save each HTML page you’ve retrieved. It takes a few iterations to get your scraping code working and it’s quicker to just have the HTML on disk so you don’t have to download it every time the script runs. Another advantage is that if you discover a mistake in your code, you don’t have to redownload all the pages you’ve already processed. It only takes a few minutes of work and worth doing.</li><li><b>Monitor actively</b><br>Scraping is prone to breaking so make sure you monitor the job as it runs. It’s likely that your code will work well on one page but will fail on others. I tend to write my code to be a bit picky at first while I work out the kinks and once I’m confident in it I will build in some logic to deal with a missing value to make sure it continues to run. As I mentioned earlier, storing the HTML of the page will save you time if you need to update your scraper and need to rerun it.</li><li><b>Throttle your requests</b><br>If you don’t want your roommates pissed pissed at you, which will happen when Yelp blocks you for 6 months, throttle your requests. The simple way to do this is to have your code wait in between downloading pages and another approach is to use proxies to hide your true IP address. This will make it seem that the requests are coming from a variety of computers and keep your roommates happy.</li></ul></div><div class=navigation><div class=row><div class="col-12 col-lg-6"><div class="mx-0 mx-md-4 mt-4 text-left"><a href=/2013/01/02/2013-goals/>« 2013 Goals</a></div></div><div class="col-12 col-lg-6"><div class="mx-0 mx-md-4 mt-4 text-right"><a href=/2013/01/11/is-the-pen-mightier-than-the-sword-in-a-social-world/>Is the pen mightier than the sword in a social world? »</a></div></div></div></div></div></div></div></div><section id=comments><div class="py-3 content"><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><div class=comments><script src=https://utteranc.es/client.js repo=dangoldin/blog.dangoldin.com-comments issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></div></div></div></div></div></section><div class="my-4 footer"><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><hr></div></div><div class="row justify-content-center"><div class="col-sm-12 col-lg-2"><div class="mx-0 mx-md-4 site-copyright">© 2024 Dan Goldin</div></div><div class="col-sm-12 col-lg-6"><div class="mx-0 mx-md-4 site-social"><ul><li><a href=mailto:dan@dangoldin.com>Contact</a></li><li><a href=https://github.com/dangoldin target=_blank>GitHub</a></li><li><a href=https://twitter.com/dangoldin target=_blank>Twitter</a></li><li><a href=https://www.linkedin.com/in/dangoldin target=_blank>LinkedIn</a></li></ul></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js defer></script><script>window.addEventListener("load",function(){hljs.initHighlighting()},!0)</script></body></html>