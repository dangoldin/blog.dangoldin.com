<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Javascript on Dan Goldin</title><link>/tags/javascript/</link><description>Recent content in Javascript on Dan Goldin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 12 Dec 2015 00:00:00 +0000</lastBuildDate><atom:link href="/tags/javascript/index.xml" rel="self" type="application/rss+xml"/><item><title>Jersey City garbage truck routes</title><link>/2015/12/12/jersey-city-garbage-truck-routes/</link><pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/12/jersey-city-garbage-truck-routes/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-garbage-trucks.png" alt="Jersey City garbage trucks" data-width="718" data-height="811" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>A couple of months ago I took a stab at plotting the Jersey City &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/">parking zones&lt;/a> after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530">comment&lt;/a> pointing out that Jersey City has a bunch of open data available, including a near-real time feed of &lt;a href="http://www.jciaonline.org/gpsMap.php?view=map">garbage truck locations&lt;/a>, a general &lt;a href="http://data.jerseycitynj.gov/">open data portal&lt;/a>, as well as the ability to &lt;a href="https://jerseycitynj.seamlessdocs.com/w/records_request">request custom data&lt;/a>. As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and the code is up on &lt;a href="https://github.com/dangoldin/jersey-city-open-data">GitHub&lt;/a>.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones II</title><link>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</link><pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-parking-zones.png" alt="Jersey City parking zones" data-width="442" data-height="640" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and includes the zones that had enough geocodeable addresses to generate a valid polygon.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones</title><link>/2015/09/12/mapping-the-jersey-city-parking-zones/</link><pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/12/mapping-the-jersey-city-parking-zones/</guid><description>&lt;p>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there&amp;rsquo;s a &lt;a href="http://jcparking.org/PDF/ZONE%20PERMITS%20ALL%20ZONES.pdf" target="_blank">PDF&lt;/a> that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.&lt;/p></description></item><item><title>Lists and localStorage</title><link>/2015/02/26/lists-and-localstorage/</link><pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/26/lists-and-localstorage/</guid><description>&lt;p>I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.&lt;/p></description></item><item><title>Lessons from Node</title><link>/2015/02/22/lessons-from-node/</link><pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/22/lessons-from-node/</guid><description>&lt;p>I&amp;rsquo;ve decided to move on from &lt;a href="http://nodejs.org/" target="_blank">Node&lt;/a> after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.&lt;/p>
&lt;ul>
&lt;li>Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write.&lt;/li>
&lt;li>It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code.&lt;/li>
&lt;li>JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want.&lt;/li>
&lt;li>Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests.&lt;/li>
&lt;li>Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.&lt;/li>
&lt;/ul>
&lt;p>I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.&lt;/p></description></item><item><title>Redirect recursion</title><link>/2014/12/31/redirect-recursion/</link><pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/31/redirect-recursion/</guid><description>&lt;p>I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.&lt;/p></description></item><item><title>More Sierpinski fun</title><link>/2014/02/21/more-sierpinski-fun/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/21/more-sierpinski-fun/</guid><description>&lt;p>As a follow up to my previous &lt;a href="http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3/">post&lt;/a>, I modified my Sierpinski generation code to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it&amp;rsquo;s difficult to confirm given a fixed screen resolution. I&amp;rsquo;d love to know what&amp;rsquo;s going on here and whether there&amp;rsquo;s a relationship between the number of sides and the distance ratio.&lt;/p></description></item><item><title>Sierpinski triangle in D3</title><link>/2014/02/19/sierpinski-triangle-in-d3/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/19/sierpinski-triangle-in-d3/</guid><description>&lt;p>There&amp;rsquo;s a little known algorithm for constructing a &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_triangle" target="_blank">Sierpinski triangle&lt;/a> that is surprisingly easy to implement.&lt;/p>
&lt;ol>
&lt;li>Start the three vertices that form a triangle&lt;/li>
&lt;li>Pick a random point inside the triangle&lt;/li>
&lt;li>Pick a random vertex&lt;/li>
&lt;li>Go halfway from a the random point to the vertex and mark that point&lt;/li>
&lt;li>Go to step 3 using the result of 4 as the starting point&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;m trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following file. Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_carpet" target="_blank">Sierpinski carpet&lt;/a> can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.&lt;/p></description></item><item><title>Drowning in JavaScript</title><link>/2013/12/01/drowning-in-javascript/</link><pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/01/drowning-in-javascript/</guid><description>&lt;p>I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.&lt;/p>
&lt;p>To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.&lt;/p></description></item><item><title>Some JavaScript Tools</title><link>/2013/10/05/some-javascript-tools/</link><pubDate>Sat, 05 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/05/some-javascript-tools/</guid><description>&lt;p>Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.&lt;/p>
&lt;p>To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &amp;lt;td&amp;gt; tag and then wrap each row in a &amp;lt;tr&amp;gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.&lt;/p></description></item><item><title>Extract info from a web page using JavaScript</title><link>/2013/08/26/extract-info-from-a-web-page-using-javascript/</link><pubDate>Mon, 26 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/26/extract-info-from-a-web-page-using-javascript/</guid><description>&lt;p>How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.&lt;/p>
&lt;p>In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.&lt;/p></description></item><item><title>D3 and Vega</title><link>/2013/07/09/d3-and-vega/</link><pubDate>Tue, 09 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/09/d3-and-vega/</guid><description>&lt;img src="/image/mcsp-star.png" alt="A data visualization I'm working on" data-width="780" data-height="780" data-layout="responsive" />
&lt;p>Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.&lt;/p></description></item></channel></rss>