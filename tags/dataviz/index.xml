<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>dataviz on Dan Goldin</title><link>/tags/dataviz/</link><description>Recent content in dataviz on Dan Goldin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 27 Jul 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/dataviz/index.xml" rel="self" type="application/rss+xml"/><item><title>Analyzing the AWS EC2 reservation options</title><link>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</guid><description>While writing the post on AWS reservations I started thinking if there&amp;rsquo;s any arbitrage opportunity in the reservations. For example - does it make sense to do a 1 year or 3 year reservation for some instance types of upgrade an instance class to get a better reservation value?
You can do this manually using ec2instances.info which provides a quick way to look at EC2 pricing info although forces you to pick the type of reservations you&amp;rsquo;re interested in.</description></item><item><title>Data analysis and visualization</title><link>/2020/04/29/data-analysis-and-visualization/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/29/data-analysis-and-visualization/</guid><description>Yesterday I had the rare chance to actually do some coding and realized how rusty I am at numerical analysis in Python. The task was simple - ingest a CSV that had a date column, two categorical columns, and a numerical column - and then generate a grid containing a series of line plots, each of which would be a combination of the two categorical columns.
I did a ton of this work years ago so knew what was possible.</description></item><item><title>Dumping Apple health data into MySQL</title><link>/2020/04/11/dumping-apple-health-data-into-mysql/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/11/dumping-apple-health-data-into-mysql/</guid><description>I apparently can&amp;rsquo;t get enough of Grafana and the latest quantified self push was to visualize the data from Apple health. Apple makes it pretty simple to export the data but it&amp;rsquo;s in XML so there&amp;rsquo;s a small bit of processing to turn into something that can be visualized. For my personal stats I&amp;rsquo;m dumping the data to MySQL and writing fairly simple queries to visualize them. Since I already did a similar export in my email-stats code I was able to reuse a fair amount.</description></item><item><title>Gamification works</title><link>/2020/04/07/gamification-works/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/07/gamification-works/</guid><description>Gamification works. Last week I hacked together a Grafana dashboard to measure the number of emails in my inbox and sure enough this gave me enough motivation to actually go through them. Earlier this week I added another metric to track the average age of an email and sure enough that caused me to go through the 4 and 5 year old emails.
It&amp;rsquo;s quite amazing how the human mind (or at least mine) works.</description></item><item><title>Visualizing my journey to Inbox Zero</title><link>/2020/03/31/visualizing-my-journey-to-inbox-zero/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/31/visualizing-my-journey-to-inbox-zero/</guid><description>I subscribe to the &amp;ldquo;Inbox Zero&amp;rdquo; philosophy and treat my email inbox as a todo list that I slowly work through. As part of the desire to get more and more quantitative I wrote a quick script to pull the number of emails from my Inbox and then insert the data as a row into a new table in my personal stats database. As usual, most of the work was in deciding to do it and once I got to coding the hacky solution was done within 20 minutes.</description></item><item><title>My personal Grafana dashboard</title><link>/2020/02/27/my-personal-grafana-dashboard/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/27/my-personal-grafana-dashboard/</guid><description>Last year I wrote about the idea of a personal dashboard and earlier this year I described my 2020 goals and how I&amp;rsquo;d go about measuring my progress. The past two days I was able to combine the two concepts and created a simple Grafana dashboard to measure my progress against the blogging goal. As with most tasks, the most difficult part was getting started and the actual exercise took a few hours.</description></item><item><title>Visualizing my 2019</title><link>/2020/01/17/visualizing-my-2019/</link><pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/17/visualizing-my-2019/</guid><description>In order to better understand myself, I&amp;rsquo;ve been collecting daily stats over the past few years with the idea that tracking various metrics would show me ways to improve. This happened to some degree - seeing many of the numbers leads to a sense of shame - but there hasn&amp;rsquo;t been a huge insight that looks at the interaction of the various items I track. To do that, I suspect I need to dive deeper into the quantified self movement and start continually tracking my physiological metrics rather than the current approach of a daily check-in describing my mood and what I consumed.</description></item><item><title>Visualizing my Twitter archive - 2019 edition</title><link>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</guid><description>GitHub sent me an alert this past weekend that a bunch of my repos were using old libraries that had security vulnerabilities. Nearly all of them were due to my usage of an old version of the requests library. Updating those was as simple as updating the requirements.txt file to the new version.
One of these repos, twitter-archive-analysis, is my most popular project on GitHub so I thought I might as well revisit it and see if I could both address the vulnerabilities and get it running again.</description></item><item><title>Google Sheets explore functionality</title><link>/2019/09/15/google-sheets-explore-functionality/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/15/google-sheets-explore-functionality/</guid><description>For the past few years I&amp;rsquo;ve been tracking a variety of daily metrics - ranging from sleep, to what I eat and drink, to my mood - in a Google spreadsheet. I have an annual tradition of analyzing and visualizing the data but I never go beyond the simple summary statistics. I always mean to do a deeper analysis but inevitably just run a script I barely touched in the past few years.</description></item><item><title>Visualizing my blog: 2018 edition</title><link>/2018/12/26/visualizing-my-blog-2018-edition/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/visualizing-my-blog-2018-edition/</guid><description>Last year I came up with a set of scripts to analyze my blog and thought it would be interesting to rerun them this year to see what&amp;rsquo;s changed. There are a ton of visualizations up on GitHub but most are just a fun visual without actually telling a story. I&amp;rsquo;ve included the most important ones below with a bit of analysis and commentary. It also looks as if the charts are getting too messy for multiple years of data so I&amp;rsquo;ll need to revisit the visualizations for 2019.</description></item><item><title>Top posts of 2018</title><link>/2018/12/25/top-posts-of-2018/</link><pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/25/top-posts-of-2018/</guid><description>In the usual end-of-year tradition I want to share the top posts of 2018 - including both the posts that were written in 2018 as well as the posts that may have been written in prior years but viewed in 2018. Given the fact that I’ve been extremely behind in writing this year and am only catching up now it’s clear that 2018 was a weaker year than previous ones.</description></item><item><title>Shell history: 2018 edition</title><link>/2018/11/28/shell-history-2018-edition/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/28/shell-history-2018-edition/</guid><description>In what has become an annual tradition I have a very simple shell script that generates a frequency of my most commonly run shell commands. This year saw a pretty big change from 2017. The most obvious difference is that I use “git” more frequently than in the past. This is a tough one to analyze by looking at the data since my usage of oh-my-zsh skews the data. It provides a variety of git aliases - for example gp for git push and gco for git checkout - that appear elsewhere in the results so my pure use of “git” is almost isolated to the cases where I do a commit.</description></item><item><title>History's largest empires</title><link>/2018/11/22/historys-largest-empires/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/22/historys-largest-empires/</guid><description>I got a bit distracted today and ended up coming across the Wikipedia page listing history’s largest empires. The page came with a list of the top 140 by land area and just by looking at them you can see there’s a huge range. The British Empire was the largest at 35.5 million square kilometers while the Sumer was the smallest at 0.05. That’s a huge difference - over 700 times - and I thought it would be interesting to plot them to visualize the distribution.</description></item><item><title>My follower factory</title><link>/2018/02/03/my-follower-factory/</link><pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/03/my-follower-factory/</guid><description>Last week, the New York Times ran an expose on the massive amount of follower fraud happening on Twitter. Unsurprisingly, when you can buy tens of thousands of followers for a few thousand dollars it’s not very likely that they’re going to be real. Anyone who has used Twitter for even a nominal amount of time would have quickly discovered that there’s a rampant amount of bots. Some leave cryptic comments, others like and retweet, while others follow; most do all of the above.</description></item><item><title>Visualizing my 2017 stats</title><link>/2017/12/27/visualizing-my-2017-stats/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/27/visualizing-my-2017-stats/</guid><description>Over the past year I’ve been collecting a bunch of statistics for each of my days in the hope that I’ll have time to dig into them and discover some interesting patterns. Unfortunately I haven’t had a chance to do anything other than some simple visualizations but even these provide some insight into my 2017. This isn’t a wholehearted adoption of the quantified self movement but it’s something I am interested in and hoping to expand in 2018.</description></item><item><title>Top posts of 2017</title><link>/2017/12/24/top-posts-of-2017/</link><pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/24/top-posts-of-2017/</guid><description>I use Google Analytics on my blog and now that the year is almost over it’s time for the annual tradition of sharing the top posts of the year. The total number of pageviews in 2017 was a remarkable 36,410, around the same I received in 2015 and 2016 but below those of 2013 and 2014 when I was both more lucky in the popularity of my posts while and more aggressive in promoting my writing on Hacker News.</description></item><item><title>Analyzing my blog: 2017 edition</title><link>/2017/12/21/analyzing-my-blog-2017-edition/</link><pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/21/analyzing-my-blog-2017-edition/</guid><description>I have a set of scripts I wrote in 2016 that aimed to analyze my posts over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.
The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.</description></item><item><title>Visualizing my meetings over time</title><link>/2017/07/28/visualizing-my-meetings-over-time/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/28/visualizing-my-meetings-over-time/</guid><description>As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would identify meeting room hogs and repurposed it to just download every one of my calendar events from when I joined TripleLift and another small script to analyze this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos.</description></item><item><title>Year in review: 2016</title><link>/2017/01/02/year-in-review-2016/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/02/year-in-review-2016/</guid><description>A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data.</description></item><item><title>Word clouds and text similarity</title><link>/2016/12/10/word-clouds-and-text-similarity/</link><pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/10/word-clouds-and-text-similarity/</guid><description>I’m a sucker for data visualizations so when I came across a simple word cloud-generating Python script I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year.</description></item><item><title>Visualizing your AWS costs</title><link>/2016/11/27/visualizing-your-aws-costs/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/27/visualizing-your-aws-costs/</guid><description>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, Trusted Advisor, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month.</description></item><item><title>Comparing the web requests made by the top sites: 2014 vs 2016</title><link>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</link><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</guid><description>A few years ago I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014.</description></item><item><title>Simple data visualizations from the command line</title><link>/2016/10/26/simple-data-visualizations-from-the-command-line/</link><pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/26/simple-data-visualizations-from-the-command-line/</guid><description>Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.</description></item><item><title>Revisiting my Twitter activity</title><link>/2016/10/19/revisiting-my-twitter-activity/</link><pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/19/revisiting-my-twitter-activity/</guid><description>While going through my old GitHub repos I discovered that the most starred repo was twitter-archive-analysis, a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working.</description></item><item><title>Visualizing fantasy football stats</title><link>/2016/09/05/visualizing-fantasy-football-stats/</link><pubDate>Mon, 05 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/05/visualizing-fantasy-football-stats/</guid><description>In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code.</description></item><item><title>Analyzing my blog</title><link>/2016/06/12/analyzing-my-blog/</link><pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/12/analyzing-my-blog/</guid><description>I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple Python script that went through each post and dumped it into a CSV file with a series of columns that would be easy to analyze via R.</description></item><item><title>Word clouds in R</title><link>/2016/06/06/word-clouds-in-r/</link><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/06/word-clouds-in-r/</guid><description>Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a quick script I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.</description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>After getting the IMDB data loaded it was time to dive in and start looking at the data. In 2012, I did an analysis to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the help of CuriousGnu, I was able to get my hands on a much larger dataset.</description></item><item><title>Jersey City garbage truck routes</title><link>/2015/12/12/jersey-city-garbage-truck-routes/</link><pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/12/jersey-city-garbage-truck-routes/</guid><description>A couple of months ago I took a stab at plotting the Jersey City parking zones after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome comment pointing out that Jersey City has a bunch of open data available, including a near-real time feed of garbage truck locations, a general open data portal, as well as the ability to request custom data.</description></item><item><title>Mapping the Jersey City parking zones II</title><link>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</link><pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</guid><description>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at https://dangoldin.github.io/jersey-city-open-data/ and includes the zones that had enough geocodeable addresses to generate a valid polygon.
As expected, most of the work was going from the PDF to a set of valid geocoded addresses.</description></item><item><title>Mapping the Jersey City parking zones</title><link>/2015/09/12/mapping-the-jersey-city-parking-zones/</link><pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/12/mapping-the-jersey-city-parking-zones/</guid><description>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there&amp;rsquo;s a PDF that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting.</description></item><item><title>Finding the optimal car to list on RelayRides</title><link>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</link><pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</guid><description>After discovering and browsing RelayRides I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.</description></item><item><title>Visualizing my browsing history</title><link>/2014/03/25/visualizing-my-browsing-history/</link><pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/25/visualizing-my-browsing-history/</guid><description>I came across a neat Chrome extension called Iconic History that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search.</description></item><item><title>Fun with the Oyster books API</title><link>/2014/03/16/fun-with-the-oyster-books-api/</link><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/16/fun-with-the-oyster-books-api/</guid><description>I’m an avid reader and signed up for Oyster as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.
One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list.</description></item><item><title>Website load times: NYC vs Beijing</title><link>/2014/03/11/website-load-times-nyc-vs-beijing/</link><pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/11/website-load-times-nyc-vs-beijing/</guid><description>Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and compare them against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from rahimnathwani who ran the script in Beijing, using home ADSL, and wanted to share his dataset.
I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites.</description></item><item><title>Examining the requests made by the top 100 sites</title><link>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</link><pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</guid><description>Since writing the Drowning in JavaScript post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files.</description></item><item><title>More Sierpinski fun</title><link>/2014/02/21/more-sierpinski-fun/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/21/more-sierpinski-fun/</guid><description>As a follow up to my previous post, I modified my Sierpinski generation code to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes.</description></item><item><title>Sierpinski triangle in D3</title><link>/2014/02/19/sierpinski-triangle-in-d3/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/19/sierpinski-triangle-in-d3/</guid><description>There&amp;rsquo;s a little known algorithm for constructing a Sierpinski triangle that is surprisingly easy to implement.
Start the three vertices that form a triangle Pick a random point inside the triangle Pick a random vertex Go halfway from a the random point to the vertex and mark that point Go to step 3 using the result of 4 as the starting point I&amp;rsquo;m trying to get better at D3 and thought it would be a good exercise to code it up.</description></item><item><title>Visualizing GPS data in R</title><link>/2014/02/05/visualizing-gps-data-in-r/</link><pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/05/visualizing-gps-data-in-r/</guid><description>Earlier today I read Nathan Yau’s post that had a quick R script to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were.</description></item><item><title>Taxi prices around the world</title><link>/2014/01/09/taxi-prices-around-the-world/</link><pubDate>Thu, 09 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/09/taxi-prices-around-the-world/</guid><description>I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on worldtaximeter.com and combining that with the data from taxiautofare.com and www.numbeo.com in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute.</description></item><item><title>Visualizing RunKeeper data in R</title><link>/2014/01/04/visualizing-runkeeper-data-in-r/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/04/visualizing-runkeeper-data-in-r/</guid><description>What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to my GitHub. If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.</description></item><item><title>D3 and Vega</title><link>/2013/07/09/d3-and-vega/</link><pubDate>Tue, 09 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/09/d3-and-vega/</guid><description>Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.</description></item><item><title>What does getting on the HN front page get you?</title><link>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</link><pubDate>Fri, 19 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</guid><description>A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.
The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday. The post ended up being featured in the NY Times Bits blog which accounted for ~2,900 of the total visits; the Gizmodo network which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.</description></item><item><title>An analysis of Lincoln's words</title><link>/2013/02/12/an-analysis-of-lincolns-words/</link><pubDate>Tue, 12 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/12/an-analysis-of-lincolns-words/</guid><description>On Saturday, I finished Team of Rivals and while looking at my calendar noticed that it was also Lincoln&amp;rsquo;s birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the 7 volume set containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.</description></item><item><title>Identifying duplicate bills across states</title><link>/2013/02/05/identifying-duplicate-bills-across-states/</link><pubDate>Tue, 05 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/05/identifying-duplicate-bills-across-states/</guid><description>This past weekend I participated in the Bicoastal Datafest hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn&amp;rsquo;t be very easy, especially in two days, but I did meet Llewellyn, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language.</description></item><item><title>Making sense of my Twitter archive</title><link>/2013/01/19/making-sense-of-my-twitter-archive/</link><pubDate>Sat, 19 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/19/making-sense-of-my-twitter-archive/</guid><description>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with matplotlib. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one &amp;ldquo;gotcha&amp;rdquo; was needing to convert the timestamp to my local time.</description></item><item><title>Trend of actor vs actress age differences</title><link>/2012/05/23/trend-of-actor-vs-actress-age-differences/</link><pubDate>Wed, 23 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/23/trend-of-actor-vs-actress-age-differences/</guid><description>I recently watched Miss Representation which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data.</description></item></channel></rss>