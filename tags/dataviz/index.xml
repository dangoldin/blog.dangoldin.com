<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dataviz on Dan Goldin</title><link>/tags/dataviz/</link><description>Recent content in Dataviz on Dan Goldin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 27 Jul 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/dataviz/index.xml" rel="self" type="application/rss+xml"/><item><title>Analyzing the AWS EC2 reservation options</title><link>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</guid><description>&lt;p>While writing the post on AWS reservations I started thinking if there&amp;rsquo;s any arbitrage opportunity in the reservations. For example - does it make sense to do a 1 year or 3 year reservation for some instance types of upgrade an instance class to get a better reservation value?&lt;/p>
&lt;p>You can do this manually using ec2instances.info which provides a quick way to look at EC2 pricing info although forces you to pick the type of reservations you&amp;rsquo;re interested in. To speed things up I just ran the &lt;a href="https://github.com/powdahound/ec2instances.info">ec2instances.info script&lt;/a> to scrape the data into a JSON with a &lt;a href="https://github.com/dangoldin/analyze-ec2instance.info">quick script&lt;/a> to extract the relevant details into a CSV file. After that, it was as simple as loading into &lt;a href="https://docs.google.com/spreadsheets/d/1h5H3vsZluk1_TGGMGrZq-KFiha-Bfhu4OBzyp5QynA8/edit#gid=0">Google sheets&lt;/a> for some analysis and visualization.&lt;/p></description></item><item><title>Data analysis and visualization</title><link>/2020/04/29/data-analysis-and-visualization/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/29/data-analysis-and-visualization/</guid><description>&lt;p>Yesterday I had the rare chance to actually do some coding and realized how rusty I am at numerical analysis in Python. The task was simple - ingest a CSV that had a date column, two categorical columns, and a numerical column - and then generate a grid containing a series of line plots, each of which would be a combination of the two categorical columns.&lt;/p>
&lt;p>I did a ton of this work years ago so knew what was possible. That&amp;rsquo;s half the battle and after a bit of searches I got a working solution. At the same time I&amp;rsquo;m disappointed it took me that long, especially after seeing the brevity of the end result. Software ate the world and now every business is generating tons of data. Being able to make sense of it is an increasingly important skill set especially if you&amp;rsquo;re a leader. It&amp;rsquo;s unfortunately an area I haven&amp;rsquo;t kept up with over the years and something I plan on remedying after this experience.&lt;/p></description></item><item><title>Dumping Apple health data into MySQL</title><link>/2020/04/11/dumping-apple-health-data-into-mysql/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/11/dumping-apple-health-data-into-mysql/</guid><description>&lt;img src="/image/grafana-health-stats.png" alt="Grafana visualization of my Apple health stat" data-width="1860" data-height="944" data-layout="responsive" />
&lt;p>I apparently can&amp;rsquo;t get enough of Grafana and the latest quantified self push was to visualize the data from Apple health. Apple makes it pretty simple to export the data but it&amp;rsquo;s in XML so there&amp;rsquo;s a small bit of processing to turn into something that can be visualized. For my personal stats I&amp;rsquo;m dumping the data to MySQL and writing fairly simple queries to visualize them. Since I already did a similar export in my &lt;a href="https://github.com/dangoldin/email-stats">email-stats&lt;/a> code I was able to reuse a fair amount. The major difference was that the Apple health export is fairly large (my export for 2020 was an 80 MB file) and it would be a shame to not apply a few optimizations.&lt;/p></description></item><item><title>Gamification works</title><link>/2020/04/07/gamification-works/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/07/gamification-works/</guid><description>&lt;img src="/image/grafana-emails-avg-age.png" alt="Grafana dashboard for emails in inbox and the avg age" data-width="1840" data-height="943" data-layout="responsive" />
&lt;p>Gamification works. Last week I hacked together a Grafana dashboard to measure the number of emails in my inbox and sure enough this gave me enough motivation to actually go through them. Earlier this week I added another metric to track the average age of an email and sure enough that caused me to go through the 4 and 5 year old emails.&lt;/p></description></item><item><title>Visualizing my journey to Inbox Zero</title><link>/2020/03/31/visualizing-my-journey-to-inbox-zero/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/31/visualizing-my-journey-to-inbox-zero/</guid><description>&lt;img src="/image/grafana-emails-in-inbox.png" alt="Number of emails in my inbox by account" data-width="1855" data-height="518" data-layout="responsive" />
&lt;p>I subscribe to the &amp;ldquo;Inbox Zero&amp;rdquo; philosophy and treat my email inbox as a todo list that I slowly work through. As part of the desire to get more and more quantitative I wrote a quick script to pull the number of emails from my Inbox and then insert the data as a row into a new table in my personal stats database. As usual, most of the work was in deciding to do it and once I got to coding the hacky solution was done within 20 minutes. The script uses Python&amp;rsquo;s built-in &lt;a href="https://docs.python.org/3/library/imaplib.html">imaplib&lt;/a> library to log in to an email provider and then a simple MySQL query to insert the resulting data. I hooked this up to run every 15 minutes via cron and put together a Grafana dashboard to plot the count over time. I&amp;rsquo;m currently not actually going through the content of the email messages themselves but there are tons of directions I can take this - for example slicing the data by sender or examining the age of the messages. For now I&amp;rsquo;m just hopeful this motivates me to keep going through that email.&lt;/p></description></item><item><title>My personal Grafana dashboard</title><link>/2020/02/27/my-personal-grafana-dashboard/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/27/my-personal-grafana-dashboard/</guid><description>&lt;img src="/image/personal-grafana-dashboard-blog-stats.png" alt="My personal Grafana dashboard for blog stats" data-width="1913" data-height="978" data-layout="responsive" />
&lt;p>Last year I wrote about the idea of a &lt;a href="/2019/07/10/personal-dashboards/">personal dashboard&lt;/a> and earlier this year I described my &lt;a href="/2020/01/28/2020-goals/">2020 goals&lt;/a> and how I&amp;rsquo;d go about measuring my progress. The past two days I was able to combine the two concepts and created a simple Grafana dashboard to measure my progress against the blogging goal. As with most tasks, the most difficult part was getting started and the actual exercise took a few hours. While it&amp;rsquo;s still fresh in my mind I want to document the step by step process in order to both provide a perspective into how I work while also giving others a guide to setting up their own.&lt;/p></description></item><item><title>Visualizing my 2019</title><link>/2020/01/17/visualizing-my-2019/</link><pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/17/visualizing-my-2019/</guid><description>&lt;p>In order to better understand myself, I&amp;rsquo;ve been collecting daily stats over the past few years with the idea that tracking various metrics would show me ways to improve. This happened to some degree - seeing many of the numbers leads to a sense of shame - but there hasn&amp;rsquo;t been a huge insight that looks at the interaction of the various items I track. To do that, I suspect I need to dive deeper into the quantified self movement and start continually tracking my physiological metrics rather than the current approach of a daily check-in describing my mood and what I consumed.&lt;/p></description></item><item><title>Visualizing my Twitter archive - 2019 edition</title><link>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</guid><description>&lt;p>GitHub sent me an alert this past weekend that a bunch of my repos were using old libraries that had security vulnerabilities. Nearly all of them were due to my usage of an old version of the &lt;a href="https://pypi.org/project/requests/">requests&lt;/a> library. Updating those was as simple as updating the requirements.txt file to the new version.&lt;/p>
&lt;p>One of these repos, &lt;a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis&lt;/a>, is my most popular project on GitHub so I thought I might as well revisit it and see if I could both address the vulnerabilities and get it running again. Upgrading the packages was straightforward but there are very few things more humbling than looking at the code you&amp;rsquo;ve written years ago. Twitter changed the format of the archive from JSON to CSV since the last time I ran the code and as part of the upgrade I did a little bit of cleanup. The &lt;a href="https://github.com/dangoldin/twitter-archive-analysis/blob/master/analyze.py">code&lt;/a> is up on GitHub and I&amp;rsquo;ve included the visualizations it generated below highlighting my Twitter behavior over the years.&lt;/p></description></item><item><title>Google Sheets explore functionality</title><link>/2019/09/15/google-sheets-explore-functionality/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/15/google-sheets-explore-functionality/</guid><description>&lt;p>For the past few years I&amp;rsquo;ve been tracking a variety of daily metrics - ranging from sleep, to what I eat and drink, to my mood - in a Google spreadsheet. I have an annual tradition of analyzing and visualizing the data but I never go beyond the simple summary statistics. I always mean to do a deeper analysis but inevitably just run a script I barely touched in the past few years.&lt;/p></description></item><item><title>Visualizing my blog: 2018 edition</title><link>/2018/12/26/visualizing-my-blog-2018-edition/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/visualizing-my-blog-2018-edition/</guid><description>&lt;p>Last year I came up with a set of scripts to analyze my blog and thought it would be interesting to rerun them this year to see what&amp;rsquo;s changed. There are a ton of visualizations up on &lt;a href="https://github.com/dangoldin/blog-analytics/tree/master/img/2018">GitHub&lt;/a> but most are just a fun visual without actually telling a story. I&amp;rsquo;ve included the most important ones below with a bit of analysis and commentary. It also looks as if the charts are getting too messy for multiple years of data so I&amp;rsquo;ll need to revisit the visualizations for 2019.&lt;/p></description></item><item><title>Top posts of 2018</title><link>/2018/12/25/top-posts-of-2018/</link><pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/25/top-posts-of-2018/</guid><description>&lt;img src="/image/2018-ga-blog-stats.png" alt="2018 Google Analytics blog stats" data-width="2594" data-height="742" data-layout="responsive" />
&lt;p>In the usual end-of-year tradition I want to share the top posts of 2018 - including both the posts that were written in 2018 as well as the posts that may have been written in prior years but viewed in 2018. Given the fact that I’ve been extremely behind in writing this year and am only catching up now it’s clear that 2018 was a weaker year than previous ones. Sorting all pages viewed in 2018 by descending pageviews the first post written in 2018 is in the 57th spot - every other post was written in prior years. That bodes well to the concept of evergreen content but it’s still disappointing that I didn’t have any noteworthy posts in 2018. The lesson for 2019 is to actually write on time and spend the time going into depth versus the more superficial and shorter posts I wrote in 2018. On a positive note it’s good to see that my blog does get a healthy flow of organic traffic despite the weak 2018 showing.&lt;/p></description></item><item><title>Shell history: 2018 edition</title><link>/2018/11/28/shell-history-2018-edition/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/28/shell-history-2018-edition/</guid><description>&lt;p>In what has become an annual tradition I have a very simple shell script that generates a frequency of my most commonly run shell commands. This year saw a pretty big change from 2017. The most obvious difference is that I use “git” more frequently than in the past. This is a tough one to analyze by looking at the data since my usage of &lt;a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh&lt;/a> skews the data. It provides a variety of git aliases - for example gp for git push and gco for git checkout - that appear elsewhere in the results so my pure use of “git” is almost isolated to the cases where I do a commit.&lt;/p></description></item><item><title>History's largest empires</title><link>/2018/11/22/historys-largest-empires/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/22/historys-largest-empires/</guid><description>&lt;p>I got a bit distracted today and ended up coming across the Wikipedia page listing &lt;a href="https://en.wikipedia.org/wiki/List_of_largest_empires">history’s largest empires&lt;/a>. The page came with a list of the top 140 by land area and just by looking at them you can see there’s a huge range. The British Empire was the largest at 35.5 million square kilometers while the Sumer was the smallest at 0.05. That’s a huge difference - over 700 times - and I thought it would be interesting to plot them to visualize the distribution. As expected, there’s a very steep drop and a long tail. If you add up the land areas of all the empires listed you get just over 455 million square kilometers. That metric itself doesn’t mean anything but it helps to normalize the land areas. The British Empire, for example, is 8% of the total and if you keep going down the list in descending order by size and sum up the percentages you get that the first 55 empires add up to 80% of the total land area. Once again, the total land area is useless metric but it allows us to see how close we are to the &lt;a href="https://en.wikipedia.org/wiki/Pareto_principle">80-20 rule&lt;/a>. It turns out not too close - 55 countries out of 140 are just over 39%. The top 20% empires add up to 52% of the land area. If you’re interested in playing around with the data it’s up on the Wikipedia page as well as an &lt;a href="/assets/static/data/largest-empires-land-area.xlsx">Excel version&lt;/a> with cleaned up data.&lt;/p></description></item><item><title>My follower factory</title><link>/2018/02/03/my-follower-factory/</link><pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/03/my-follower-factory/</guid><description>&lt;p>Last week, the New York Times &lt;a href="https://www.nytimes.com/interactive/2018/01/27/technology/social-media-bots.html">ran an expose&lt;/a> on the massive amount of follower fraud happening on Twitter. Unsurprisingly, when you can buy tens of thousands of followers for a few thousand dollars it’s not very likely that they’re going to be real. Anyone who has used Twitter for even a nominal amount of time would have quickly discovered that there’s a rampant amount of bots. Some leave cryptic comments, others like and retweet, while others follow; most do all of the above.&lt;/p></description></item><item><title>Visualizing my 2017 stats</title><link>/2017/12/27/visualizing-my-2017-stats/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/27/visualizing-my-2017-stats/</guid><description>&lt;p>Over the past year I’ve been collecting a bunch of statistics for each of my days in the hope that I’ll have time to dig into them and discover some interesting patterns. Unfortunately I haven’t had a chance to do anything other than some simple visualizations but even these provide some insight into my 2017. This isn’t a wholehearted adoption of the quantified self movement but it’s something I am interested in and hoping to expand in 2018. A goal has always been to move beyond visualization and into actual analysis and actionable insights that can help me improve my lifestyle and behavior. I did the same set of &lt;a href="/2017/01/02/year-in-review-2016/">visualizations in 2016&lt;/a> so it&amp;rsquo;s useful to compare them year over and year and see how, and if, my habits have changed.&lt;/p></description></item><item><title>Top posts of 2017</title><link>/2017/12/24/top-posts-of-2017/</link><pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/24/top-posts-of-2017/</guid><description>&lt;img src="/image/2017-pageviews.png" alt="Pageviews of my posts in 2017" data-width="2720" data-height="384" data-layout="responsive" />
&lt;p>I use Google Analytics on my blog and now that the year is almost over it’s time for the annual tradition of sharing the top posts of the year. The total number of pageviews in 2017 was a remarkable 36,410, around the same I received in 2015 and 2016 but below those of 2013 and 2014 when I was both more lucky in the popularity of my posts while and more aggressive in promoting my writing on Hacker News. While I feel my writing has improved over the years I feel my content has atrophied and is something I’d like to correct in 2018. What’s interesting is that out of all the views this year, 17% were from posts written in 2017, 74% were from posts written in previous years, and the remaining 9% were from non post pages. One argument is that as I build up a larger stable of content it will keep growing in percentage every year. At the same time I should start seeing an overall growth in views which hasn’t happened over the past few years. A worthy goal is to keep growing the total number of views of previous years’ posts while also making sure the current year’s are driving even more views and become the evergreen content for the future.&lt;/p></description></item><item><title>Analyzing my blog: 2017 edition</title><link>/2017/12/21/analyzing-my-blog-2017-edition/</link><pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/21/analyzing-my-blog-2017-edition/</guid><description>&lt;p>I have a set of &lt;a href="https://github.com/dangoldin/blog-analytics">scripts&lt;/a> I wrote in 2016 that aimed to &lt;a href="/2016/06/12/analyzing-my-blog/">analyze my posts&lt;/a> over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.&lt;/p>
&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/wordcloud_2017.png" alt="2017 word cloud" data-width="600" data-height="600" data-layout="responsive" />
 &lt;p>The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.&lt;/p></description></item><item><title>Visualizing my meetings over time</title><link>/2017/07/28/visualizing-my-meetings-over-time/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/28/visualizing-my-meetings-over-time/</guid><description>&lt;p>As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would &lt;a href="http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs/">identify meeting room hogs&lt;/a> and &lt;a href="https://github.com/dangoldin/gcal-shaming/blob/master/meeting_duration_growth.py">repurposed it&lt;/a> to just download every one of my calendar events from when I joined TripleLift and another small script to &lt;a href="https://github.com/dangoldin/gcal-shaming/blob/master/analyze.py">analyze&lt;/a> this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos. Unsurprisingly, there was a pretty large increase over time as we grew from a scrappy startup of 15 people to one with over 150 and as my role evolved from an individual contributor to a manager and then to the head of the engineering team.&lt;/p></description></item><item><title>Year in review: 2016</title><link>/2017/01/02/year-in-review-2016/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/02/year-in-review-2016/</guid><description>&lt;p>A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data. The goal is to identify healthy and unhealthy trends over time and use that information to make changes in my life. At the moment the stats are mostly high level summaries but what I want to do is use this data in order to identify hidden relationships in order to improve my physical state and mental moods. This is a work in progress but I hope to do more of that this coming year as well as improve the way I’m gathering this data. The analysis &lt;a href="https://github.com/dangoldin/annual-stats-analysis">code is up on GitHub&lt;/a> with a guide and a sample file that can be analyzed. And now on to the data:&lt;/p></description></item><item><title>Word clouds and text similarity</title><link>/2016/12/10/word-clouds-and-text-similarity/</link><pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/10/word-clouds-and-text-similarity/</guid><description>&lt;p>I’m a sucker for data visualizations so when I came across a simple word cloud-generating &lt;a href="https://github.com/amueller/word_cloud">Python script&lt;/a> I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year. This didn’t reveal too much but got me thinking about how my writing has changed over the years. This led my discovery of a &lt;a href="http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents">script on StackOverflow&lt;/a> that works by translating each block of text into an tf-idf (term frequency - inverse document frequency) vector and then calculating the cosine distance between them. This intuitively makes sense. The tf-idf vector is used to highlight and quantify the unique words in a given document as a vector and the cosine distance is used to compare the similarities between them - if they vectors are equivalent the angle between them is 0 which has a cosine of 1. Turns out that high school math is incredibly useful.&lt;/p></description></item><item><title>Visualizing your AWS costs</title><link>/2016/11/27/visualizing-your-aws-costs/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/27/visualizing-your-aws-costs/</guid><description>&lt;p>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, &lt;a href="https://aws.amazon.com/premiumsupport/trustedadvisor/">Trusted Advisor&lt;/a>, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: &lt;a href="https://www.cloudability.com">Cloudability&lt;/a> starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.&lt;/p></description></item><item><title>Comparing the web requests made by the top sites: 2014 vs 2016</title><link>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</link><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</guid><description>&lt;p>A &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">few years ago&lt;/a> I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they&amp;rsquo;re making.&lt;/p></description></item><item><title>Simple data visualizations from the command line</title><link>/2016/10/26/simple-data-visualizations-from-the-command-line/</link><pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/26/simple-data-visualizations-from-the-command-line/</guid><description>&lt;p>Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.&lt;/p></description></item><item><title>Revisiting my Twitter activity</title><link>/2016/10/19/revisiting-my-twitter-activity/</link><pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/19/revisiting-my-twitter-activity/</guid><description>&lt;p>While going through my old GitHub repos I discovered that the most starred repo was &lt;a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis&lt;/a>, a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.&lt;/p></description></item><item><title>Visualizing fantasy football stats</title><link>/2016/09/05/visualizing-fantasy-football-stats/</link><pubDate>Mon, 05 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/05/visualizing-fantasy-football-stats/</guid><description>&lt;p>In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on &lt;a href="https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R">GitHub&lt;/a> and I’ll keep updating it as I keep adding newer visualizations and analyses.&lt;/p></description></item><item><title>Analyzing my blog</title><link>/2016/06/12/analyzing-my-blog/</link><pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/12/analyzing-my-blog/</guid><description>&lt;p>I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple &lt;a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.py">Python script&lt;/a> that went through each post and dumped it into a CSV file with a series of columns that would be easy to &lt;a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.R">analyze via R&lt;/a>. The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.&lt;/p></description></item><item><title>Word clouds in R</title><link>/2016/06/06/word-clouds-in-r/</link><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/06/word-clouds-in-r/</guid><description>&lt;p>Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a &lt;a href="http://www.r-bloggers.com/building-wordclouds-in-r/">quick script&lt;/a> I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.&lt;/p></description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>&lt;p>After getting the &lt;a href="http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/">IMDB data loaded&lt;/a> it was time to dive in and start looking at the data. In 2012, I did an &lt;a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">analysis&lt;/a> to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the &lt;a href="https://www.curiousgnu.com/imdb-age-distribution">help of CuriousGnu&lt;/a>, I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on &lt;a href="https://github.com/dangoldin/imdb">GitHub&lt;/a> and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.&lt;/p></description></item><item><title>Jersey City garbage truck routes</title><link>/2015/12/12/jersey-city-garbage-truck-routes/</link><pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/12/jersey-city-garbage-truck-routes/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-garbage-trucks.png" alt="Jersey City garbage trucks" data-width="718" data-height="811" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>A couple of months ago I took a stab at plotting the Jersey City &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/">parking zones&lt;/a> after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530">comment&lt;/a> pointing out that Jersey City has a bunch of open data available, including a near-real time feed of &lt;a href="http://www.jciaonline.org/gpsMap.php?view=map">garbage truck locations&lt;/a>, a general &lt;a href="http://data.jerseycitynj.gov/">open data portal&lt;/a>, as well as the ability to &lt;a href="https://jerseycitynj.seamlessdocs.com/w/records_request">request custom data&lt;/a>. As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and the code is up on &lt;a href="https://github.com/dangoldin/jersey-city-open-data">GitHub&lt;/a>.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones II</title><link>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</link><pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-parking-zones.png" alt="Jersey City parking zones" data-width="442" data-height="640" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and includes the zones that had enough geocodeable addresses to generate a valid polygon.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones</title><link>/2015/09/12/mapping-the-jersey-city-parking-zones/</link><pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/12/mapping-the-jersey-city-parking-zones/</guid><description>&lt;p>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there&amp;rsquo;s a &lt;a href="http://jcparking.org/PDF/ZONE%20PERMITS%20ALL%20ZONES.pdf" target="_blank">PDF&lt;/a> that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.&lt;/p></description></item><item><title>Finding the optimal car to list on RelayRides</title><link>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</link><pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</guid><description>&lt;p>After discovering and browsing &lt;a href="https://relayrides.com/">RelayRides&lt;/a> I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.&lt;/p></description></item><item><title>Visualizing my browsing history</title><link>/2014/03/25/visualizing-my-browsing-history/</link><pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/25/visualizing-my-browsing-history/</guid><description>&lt;p>I came across a neat Chrome extension called &lt;a href="http://shan-huang.com/browserdatavis/">Iconic History&lt;/a> that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.&lt;/p></description></item><item><title>Fun with the Oyster books API</title><link>/2014/03/16/fun-with-the-oyster-books-api/</link><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/16/fun-with-the-oyster-books-api/</guid><description>&lt;p>I’m an avid reader and signed up for &lt;a href="http://oysterbooks.com/" target="_blank">Oyster&lt;/a> as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.&lt;/p>
&lt;p>One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.&lt;/p></description></item><item><title>Website load times: NYC vs Beijing</title><link>/2014/03/11/website-load-times-nyc-vs-beijing/</link><pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/11/website-load-times-nyc-vs-beijing/</guid><description>&lt;p>Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">compare them&lt;/a> against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from &lt;a href="https://github.com/rahimnathwani" target="_blank">rahimnathwani&lt;/a> who ran the script in Beijing, using home ADSL, and wanted to share his dataset.&lt;/p>
&lt;p>I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.&lt;/p></description></item><item><title>Examining the requests made by the top 100 sites</title><link>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</link><pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</guid><description>&lt;p>Since writing the &lt;a href="http://dangoldin.com/2013/12/01/drowning-in-javascript/">Drowning in JavaScript&lt;/a> post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.&lt;/p></description></item><item><title>More Sierpinski fun</title><link>/2014/02/21/more-sierpinski-fun/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/21/more-sierpinski-fun/</guid><description>&lt;p>As a follow up to my previous &lt;a href="http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3/">post&lt;/a>, I modified my Sierpinski generation code to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it&amp;rsquo;s difficult to confirm given a fixed screen resolution. I&amp;rsquo;d love to know what&amp;rsquo;s going on here and whether there&amp;rsquo;s a relationship between the number of sides and the distance ratio.&lt;/p></description></item><item><title>Sierpinski triangle in D3</title><link>/2014/02/19/sierpinski-triangle-in-d3/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/19/sierpinski-triangle-in-d3/</guid><description>&lt;p>There&amp;rsquo;s a little known algorithm for constructing a &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_triangle" target="_blank">Sierpinski triangle&lt;/a> that is surprisingly easy to implement.&lt;/p>
&lt;ol>
&lt;li>Start the three vertices that form a triangle&lt;/li>
&lt;li>Pick a random point inside the triangle&lt;/li>
&lt;li>Pick a random vertex&lt;/li>
&lt;li>Go halfway from a the random point to the vertex and mark that point&lt;/li>
&lt;li>Go to step 3 using the result of 4 as the starting point&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;m trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following file. Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_carpet" target="_blank">Sierpinski carpet&lt;/a> can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.&lt;/p></description></item><item><title>Visualizing GPS data in R</title><link>/2014/02/05/visualizing-gps-data-in-r/</link><pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/05/visualizing-gps-data-in-r/</guid><description>&lt;p>Earlier today I read Nathan Yau’s &lt;a href="http://flowingdata.com/2014/02/05/where-people-run/" target="_blank">post&lt;/a> that had a quick &lt;a href="http://projects.flowingdata.com/tut/map-routes.R" target="_blank">R script&lt;/a> to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.&lt;/p></description></item><item><title>Taxi prices around the world</title><link>/2014/01/09/taxi-prices-around-the-world/</link><pubDate>Thu, 09 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/09/taxi-prices-around-the-world/</guid><description>&lt;p>I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on &lt;a href="http://www.worldtaximeter.com" target="_blank" rel="nofollow">worldtaximeter.com&lt;/a> and combining that with the data from &lt;a href="http://www.taxiautofare.com" target="_blank" rel="nofollow">taxiautofare.com&lt;/a> and &lt;a href="http://www.numbeo.com/taxi-fare/" target="_blank">&lt;a href="https://www.numbeo.com">www.numbeo.com&lt;/a>&lt;/a> in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on &lt;a href="https://github.com/dangoldin/taxi-pricing" target="_blank">GitHub&lt;/a> with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.&lt;/p></description></item><item><title>Visualizing RunKeeper data in R</title><link>/2014/01/04/visualizing-runkeeper-data-in-r/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/04/visualizing-runkeeper-data-in-r/</guid><description>&lt;p>What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to &lt;a href="https://github.com/dangoldin/runkeeper-stats" target="_blank">my GitHub&lt;/a>. If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.&lt;/p></description></item><item><title>D3 and Vega</title><link>/2013/07/09/d3-and-vega/</link><pubDate>Tue, 09 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/09/d3-and-vega/</guid><description>&lt;img src="/image/mcsp-star.png" alt="A data visualization I'm working on" data-width="780" data-height="780" data-layout="responsive" />
&lt;p>Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.&lt;/p></description></item><item><title>What does getting on the HN front page get you?</title><link>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</link><pubDate>Fri, 19 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</guid><description>&lt;p>A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.&lt;/p>
&lt;ul>
 &lt;li>The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.
 &lt;img src="/image/ga-blog-post-spike.png" alt="GA Blog Post Spike" data-width="1167" data-height="420" data-layout="responsive" />
 &lt;/li>
 &lt;li>The post ended up being featured in the NY Times &lt;a href="http://bits.blogs.nytimes.com/2013/04/12/todays-scuttlebot-e-mail-cachet-and-cellphone-dial-tones/" target="_blank">Bits blog&lt;/a> which accounted for ~2,900 of the total visits; the &lt;a href="http://gizmodo.com/dial-tones/" target="_blank">Gizmodo network&lt;/a> which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.&lt;/li>
 &lt;li>The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the &lt;a href="http://www.gizmodo.co.uk/2013/04/why-your-mobile-phone-doesnt-have-a-dial-tone/" target="_blank">UK&lt;/a> on Sunday and &lt;a href="http://www.gizmodo.fr/2013/04/16/pourquoi-telephone-portable-tonalite.html" target="_blank">France&lt;/a> on Tuesday. &lt;a href="http://www.cnet.com/8301-13952_1-57579660-81/the-404-1249-where-we-get-the-senior-discount-podcast/" target="_blank">CNET&lt;/a> and the &lt;a href="http://www.guardian.co.uk/technology/blog/2013/apr/15/technology-links-newsbucket" target="_blank">Guardian&lt;/a> both picked it up on Monday.&lt;/li>
 &lt;li>Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.&lt;/li>
 &lt;li>Only 80 people ended up visiting the &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a> site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.&lt;/li>
 &lt;li>I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.&lt;/li>
 &lt;li>It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.&lt;/li>
 &lt;li>If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a &lt;a href="http://nathanmarz.com/blog/you-should-blog-even-if-you-have-no-readers.html" target="_blank">great post&lt;/a> that everyone who's interested in blogging should read.&lt;/li>
 &lt;li>It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.&lt;/li>
&lt;/ul></description></item><item><title>An analysis of Lincoln's words</title><link>/2013/02/12/an-analysis-of-lincolns-words/</link><pubDate>Tue, 12 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/12/an-analysis-of-lincolns-words/</guid><description>&lt;p>On Saturday, I finished &lt;a href="http://www.amazon.com/Team-Rivals-Political-Abraham-Lincoln/dp/0743270754">Team of Rivals&lt;/a> and while looking at my calendar noticed that it was also &lt;a href="http://en.wikipedia.org/wiki/Abraham_Lincoln">Lincoln&amp;rsquo;s&lt;/a> birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the &lt;a href="http://www.gutenberg.org/files/3253/3253-h/3253-h.htm">7 volume set&lt;/a> containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.&lt;/p></description></item><item><title>Identifying duplicate bills across states</title><link>/2013/02/05/identifying-duplicate-bills-across-states/</link><pubDate>Tue, 05 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/05/identifying-duplicate-bills-across-states/</guid><description>&lt;p>This past weekend I participated in the &lt;a href="http://www.bdatafest.computationalreporting.com/">Bicoastal Datafest&lt;/a> hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn&amp;rsquo;t be very easy, especially in two days, but I did meet &lt;a href="https://twitter.com/llewellynhinkes">Llewellyn&lt;/a>, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.&lt;/p></description></item><item><title>Making sense of my Twitter archive</title><link>/2013/01/19/making-sense-of-my-twitter-archive/</link><pubDate>Sat, 19 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/19/making-sense-of-my-twitter-archive/</guid><description>&lt;p>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with &lt;a href="http://matplotlib.org/">matplotlib&lt;/a>. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one &amp;ldquo;gotcha&amp;rdquo; was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I&amp;rsquo;m more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I&amp;rsquo;d like to explore how my usage of Twitter has evolved and also get to play around with the &lt;a href="http://nltk.org/">NLTK library&lt;/a>.&lt;/p></description></item><item><title>Trend of actor vs actress age differences</title><link>/2012/05/23/trend-of-actor-vs-actress-age-differences/</link><pubDate>Wed, 23 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/23/trend-of-actor-vs-actress-age-differences/</guid><description>&lt;p>I recently watched &lt;a href="http://www.missrepresentation.org/" title="Miss Representation" target="_blank">Miss Representation&lt;/a> which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.&lt;br/>&lt;br/>I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via &lt;a href="http://www.imdb.com/chart/1910s">&lt;span>&lt;a href="http://www.imdb.com/chart/1910s">&lt;a href="http://www.imdb.com/chart/1910s">http://www.imdb.com/chart/1910s&lt;/a>&lt;/a>&lt;/span>&lt;/a> - &lt;a href="http://www.imdb.com/chart/2010s)">&lt;span>&lt;a href="http://www.imdb.com/chart/2010s">&lt;a href="http://www.imdb.com/chart/2010s">http://www.imdb.com/chart/2010s&lt;/a>&lt;/a>)&lt;/span>&lt;/a> and then pulling the top 5 cast members for each movie (via &lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast)">&lt;span>&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">http://www.imdb.com/title/tt1375666/fullcredits#cast&lt;/a>&lt;/a>)&lt;/span>&lt;/a>. I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE" target="_blank">Google Spreadsheets&lt;/a> to clean it up. &lt;br/>&lt;br/>Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.&lt;/p></description></item></channel></rss>