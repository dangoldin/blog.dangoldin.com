<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data on Dan Goldin</title><link>/tags/data/</link><description>Recent content in data on Dan Goldin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 16 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/data/index.xml" rel="self" type="application/rss+xml"/><item><title>Big money, not big data</title><link>/2020/08/16/big-money-not-big-data/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/16/big-money-not-big-data/</guid><description>It&amp;rsquo;s common for companies to complain about the challenges of running big data and how difficult it is. The reality is that unless you&amp;rsquo;re running at massive scale (Google or Facebook) your problems are more to do with big money rather than big data. It&amp;rsquo;s expensive to store, process, and expose terabytes and the difficulty is in doing it cost effectively, not in simply doing it. There are enough modern tools out there across all the cloud providers (AWS, GCP, Azure) and vendors (Snowflake, Databricks) that it&amp;rsquo;s possible to do nearly everything you want but you&amp;rsquo;ll just have to pay for it.</description></item><item><title>Open, Public, Electronic, and Necessary Government Data Act</title><link>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</guid><description>The US Congress recently passed HR-4174 (The Open, Public, Electronic, and Necessary Government Data Act) which is intended to make all public government data available and accessible. Over the years I’ve done my fair share of poking around various government datasets - both public and private - and while the data was generally available it was rarely accessible. More often than not the data would be available via a scanned PDF which required some heavy OCR work to extract anything useful or the slightly easier PDF parsing code.</description></item><item><title>My DataEngConf 2018 talk</title><link>/2018/11/20/my-dataengconf-2018-talk/</link><pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/20/my-dataengconf-2018-talk/</guid><description>On November 9th I had the privilege of speaking at DataEngConf under the “Hero Engineering” track. My talk was titled “The Highs and Lows of Building an AdTech Data Pipeline” and I covered our evolution from a dead simple, sampled approach that had nothing to do with big data to the latest version which is leveraging a variety of modern open source data technologies.
I spoke about the motivation, challenges, and lessons learned during each iteration and ended the talk with the top 3 lessons learned across the various iterations of the pipeline.</description></item><item><title>Facebook's "breach"</title><link>/2018/03/18/facebooks-breach/</link><pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/18/facebooks-breach/</guid><description>The big news this weekend was that Facebook suspended Cambridge Analytica, a company that leveraged behavioral data to come up with very focused and accurate political ads, for using data that they were not supposed to have as well as not deleting it when caught. Everyone seems to be surprised by this revelation but I’m honestly surprised it took this long and I wouldn’t be surprised if there are still hundreds, or even thousands, of companies in the same situation as Cambridge Analytica, albeit at a smaller scale.</description></item><item><title>Crowdsourced data</title><link>/2018/03/11/crowdsourced-data/</link><pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/11/crowdsourced-data/</guid><description>Open source has become a critical part of modern software development that allows small teams to move quickly and do in months what used to take years. This has been driven by massive platforms, such as GitHub, that make it extremely easy to find useful code, contribute back, and provide feedback, comments, and requests.
Unfortunately, data hasn’t seen as strong of an open sourcing trend. There are a few sites - ranging from data.</description></item><item><title>Slides from my talk at DataEngConf</title><link>/2017/10/30/slides-from-my-talk-at-dataengconf/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/30/slides-from-my-talk-at-dataengconf/</guid><description>I had the privilege of giving a talk today at DataEngConf. Unfortunately the talk was not recorded but you can grab the slides [here]({{DATA_PATH }}/DataEngCof_NYC_Data_Startups_Dan_Goldin-Scaling-a-Data-Pipeline-Mystery-to-Mastery.pdf). The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture. Take a look at the slides and if you have any questions I’d be glad to answer them in the comments.</description></item><item><title>Type dependent databases</title><link>/2017/05/23/type-dependent-databases/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/23/type-dependent-databases/</guid><description>I’m a huge proponent of strong types when it comes to coding. Unless it’s a throwaway project it’s always worth spending the extra time to define your objects and the way they will be exposed in the code. This investment makes it more likely that you’ve thought through the way the code will need to evolve and the various edge cases you need to handle.
This philosophy is even more important when thinking about your database structure since that’s going to be even more difficult to change than your code.</description></item><item><title>The golden age of big data tools</title><link>/2017/04/23/the-golden-age-of-big-data-tools/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/23/the-golden-age-of-big-data-tools/</guid><description>I really dislike using the phrase “big data” but it is catchy so I’m going with it. It really does feel we’re in the golden age of big data tools. The rise of cloud computing, distributed storage, and the proliferation of open source have led to multiple orders of magnitude more data generated now than a decade ago. It’s an impossible number to calculate but some project that between 2010 and 2020 there will be a 50 fold increase in the amount of data collected.</description></item><item><title>A poor man's data pipeline</title><link>/2016/11/12/a-poor-mans-data-pipeline/</link><pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/12/a-poor-mans-data-pipeline/</guid><description>Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.</description></item><item><title>Setting up secor for Kafka 0.10</title><link>/2016/10/10/setting-up-secor-for-kafka-0.10/</link><pubDate>Mon, 10 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/10/setting-up-secor-for-kafka-0.10/</guid><description>Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that secor, a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly.</description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>After getting the IMDB data loaded it was time to dive in and start looking at the data. In 2012, I did an analysis to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the help of CuriousGnu, I was able to get my hands on a much larger dataset.</description></item><item><title>Analyzing IMDB data: Step 1 - Cleaning and QA</title><link>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</guid><description>In 2012 I did a simple analysis of IMDB to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great post by CuriousGnu that did a similar analysis across a larger set of movies but limited to movies since 2000.</description></item><item><title>Analyzing large networks</title><link>/2015/11/26/analyzing-large-networks/</link><pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/26/analyzing-large-networks/</guid><description>While going through some old repos I came across an old project I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections.</description></item><item><title>Finding the optimal car to list on RelayRides</title><link>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</link><pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</guid><description>After discovering and browsing RelayRides I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.</description></item><item><title>Don't scrape into a Dropbox folder</title><link>/2015/04/19/dont-scrape-into-a-dropbox-folder/</link><pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/19/dont-scrape-into-a-dropbox-folder/</guid><description>Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.
Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted.</description></item><item><title>Don't trust client side data</title><link>/2015/01/24/dont-trust-client-side-data/</link><pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/24/dont-trust-client-side-data/</guid><description>At TripleLift, we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins.</description></item><item><title>2014 stats</title><link>/2015/01/11/2014-stats/</link><pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/11/2014-stats/</guid><description>At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year.</description></item><item><title>Some quick Twitter analytics analysis</title><link>/2015/01/06/some-quick-twitter-analytics-analysis/</link><pubDate>Tue, 06 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/06/some-quick-twitter-analytics-analysis/</guid><description>I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.</description></item><item><title>Piketty and the power of data</title><link>/2014/12/09/piketty-and-the-power-of-data/</link><pubDate>Tue, 09 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/09/piketty-and-the-power-of-data/</guid><description>I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions.</description></item><item><title>Normalizing a CSV file using MySQL</title><link>/2014/10/01/normalizing-a-csv-file-using-mysql/</link><pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/01/normalizing-a-csv-file-using-mysql/</guid><description>As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database.</description></item><item><title>Yahoo fantasy football stats - 2014 edition</title><link>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</link><pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</guid><description>This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and updated it to work for the 2014 season. The old version used the great Scrapy framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses Selenium to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.</description></item><item><title>A MySQL class proposal</title><link>/2014/08/17/a-mysql-class-proposal/</link><pubDate>Sun, 17 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/17/a-mysql-class-proposal/</guid><description>I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with TEALS, a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the Coalition 4 Queens program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it.</description></item><item><title>The future of databases</title><link>/2014/07/05/the-future-of-databases/</link><pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/05/the-future-of-databases/</guid><description>A couple of weeks ago I attended a talk by Professor Michael Stonebraker. For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.</description></item><item><title>Beware the data monopoly</title><link>/2013/07/21/beware-the-data-monopoly/</link><pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/21/beware-the-data-monopoly/</guid><description>I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.
I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies.</description></item></channel></rss>