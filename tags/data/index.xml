<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data on Dan Goldin</title><link>/tags/data/</link><description>Recent content in Data on Dan Goldin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 16 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/data/index.xml" rel="self" type="application/rss+xml"/><item><title>Big money, not big data</title><link>/2020/08/16/big-money-not-big-data/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/16/big-money-not-big-data/</guid><description>&lt;p>It&amp;rsquo;s common for companies to complain about the challenges of running big data and how difficult it is. The reality is that unless you&amp;rsquo;re running at massive scale (Google or Facebook) your problems are more to do with big money rather than big data. It&amp;rsquo;s expensive to store, process, and expose terabytes and the difficulty is in doing it cost effectively, not in simply doing it. There are enough modern tools out there across all the cloud providers (AWS, GCP, Azure) and vendors (Snowflake, Databricks) that it&amp;rsquo;s possible to do nearly everything you want but you&amp;rsquo;ll just have to pay for it.&lt;/p></description></item><item><title>Open, Public, Electronic, and Necessary Government Data Act</title><link>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</guid><description>&lt;p>The US Congress recently &lt;a href="https://e-pluribusunum.org/2018/12/21/congress-made-open-government-data-the-default-in-the-united-states/">passed&lt;/a> &lt;a href="https://www.govtrack.us/congress/bills/115/hr4174">HR-4174&lt;/a> (The Open, Public, Electronic, and Necessary Government Data Act) which is intended to make all public government data available and accessible. Over the years I’ve done my fair share of poking around various government datasets - both public and private - and while the data was generally available it was rarely accessible. More often than not the data would be available via a scanned PDF which required some heavy OCR work to extract anything useful or the slightly easier PDF parsing code. Even when the data was in CSV files I often ran into formatting issues or inconsistency between the column documentation and the data contents themselves. The most important available datasets will always have people willing to go through the grunt work of cleaning them up but it’s the fringe datasets that end up having too much friction for researchers and developers to dig into them. I’m glad the government is moving to make the data accessible as well since it is the strongest way to make it actionable.&lt;/p></description></item><item><title>My DataEngConf 2018 talk</title><link>/2018/11/20/my-dataengconf-2018-talk/</link><pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/20/my-dataengconf-2018-talk/</guid><description>&lt;p>On November 9th I had the privilege of speaking at DataEngConf under the “Hero Engineering” track. My talk was titled “The Highs and Lows of Building an AdTech Data Pipeline” and I covered our evolution from a dead simple, sampled approach that had nothing to do with big data to the latest version which is leveraging a variety of modern open source data technologies.&lt;/p>
&lt;p>I spoke about the motivation, challenges, and lessons learned during each iteration and ended the talk with the top 3 lessons learned across the various iterations of the pipeline. If you’re interested in the details you can grab the slides as either &lt;a href="https://docs.google.com/presentation/d/1XmOPgsbxoah2Pulw3eRvzjClOM5A5Dq-B2MWfot0guo/edit#slide=id.p">Google Slides&lt;/a> or as PowerPoint from the &lt;a href="https://www.dataengconf.com/speaker/the-highs-and-lows-of-building-an-adtech-data-pipeline">DataEngConf site&lt;/a>. Note that there was also a recording made but I’m still waiting for it to be processed and uploaded it to YouTube and will share that when it’s available.&lt;/p></description></item><item><title>Facebook's "breach"</title><link>/2018/03/18/facebooks-breach/</link><pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/18/facebooks-breach/</guid><description>&lt;p>The big news this weekend was that Facebook suspended Cambridge Analytica, a company that leveraged behavioral data to come up with very focused and accurate political ads, for using data that they were not supposed to have as well as not deleting it when caught. Everyone seems to be surprised by this revelation but I’m honestly surprised it took this long and I wouldn’t be surprised if there are still hundreds, or even thousands, of companies in the same situation as Cambridge Analytica, albeit at a smaller scale.&lt;/p></description></item><item><title>Crowdsourced data</title><link>/2018/03/11/crowdsourced-data/</link><pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/11/crowdsourced-data/</guid><description>&lt;p>Open source has become a critical part of modern software development that allows small teams to move quickly and do in months what used to take years. This has been driven by massive platforms, such as GitHub, that make it extremely easy to find useful code, contribute back, and provide feedback, comments, and requests.&lt;/p>
&lt;p>Unfortunately, data hasn’t seen as strong of an open sourcing trend. There are a few sites - ranging from &lt;a href="https://www.data.gov/developers/open-source">data.gov&lt;/a> for government data to various &lt;a href="https://github.com/awesomedata/awesome-public-datasets">aggregators&lt;/a> that offer various datasets for download but the formats are inconsistent and some even come in PDF. There just hasn’t been a single open data standard that’s been globally adopted. Instead we have cities offering PDF and CSV files for download and companies offering throttled APIs to their proprietary data.&lt;/p></description></item><item><title>Slides from my talk at DataEngConf</title><link>/2017/10/30/slides-from-my-talk-at-dataengconf/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/30/slides-from-my-talk-at-dataengconf/</guid><description>&lt;p>I had the privilege of giving a talk today at DataEngConf. Unfortunately the talk was not recorded but you can grab the slides [here]({{DATA_PATH }}/DataEngCof_NYC_Data_Startups_Dan_Goldin-Scaling-a-Data-Pipeline-Mystery-to-Mastery.pdf). The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture. Take a look at the slides and if you have any questions I’d be glad to answer them in the comments.&lt;/p></description></item><item><title>Type dependent databases</title><link>/2017/05/23/type-dependent-databases/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/23/type-dependent-databases/</guid><description>&lt;p>I’m a huge proponent of strong types when it comes to coding. Unless it’s a throwaway project it’s always worth spending the extra time to define your objects and the way they will be exposed in the code. This investment makes it more likely that you’ve thought through the way the code will need to evolve and the various edge cases you need to handle.&lt;/p>
&lt;p>This philosophy is even more important when thinking about your database structure since that’s going to be even more difficult to change than your code. Changing the code requires a deploy while changing a database schema will require a migration and a series of corresponding code changes.&lt;/p></description></item><item><title>The golden age of big data tools</title><link>/2017/04/23/the-golden-age-of-big-data-tools/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/23/the-golden-age-of-big-data-tools/</guid><description>&lt;p>I really dislike using the phrase “big data” but it is catchy so I’m going with it. It really does feel we’re in the golden age of big data tools. The rise of cloud computing, distributed storage, and the proliferation of open source have led to multiple orders of magnitude more data generated now than a decade ago. It’s an impossible number to calculate but some project that between 2010 and 2020 there will be a 50 fold increase in the amount of data collected. And the rate is only increasing as more and more people around the world get smartphones and the internet of things starts becoming a part of daily life.&lt;/p></description></item><item><title>A poor man's data pipeline</title><link>/2016/11/12/a-poor-mans-data-pipeline/</link><pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/12/a-poor-mans-data-pipeline/</guid><description>&lt;p>Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.&lt;/p></description></item><item><title>Setting up secor for Kafka 0.10</title><link>/2016/10/10/setting-up-secor-for-kafka-0.10/</link><pubDate>Mon, 10 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/10/setting-up-secor-for-kafka-0.10/</guid><description>&lt;p>Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that &lt;a href="https://github.com/pinterest/secor">secor&lt;/a>, a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.&lt;/p></description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>&lt;p>After getting the &lt;a href="http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/">IMDB data loaded&lt;/a> it was time to dive in and start looking at the data. In 2012, I did an &lt;a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">analysis&lt;/a> to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the &lt;a href="https://www.curiousgnu.com/imdb-age-distribution">help of CuriousGnu&lt;/a>, I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on &lt;a href="https://github.com/dangoldin/imdb">GitHub&lt;/a> and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.&lt;/p></description></item><item><title>Analyzing IMDB data: Step 1 - Cleaning and QA</title><link>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</guid><description>&lt;p>In 2012 I did a &lt;a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">simple analysis of IMDB&lt;/a> to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great &lt;a href="https://www.curiousgnu.com/imdb-age-distribution">post by CuriousGnu&lt;/a> that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The &lt;a href="http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0">NY Times interviewed&lt;/a> a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.&lt;/p></description></item><item><title>Analyzing large networks</title><link>/2015/11/26/analyzing-large-networks/</link><pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/26/analyzing-large-networks/</guid><description>&lt;p>While going through some old repos I came across an old &lt;a href="https://github.com/dangoldin/meerkat-crawl">project&lt;/a> I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including &lt;a href="http://gephi.github.io/">Gephi&lt;/a>, &lt;a href="http://www.cytoscape.org/">Cytoscape&lt;/a>, and &lt;a href="https://networkx.github.io/">NetworkX&lt;/a> - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.&lt;/p></description></item><item><title>Finding the optimal car to list on RelayRides</title><link>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</link><pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</guid><description>&lt;p>After discovering and browsing &lt;a href="https://relayrides.com/">RelayRides&lt;/a> I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.&lt;/p></description></item><item><title>Don't scrape into a Dropbox folder</title><link>/2015/04/19/dont-scrape-into-a-dropbox-folder/</link><pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/19/dont-scrape-into-a-dropbox-folder/</guid><description>&lt;p>Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.&lt;/p>
&lt;p>Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.&lt;/p></description></item><item><title>Don't trust client side data</title><link>/2015/01/24/dont-trust-client-side-data/</link><pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/24/dont-trust-client-side-data/</guid><description>&lt;p>At &lt;a href="http://triplelift.com/" target="_blank">TripleLift&lt;/a>, we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.&lt;/p></description></item><item><title>2014 stats</title><link>/2015/01/11/2014-stats/</link><pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/11/2014-stats/</guid><description>&lt;p>At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a &lt;a href="https://gist.github.com/dangoldin/14906d4f863cd83f3008" target="_blank">simple Python script&lt;/a>. For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.&lt;/p></description></item><item><title>Some quick Twitter analytics analysis</title><link>/2015/01/06/some-quick-twitter-analytics-analysis/</link><pubDate>Tue, 06 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/06/some-quick-twitter-analytics-analysis/</guid><description>&lt;p>I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.&lt;/p></description></item><item><title>Piketty and the power of data</title><link>/2014/12/09/piketty-and-the-power-of-data/</link><pubDate>Tue, 09 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/09/piketty-and-the-power-of-data/</guid><description>&lt;p>I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.&lt;/p></description></item><item><title>Normalizing a CSV file using MySQL</title><link>/2014/10/01/normalizing-a-csv-file-using-mysql/</link><pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/01/normalizing-a-csv-file-using-mysql/</guid><description>&lt;p>As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.&lt;/p></description></item><item><title>Yahoo fantasy football stats - 2014 edition</title><link>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</link><pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</guid><description>&lt;p>This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and &lt;a href="https://github.com/dangoldin/yahoo-ffl" target="_blank">updated it to work&lt;/a> for the 2014 season. The old version used the great &lt;a href="http://scrapy.org/" target="_blank">Scrapy&lt;/a> framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses &lt;a href="http://www.seleniumhq.org/" target="_blank">Selenium&lt;/a> to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.&lt;/p></description></item><item><title>A MySQL class proposal</title><link>/2014/08/17/a-mysql-class-proposal/</link><pubDate>Sun, 17 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/17/a-mysql-class-proposal/</guid><description>&lt;p>I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with &lt;a href="http://www.tealsk12.org/" target="_blank">TEALS&lt;/a>, a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the &lt;a href="http://www.c4q.nyc/" target="_blank">Coalition 4 Queens&lt;/a> program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.&lt;/p></description></item><item><title>The future of databases</title><link>/2014/07/05/the-future-of-databases/</link><pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/05/the-future-of-databases/</guid><description>&lt;p>A couple of weeks ago I attended a talk by &lt;a href="https://en.wikipedia.org/wiki/Michael_Stonebraker" target="_blank">Professor Michael Stonebraker&lt;/a>. For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.&lt;/p></description></item><item><title>Beware the data monopoly</title><link>/2013/07/21/beware-the-data-monopoly/</link><pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/21/beware-the-data-monopoly/</guid><description>&lt;p>I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.&lt;/p>
&lt;p>I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to &lt;a href="http://www.infoworld.com/t/data-management/google-wants-your-phonemes-539" target="_blank">subsidize the cost&lt;/a> of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.&lt;/p></description></item></channel></rss>