<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>devops on Dan Goldin</title><link>/tags/devops/</link><description>Recent content in devops on Dan Goldin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 28 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>Increase reliability through loose coupling: An adtech example</title><link>/2020/12/28/increase-reliability-through-loose-coupling-an-adtech-example/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/28/increase-reliability-through-loose-coupling-an-adtech-example/</guid><description>A few weeks ago I evangelized degrading functionality in the case of system outages as an option versus building out full regional resiliency. Someone asked me to dig into my example more so I&amp;rsquo;ll share a bit of how one of our core systems at TripleLift is set up.
We run an advertising exchange handing tens of billions of ad requests each day. AdTech has a ton of different companies with each doing a bit of everything but a simple way to think of us is an &amp;ldquo;exchange&amp;rdquo; that sits in between buyers and sellers of ads and ad space.</description></item><item><title>Removing duplicate files in S3</title><link>/2020/12/18/removing-duplicate-files-in-s3/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/18/removing-duplicate-files-in-s3/</guid><description>I&amp;rsquo;m a digital hoarder and whenever I had to switch computers, I was always worried about losing files. These days it&amp;rsquo;s both lower risk since so much is scattered across the cloud but with the ascent of AWS I&amp;rsquo;ve resorted to just backing up my computers onto S3.
I simply do a recursive copy of my home folder to S3 and call it a day. One problem this exposes is that there are duplicate files scattered all over the place.</description></item><item><title>What do adtech and BGP have in common?</title><link>/2020/12/17/what-do-adtech-and-bgp-have-in-common/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/17/what-do-adtech-and-bgp-have-in-common/</guid><description>Despite writing web applications many engineers are not familiar with the infrastructure side of the internet - DNS, BGP, TCP/IP - and yet depend on it all working. Over the years I&amp;rsquo;ve gotten a better understanding of the guts of the internet but only in the past few weeks have I discovered Resource Public Key Infrastructure (RPKI) on top of BGP. RPKI is meant to add a layer of trust on top of the existing infrastructure which was designed many decades ago without worrying about malicious actors.</description></item><item><title>Degrade functionality instead of building cross region availablity</title><link>/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/</guid><description>AWS had a large multi-hour Kinesis outage last Wednesday that affected a variety of dependent services, including Cloudwatch, Lambda, ECS, and EKS. These systems are complicated and highlight the scale and complexity of modern cloud computing.
It&amp;rsquo;s impossible to be perfect but it&amp;rsquo;s a reminder for all of us to think through our applications and identify their failure cases. Just because there was a major AWS failure that affected your service does not mean you should drop everything and convert your application to be highly-available with regional failover.</description></item><item><title>Static code analysis with DeepSource</title><link>/2020/11/26/static-code-analysis-with-deepsource/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/26/static-code-analysis-with-deepsource/</guid><description>While listening to the Software Engineering Daily podcast I came across an interview with Jai Pradeesh and Sanket Saurav who are the founders of DeepSource, a modern code analysis tool.
I&amp;rsquo;m a sucker for these types of tools and willing to try anything that&amp;rsquo;s low friction and promises to me more productive so I gave it a shot on two of my open source repos - health-stats and blog-analytics.
There have been quite a few of these tools - for example SonarQube and Amazon&amp;rsquo;s CodeGuru - but DeepSource definitely felt more modern.</description></item><item><title>Analyzing the AWS EC2 reservation options</title><link>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</guid><description>While writing the post on AWS reservations I started thinking if there&amp;rsquo;s any arbitrage opportunity in the reservations. For example - does it make sense to do a 1 year or 3 year reservation for some instance types of upgrade an instance class to get a better reservation value?
You can do this manually using ec2instances.info which provides a quick way to look at EC2 pricing info although forces you to pick the type of reservations you&amp;rsquo;re interested in.</description></item><item><title>Optimizing AWS reservations</title><link>/2020/07/08/optimizing-aws-reservations/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/08/optimizing-aws-reservations/</guid><description>The Information has an article making the case that Uber was better off having their own data centers versus relying on the cloud given the impact of COVID but that would depend on their reservation strategy. Sure if they reserved AWS capacity then they would be on the hook. Alternatively, if they had no reservations and were running everything on demand they would have incurred higher previous costs but would immediately be able to shut things down.</description></item><item><title>Repository of configurations</title><link>/2020/02/07/repository-of-configurations/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/07/repository-of-configurations/</guid><description>While tuning Kafka yet another time I started thinking of how useful a repository of configuration files would be. Very often we install an open-source project and default to the standard options. This works great when you&amp;rsquo;re starting out but as you grow you realize the deployment is no longer cutting it. So you go back to the documentation and do a few searches to see how others have it tuned.</description></item><item><title>Importing resources into terraform</title><link>/2019/11/06/importing-resources-into-terraform/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/06/importing-resources-into-terraform/</guid><description>You don&amp;rsquo;t appreciate Terraform until you have hundreds of AWS resources with dozens of security rules. But once you develop that appreciate it becomes an indispensable tool in managing and scaling your infrastructure. I&amp;rsquo;m a huge fan of Terraform and recently started moving a bunch of my personal project resources into Terraform.
I imagine for most people it&amp;rsquo;s a chore but I find the entire process zen-like. I take something that was a mess and clean it up while learning something new.</description></item><item><title>Deplying Docker using systemd</title><link>/2019/10/28/deplying-docker-using-systemd/</link><pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/28/deplying-docker-using-systemd/</guid><description>A few months ago in a burst of inspiration I converted a bunch of my side projects to run inside Docker. Unfortunately, I didn&amp;rsquo;t do the follow up work of actually creating a Kubernetes cluster and instead came up with a ridiculously hacky process to get them running. I created a simple shell script that would just build and run a Docker image and have just been running it inside a screen session.</description></item><item><title>Upgrading pip packages within a Dockerfile</title><link>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</guid><description>I have an old project, makersalley.com, that used to run on an old version of Python (2.7) and an archaic version of Django (1.4). Earlier this year I overhauled it to run on a newer version of Django (1.11) and Dockerized the entire setup which required all sorts of changes and library fixes.
Last night, I took it one step further by upgrading it to the latest versions of both Python (3.</description></item><item><title>Visualizing Kafka partition changes</title><link>/2019/10/01/visualizing-kafka-partition-changes/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/01/visualizing-kafka-partition-changes/</guid><description>Earlier this week we scaled up our Kafka cluster to take advantage of more availability zones and increase the replication for some of our key topics. After making sure the new brokers joined the existing cluster we needed to redo the partitioning to take advantage of these newly available brokers.
I&amp;rsquo;m sure there are better and more modern tools out there but we&amp;rsquo;ve been using SiftScience&amp;rsquo;s kafka-assigner. Rather than being a naive partitioning it looks at the existing assignments and optimizes the new assignment to minimize the number of moves while striving to keep the partitions evenly distributed across all brokers.</description></item><item><title>Open sourcing configurations</title><link>/2019/07/28/open-sourcing-configurations/</link><pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/28/open-sourcing-configurations/</guid><description>Open source has been an incredible boon to software. It is the foundation of every modern startup and has allowed small teams to outbuild their larger rivals. As critical as open source software is I wish there was a single repository containing production deployment stats and configurations. For example, if you look at the Kafka documentation you quickly discover there are hundreds of options that can be set. Some matter more than others but it&amp;rsquo;s a larger undertaking to properly configure a cluster and understand the impact each of these configuration options will have.</description></item><item><title>PagerDuty's incident response guide</title><link>/2019/07/26/pagerdutys-incident-response-guide/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/26/pagerdutys-incident-response-guide/</guid><description>As companies grow they need to develop their incident response processes. The risks are greater and the systems become increasingly more complicated with significantly more specialized knowledge. We&amp;rsquo;re going through a process to redo our incident response process and while doing research I came across PagerDuty&amp;rsquo;s guide which has been incredibly helpful in guiding our approach.
It&amp;rsquo;s extremely thorough and goes into painstaking detail that describes everything one needs to do to build a mature incident response process.</description></item><item><title>Cloudflare outage postmortem</title><link>/2019/07/16/cloudflare-outage-postmortem/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/16/cloudflare-outage-postmortem/</guid><description>Postmortems are one of the best practices of modern software engineering. They allow engineering teams to learn from mistakes and drive changes that eliminate entire categories of problems. They&amp;rsquo;re a great way to own issues, and if shared publicly, provide transparency to customers and describe what will be done to prevent these types of issues in the future.
As an engineer, it&amp;rsquo;s incredibly valuable and interesting to read these public postmortems.</description></item><item><title>DNS resolution gotchas</title><link>/2019/07/14/dns-resolution-gotchas/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/14/dns-resolution-gotchas/</guid><description>A few months ago we ran a pretty complex migration from AWS Route 53 to NS1 and ran into a few gotchas that I wanted to share. On the surface, DNS seems simple: you associate subdomains to specific records and then rely on the DNS provider to handle that resolution.
The nuance occurs when you have a deeper structure where it takes a few steps to get to the final IP address.</description></item><item><title>Keeping Ubuntu computers in sync</title><link>/2019/07/13/keeping-ubuntu-computers-in-sync/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/13/keeping-ubuntu-computers-in-sync/</guid><description>I switched to Ubuntu a few years ago and along with the change decided to keep as much of my environment setup in version control as possible. The motivation was to make the process of setting up a new computer as simple and repeatable as possible since I planned on shifting both my personal and work computers to Ubuntu and wanted to keep them in sync as much as possible.</description></item><item><title>Future of cloud is hybrid</title><link>/2019/07/05/future-of-cloud-is-hybrid/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/05/future-of-cloud-is-hybrid/</guid><description>Cloud providers are doing all they can to lock companies into their own offerings but my suspicion is that the future looks hybrid. On the surface they all offer the same base functionality but there&amp;rsquo;s a variety of specialization on the edges offered by Amazon vs Microsoft vs Google vs your own data center. My suspicion is that over time more and more companies will adopt hybrid setups that allow them to leverage the strengths of each platform.</description></item><item><title>Secret management across computers</title><link>/2018/12/26/secret-management-across-computers/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/secret-management-across-computers/</guid><description>As part of the Cyber Monday nonsense I convinced myself to purchase a ThinkPad in order to run Linux. I have a separate post coming about the transition from OS X to Ubuntu but one area I’m still trying to get under control is secret management. I have a ton of code on the old computer and almost all of it is on GitHub. The biggest challenge so far has been migrating secrets across computers.</description></item><item><title>New iteration of DevOps</title><link>/2018/12/19/new-iteration-of-devops/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/19/new-iteration-of-devops/</guid><description>While catching up on some tech news today I discovered OpsMop and Vespene. Both of these are new DevOps tools from Michael DeHaan, the creator of Ansible. Before we had a DevOps team I was doing the bulk of our AWS management through the AWS console as well as a few command line scripts but as soon as we had a real DevOps that introduced the modern DevOps stack (Ansible, Terraform, Packer, Kubernetes) I was hooked.</description></item><item><title>EMR vs Databricks costs</title><link>/2018/12/10/emr-vs-databricks-costs/</link><pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/10/emr-vs-databricks-costs/</guid><description>It’s frustrating when vendors introduce their own currency in what seems to be a way to obfuscate pricing. The most recent example is Databricks which offers a slick Spark hosting solution on top of AWS and Azure. Unfortunately, instead of being explicit about the prices they introduced a Databricks Unit (DBU) currency type that then translates into dollars based on the type of usage - ranging from a simple Spark cluster with limited optimizations (Basic Plan) to an interactive one with all sorts of behind the scenes performance tweaks (Data Analytics Plan).</description></item><item><title>Using personal AWS credentials in production</title><link>/2018/06/28/using-personal-aws-credentials-in-production/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>/2018/06/28/using-personal-aws-credentials-in-production/</guid><description>Earlier this week in a fit of security I went into AWS and revoked my old AWS credentials. I assumed that all would be well but unfortunately didn’t realize that my AWS credentials were being used on a production system that wrote data to S3. Before I revoked them I did see that the recent activity contained S3 but assumed it was just me playing around with the AWS CLI. Of course I shouldn’t have had my AWS credentials used on a live system and of course we updated the application to use its own account.</description></item><item><title>Secure at the network level</title><link>/2018/04/17/secure-at-the-network-level/</link><pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/17/secure-at-the-network-level/</guid><description>Two weeks ago Travis CI published a postmortem describing an outage that was caused by a script that truncated all tables on a production database. The script was designed to run against a test database but instead ended up wiping the production one. The remediation steps highlighted are a great start but I’m surprised they didn’t pick the most obvious one - protect systems at the network level.
Relying on confirmation steps, user permissions, and unique credentials per environment are great steps and should be best practices but they don’t actually stop malicious or accidental behavior.</description></item><item><title>Load testing</title><link>/2018/04/12/load-testing/</link><pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/12/load-testing/</guid><description>I started writing this post about using Siege to do load testing but got carried away and ended up discovering how much I don’t know. In particular, I ended up stumbling unto Gil Tene’s talk on measuring latency and how nearly every tool gets it wrong due to the bias in the tools themselves. The general idea is that most tools measure service time rather than request time. Service time is how long it takes your application to handle a request while request time is the time it takes for the user to receive a response.</description></item><item><title>Investigating application issues</title><link>/2017/06/17/investigating-application-issues/</link><pubDate>Sat, 17 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/17/investigating-application-issues/</guid><description>A skill that seems lacking is the ability to debug large scale applications. Most people are comfortable looking at exceptions or log files and working their way back to an issue in the code but given the complexity of modern applications that’s not enough. These days applications are hosted across dozens of cloud instances while utilizing a ton of cloud services. This makes it easier to ship applications but also makes it more difficult to isolate and identify issues since they’re no longer isolated to a single service or application.</description></item><item><title>Security across multiple AWS regions</title><link>/2017/05/04/security-across-multiple-aws-regions/</link><pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/04/security-across-multiple-aws-regions/</guid><description>As great as AWS is there’s still a major gap in the way cross-region support are handled. It’s boggling that there’s no single screen to see every one of your instances and you’re forced to do it a region at a time. Beyond the cosmetic it’s not-obvious how to get instances from multiple regions to communicate securely with one another. On one hand Amazon has the neat concept of a Virtual Private Cloud (VPC) that allows you to create a group of machines that act as if they’re on the same network.</description></item><item><title>Lessons learned from today's S3 failure</title><link>/2017/02/28/lessons-learned-from-todays-s3-failure/</link><pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/28/lessons-learned-from-todays-s3-failure/</guid><description>Today was quite a day. S3, the most resilient of Amazon’s services went down for a few hours in the US-EAST-1 zone and led to a series of failures across a variety of services. There are a ton of lessons one should take away from this - ranging from running across multiple availability zones to being integrated with a variety of cloud providers. The challenge is that it’s not easy; especially when you’re small.</description></item><item><title>Read the release notes</title><link>/2016/12/03/read-the-release-notes/</link><pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/03/read-the-release-notes/</guid><description>I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade.</description></item><item><title>Visualizing your AWS costs</title><link>/2016/11/27/visualizing-your-aws-costs/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/27/visualizing-your-aws-costs/</guid><description>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, Trusted Advisor, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month.</description></item><item><title>Setting up secor for Kafka 0.10</title><link>/2016/10/10/setting-up-secor-for-kafka-0.10/</link><pubDate>Mon, 10 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/10/setting-up-secor-for-kafka-0.10/</guid><description>Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that secor, a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly.</description></item><item><title>Let's Encrypt</title><link>/2016/02/20/lets-encrypt/</link><pubDate>Sat, 20 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/20/lets-encrypt/</guid><description>I’ve been meaning to mess around with Let’s Encrypt since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.
Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy.</description></item><item><title>Design your database for flexibility</title><link>/2016/02/15/design-your-database-for-flexibility/</link><pubDate>Mon, 15 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/15/design-your-database-for-flexibility/</guid><description>One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.
With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have.</description></item><item><title>Have consistent development environments</title><link>/2016/01/09/have-consistent-development-environments/</link><pubDate>Sat, 09 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/09/have-consistent-development-environments/</guid><description>An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries.</description></item><item><title>Good code is easy to build and deploy</title><link>/2015/11/01/good-code-is-easy-to-build-and-deploy/</link><pubDate>Sun, 01 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/01/good-code-is-easy-to-build-and-deploy/</guid><description>A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.</description></item><item><title>Production makes fools of us all</title><link>/2015/10/25/production-makes-fools-of-us-all/</link><pubDate>Sun, 25 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/25/production-makes-fools-of-us-all/</guid><description>The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don&amp;rsquo;t scale on production. Ideally your development environment mirrors production and has the same load and hardware but that&amp;rsquo;s rarely the case.</description></item><item><title>Writing code? Think about the deployment</title><link>/2015/10/18/writing-code-think-about-the-deployment/</link><pubDate>Sun, 18 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/18/writing-code-think-about-the-deployment/</guid><description>The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand.</description></item><item><title>Learn the application architecture through AWS</title><link>/2015/08/23/learn-the-application-architecture-through-aws/</link><pubDate>Sun, 23 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/23/learn-the-application-architecture-through-aws/</guid><description>Last month I wrote that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.</description></item><item><title>Comparing SQL schemas</title><link>/2015/07/12/comparing-sql-schemas/</link><pubDate>Sun, 12 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/12/comparing-sql-schemas/</guid><description>During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn&amp;rsquo;t necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state.</description></item><item><title>Zsh and Oh My Zsh</title><link>/2015/07/06/zsh-and-oh-my-zsh/</link><pubDate>Mon, 06 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/06/zsh-and-oh-my-zsh/</guid><description>I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful oh-my-zsh framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts.</description></item><item><title>Mosh trumps shoddy internet</title><link>/2015/05/12/mosh-trumps-shoddy-internet/</link><pubDate>Tue, 12 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/12/mosh-trumps-shoddy-internet/</guid><description>Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was Mosh. Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions.</description></item><item><title>AWS service limits</title><link>/2015/04/26/aws-service-limits/</link><pubDate>Sun, 26 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/26/aws-service-limits/</guid><description>Something I haven’t seen mentioned much is that AWS has service limits. The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.
This is a serious problem when you’re at the instance limit and depend on auto scaling for high loads.</description></item><item><title>Adding columns in PostgreSQL and Redshift</title><link>/2015/04/23/adding-columns-in-postgresql-and-redshift/</link><pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/23/adding-columns-in-postgresql-and-redshift/</guid><description>A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.</description></item><item><title>Dealing with an unresponsive Google CDN</title><link>/2015/04/12/dealing-with-an-unresponsive-google-cdn/</link><pubDate>Sun, 12 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/12/dealing-with-an-unresponsive-google-cdn/</guid><description>I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to Google&amp;rsquo;s CDN were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.
After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.</description></item><item><title>Migrating a simple HTTP application on AWS</title><link>/2015/01/29/migrating-a-simple-http-application-on-aws/</link><pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/29/migrating-a-simple-http-application-on-aws/</guid><description>A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.</description></item><item><title>DevOps for the rest of us</title><link>/2014/12/26/devops-for-the-rest-of-us/</link><pubDate>Fri, 26 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/26/devops-for-the-rest-of-us/</guid><description>I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen.</description></item><item><title>Symptom based monitoring</title><link>/2014/12/01/symptom-based-monitoring/</link><pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/01/symptom-based-monitoring/</guid><description>A month or so ago I read Rob Ewaschuk’s philosophy on alerting and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level.</description></item><item><title>AWS Glacier</title><link>/2014/10/12/aws-glacier/</link><pubDate>Sun, 12 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/12/aws-glacier/</guid><description>I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click.</description></item><item><title>Dev tools matter</title><link>/2014/09/25/dev-tools-matter/</link><pubDate>Thu, 25 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/25/dev-tools-matter/</guid><description>It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.
After Java, Python felt like a breath of fresh air.</description></item><item><title>Dealing with an RDS replication issue</title><link>/2014/09/20/dealing-with-an-rds-replication-issue/</link><pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/20/dealing-with-an-rds-replication-issue/</guid><description>Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn&amp;rsquo;t a huge deal since our production system relies on the master database but we did have to spend time dealing with this.</description></item><item><title>Managing settings files in Django projects</title><link>/2014/08/30/managing-settings-files-in-django-projects/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/30/managing-settings-files-in-django-projects/</guid><description>I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.</description></item><item><title>Evolution of code deployment</title><link>/2014/08/09/evolution-of-code-deployment/</link><pubDate>Sat, 09 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/09/evolution-of-code-deployment/</guid><description>I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I&amp;rsquo;m hoping to get.
FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP.</description></item><item><title>A simple way to automate AWS deployments</title><link>/2014/07/16/a-simple-way-to-automate-aws-deployments/</link><pubDate>Wed, 16 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/16/a-simple-way-to-automate-aws-deployments/</guid><description>A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of other instance information, including the public hostname, the instance region, and instance type.
This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles.</description></item><item><title>Set up HTTPS on EC2 running Nginx without ELB</title><link>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</link><pubDate>Tue, 15 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</guid><description>I recently needed to set up HTTPS for my side project, better404.com. Amazon makes it easy to set up by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.</description></item><item><title>Generate fake SQL data using JavaScript</title><link>/2014/05/29/generate-fake-sql-data-using-javascript/</link><pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/29/generate-fake-sql-data-using-javascript/</guid><description>A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and hacked one together in JavaScript.</description></item><item><title>Examining ssh login requests</title><link>/2014/05/16/examining-ssh-login-requests/</link><pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/16/examining-ssh-login-requests/</guid><description>I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.
grep &amp;#34;Invalid user &amp;#34; /var/log/auth.</description></item><item><title>Site down? Fall back to S3</title><link>/2014/05/10/site-down-fall-back-to-s3/</link><pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/10/site-down-fall-back-to-s3/</guid><description>An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.
Reddit gives logged out users cached content rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site.</description></item><item><title>Migrating from Linode to Digital Ocean</title><link>/2014/05/02/migrating-from-linode-to-digital-ocean/</link><pubDate>Fri, 02 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/02/migrating-from-linode-to-digital-ocean/</guid><description>Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.
My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects.</description></item></channel></rss>