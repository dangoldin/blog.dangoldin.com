<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>code on Dan Goldin</title><link>/tags/code/</link><description>Recent content in code on Dan Goldin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 29 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/code/index.xml" rel="self" type="application/rss+xml"/><item><title>Challenges in identifying a spawned OS X process</title><link>/2020/12/29/challenges-in-identifying-a-spawned-os-x-process/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/challenges-in-identifying-a-spawned-os-x-process/</guid><description>As part of my project to come up with a utility to deduplicate images on S3 I started working on a proof of concept to validate the flow and functionality. I know that the AWS S3 API has everything I need but I wanted to see whether I could use the built in OS tools to handle the image comparison without resorting to a webapp. The steps I had in mind were pretty simple:</description></item><item><title>Improving the accuracy of evergreen content</title><link>/2020/12/29/improving-the-accuracy-of-evergreen-content/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/improving-the-accuracy-of-evergreen-content/</guid><description>While looking through my blog&amp;rsquo;s analytics I noticed that I have a few posts that have views despite being written years ago. Some of these are still relevant while others are woefully out of date. For example, the difference between yyyy and YYYY in Java&amp;rsquo;s SimpleDateFormat is still relevant but how to scrape web pages has changed significantly since 2013 and anything that has pricing or performance information is likely wrong.</description></item><item><title>Removing duplicate files in S3</title><link>/2020/12/18/removing-duplicate-files-in-s3/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/18/removing-duplicate-files-in-s3/</guid><description>I&amp;rsquo;m a digital hoarder and whenever I had to switch computers, I was always worried about losing files. These days it&amp;rsquo;s both lower risk since so much is scattered across the cloud but with the ascent of AWS I&amp;rsquo;ve resorted to just backing up my computers onto S3.
I simply do a recursive copy of my home folder to S3 and call it a day. One problem this exposes is that there are duplicate files scattered all over the place.</description></item><item><title>Amazon owns more than $2B worth of IPV4 addresses</title><link>/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/</guid><description>While listening to a podcast discussing BGP I heard the fact that AWS owns more than $2B worth of IP addresses. I knew AWS was massive but this came as a big shock so I decided to do some digging around. I came across a site that listed the market prices of IP addresses and the range looks to be anywhere from $20 to $30 per IP depending on the block size.</description></item><item><title>Bringing a user centric approach to the command line</title><link>/2020/11/25/bringing-a-user-centric-approach-to-the-command-line/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/25/bringing-a-user-centric-approach-to-the-command-line/</guid><description>I have a small script that takes an export of Apple Health data and then dumps it into a MySQL database that I then use to visualize my health metrics over time. My prior workflow was to open the app and then Airdrop the file over to my computer at which point I&amp;rsquo;d unzip it, copy the relevant file over into the script directory, and simply run the script.
Clearly this was not the smoothest experience and I figured it was finally time to deal with it.</description></item><item><title>Yahoo fantasy football stats: 2020-2021 edition</title><link>/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/</guid><description>For the fifth year in a row I&amp;rsquo;ve updated my script to fetch the projected stats for the upcoming fantasy football season. These days I&amp;rsquo;m torn on football as a whole - both due to its politics and dangers - and don&amp;rsquo;t plan on watching too many games. Yet I enjoy the competition with my friends and the rote work of updating my scraping script to work every year.
This time around there haven&amp;rsquo;t been too many changes: Yahoo changed the order of a few columns and introduced some minor stylistic changes but the code only needed a few changes to work - much simpler than last year which required running with an adblocking extension to bypass a script blocker.</description></item><item><title>Coupling in action: protobuf enum fields</title><link>/2020/08/17/coupling-in-action-protobuf-enum-fields/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/17/coupling-in-action-protobuf-enum-fields/</guid><description>The next few blog posts are all topics that have come up recently in my 1-1s and serve as good examples for how I think about code and software engineering. The first post is a discussion around coupling and the tradeoffs we considered.
Our pipeline was covered in depth on the high scalability blog but the important part for this post is our event collection piece which consists of the following components:</description></item><item><title>Overthinking how to load data into MySQL</title><link>/2020/06/30/overthinking-how-to-load-data-into-mysql/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/30/overthinking-how-to-load-data-into-mysql/</guid><description>I have two projects that generate data to dump into my personal dashboard. One loads the health export from my Apple watch into MySQL and the other analyzes my blog posts and generates a CSV file of statistics that I then load into MySQL. The input to both is basically the same - either a file or a directory - and yet two different approaches to the processing.
The blog analytics script doesn&amp;rsquo;t depend on any third party libraries and just generates a CSV file that can then be loaded into MySQL through a query.</description></item><item><title>Anatomy of a crypto mining hack</title><link>/2020/05/29/anatomy-of-a-crypto-mining-hack/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/29/anatomy-of-a-crypto-mining-hack/</guid><description>A few months ago I set up a simple ftp server to help a friend. I took a few security shortcuts which came to bite me this week when I received an alert from DigitalOcean that an instance was running hot.
I dug into it and noticed a series of processes being run by the ftp_user - the most impactful was a command called rsync. I&amp;rsquo;m familiar with rsync which syncs files across devices - and this was nothing like that.</description></item><item><title>Metrics, logging, and error reporting</title><link>/2020/05/27/metrics-logging-and-error-reporting/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/27/metrics-logging-and-error-reporting/</guid><description>As software engineers it’s vital to understand how our applications are performing. The more information we have the better we can address problems, improve performance, and generally better solve problems for our customers. Imagine releasing a product with nothing being collected - you&amp;rsquo;d be flying blind.
I&amp;rsquo;m a huge advocate for measuring application performance in terms of business metrics and aligning it as much with the customer experience as possible. This post does not go into that.</description></item><item><title>Data analysis and visualization</title><link>/2020/04/29/data-analysis-and-visualization/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/29/data-analysis-and-visualization/</guid><description>Yesterday I had the rare chance to actually do some coding and realized how rusty I am at numerical analysis in Python. The task was simple - ingest a CSV that had a date column, two categorical columns, and a numerical column - and then generate a grid containing a series of line plots, each of which would be a combination of the two categorical columns.
I did a ton of this work years ago so knew what was possible.</description></item><item><title>Optimizing code? Think theoretical limits</title><link>/2020/04/18/optimizing-code-think-theoretical-limits/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/18/optimizing-code-think-theoretical-limits/</guid><description>This post was inspired from a conversation with an engineer who was tasked with optimizing the performance of a heavily used static JavaScript script. This code gets loaded billions of times a day across a variety of devices and small improvements to its load time and performance can drive significant value to our customers and us.
When you start it&amp;rsquo;s easy to find the low hanging fruit and get the simple wins.</description></item><item><title>Dumping Apple health data into MySQL</title><link>/2020/04/11/dumping-apple-health-data-into-mysql/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/11/dumping-apple-health-data-into-mysql/</guid><description>I apparently can&amp;rsquo;t get enough of Grafana and the latest quantified self push was to visualize the data from Apple health. Apple makes it pretty simple to export the data but it&amp;rsquo;s in XML so there&amp;rsquo;s a small bit of processing to turn into something that can be visualized. For my personal stats I&amp;rsquo;m dumping the data to MySQL and writing fairly simple queries to visualize them. Since I already did a similar export in my email-stats code I was able to reuse a fair amount.</description></item><item><title>Visualizing my journey to Inbox Zero</title><link>/2020/03/31/visualizing-my-journey-to-inbox-zero/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/31/visualizing-my-journey-to-inbox-zero/</guid><description>I subscribe to the &amp;ldquo;Inbox Zero&amp;rdquo; philosophy and treat my email inbox as a todo list that I slowly work through. As part of the desire to get more and more quantitative I wrote a quick script to pull the number of emails from my Inbox and then insert the data as a row into a new table in my personal stats database. As usual, most of the work was in deciding to do it and once I got to coding the hacky solution was done within 20 minutes.</description></item><item><title>My personal Grafana dashboard</title><link>/2020/02/27/my-personal-grafana-dashboard/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/27/my-personal-grafana-dashboard/</guid><description>Last year I wrote about the idea of a personal dashboard and earlier this year I described my 2020 goals and how I&amp;rsquo;d go about measuring my progress. The past two days I was able to combine the two concepts and created a simple Grafana dashboard to measure my progress against the blogging goal. As with most tasks, the most difficult part was getting started and the actual exercise took a few hours.</description></item><item><title>Finding parking spots using YOLO</title><link>/2020/01/31/finding-parking-spots-using-yolo/</link><pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/31/finding-parking-spots-using-yolo/</guid><description>I finally managed to make a bit more progress on the Parking Spot Finder project. Last time I tested the computer vision products offered by Google and AWS to see how well they were able to detect cars. This time around I decided to actually start working on the computer vision side and found a nice tutorial that allowed me to quickly try out the YOLO computer vision library. Surprisingly, it did significantly better than AWS but was more mixed against Google.</description></item><item><title>Parking spot finder</title><link>/2019/12/26/parking-spot-finder/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/26/parking-spot-finder/</guid><description>I live in Jersey City and rely on street parking rather than paying for a lot. Jersey City also some aggressive alternate side parking rules with many streets having twice-a-week cleanings which in the worst case requires me to move my car 4 times a week.
A few years ago I decided to build a simple tool that would be hooked up to a camera and would notify me whenever a spot became available.</description></item><item><title>SMS based password manager</title><link>/2019/11/19/sms-based-password-manager/</link><pubDate>Tue, 19 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/19/sms-based-password-manager/</guid><description>Two Fridays ago I wrote about my home-built password manager and while working well on a computer it was a pain to use on mobile. I&amp;rsquo;ve been trying to think of a more friendly way of supporting this on mobile and came up with the idea of retrieving the passwords via SMS. The workflow would be to send a text to a number with a search term and then get the password back as a response.</description></item><item><title>DYI password manager</title><link>/2019/11/08/dyi-password-manager/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/08/dyi-password-manager/</guid><description>I prefer plaintext for all my note taking. Text files are extremely portable and serve as a flexible foundation for anything I&amp;rsquo;d want to do. For example, I can use grep to perform complex regex searches across thousands of files. If I want to apply a bulk operation to my notes I can write a quick script to do so. This is all possible because there&amp;rsquo;s no proprietary format backing them and I&amp;rsquo;m able to leverage the power of the command line.</description></item><item><title>Upgrade your libraries</title><link>/2019/11/04/upgrade-your-libraries/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/04/upgrade-your-libraries/</guid><description>This blog is hosted on GitHub and built using Jekyll. Jekyll is a simple static site generator that&amp;rsquo;s been working well for me and was flexible enough to allow me to switch the entire site over to AMP. Unfortunately, the switch to AMP led to the site generation becoming significant slower due to the CSS-inlining requirement. Two years ago I started profiling and dropped the site generation time down to about 15 seconds from over 4 minutes by generating the CSS once and then reusing it for all future pages.</description></item><item><title>Deplying Docker using systemd</title><link>/2019/10/28/deplying-docker-using-systemd/</link><pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/28/deplying-docker-using-systemd/</guid><description>A few months ago in a burst of inspiration I converted a bunch of my side projects to run inside Docker. Unfortunately, I didn&amp;rsquo;t do the follow up work of actually creating a Kubernetes cluster and instead came up with a ridiculously hacky process to get them running. I created a simple shell script that would just build and run a Docker image and have just been running it inside a screen session.</description></item><item><title>File size histogram via the command line</title><link>/2019/10/20/file-size-histogram-via-the-command-line/</link><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/20/file-size-histogram-via-the-command-line/</guid><description>I&amp;rsquo;m a sucker for a good shell command and recently discovered (via StackOverflow) the most complex one yet - a one liner to generate a histogram of file sizes within a directory. The sizes are in powers of two but it&amp;rsquo;s a great way to get some simple summary statistics of files inside a directory. I still find awk mystifying to write but nearly every advanced shell command uses awk in some way.</description></item><item><title>Visualizing my Twitter archive - 2019 edition</title><link>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</guid><description>GitHub sent me an alert this past weekend that a bunch of my repos were using old libraries that had security vulnerabilities. Nearly all of them were due to my usage of an old version of the requests library. Updating those was as simple as updating the requirements.txt file to the new version.
One of these repos, twitter-archive-analysis, is my most popular project on GitHub so I thought I might as well revisit it and see if I could both address the vulnerabilities and get it running again.</description></item><item><title>Upgrading pip packages within a Dockerfile</title><link>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</guid><description>I have an old project, makersalley.com, that used to run on an old version of Python (2.7) and an archaic version of Django (1.4). Earlier this year I overhauled it to run on a newer version of Django (1.11) and Dockerized the entire setup which required all sorts of changes and library fixes.
Last night, I took it one step further by upgrading it to the latest versions of both Python (3.</description></item><item><title>Visualizing Kafka partition changes</title><link>/2019/10/01/visualizing-kafka-partition-changes/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/01/visualizing-kafka-partition-changes/</guid><description>Earlier this week we scaled up our Kafka cluster to take advantage of more availability zones and increase the replication for some of our key topics. After making sure the new brokers joined the existing cluster we needed to redo the partitioning to take advantage of these newly available brokers.
I&amp;rsquo;m sure there are better and more modern tools out there but we&amp;rsquo;ve been using SiftScience&amp;rsquo;s kafka-assigner. Rather than being a naive partitioning it looks at the existing assignments and optimizes the new assignment to minimize the number of moves while striving to keep the partitions evenly distributed across all brokers.</description></item><item><title>Solving Num: A combinatoric math game</title><link>/2019/09/14/solving-num-a-combinatoric-math-game/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/14/solving-num-a-combinatoric-math-game/</guid><description>I don&amp;rsquo;t play games on my phone but one game I keep going back to is &amp;ldquo;Num&amp;rdquo; - an &amp;ldquo;insanely hard math game.&amp;rdquo; The premise is pretty simple - you have a few numbers that you need to combine, using the four basic math operations, so it computes to a specific number.
The levels start off simple but it gets more difficult with a lot of trial and error at the higher levels.</description></item><item><title>In praise of tcpdump</title><link>/2019/09/13/in-praise-of-tcpdump/</link><pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/13/in-praise-of-tcpdump/</guid><description>We switched over to Prometheus and Grafana for our monitoring but some of our older systems are still on Graphite and StatsD. One of these is an alert for disk usage that started going off a few weeks ago. Over the course of the day it kept fluctuating from 100% disk usage to ~40% and whenever we dug into it we only saw the 40% number. Since StatsD is push based we assumed it was another instance that was submitting its metrics under the same key.</description></item><item><title>Explain the why in code reviews</title><link>/2019/08/25/explain-the-why-in-code-reviews/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/25/explain-the-why-in-code-reviews/</guid><description>This may be obvious to most people but it&amp;rsquo;s still worth reiterating: if you&amp;rsquo;re leaving a comment on a code review make sure to explain your reasoning. Code reviews are a key component in writing high quality code, improving everyone&amp;rsquo;s skills and knowledge, and encouraging a strong and collaborative team.
Code reviews can be a bit clinical with feedback being blunt and a few words and it&amp;rsquo;s important to add context to all but the simplest of feedback.</description></item><item><title>Yahoo fantasy football stats: 2019-2020 edition</title><link>/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition/</guid><description>In what has become an annual tradition I&amp;rsquo;ve updated my Yahoo Fantasy Football scraping script for the 2019-2020 NFL season. The script works by logging into the Yahoo Fantasy Football site and downloading the first 4 pages of projected stats for each week. The code is up on GitHub as well as the stats in a CSV file.
Every year there&amp;rsquo;s been something that Yahoo did to break my script and this year was no different.</description></item><item><title>Keeping Ubuntu computers in sync</title><link>/2019/07/13/keeping-ubuntu-computers-in-sync/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/13/keeping-ubuntu-computers-in-sync/</guid><description>I switched to Ubuntu a few years ago and along with the change decided to keep as much of my environment setup in version control as possible. The motivation was to make the process of setting up a new computer as simple and repeatable as possible since I planned on shifting both my personal and work computers to Ubuntu and wanted to keep them in sync as much as possible.</description></item><item><title>Video of my Data Council NYC talk</title><link>/2019/07/07/video-of-my-data-council-nyc-talk/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/07/video-of-my-data-council-nyc-talk/</guid><description>Last November I gave a talk at Data Council NYC &amp;lsquo;18 titled &amp;ldquo;The Highs and Lows of Building an Adtech Data Pipeline&amp;rdquo; and finally saw that the video has been uploaded to YouTube. If you&amp;rsquo;re interested in hearing a runthrough of the different iterations our data pipeline went through over the course of 6 years definitely give it a watch and leave comments and feedback. I&amp;rsquo;m not the most natural of public speakers and there were moments I spoke much quicker than I should have but hopefully having the slides on the side make it a tad more understandable.</description></item><item><title>Tightly coupled data loss</title><link>/2019/06/15/tightly-coupled-data-loss/</link><pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate><guid>/2019/06/15/tightly-coupled-data-loss/</guid><description>A few months while doing some Kafka maintenance we ran into an issue that caused us to lose approximately 10% of records across all our topics over the course of an hour. It was a big screw up but what made it worse was the interaction between the records. Our records represent the life cycle of an ad from the auction, to the render, to whether it was in view, and beyond.</description></item><item><title>Modernizing Makers Alley and Better404</title><link>/2019/01/27/modernizing-makers-alley-and-better404/</link><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><guid>/2019/01/27/modernizing-makers-alley-and-better404/</guid><description>Years ago I started two companies, Makers Alley and Better404, and while they were both failures I didn’t have the heart to shut down the site. I put a lot of heart into building them and just pulling the plug felt cold. Instead I just paid the annual hosting fee and had them both running on a small AWS instance. This worked fine for years but unfortunately while I was messing around with Terraform I ended up terminating the instance entirely.</description></item><item><title>Secret management across computers</title><link>/2018/12/26/secret-management-across-computers/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/secret-management-across-computers/</guid><description>As part of the Cyber Monday nonsense I convinced myself to purchase a ThinkPad in order to run Linux. I have a separate post coming about the transition from OS X to Ubuntu but one area I’m still trying to get under control is secret management. I have a ton of code on the old computer and almost all of it is on GitHub. The biggest challenge so far has been migrating secrets across computers.</description></item><item><title>New code is not a linear increase in complexity</title><link>/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/</link><pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/</guid><description>When it comes to software development we often add features simply because it’s easy. And almost always they are - just add an additional optional argument or two to a function and suddenly you’ve expanded your application’s functionality. The catch is that this assumes that this new code is a linear increase in complexity but it’s not.
Computer science has the concept of “Big O notation” to measure how a function behaves as a function of it’s input.</description></item><item><title>Counting the number of lines of code in a GitHub account</title><link>/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/</link><pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/</guid><description>It’s surprisingly difficult to count the number of lines of code in a GitHub account. One day I’d like to come up with a fully automated solution but in the meantime I’ve come up with a workaround that gets me what I need.
Follow the steps in the following Stack Overflow answer to create your own command, cloc-git, that fetches a repo and runs another utility, cloc, that counts the number of lines in a git repo.</description></item><item><title>Aggressive code deprecation</title><link>/2018/11/25/aggressive-code-deprecation/</link><pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/25/aggressive-code-deprecation/</guid><description>Part of writing high quality code quickly is deprecating no longer used features and functionality. It sounds simple but more often than not there’s an abundance of references throughout - some tightly coupled and others loosely coupled - that make a full deprecation difficult. In some cases it’s is as simple as an isolated code change while in other cases it’s removing code along with some database migrations and in the extreme case it may be removing an entire service.</description></item><item><title>Code without online help</title><link>/2018/11/24/code-without-online-help/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/24/code-without-online-help/</guid><description>Whenever I need some coding help my first step is to do an online search which usually leads me to either the library documentation or a StackOverflow page. This is a poor habit and something I’m trying to move away from. While I’m almost always online it’s dangerous to rely on the internet to code - both because there will be times you may not have internet access but also because you lose the ability to do your own investigation, discovery, and critical thinking.</description></item><item><title>Python 3 and aiohttp</title><link>/2018/11/16/python-3-and-aiohttp/</link><pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/16/python-3-and-aiohttp/</guid><description>A few months back I read about aiohttp and asyncio and finally got the chance to play around with it a few weeks back. The project was a quick one-off scrape of a few thousand domains to see what percentage had implemented the pubvendors.json spec, an extension of GDPR that allows publishers to specify the vendors they’re working with.
My initial reaction was to do it in the way I’ve done it countless times before: the requests library in a for loop.</description></item><item><title>Yahoo fantasy football stats: 2018-2019 edition</title><link>/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/</link><pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate><guid>/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/</guid><description>This is much later than in previous years but hopefully that just makes the data more accurate. I updated my Yahoo fantasy football stats scraper to account for the slightly different design for the upcoming season. It still works as before and uses Selenium to open up Chrome and scrape the projected stats by week. The change this year involved shifting the columns around a tiny bit as Yahoo changed the order but other than that there were no changes.</description></item><item><title>MySQL foreign keys</title><link>/2018/07/07/mysql-foreign-keys/</link><pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/07/mysql-foreign-keys/</guid><description>Databases are the last layer of defense against corrupt data and the more restrictive you can make them the better. No matter how much validation you may have missed in your code having a strong and restrictive database schema will protect your data. One of the best approaches to building a restrictive schema is using foreign keys which specify how fields from one table relate to the fields of another table.</description></item><item><title>ALB and ELB access log schemas for Redshift</title><link>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</link><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</guid><description>Back in February I wrote about using Redshift to quickly analyze ELB access logs. This worked great until we switched from using ELBs to using ALBs. Unsurprisingly in hindsight but frustrating at the time the ALBs have a different log schema. Both the Classic and Application Load Balancer logs are well documented on the AWS site but unfortunately the code to create the appropriate Redshift schema is not. In the hope of helping others and passing it forward I wanted to share the Redshift schemas for both types of access logs.</description></item><item><title>Power of shell commands</title><link>/2018/05/26/power-of-shell-commands/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>/2018/05/26/power-of-shell-commands/</guid><description>It’s surprising how unappreciated shell commands are. They’re incredibly powerful and once understood are able to handle small one-off tasks much quicker than writing even simple scripts. Earlier this week I ran into a small task that highlights the power and ability of the shell.
A few of our applications use the same configuration file which contains a variety of URLs, secrets, and passwords. If any of these applications require a field the it gets added to this growing configuration file.</description></item><item><title>Connect Four bot competition</title><link>/2018/04/25/connect-four-bot-competition/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/25/connect-four-bot-competition/</guid><description>Years ago when I worked at Yodle the engineering team held a Connect Four bot competition. The goal was for each person to write a Connect Four playing bot and then let them loose to determine the winner. We had either a few days or a few weeks to do this and my failed approach was to use genetic programming to evolve a bot. The best it did was beat a completely random bot 80% of the time while the winning entry leveraged Minimax with Alpha Beta Pruning.</description></item><item><title>Analyzing AWS ELB logs</title><link>/2018/02/20/analyzing-aws-elb-logs/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/20/analyzing-aws-elb-logs/</guid><description>Logging HTTP requests should be enabled for every application you run. When things go wrong, and they will, it’s often the first step to understand the problem. Unfortunately, logging isn’t always top of mind and is often forgotten. Luckily, if you use the Elastic Load Balancer (ELB) functionality within AWS you’re able to set up ELB logs that track every request and write it to an S3 bucket. The documentation is up on the Amazon site but there’s a surprising amount of information that’s hidden away in the logs.</description></item><item><title>Phonetic distance</title><link>/2018/01/16/phonetic-distance/</link><pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/16/phonetic-distance/</guid><description>Last year I wrote a simple script to automate posting our On-Call schedule. It worked by reading the schedule from a Google Spreadsheet, looking up the names in Slack, and then sharing these usernames on Slack. A tiny problem I ran into was the fact that since I was using an exact match the names in the spreadsheet had to match the names in Slack. This is a trivial problem to solve since we have a finite number of engineers but it still felt a bit too sensitive.</description></item><item><title>Learning Docker</title><link>/2018/01/03/learning-docker/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/03/learning-docker/</guid><description>I’m a bit embarrassed to admit this but I’ve been a bit behind the Docker craze. Sure I’ve done the tutorials when it came out but never really applied it to any of my actual projects. Given that nearly everyone is using Docker in some shape or form, I decided it was finally time to give it an honest effort.
I had a small Python script that I’ve been running weekly off of my laptop and wanted to come up with a better solution.</description></item><item><title>Analyzing my blog: 2017 edition</title><link>/2017/12/21/analyzing-my-blog-2017-edition/</link><pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/21/analyzing-my-blog-2017-edition/</guid><description>I have a set of scripts I wrote in 2016 that aimed to analyze my posts over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.
The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.</description></item><item><title>Jira analysis script</title><link>/2017/12/07/jira-analysis-script/</link><pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/07/jira-analysis-script/</guid><description>A few days ago I wrote about using average number of sprints to complete a story as a way to measure a team’s sprint efficiency. Unfortunately at that time I had a pretty hacky Jira analysis script that I was too ashamed to share but it has been cleaned up enough for me to not feel too much guilt. It’s available on GitHub and comes with a few additional bells and whistles.</description></item><item><title>Improving Jekyll generation speed for AMP pages</title><link>/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/</link><pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/</guid><description>Last September I migrated my blog over to AMP which entailed a variety of challenges ranging from converting every img tag to an amp-img tag with some additional metadata to figuring out how to support Disqus. I tackled the critical ones but the one I never got to was speeding up the build time since it had no impact on the actual reader experience and just slowed down my build and commit process.</description></item><item><title>Archiving large MySQL tables</title><link>/2017/11/18/archiving-large-mysql-tables/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/18/archiving-large-mysql-tables/</guid><description>One of the major changes we made when building the latest iteration of our data pipeline was moving our key agg tables over from MySQL to Redshift. Despite the migration we thought it would be prudent to archive these tables. The challenge was that some of these tables were hundreds of gigabytes so doing a simple mysqldump wouldn’t work. The reason these tables were so large is because they included a date dimension which led to our archive script.</description></item><item><title>Spark's read.jdbc</title><link>/2017/11/07/sparks-read.jdbc/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/07/sparks-read.jdbc/</guid><description>Yesterday I spent a bit of time investigating one of our Spark jobs that had suddenly shot up in run time. The purpose of our job is to collect all the events we see in an hour and generate a variety of aggregate tables and files that can then be loaded into various systems. When we first wrote the job it took about 45 minutes to run but as we&amp;rsquo;ve started seeing much higher data volume the job time has crept up to to 90 minutes.</description></item><item><title>Philosophy of code</title><link>/2017/10/29/philosophy-of-code/</link><pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/29/philosophy-of-code/</guid><description>After writing my post on the code review pyramid I realized that I had many more thoughts about the highest level, code philosophy, and wanted to dedicate a full post to dig into it. The general idea is that a highly functional engineering team is way past the point of arguing over style and syntax and has reached the point where they share the same code philosophy. At this point all members of the team have an instinctive sense of how and where new code should be written - even if they can’t necessarily explain it.</description></item><item><title>The code review pyramid</title><link>/2017/10/27/the-code-review-pyramid/</link><pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/27/the-code-review-pyramid/</guid><description>Every modern software development process contains some form of code reviews. They ensure that all code is looked at by someone other than the author. This improves context, increases code quality, and generally leads to a stronger team and product. Yet there’s a world of difference in code reviews and I I’ve started to think of the different types as a pyramid. The peak is at the highest level but it’s not possible to get to that without going through the lower tiers.</description></item><item><title>Schedule automation using Google spreadsheets and Slack</title><link>/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/</link><pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/</guid><description>Back in March I wrote a script that would go through an on-call calendar kept in a Google spreadsheet and then post the current week’s schedule to a Slack channel. This worked surprisingly well and I thought of doing something similar for the other engineering team calendars. In addition to the on call rotation, we have a dedicated time for internal tech talks as well as a session to cover the news in the industry.</description></item><item><title>Downloading your AIM buddy list</title><link>/2017/10/09/downloading-your-aim-buddy-list/</link><pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/09/downloading-your-aim-buddy-list/</guid><description>While writing the most recent post about the impending AIM shut down I became curious and logged in to see what what I’ve been missing. The application felt worse but seeing my buddy list made me nostalgic and going through the usernames brought back some fond memories as I tried to remember who each screen name belonged to.
I’m a bit of a hoarded, across both the physical and digital worlds, so didn’t want to lose my buddy list after the shut down.</description></item><item><title>Generating a series of commands covering a date range</title><link>/2017/10/05/generating-a-series-of-commands-covering-a-date-range/</link><pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/05/generating-a-series-of-commands-covering-a-date-range/</guid><description>I know the title of the post is terrible but I found it difficult to describe the content in another way.
Lately I’ve been spending a decent amount of my time in SQL-land and running some pretty repetitive queries where only some of the arguments are changed. These run the gamut from exporting some data for a date range by day to adding a series of date partitions while messing around with Spectrum.</description></item><item><title>Examining my shell command history</title><link>/2017/09/21/examining-my-shell-command-history/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/21/examining-my-shell-command-history/</guid><description>A few years ago I wrote a simple script to analyze my shell history in order to examine my most frequently run shell commands. Being in dire need of a new blog post and suffering from a pretty heavy bout of writer’s block I thought it would be interesting to rerun the analysis and see how it compared to results from over 3 years ago.
It’s tough to say whether my usage has changed significantly.</description></item><item><title>A unified Lambda architecture</title><link>/2017/08/25/a-unified-lambda-architecture/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/25/a-unified-lambda-architecture/</guid><description>Lately I’ve been thinking about the Lambda architecture used in modern data pipelines. Lambda architectures are designed for systems that contain massive amounts of streaming data that needs to be processed and exposed quickly. The architecture consists of two different systems. One is a real time pipeline that’s not perfectly accurate but is able to handle large volumes while providing a solid estimate quickly. The other is a batch process that is accurate but runs on a delay.</description></item><item><title>JSON to CSV</title><link>/2017/08/20/json-to-csv/</link><pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/20/json-to-csv/</guid><description>A while back I needed to dump some some EC2 instance information into a CSV file for a quick analysis. Just to get it done I took the immediate approach of using the AWS API to pull the details and then just navigating the massively deep structure. This approach required code designed for that exact structure so it got me thinking of a more generic approach that would be able to extract CSV data from an arbitrary JSON structure.</description></item><item><title>Visualizing my meetings over time</title><link>/2017/07/28/visualizing-my-meetings-over-time/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/28/visualizing-my-meetings-over-time/</guid><description>As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would identify meeting room hogs and repurposed it to just download every one of my calendar events from when I joined TripleLift and another small script to analyze this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos.</description></item><item><title>Yahoo fantasy football stats: 2017-2018 edition</title><link>/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/</link><pubDate>Sat, 08 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/</guid><description>In what has become an annual tradition I updated my Yahoo Fantasy Football scraping bot for the 2017-2018 season. Every year Yahoo makes a few changes to their page and this year was no different. It’s always fun to cross my fingers, run the script, and see what breaks. This year the changes were surprisingly minor. For some reason Yahoo changed the name attribute of the password field from “passwd” to “password” and made a few tweaks to the table structure which required updating the XPath selectors.</description></item><item><title>Thoughtful code</title><link>/2017/07/04/thoughtful-code/</link><pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/04/thoughtful-code/</guid><description>Lately I’ve found myself thinking more deeply about the code I’m writing. No matter how small the task or script I’ll think through the implications of my approach and whether I should be doing anything differently. This doesn’t mean I’ll always pick the more correct and flexible approach and more often than not I’ll choose the quick and dirty one to save time but the thought process itself is valuable since it gets me in the habit of questioning and constantly improving my code.</description></item><item><title>Send private messages to all members of a Slack channel</title><link>/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/</link><pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/</guid><description>One of my more recent “management automation” tricks was to write a simple script that gets all active members of a Slack channel and then sends them a direct message. I’ll often want to poll the entire team and ask them to fill out a survey or submit a questionnaire but the response rates tend to be poor. But if I send a message to people directly I end up with a much better response rate.</description></item><item><title>Getting AMP into RSS</title><link>/2017/06/20/getting-amp-into-rss/</link><pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/20/getting-amp-into-rss/</guid><description>A little less than a year ago I migrated this blog over to AMP which required a lot of small tweaks - ranging from automating the markup changes to getting the Disqus plugin to work. One thing I didn’t get a chance to finish until earlier this week was supporting the RSS feed. This blog is hosted on GitHub pages which is powered by Jekyll and comes with a pretty powerful templating engine.</description></item><item><title>Copying production SQL data to other environments</title><link>/2017/05/20/copying-production-sql-data-to-other-environments/</link><pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/20/copying-production-sql-data-to-other-environments/</guid><description>I suspect most developers have encountered this problem at least once: how do I copy some production data to my test or development environment? This can stem from needing to fix a bug that only manifests in production or just getting a more complicated, real-world dataset that doesn’t yet exist in the test environment. In an ideal world we’d have everything we need in fixtures and properly tested but in the cases we don’t it seems simpler to just copy the data over from the production environment.</description></item><item><title>Security across multiple AWS regions</title><link>/2017/05/04/security-across-multiple-aws-regions/</link><pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/04/security-across-multiple-aws-regions/</guid><description>As great as AWS is there’s still a major gap in the way cross-region support are handled. It’s boggling that there’s no single screen to see every one of your instances and you’re forced to do it a region at a time. Beyond the cosmetic it’s not-obvious how to get instances from multiple regions to communicate securely with one another. On one hand Amazon has the neat concept of a Virtual Private Cloud (VPC) that allows you to create a group of machines that act as if they’re on the same network.</description></item><item><title>Having some fun with the RGB color model</title><link>/2017/04/30/having-some-fun-with-the-rgb-color-model/</link><pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/30/having-some-fun-with-the-rgb-color-model/</guid><description>The best way to learn a new technology is to play with it so to learn React I started a simple project I termed “color-fun&amp;quot; (GitHub). The general idea is to let you specify a starting color along with a step size for each of the digital primary colors and see the color progression. By messing around with various combinations one can get a pretty good sense of the way the RGB color scheme works.</description></item><item><title>Refactor driven development</title><link>/2017/03/19/refactor-driven-development/</link><pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/19/refactor-driven-development/</guid><description>There are a variety of software development methodologies and I’d like to throw another one into the fray - refactor driven development. Rather than focusing on tests or models or functions the focus should be on expressive and maintainable code. Imagine spending 80% of your development time on refactoring old code and laying a solid foundation for all future work. Then the remaining 20% of the time can be spent on writing new features and functionality that drive the product forward.</description></item><item><title>In praise of long running code</title><link>/2017/03/12/in-praise-of-long-running-code/</link><pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/12/in-praise-of-long-running-code/</guid><description>There’s something spectacular in checking in on a project you worked on years ago and discovering it&amp;rsquo;s still running years later. This past Friday I got an HTTPS alert from Let’s Encrypt reminding me that my SSL certificate for https://yahnr.dangoldin.com was set to expire. I checked it out and remarkably it&amp;rsquo;s still up and running. I built that in March of 2013 as a proof of concept of what I termed a “pseudo-static site.</description></item><item><title>Automating admin work: Spreadsheets to Slack</title><link>/2017/03/04/automating-admin-work-spreadsheets-to-slack/</link><pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/04/automating-admin-work-spreadsheets-to-slack/</guid><description>Recently we adopted the concept of owning your own up time for our engineering teams. The goal is to encourage a stronger sense of ownership and actually give the teams the autonomy to approach their development and release process the way they’re comfortable with. Before this we relied on a single on call every week that would be responsible for monitoring all issues and escalating them to the appropriate team. One minor side effect of this change was that I now had to manage the on call calendar and post the new rotation on Slack every week.</description></item><item><title>Lessons learned from today's S3 failure</title><link>/2017/02/28/lessons-learned-from-todays-s3-failure/</link><pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/28/lessons-learned-from-todays-s3-failure/</guid><description>Today was quite a day. S3, the most resilient of Amazon’s services went down for a few hours in the US-EAST-1 zone and led to a series of failures across a variety of services. There are a ton of lessons one should take away from this - ranging from running across multiple availability zones to being integrated with a variety of cloud providers. The challenge is that it’s not easy; especially when you’re small.</description></item><item><title>Learning modern frontend development</title><link>/2017/02/01/learning-modern-frontend-development/</link><pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/01/learning-modern-frontend-development/</guid><description>Until a few weeks ago my frontend programming experience ended with jQuery so I decided to do something about it and start getting up to speed with modern frontend development. This ranged from starting to mess around with React, to using ES6, to integrating webpack and Babel in these projects. I’ve been using Sublime Text for the past 6 years but am switching to Visual Code Studio as my primary editor.</description></item><item><title>Year in review: 2016</title><link>/2017/01/02/year-in-review-2016/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/02/year-in-review-2016/</guid><description>A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data.</description></item><item><title>Automatically taking screenshots of HTML elements</title><link>/2016/12/13/automatically-taking-screenshots-of-html-elements/</link><pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/13/automatically-taking-screenshots-of-html-elements/</guid><description>I’ve worked on a variety of scraping projects that required spinning up a browser (via selenium) and having it browse a variety of pages unattended in order to capture some data. The two most recents ones scraping my account data from Turo and the fantasy football stats from Yahoo. These were relatively straightforward since the browser was used purely to navigate from page to page with the actual data capture done by parsing the underlying HTML.</description></item><item><title>Word clouds and text similarity</title><link>/2016/12/10/word-clouds-and-text-similarity/</link><pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/10/word-clouds-and-text-similarity/</guid><description>I’m a sucker for data visualizations so when I came across a simple word cloud-generating Python script I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year.</description></item><item><title>Efficiency vs expressiveness</title><link>/2016/12/06/efficiency-vs-expressiveness/</link><pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/06/efficiency-vs-expressiveness/</guid><description>The ideal code is both efficient and expressive but they’re often at odds with one another. Last week I was working on a simple script to parse and visualize a detailed AWS bill across a variety of dimensions and came across a clear example. The script loads a CSV file into a Pandas dataframe and adds a few columns based on the values of some others. The challenge is that the CSV file can be millions of rows so minor improvements can lead to significant efficiency gains.</description></item><item><title>Visualizing your AWS costs</title><link>/2016/11/27/visualizing-your-aws-costs/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/27/visualizing-your-aws-costs/</guid><description>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, Trusted Advisor, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month.</description></item><item><title>Recursive redirects with AWS Lambda</title><link>/2016/11/13/recursive-redirects-with-aws-lambda/</link><pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/13/recursive-redirects-with-aws-lambda/</guid><description>Two years ago I toyed around with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS Lambda binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda.</description></item><item><title>A poor man's data pipeline</title><link>/2016/11/12/a-poor-mans-data-pipeline/</link><pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/12/a-poor-mans-data-pipeline/</guid><description>Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.</description></item><item><title>Simple data visualizations from the command line</title><link>/2016/10/26/simple-data-visualizations-from-the-command-line/</link><pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/26/simple-data-visualizations-from-the-command-line/</guid><description>Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.</description></item><item><title>Revisiting my Twitter activity</title><link>/2016/10/19/revisiting-my-twitter-activity/</link><pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/19/revisiting-my-twitter-activity/</guid><description>While going through my old GitHub repos I discovered that the most starred repo was twitter-archive-analysis, a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working.</description></item><item><title>Shaming meeting room hogs</title><link>/2016/10/01/shaming-meeting-room-hogs/</link><pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/01/shaming-meeting-room-hogs/</guid><description>One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.</description></item><item><title>Supporting Disqus in AMP</title><link>/2016/09/13/supporting-disqus-in-amp/</link><pubDate>Tue, 13 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/13/supporting-disqus-in-amp/</guid><description>After migrating my blog to AMP the last task was getting Disqus working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos.</description></item><item><title>AMP migration scripts</title><link>/2016/09/08/amp-migration-scripts/</link><pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/08/amp-migration-scripts/</guid><description>Over Labor Day weekend I migrated my blog to use AMP but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &amp;lt;img&amp;gt; tag with &amp;lt;amp-img&amp;gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &amp;lt;img&amp;gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version.</description></item><item><title>Food identification with Google's Cloud Vision</title><link>/2016/08/29/food-identification-with-googles-cloud-vision/</link><pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/29/food-identification-with-googles-cloud-vision/</guid><description>Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last.</description></item><item><title>Writing scrapers as APIs</title><link>/2016/08/24/writing-scrapers-as-apis/</link><pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/24/writing-scrapers-as-apis/</guid><description>While building the Turo scraper I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API.</description></item><item><title>Downloading your Turo ride history</title><link>/2016/08/21/downloading-your-turo-ride-history/</link><pubDate>Sun, 21 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/21/downloading-your-turo-ride-history/</guid><description>I&amp;rsquo;ve been using Turo to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: scraping. Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.</description></item><item><title>Integrating poorly documented Open Source libraries</title><link>/2016/08/14/integrating-poorly-documented-open-source-libraries/</link><pubDate>Sun, 14 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/14/integrating-poorly-documented-open-source-libraries/</guid><description>Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected.</description></item><item><title>Fantasy football stats: 2016-2017 edition</title><link>/2016/08/13/fantasy-football-stats-2016-2017-edition/</link><pubDate>Sat, 13 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/13/fantasy-football-stats-2016-2017-edition/</guid><description>This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on GitHub. If all you care about is the raw data you can just download the CSV.</description></item><item><title>One of my favorite programs</title><link>/2016/08/06/one-of-my-favorite-programs/</link><pubDate>Sat, 06 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/06/one-of-my-favorite-programs/</guid><description>While working on a small programming puzzle I remembered Peter Norvig’s spell checker and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.</description></item><item><title>Coding puzzle: Word transformation through valid words</title><link>/2016/07/17/coding-puzzle-word-transformation-through-valid-words/</link><pubDate>Sun, 17 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/17/coding-puzzle-word-transformation-through-valid-words/</guid><description>A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.</description></item><item><title>Maximize the potential energy of your code</title><link>/2016/07/04/maximize-the-potential-energy-of-your-code/</link><pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/04/maximize-the-potential-energy-of-your-code/</guid><description>Potential energy: the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system. Dictionary.com Kinetic energy: the energy of a body or a system with respect to the motion of the body or of the particles in the system. Dictionary.com I’m constantly striving to discover new ways of thinking about code and my latest is thinking about it through what many of us learned in high school physics - potential and kinetic energy.</description></item><item><title>Analyzing my blog</title><link>/2016/06/12/analyzing-my-blog/</link><pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/12/analyzing-my-blog/</guid><description>I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple Python script that went through each post and dumped it into a CSV file with a series of columns that would be easy to analyze via R.</description></item><item><title>Word clouds in R</title><link>/2016/06/06/word-clouds-in-r/</link><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/06/word-clouds-in-r/</guid><description>Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a quick script I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.</description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>After getting the IMDB data loaded it was time to dive in and start looking at the data. In 2012, I did an analysis to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the help of CuriousGnu, I was able to get my hands on a much larger dataset.</description></item><item><title>Analyzing IMDB data: Step 1 - Cleaning and QA</title><link>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</guid><description>In 2012 I did a simple analysis of IMDB to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great post by CuriousGnu that did a similar analysis across a larger set of movies but limited to movies since 2000.</description></item><item><title>Identifying unused database tables</title><link>/2016/05/11/identifying-unused-database-tables/</link><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/11/identifying-unused-database-tables/</guid><description>When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.
Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them.</description></item><item><title>A Telegram blog bot</title><link>/2016/04/23/a-telegram-blog-bot/</link><pubDate>Sat, 23 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/23/a-telegram-blog-bot/</guid><description>A combination of bots being in vogue and Telegram offering $1M in bot prizes got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup.</description></item><item><title>Unintended consequences</title><link>/2016/04/10/unintended-consequences/</link><pubDate>Sun, 10 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/10/unintended-consequences/</guid><description>Earlier today I read an article about MaxMind, a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations.</description></item><item><title>Generalize at n=3</title><link>/2016/04/07/generalize-at-n3/</link><pubDate>Thu, 07 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/07/generalize-at-n3/</guid><description>Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.
If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios.</description></item><item><title>The MySQL enum type</title><link>/2016/03/10/the-mysql-enum-type/</link><pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/10/the-mysql-enum-type/</guid><description>The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected.</description></item><item><title>Cleanest way to read a CSV file with Python</title><link>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</link><pubDate>Sun, 10 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</guid><description>Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They&amp;rsquo;re related but not synonymous.
#!/usr/bin/python import csv from collections import namedtuple # Can add whatever columns you want to parse here # Can also generate this via the header (skipped in this example) Row = namedtuple(&amp;#39;Row&amp;#39;, (&amp;#39;ymd&amp;#39;, &amp;#39;state&amp;#39;, &amp;#39;size&amp;#39;, &amp;#39;count&amp;#39;)) with open(&amp;#39;file.</description></item><item><title>Jersey City garbage truck routes</title><link>/2015/12/12/jersey-city-garbage-truck-routes/</link><pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/12/jersey-city-garbage-truck-routes/</guid><description>A couple of months ago I took a stab at plotting the Jersey City parking zones after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome comment pointing out that Jersey City has a bunch of open data available, including a near-real time feed of garbage truck locations, a general open data portal, as well as the ability to request custom data.</description></item><item><title>Think interfaces, not implementation</title><link>/2015/12/02/think-interfaces-not-implementation/</link><pubDate>Wed, 02 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/02/think-interfaces-not-implementation/</guid><description>An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future.</description></item><item><title>More MySQL fun</title><link>/2015/11/21/more-mysql-fun/</link><pubDate>Sat, 21 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/21/more-mysql-fun/</guid><description>I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.
drop table if exists dan_test; create table dan_test ( id int not null, id2 int not null ); insert into dan_test (id, id2) values (1,1), (2,2), (3,3); select * from dan_test; select id, case when id = 1 then 2 else id end as id, id2 from dan_test; select id, sum(id2) from dan_test group by id; select case when id = 1 then 2 else id end as id, sum(id2) from dan_test group by id; select case when id = 1 then 2 else id end as new_id, sum(id2) from dan_test group by new_id; {% endhighlight sql %} With the second to last query it’s not obvious which id field the group by is referring to: the original from the table or the derived field?</description></item><item><title>My old projects</title><link>/2015/11/12/my-old-projects/</link><pubDate>Thu, 12 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/12/my-old-projects/</guid><description>Writing up my old projects got me browsing through my GitHub account to see what else I&amp;rsquo;ve worked on. Some I&amp;rsquo;ll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don&amp;rsquo;t require any setup or configuration to start using.</description></item><item><title>Jsonify.me 2.0</title><link>/2015/10/21/jsonify.me-2.0/</link><pubDate>Wed, 21 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/21/jsonify.me-2.0/</guid><description>A couple of weeks ago I wrote about the idea of having a “go to” project that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of jsonify.me. It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.</description></item><item><title>Dates in the shell</title><link>/2015/10/19/dates-in-the-shell/</link><pubDate>Mon, 19 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/19/dates-in-the-shell/</guid><description>The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.</description></item><item><title>Mapping the Jersey City parking zones II</title><link>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</link><pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</guid><description>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at https://dangoldin.github.io/jersey-city-open-data/ and includes the zones that had enough geocodeable addresses to generate a valid polygon.
As expected, most of the work was going from the PDF to a set of valid geocoded addresses.</description></item><item><title>Mapping the Jersey City parking zones</title><link>/2015/09/12/mapping-the-jersey-city-parking-zones/</link><pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/12/mapping-the-jersey-city-parking-zones/</guid><description>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there&amp;rsquo;s a PDF that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting.</description></item><item><title>Reprioritizing a non priority RabbitMQ queue</title><link>/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue/</link><pubDate>Wed, 12 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue/</guid><description>Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.</description></item><item><title>The Go interface</title><link>/2015/07/29/the-go-interface/</link><pubDate>Wed, 29 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/29/the-go-interface/</guid><description>I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:
interface Animal { public boolean isFurry(); public String speak(); } class Dog implements Animal { public boolean isFurry() { return true; } public String speak() { return &amp;#34;Woof&amp;#34;; } } public void aRandomFunction(Animal a) { .</description></item><item><title>Comparing SQL schemas</title><link>/2015/07/12/comparing-sql-schemas/</link><pubDate>Sun, 12 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/12/comparing-sql-schemas/</guid><description>During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn&amp;rsquo;t necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state.</description></item><item><title>Domain specific API definitions</title><link>/2015/06/23/domain-specific-api-definitions/</link><pubDate>Tue, 23 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/23/domain-specific-api-definitions/</guid><description>Yesterday, Amazon announced a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a standardized representation for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I&amp;rsquo;ve included a subset of the EC2 definition below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.</description></item><item><title>Date range generation</title><link>/2015/05/30/date-range-generation/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/30/date-range-generation/</guid><description>I finally had the chance to go back and add another quick tool to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges.</description></item><item><title>Adding columns in PostgreSQL and Redshift</title><link>/2015/04/23/adding-columns-in-postgresql-and-redshift/</link><pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/23/adding-columns-in-postgresql-and-redshift/</guid><description>A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.</description></item><item><title>Redshift meets Excel</title><link>/2015/04/06/redshift-meets-excel/</link><pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/06/redshift-meets-excel/</guid><description>As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it.</description></item><item><title>Getting the most out of log4j</title><link>/2015/02/28/getting-the-most-out-of-log4j/</link><pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/28/getting-the-most-out-of-log4j/</guid><description>Something that’s incredibly helpful when writing Java code is customizing log4j. There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.
One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.</description></item><item><title>Lists and localStorage</title><link>/2015/02/26/lists-and-localstorage/</link><pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/26/lists-and-localstorage/</guid><description>I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.</description></item><item><title>URL redirection app</title><link>/2015/02/07/url-redirection-app/</link><pubDate>Sat, 07 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/07/url-redirection-app/</guid><description>At TripleLift, we’re big fans of the Switcheroo plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.</description></item><item><title>Fun with GitHub's map tools</title><link>/2015/01/18/fun-with-githubs-map-tools/</link><pubDate>Sun, 18 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/18/fun-with-githubs-map-tools/</guid><description>After discovering GitHub&amp;rsquo;s map visualization feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short Python script later and I was able to convert the GPX data into geojson. The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize.</description></item><item><title>2014 stats</title><link>/2015/01/11/2014-stats/</link><pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/11/2014-stats/</guid><description>At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year.</description></item><item><title>Redirect recursion</title><link>/2014/12/31/redirect-recursion/</link><pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/31/redirect-recursion/</guid><description>I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.</description></item><item><title>Some simple AWS tools</title><link>/2014/11/09/some-simple-aws-tools/</link><pubDate>Sun, 09 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/09/some-simple-aws-tools/</guid><description>Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.</description></item><item><title>Top down vs bottom up coding</title><link>/2014/09/16/top-down-vs-bottom-up-coding/</link><pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/16/top-down-vs-bottom-up-coding/</guid><description>Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.
Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation.</description></item><item><title>Managing settings files in Django projects</title><link>/2014/08/30/managing-settings-files-in-django-projects/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/30/managing-settings-files-in-django-projects/</guid><description>I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.</description></item><item><title>Yahoo fantasy football stats - 2014 edition</title><link>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</link><pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</guid><description>This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and updated it to work for the 2014 season. The old version used the great Scrapy framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses Selenium to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.</description></item><item><title>Set up HTTPS on EC2 running Nginx without ELB</title><link>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</link><pubDate>Tue, 15 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</guid><description>I recently needed to set up HTTPS for my side project, better404.com. Amazon makes it easy to set up by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.</description></item><item><title>Retrieving a Twitter user's followers and followees</title><link>/2014/07/07/retrieving-a-twitter-users-followers-and-followees/</link><pubDate>Mon, 07 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/07/retrieving-a-twitter-users-followers-and-followees/</guid><description>After reading Gilad Lotan’s post where Gilad bought 4,000 Twitter followers in order to analyze them, a friend of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.</description></item><item><title>Debugging a reverting database update</title><link>/2014/06/07/debugging-a-reverting-database-update/</link><pubDate>Sat, 07 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/07/debugging-a-reverting-database-update/</guid><description>I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For Better404, a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site.</description></item><item><title>Generate fake SQL data using JavaScript</title><link>/2014/05/29/generate-fake-sql-data-using-javascript/</link><pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/29/generate-fake-sql-data-using-javascript/</guid><description>A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and hacked one together in JavaScript.</description></item><item><title>Examining ssh login requests</title><link>/2014/05/16/examining-ssh-login-requests/</link><pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/16/examining-ssh-login-requests/</guid><description>I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.
grep &amp;#34;Invalid user &amp;#34; /var/log/auth.</description></item><item><title>Most commonly used shell commands</title><link>/2014/05/12/most-commonly-used-shell-commands/</link><pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/12/most-commonly-used-shell-commands/</guid><description>I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.
history | cut -d&amp;#39; &amp;#39; -f4 | awk &amp;#39;{a[$0]++}END{for(i in a)print i,a[i]}&amp;#39; | sort -k 2 -n -r The script is simple - look through my command history, extract the first word, and count the number of times that word appears.</description></item><item><title>Gap fills and cross joins in Excel</title><link>/2014/05/03/gap-fills-and-cross-joins-in-excel/</link><pubDate>Sat, 03 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/03/gap-fills-and-cross-joins-in-excel/</guid><description>During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.
The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula.</description></item><item><title>Vertical integration and web development</title><link>/2014/04/16/vertical-integration-and-web-development/</link><pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/16/vertical-integration-and-web-development/</guid><description>Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.
Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure.</description></item><item><title>PostgreSQL Fibonacci</title><link>/2014/04/04/postgresql-fibonacci/</link><pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/04/postgresql-fibonacci/</guid><description>Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a short PostgreSQL query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.
with recursive f as ( select 0 as a, 1 as b union all select b as a, a+b from f where a &amp;lt; 100000 ) select a from f It’s interesting to see the edge features of a language and I find that query languages tend to have the most striking ones.</description></item><item><title>More Sierpinski fun</title><link>/2014/02/21/more-sierpinski-fun/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/21/more-sierpinski-fun/</guid><description>As a follow up to my previous post, I modified my Sierpinski generation code to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes.</description></item><item><title>Sierpinski triangle in D3</title><link>/2014/02/19/sierpinski-triangle-in-d3/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/19/sierpinski-triangle-in-d3/</guid><description>There&amp;rsquo;s a little known algorithm for constructing a Sierpinski triangle that is surprisingly easy to implement.
Start the three vertices that form a triangle Pick a random point inside the triangle Pick a random vertex Go halfway from a the random point to the vertex and mark that point Go to step 3 using the result of 4 as the starting point I&amp;rsquo;m trying to get better at D3 and thought it would be a good exercise to code it up.</description></item><item><title>Using virtualenv in production</title><link>/2014/02/10/using-virtualenv-in-production/</link><pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/10/using-virtualenv-in-production/</guid><description>One of my favorite things about Python is being able to use virtualenv to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.
The thing that&amp;rsquo;s tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services.</description></item><item><title>Visualizing GPS data in R</title><link>/2014/02/05/visualizing-gps-data-in-r/</link><pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/05/visualizing-gps-data-in-r/</guid><description>Earlier today I read Nathan Yau’s post that had a quick R script to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were.</description></item><item><title>Solving coding tests in PostgreSQL</title><link>/2014/01/25/solving-coding-tests-in-postgresql/</link><pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/25/solving-coding-tests-in-postgresql/</guid><description>Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:</description></item><item><title>Developing on a remote instance</title><link>/2014/01/23/developing-on-a-remote-instance/</link><pubDate>Thu, 23 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/23/developing-on-a-remote-instance/</guid><description>One of the first things I was given when joining TripleLift was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I&amp;rsquo;m a fan of this approach.</description></item><item><title>Taxi prices around the world</title><link>/2014/01/09/taxi-prices-around-the-world/</link><pubDate>Thu, 09 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/09/taxi-prices-around-the-world/</guid><description>I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on worldtaximeter.com and combining that with the data from taxiautofare.com and www.numbeo.com in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute.</description></item><item><title>Visualizing RunKeeper data in R</title><link>/2014/01/04/visualizing-runkeeper-data-in-r/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/04/visualizing-runkeeper-data-in-r/</guid><description>What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to my GitHub. If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.</description></item><item><title>Using the information_schema.columns table</title><link>/2013/12/15/using-the-information_schema.columns-table/</link><pubDate>Sun, 15 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/15/using-the-information_schema.columns-table/</guid><description>Something that’s been really helpful to me in understanding a MySQL database is the built in information_schema.columns table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in.</description></item><item><title>Drowning in JavaScript</title><link>/2013/12/01/drowning-in-javascript/</link><pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/01/drowning-in-javascript/</guid><description>I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.
To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries.</description></item><item><title>Genetic programming Connect 4</title><link>/2013/11/30/genetic-programming-connect-4/</link><pubDate>Sat, 30 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/30/genetic-programming-connect-4/</guid><description>Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a Connect 4 bot that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation.</description></item><item><title>RDS and R</title><link>/2013/11/15/rds-and-r/</link><pubDate>Fri, 15 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/15/rds-and-r/</guid><description>In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better.</description></item><item><title>Some JavaScript Tools</title><link>/2013/10/05/some-javascript-tools/</link><pubDate>Sat, 05 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/05/some-javascript-tools/</guid><description>Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.
To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &amp;lt;td&amp;gt; tag and then wrap each row in a &amp;lt;tr&amp;gt;tag.</description></item><item><title>Extract info from a web page using JavaScript</title><link>/2013/08/26/extract-info-from-a-web-page-using-javascript/</link><pubDate>Mon, 26 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/26/extract-info-from-a-web-page-using-javascript/</guid><description>How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.
In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend.</description></item><item><title>Run Django under Nginx, Virtualenv and Supervisor</title><link>/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/</link><pubDate>Tue, 30 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/</guid><description>After yet another attempt to deploy a Django application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.
In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.</description></item><item><title>Scraping Yahoo fantasy football stats with Scrapy</title><link>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</link><pubDate>Wed, 17 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</guid><description>Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the Scrapy framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.</description></item><item><title>Fun with Prolog: Priceonomics Puzzle</title><link>/2013/06/07/fun-with-prolog-priceonomics-puzzle/</link><pubDate>Fri, 07 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/07/fun-with-prolog-priceonomics-puzzle/</guid><description>The Priceonomics blog is one of my favorites so when I saw that they had a programming puzzle up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.
Prolog is pretty different than the mainstream programming languages, it belongs to the logic programming language category and relies on defining a variety of relations and then querying these relationships to get results.</description></item><item><title>Citibike Directions: Second Attempt</title><link>/2013/06/04/citibike-directions-second-attempt/</link><pubDate>Tue, 04 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/04/citibike-directions-second-attempt/</guid><description>To coincide with the launch of Citibike, I wrote a simple web app that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I updated the app to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination.</description></item><item><title>Citibike Station to Station Directions</title><link>/2013/06/01/citibike-station-to-station-directions/</link><pubDate>Sat, 01 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/01/citibike-station-to-station-directions/</guid><description>Photo by @rafat
On Wednesday, I took my first bike ride using New York City&amp;rsquo;s new Citibike program. So far it&amp;rsquo;s been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn&amp;rsquo;t take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn&amp;rsquo;t make it easy to find directions from one station to another unless you&amp;rsquo;re already at one of them.</description></item><item><title>Adding attachments to django-postman</title><link>/2013/05/17/adding-attachments-to-django-postman/</link><pubDate>Fri, 17 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/17/adding-attachments-to-django-postman/</guid><description>After doing a round of customer development for Makers Alley, we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using django-postman to handle the user to user communication. Within a few days, we quickly discovered that text messages weren&amp;rsquo;t enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like.</description></item><item><title>Eighteen Months of Django: Part 2</title><link>/2013/05/10/eighteen-months-of-django-part-2/</link><pubDate>Fri, 10 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/10/eighteen-months-of-django-part-2/</guid><description>On Tuesday, I shared some best practices I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.
django-registration and django-social-auth: Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter.</description></item><item><title>Eighteen months of Django</title><link>/2013/05/07/eighteen-months-of-django/</link><pubDate>Tue, 07 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/07/eighteen-months-of-django/</guid><description>I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning.</description></item><item><title>Raspbmc</title><link>/2013/04/10/raspbmc/</link><pubDate>Wed, 10 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/10/raspbmc/</guid><description>I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks Simon!) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.</description></item><item><title>Mmmm... pseudo static sites</title><link>/2013/03/12/mmmm...-pseudo-static-sites/</link><pubDate>Tue, 12 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/12/mmmm...-pseudo-static-sites/</guid><description>Reading Katie Zhu’s post on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:
Cost: S3 is cheaper than EC2. Reliable: S3 doesn’t go down near as frequently as EC2. Scalable: Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.</description></item><item><title>Identifying duplicate bills across states</title><link>/2013/02/05/identifying-duplicate-bills-across-states/</link><pubDate>Tue, 05 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/05/identifying-duplicate-bills-across-states/</guid><description>This past weekend I participated in the Bicoastal Datafest hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn&amp;rsquo;t be very easy, especially in two days, but I did meet Llewellyn, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language.</description></item><item><title>Making sense of my Twitter archive</title><link>/2013/01/19/making-sense-of-my-twitter-archive/</link><pubDate>Sat, 19 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/19/making-sense-of-my-twitter-archive/</guid><description>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with matplotlib. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one &amp;ldquo;gotcha&amp;rdquo; was needing to convert the timestamp to my local time.</description></item><item><title>Web scraping like a pro</title><link>/2013/01/09/web-scraping-like-a-pro/</link><pubDate>Wed, 09 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/09/web-scraping-like-a-pro/</guid><description>I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.
Hope they’re useful!</description></item><item><title>Self hosted Instagram export</title><link>/2012/12/19/self-hosted-instagram-export/</link><pubDate>Wed, 19 Dec 2012 00:00:00 +0000</pubDate><guid>/2012/12/19/self-hosted-instagram-export/</guid><description>I just hacked together a quick app to help download Instagram photos. At first, I tried using Instaport and OpenPhoto but both of them were backed up with others trying to do the same so I decided to create my own. It&amp;rsquo;s basically a really simple python web app that allows you do a quick authentication with Instagram and then lets you downloads all your images to your hard drive.</description></item><item><title>Trend of actor vs actress age differences</title><link>/2012/05/23/trend-of-actor-vs-actress-age-differences/</link><pubDate>Wed, 23 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/23/trend-of-actor-vs-actress-age-differences/</guid><description>I recently watched Miss Representation which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data.</description></item><item><title>Tech interview question</title><link>/2011/01/08/tech-interview-question/</link><pubDate>Sat, 08 Jan 2011 00:00:00 +0000</pubDate><guid>/2011/01/08/tech-interview-question/</guid><description>When conducting interviews, I&amp;rsquo;ve developed the following criteria for a good interview problem:
Avoid brain teasers - they tend to be hit/miss and some people don't really do well under this type of problem Challenging - the answer should not be immediately obvious and the should require some creativity Rare - similar to above, the problem should not be a common question in order to get Flexible - the problem has multiple solutions and can be modified on the fly for different skill levels I&amp;rsquo;ve found that the following problem satisfies the criteria and gives a pretty good sense of a developer&amp;rsquo;s skill level.</description></item><item><title>Enabling modules in Apache2 under Ubuntu</title><link>/2009/01/23/enabling-modules-in-apache2-under-ubuntu/</link><pubDate>Fri, 23 Jan 2009 00:00:00 +0000</pubDate><guid>/2009/01/23/enabling-modules-in-apache2-under-ubuntu/</guid><description>The Apache enabled modules are found in "/etc/apache2/mods-enabled" as a set of .load and .conf files. If the modules you want are in the /etc/apache2/mods-available folder but not in "/etc/apache2/mods-enabled" folder, just copy the .load and .conf files over (note that the .conf file may not exist).
If there is no file in the mods-availble folder, you will need to create a new .load file in the mods-available folder to point to a module in "</description></item><item><title>Interesting Perl behavior</title><link>/2008/05/30/interesting-perl-behavior/</link><pubDate>Fri, 30 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/30/interesting-perl-behavior/</guid><description>I ran into this problem a while back and wanted to share it. It was a bit unintuitive but documentd so I guess I shouldn't be surprised by the results. Hopefully this will help someone else avoid this pitfall.
It looks as if declaring a variable with the "my" statement but then guarded with an "if" statement causes the scope of the variable to be global - note that the "</description></item></channel></rss>