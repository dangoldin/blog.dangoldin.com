<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Datascience on Dan Goldin</title><link>/tags/datascience/</link><description>Recent content in Datascience on Dan Goldin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 18 Nov 2016 00:00:00 +0000</lastBuildDate><atom:link href="/tags/datascience/index.xml" rel="self" type="application/rss+xml"/><item><title>Comparing the web requests made by the top sites: 2014 vs 2016</title><link>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</link><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</guid><description>&lt;p>A &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">few years ago&lt;/a> I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they&amp;rsquo;re making.&lt;/p></description></item><item><title>Approachable data science</title><link>/2014/06/02/approachable-data-science/</link><pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/02/approachable-data-science/</guid><description>&lt;p>Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn&amp;rsquo;t have to be this way. The goal of data science is simply to unlock insights and value from data. There&amp;rsquo;s no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.&lt;/p></description></item><item><title>Fun with the Oyster books API</title><link>/2014/03/16/fun-with-the-oyster-books-api/</link><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/16/fun-with-the-oyster-books-api/</guid><description>&lt;p>I’m an avid reader and signed up for &lt;a href="http://oysterbooks.com/" target="_blank">Oyster&lt;/a> as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.&lt;/p>
&lt;p>One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.&lt;/p></description></item><item><title>Website load times: NYC vs Beijing</title><link>/2014/03/11/website-load-times-nyc-vs-beijing/</link><pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/11/website-load-times-nyc-vs-beijing/</guid><description>&lt;p>Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">compare them&lt;/a> against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from &lt;a href="https://github.com/rahimnathwani" target="_blank">rahimnathwani&lt;/a> who ran the script in Beijing, using home ADSL, and wanted to share his dataset.&lt;/p>
&lt;p>I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.&lt;/p></description></item><item><title>Examining the requests made by the top 100 sites</title><link>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</link><pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</guid><description>&lt;p>Since writing the &lt;a href="http://dangoldin.com/2013/12/01/drowning-in-javascript/">Drowning in JavaScript&lt;/a> post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.&lt;/p></description></item><item><title>Heuristic vs algorithmic approaches</title><link>/2014/02/15/heuristic-vs-algorithmic-approaches/</link><pubDate>Sat, 15 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/15/heuristic-vs-algorithmic-approaches/</guid><description>&lt;p>Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.&lt;/p></description></item><item><title>Taxi pricing in NYC vs Mumbai</title><link>/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/</link><pubDate>Sun, 29 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/</guid><description>&lt;p>Something else that struck me during my trip to India was the difference in taxi fare between &lt;a href="http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml" target="_blank">New York City&lt;/a> and &lt;a href="http://www.taxiautofare.com/taxi-fare-card/Mumbai-Taxi-fare" target="_blank">Mumbai&lt;/a>. I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.&lt;/p></description></item><item><title>Is Excel on a Mac intentionally hobbled?</title><link>/2013/11/09/is-excel-on-a-mac-intentionally-hobbled/</link><pubDate>Sat, 09 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/09/is-excel-on-a-mac-intentionally-hobbled/</guid><description>&lt;p>The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.&lt;/p></description></item><item><title>In defense of Excel</title><link>/2013/09/20/in-defense-of-excel/</link><pubDate>Fri, 20 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/20/in-defense-of-excel/</guid><description>&lt;div class="right10">
 &lt;img src="/image/excel-logo-2013.png" alt="Excel 2013 logo" data-width="256" data-height="256" data-layout="responsive" />
&lt;/div>
&lt;p>Excel has developed a reputation of being bloated, slow, error prone and used primarily by &amp;ldquo;business people&amp;rdquo; who don&amp;rsquo;t have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.&lt;/p>
&lt;p>The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they&amp;rsquo;re still able to run some neat analyses and derive useful insights.&lt;/p></description></item><item><title>Programming and math</title><link>/2013/09/17/programming-and-math/</link><pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/17/programming-and-math/</guid><description>&lt;p>The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.&lt;/p></description></item><item><title>Scraping Yahoo fantasy football stats with Scrapy</title><link>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</link><pubDate>Wed, 17 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</guid><description>&lt;p>Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the &lt;a href="http://scrapy.org/" target="_blank">Scrapy&lt;/a> framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.&lt;/p></description></item><item><title>An analysis of Lincoln's words</title><link>/2013/02/12/an-analysis-of-lincolns-words/</link><pubDate>Tue, 12 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/12/an-analysis-of-lincolns-words/</guid><description>&lt;p>On Saturday, I finished &lt;a href="http://www.amazon.com/Team-Rivals-Political-Abraham-Lincoln/dp/0743270754">Team of Rivals&lt;/a> and while looking at my calendar noticed that it was also &lt;a href="http://en.wikipedia.org/wiki/Abraham_Lincoln">Lincoln&amp;rsquo;s&lt;/a> birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the &lt;a href="http://www.gutenberg.org/files/3253/3253-h/3253-h.htm">7 volume set&lt;/a> containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.&lt;/p></description></item><item><title>Identifying duplicate bills across states</title><link>/2013/02/05/identifying-duplicate-bills-across-states/</link><pubDate>Tue, 05 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/05/identifying-duplicate-bills-across-states/</guid><description>&lt;p>This past weekend I participated in the &lt;a href="http://www.bdatafest.computationalreporting.com/">Bicoastal Datafest&lt;/a> hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn&amp;rsquo;t be very easy, especially in two days, but I did meet &lt;a href="https://twitter.com/llewellynhinkes">Llewellyn&lt;/a>, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.&lt;/p></description></item><item><title>Making sense of my Twitter archive</title><link>/2013/01/19/making-sense-of-my-twitter-archive/</link><pubDate>Sat, 19 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/19/making-sense-of-my-twitter-archive/</guid><description>&lt;p>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with &lt;a href="http://matplotlib.org/">matplotlib&lt;/a>. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one &amp;ldquo;gotcha&amp;rdquo; was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I&amp;rsquo;m more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I&amp;rsquo;d like to explore how my usage of Twitter has evolved and also get to play around with the &lt;a href="http://nltk.org/">NLTK library&lt;/a>.&lt;/p></description></item><item><title>Trend of actor vs actress age differences</title><link>/2012/05/23/trend-of-actor-vs-actress-age-differences/</link><pubDate>Wed, 23 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/23/trend-of-actor-vs-actress-age-differences/</guid><description>&lt;p>I recently watched &lt;a href="http://www.missrepresentation.org/" title="Miss Representation" target="_blank">Miss Representation&lt;/a> which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.&lt;br/>&lt;br/>I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via &lt;a href="http://www.imdb.com/chart/1910s">&lt;span>&lt;a href="http://www.imdb.com/chart/1910s">&lt;a href="http://www.imdb.com/chart/1910s">http://www.imdb.com/chart/1910s&lt;/a>&lt;/a>&lt;/span>&lt;/a> - &lt;a href="http://www.imdb.com/chart/2010s)">&lt;span>&lt;a href="http://www.imdb.com/chart/2010s">&lt;a href="http://www.imdb.com/chart/2010s">http://www.imdb.com/chart/2010s&lt;/a>&lt;/a>)&lt;/span>&lt;/a> and then pulling the top 5 cast members for each movie (via &lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast)">&lt;span>&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">http://www.imdb.com/title/tt1375666/fullcredits#cast&lt;/a>&lt;/a>)&lt;/span>&lt;/a>. I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE" target="_blank">Google Spreadsheets&lt;/a> to clean it up. &lt;br/>&lt;br/>Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.&lt;/p></description></item><item><title>What's the easiest way to be elected president?</title><link>/2009/01/21/whats-the-easiest-way-to-be-elected-president/</link><pubDate>Wed, 21 Jan 2009 00:00:00 +0000</pubDate><guid>/2009/01/21/whats-the-easiest-way-to-be-elected-president/</guid><description>&lt;p>Answer: Be elected for a first term, the second term will follow.&lt;/p>
&lt;p>It turns out it&amp;rsquo;s pretty likely that a president will be elected to a second term. If we examine all previous Presidential Elections, we will see 8 presidents who failed to get reelected:&lt;/p>
&lt;table class="table">
&lt;thead>
&lt;tr>
 &lt;th>President&lt;/th>
 &lt;th>Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Benjamin Harrison&lt;/td>
&lt;td>Failed to get reelected in 1892&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>George H. W. Bush&lt;/td>
&lt;td>Failed to get reelected in 1992&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Herbert Hoover&lt;/td>
&lt;td>Failed to get reelected in 1932&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Jimmy Carter&lt;/td>
&lt;td>Failed to get reelected in 1980&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>John Quincy Adams&lt;/td>
&lt;td>Failed to get reelected in 1828&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Theodore Roosevelt&lt;/td>
&lt;td>Failed to get reelected in 1912&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>William Henry Harrison&lt;/td>
&lt;td>Failed to get elected in 1836&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>William Howard Taft&lt;/td>
&lt;td>Failed to get reelected in 1912&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;p>On the other hand, if we look at all presidents with 2 or more terms, we only see a few Presidents who have failed to get elected. Some of these, like Andrew Jackson, failed to get elected initially but were then able to get 2 terms in office. Grover Cleveland had non consecutive terms in office. In total, there were 16 presidents who had a second term.&lt;/p></description></item><item><title>On weekend voting</title><link>/2008/10/24/on-weekend-voting/</link><pubDate>Fri, 24 Oct 2008 00:00:00 +0000</pubDate><guid>/2008/10/24/on-weekend-voting/</guid><description>&lt;p>I found an &lt;a href="http://www.nytimes.com/2008/10/24/opinion/24ornstein.html?ref=opinion">op-ed&lt;/a> in the NY Times that claimed that the best way to increase voter turnout was by having election day fall on a weekend. They provide a few examples but nothing too detailed. I tried pulling in some data and seeing if I could come to the same conclusion. I used two data sets: &lt;a href="http://en.wikipedia.org/wiki/Voter_turnout ">voter turn out by country&lt;/a> and &lt;a href="http://www.electionguide.org/calendar.php ">election dates by country&lt;/a>.&lt;/p></description></item></channel></rss>