---
layout: post
title: "A new Turing test"
description: "Instead of focusing on making an AI act as a human it might be interesting to try a form of the Turing test where a human acts as an AI."
keywords: "turing test, ai"
image_url:
category:
tags: ["#meta"]
---
{% include JB/setup %}
A friend sent me <a href="http://www.bloombergview.com/articles/2014-06-09/fake-victory-for-artificial-intelligence" target="_blank">an article</a> where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.

Regardless of whether modern AIs can beat the Turing test I think it’s inevitable that an AI will conclusively beat the Turing test in the coming years. A more interesting question is whether a human can trick another human into thinking he or she is an AI. It’s similar to the Turing test in that it’s supposed to make the AI and human indistinguishable to a judge but instead of making the AI smarter we’re dumbing down the human.

The nice thing about this approach is that historically it was very easy for a human to act as an AI by making dumb mistakes and responding with non-sequiturs. I suspect it’s currently quite difficult to respond in a way that would convince someone you’re an AI, even after enough time speaking with one, and I’d love to see this attempted. In the end both of these approaches converge to the same goal of making AIs and humans indistinguishable and this is just another way of looking at it.