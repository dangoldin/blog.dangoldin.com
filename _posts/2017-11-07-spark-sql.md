---
layout: post
title: "Spark SQL"
description: ""
keywords:
image_url:
category:
tags: []
---
{% include setup %}
Yesterday I spent a bit of time investigating a job that shot up in time from just over an hour to not being anywhere close to done after 4 and needing to be killed.

Digging into it I learned a little bit about the way Spark reads SQL tables via the JDBC driver.

Two queries
- SELECT * FROM table WHERE 1=0;
- SELECT a, b, c from table;

This would work great in practice but we had some creative code that instead of a table would insert a subquery (SELECT * FROM table) table. As you can imagine this caused quite a hit to performance.

Queries now complete within seconds whereas before some would run for over half an hour.

Wasteful when you have a large data processing system like Spark waiting on a little bit of data from SQL.
