<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dan Goldin</title><link>/</link><description>Recent content on Dan Goldin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 29 Aug 2021 15:53:06 -0500</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Yahoo fantasy football stats: 2021-2022 edition</title><link>/2021/08/29/yahoo-fantasy-football-stats-2021-2022-edition/</link><pubDate>Sun, 29 Aug 2021 15:53:06 -0500</pubDate><guid>/2021/08/29/yahoo-fantasy-football-stats-2021-2022-edition/</guid><description>&lt;p>I had a bit of a blogging break this year but the pull of tradition is too strong. For the sixth year in a row I updated my Yahoo fantasy football stats scraper to pull Yahoo&amp;rsquo;s projected fantasy stats for the upcoming season. The only change this year was the addition of a &amp;ldquo;O Rank&amp;rdquo; column which was simple to implement so I have no excuse for pushing this off till the last minute. Hopefully this shames me into writing more.&lt;/p></description></item><item><title>Instacart price markup</title><link>/2021/02/15/instacart-price-markup/</link><pubDate>Mon, 15 Feb 2021 15:53:06 -0500</pubDate><guid>/2021/02/15/instacart-price-markup/</guid><description>&lt;p>Last year I &lt;a href="/2019/10/12/online-food-delivery-markups-seamless-vs-ubereats-vs-doordash/">looked at the markup&lt;/a> of a few different food delivery services by comparing their prices against the ones on the restaurant&amp;rsquo;s menu. I got the chance to do the same thing for Instacart a few days ago when the receipt was included alongside the delivery.&lt;/p>
&lt;p>The markup wasn&amp;rsquo;t as significant or variable as the food delivery apps and was just over 9% for the majority of items. I imagine the intention was to keep it single digit in case anyone did the math and 10% does seem like a reasonable markup - even with the additional tip.&lt;/p></description></item><item><title>Slack Idea: Hierarchical Channels</title><link>/2021/02/14/slack-idea-hierarchical-channels/</link><pubDate>Sun, 14 Feb 2021 15:32:14 -0500</pubDate><guid>/2021/02/14/slack-idea-hierarchical-channels/</guid><description>&lt;p>While there are a lot of strong opinions about Slack on both sides, it clearly changed the way work gets done. I&amp;rsquo;m mixed. I appreciate the ability to quickly drop a question or response asynchronous yet acknowledge how disruptive it is and prevents deep work.&lt;/p>
&lt;p>The one thing that I did notice in our usage of Slack is that quite often the same message ends up being posted across multiple channels due to relevance for multiple groups. Yet many members exist in both groups and the repetition is redundant and inefficient. What if Slack had a way to have hierarchical channels?&lt;/p></description></item><item><title>Clubhouse as a solution to Deepfakes</title><link>/2021/01/31/clubhouse-as-a-solution-to-deepfakes/</link><pubDate>Sun, 31 Jan 2021 23:37:18 -0500</pubDate><guid>/2021/01/31/clubhouse-as-a-solution-to-deepfakes/</guid><description>&lt;p>My mind started wandering while half-listening to a Clubhouse discussion and I had the thought that Clubhouse - and generally any real time communication app - can end up being a solution to Deepfakes. At some point Deepfakes will be good enough to render in real time but until then having synchronous communication is a way to confirm the authenticity of the participants. On top of that I&amp;rsquo;m sure you can find some blockchain way to authenticate the session to prove that it was done live and was unaltered.&lt;/p></description></item><item><title>Pre-internet businesses were tough</title><link>/2021/01/23/pre-internet-businesses-were-tough/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>/2021/01/23/pre-internet-businesses-were-tough/</guid><description>&lt;p>One of my favorite podcasts is NPR&amp;rsquo;s How I Built This that conducts interviews with entrepreneurs across a variety of industries, not just tech. A recent episode was an &lt;a href="https://www.npr.org/2021/01/15/957229311/jazzercise-judi-sheppard-missett">interview with Judi Sheppard Misset&lt;/a>, the creator of &lt;a href="https://www.jazzercise.com/">Jazzercise&lt;/a> - a dance fitness class that she launched in 1969. We all realize that there&amp;rsquo;s much less friction to launch a business these days but the interview makes you see how significant the changes have been.&lt;/p></description></item><item><title>Billion dollar features</title><link>/2021/01/22/billion-dollar-features/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>/2021/01/22/billion-dollar-features/</guid><description>&lt;p>There&amp;rsquo;s an &lt;a href="https://www.businessinsider.com/drew-houston-dropbox-steve-jobs-2017-6">old story&lt;/a> of Steve Jobs meeting Drew Houston in order to acquire Dropbox but after being refused calling Dropbox a feature. Dropbox kept innovating, building a great product, went public and is currently a $9 billion company. Clearly Dropbox proved Steve Jobs wrong. But did it really? Dropbox recently announced a round of layoffs and their stock has dropped 20% since they went public. Even on a personal note I plan on moving my files from Dropbox to Google Drive.&lt;/p></description></item><item><title>From AMP on Jekyll to Hugo and GitHub Actions</title><link>/2021/01/08/from-amp-on-jekyll-to-hugo-and-github-actions/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>/2021/01/08/from-amp-on-jekyll-to-hugo-and-github-actions/</guid><description>&lt;p>This the first post written on my new blogging setup so what better topic than the new blog setup? Years ago I redesigned my entire site to use AMP across all devices, powered by Jekyll, and hosted on GitHub pages. At the end of 2020 I decided I was ready to abandon AMP and move toward a more traditional and straightforward setup while leveraging a more modern toolchain.&lt;/p>
&lt;p>Due to my adoption of AMP my Jekyll/GitHub Pages setup was highly custom. I had to build it locally and push the rendered version to GitHub since I had custom ruby scripts handling a lot of the AMP conversions. In addition, I had to resort to a variety of workarounds to get Disqus working within AMP. Also, since AMP inlines all the CSS the build took almost 8 seconds despite being highly optimized and running a pretty powerful computer.&lt;/p></description></item><item><title>Everyone's a hustler</title><link>/2020/12/30/everyones-a-hustler/</link><pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/30/everyones-a-hustler/</guid><description>&lt;p>It&amp;rsquo;s the holiday season and mine has been filled with a lot of &lt;a href="https://en.wikipedia.org/wiki/Bing_Crosby">Bing Crosby&lt;/a>. I&amp;rsquo;m one of those people that will Wikipedia anything new and came across this sentence that describes his popularity:&lt;/p>
&lt;p>&amp;ldquo;Also in 1948, Music Digest estimated that his recordings filled more than half of the 80,000 weekly hours allocated to recorded radio music&amp;rdquo;&lt;/p>
&lt;p>This reminded me of another celebrity from that era, &lt;a href="https://en.wikipedia.org/wiki/Charles_Lindbergh">Charles Lindbergh&lt;/a>, and his popularity:&lt;/p></description></item><item><title>Science vs policy</title><link>/2020/12/30/science-vs-policy/</link><pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/30/science-vs-policy/</guid><description>&lt;p>In what seems to be a part of the modern American experience there&amp;rsquo;s &lt;a href="https://www.wsj.com/amp/articles/new-u-s-dietary-guidelines-reject-recommendation-to-cut-sugar-alcohol-intake-11609254000">another case&lt;/a> of a scientific group proposing some changes and a regulatory agency ignoring the suggestions. In this case, a group of scientists proposed further limiting sugar and alcohol but the USDA didn&amp;rsquo;t include these in their official guidance. There may be some truth to the USDA&amp;rsquo;s claim that there wasn&amp;rsquo;t enough evidence to justify a change but I can&amp;rsquo;t help but think this is another example of strong private interest groups sacrificing the public good for themselves.&lt;/p></description></item><item><title>2021 goals</title><link>/2020/12/29/2021-goals/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/2021-goals/</guid><description>&lt;p>Despite 2020 showing us how unpredictable the world can be, it&amp;rsquo;s that wonderful time of the year when everyone comes up with goals for the upcoming year. On one hand it&amp;rsquo;s impossible to predict what will happen but on the other hand it is useful to give some sense of purpose and direction. Being thoughtful and public around our goals is also a way to encourage us to commit and see them through completion. I &lt;a href="/2020/12/21/2020-goal-review/">didn&amp;rsquo;t do so well&lt;/a> with my 2020 goals but even the process of generating them was useful to help me understand where I am and what I&amp;rsquo;m interested in. For 2021 I&amp;rsquo;m adopting a similar approach but rather than get too specific I want to think more thematically and use 2021 to recover from the bad habits introduced in 2020.&lt;/p></description></item><item><title>Challenges in identifying a spawned OS X process</title><link>/2020/12/29/challenges-in-identifying-a-spawned-os-x-process/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/challenges-in-identifying-a-spawned-os-x-process/</guid><description>&lt;p>As part of my project to come up with a utility to deduplicate images on S3 I started working on a proof of concept to validate the flow and functionality. I know that the AWS S3 API has everything I need but I wanted to see whether I could use the built in OS tools to handle the image comparison without resorting to a webapp. The steps I had in mind were pretty simple:&lt;/p></description></item><item><title>Improving the accuracy of evergreen content</title><link>/2020/12/29/improving-the-accuracy-of-evergreen-content/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/improving-the-accuracy-of-evergreen-content/</guid><description>&lt;p>&lt;img src="/image/google-analytics-evergreen-posts.png" alt="Google Analytics highlighting my evergreen posts">&lt;/p>
&lt;p>While looking through my blog&amp;rsquo;s analytics I noticed that I have a few posts that have views despite being written years ago. Some of these are still relevant while others are woefully out of date. For example, the difference between yyyy and YYYY in Java&amp;rsquo;s SimpleDateFormat is still relevant but how to scrape web pages has changed significantly since 2013 and anything that has pricing or performance information is likely wrong.&lt;/p></description></item><item><title>Time to remove AMP from my blog</title><link>/2020/12/29/time-to-remove-amp-from-my-blog/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/29/time-to-remove-amp-from-my-blog/</guid><description>&lt;p>A few years ago I redid this entire blog to use AMP - not just for mobile but also for desktop. The motivation was to improve the performance while scratching my curiosity itch. It was a fun experiment but these days I&amp;rsquo;m more averse to Google&amp;rsquo;s dominance and think I can do even better by taking what I liked from AMP but going even simpler. It took quite a bit of work to get everything working the way I wanted to in AMP - for example converting AMP markup into a valid RSS feed and getting Disqus working - and while it&amp;rsquo;ll be a shame to undo all that work it feels like the right time. The current deployment is custom Jekyll on top of GitHub Pages but I&amp;rsquo;m hoping to simplify the entire workflow and take advantage of some of the modern tools - for example GitHub Actions.&lt;/p></description></item><item><title>Blogging: 2020 and beyond</title><link>/2020/12/28/blogging-2020-and-beyond/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/28/blogging-2020-and-beyond/</guid><description>&lt;p>I have a &lt;a href="https://github.com/dangoldin/blog-analytics/">script&lt;/a> I use to analyze and visualize my blogging. One script generates a simple CSV file containing a variety of metadata about my blogging that I then feed into a Grafana dashboard to see how I&amp;rsquo;m doing. Another script does some simple visualizations. In the past years I had an annual tradition of sharing these visualizations and describing each one but I took a look at this year&amp;rsquo;s and for the most part there&amp;rsquo;s nothing that interesting.&lt;/p></description></item><item><title>Increase reliability through loose coupling: An adtech example</title><link>/2020/12/28/increase-reliability-through-loose-coupling-an-adtech-example/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/28/increase-reliability-through-loose-coupling-an-adtech-example/</guid><description>&lt;p>A few weeks ago I &lt;a href="/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/">evangelized degrading functionality&lt;/a> in the case of system outages as an option versus building out full regional resiliency. Someone asked me to dig into my example more so I&amp;rsquo;ll share a bit of how one of our core systems at TripleLift is set up.&lt;/p>
&lt;p>We run an advertising exchange handing tens of billions of ad requests each day. AdTech has a ton of different companies with each doing a bit of everything but a simple way to think of us is an &amp;ldquo;exchange&amp;rdquo; that sits in between buyers and sellers of ads and ad space. In this case, a seller would be a publisher, such as the NY Times that has ad opportunities available. The buy side would be a Demand Side Platform (DSP) that has an API integration and submits bids on behalf of advertisers.&lt;/p></description></item><item><title>SpaceX Starship vs Saturn V</title><link>/2020/12/27/spacex-starship-vs-saturn-v/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/27/spacex-starship-vs-saturn-v/</guid><description>&lt;p>As a kid I used to be obsessed with all things space. A big part was having the same name as the head of NASA, Daniel Goldin, where I harbored illusions that they wouldn&amp;rsquo;t even have to change the nameplate on the door. My parents supported this interest and got me subscriptions to a variety of space and science magazines. I did not go the space route but despite that I&amp;rsquo;m sure that passion and curiosity contributed to my growth and I still have a soft spot for space.&lt;/p></description></item><item><title>2020 donations</title><link>/2020/12/26/2020-donations/</link><pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/26/2020-donations/</guid><description>&lt;p>Last year I wrote a &lt;a href="/2019/12/17/2019-donations/">short post&lt;/a> highlighting my donations with the hope that they encourage others to donate. Given this year&amp;rsquo;s trials and tribulations it&amp;rsquo;s even more important to donate and help others if you have the means.&lt;/p>
&lt;p>I continued my donations from last year and donated to the EFF, ACLU, Wikipedia, and Planned Parenthood for the &lt;a href="/2019/12/17/2019-donations/">same reasons&lt;/a> as last year. The additions this year have been a function of everything that&amp;rsquo;s happened. Earlier in the I made multiple donations to various BLM charities and most recently contributed to &lt;a href="https://fairfight.com/">Fair Fight&lt;/a> to help with the Georgia election.&lt;/p></description></item><item><title>Tools of 2020</title><link>/2020/12/26/tools-of-2020/</link><pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/26/tools-of-2020/</guid><description>&lt;p>Despite not being in the code too much these days I still spend a significant amount of time on the computer so thought it would be fun to share my commonly used tools. Maybe there&amp;rsquo;s something here you&amp;rsquo;re not familiar with and it might be enough to encourage you to give it a shot.&lt;/p>
&lt;h3 id="hardware">Hardware&lt;/h3>
&lt;p>For the first half of the year I used Ubuntu on a ThinkPad X1 but switched back to a Macbook (Early 2019 Pro) due to a variety of stability issues. The keyboard isn&amp;rsquo;t as nice but I&amp;rsquo;m overall more productive and glad to have the Microsoft Office suite back.&lt;/p></description></item><item><title>My Tesla stock trade</title><link>/2020/12/25/my-tesla-stock-trade/</link><pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/25/my-tesla-stock-trade/</guid><description>&lt;p>&lt;img src="/image/tesla-stock.png" alt="Tesla Stock Chart">&lt;/p>
&lt;p>The above is Tesla&amp;rsquo;s stock price since its IPO in 2010. On its first day it closed at $3.84 and was relatively flat until earlier this year at which point it shot up closing at $661.77 on December 24. If you invested $1,000 at the IPO that would be worth over $172,000. I&amp;rsquo;m one of those unlucky fools that purchased Tesla early - in 2011 - but then sold it in 2015. My $1,148.20 investment resulted in a pre-tax profit of $7,691.26. A nice gain until you consider what could have been.&lt;/p></description></item><item><title>Incident response</title><link>/2020/12/24/incident-response/</link><pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/24/incident-response/</guid><description>&lt;p>In honor of everyone on-call during the holidays I thought I&amp;rsquo;d share my evolving thoughts around incident response. Every company has something here and as much as we can strive for perfection it&amp;rsquo;s impossible and we&amp;rsquo;ll inevitably have incidents that need handling.&lt;/p>
&lt;p>I&amp;rsquo;m still thinking through the exact way to approach and summarize my thoughts but I find it useful to start with goals and first principles. The ultimate goals are to resolve the issue as quickly as possible and prevent them and other similar issues from recurring. In terms of axioms, we want to align incentives and make those able to solve the problem own the problem. In addition, we need to embrace that we work on highly dynamic systems - both the systems and people change. And finally, we need to consider incident response as one of the many responsibilities engineers have.&lt;/p></description></item><item><title>An ounce of prevention is worth a pound of cure</title><link>/2020/12/23/an-ounce-of-prevention-is-worth-a-pound-of-cure/</link><pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/23/an-ounce-of-prevention-is-worth-a-pound-of-cure/</guid><description>&lt;p>As we get into the last week of the year many software teams are in a code freeze - avoiding any deployments that may cause an incident and cause a dreaded holiday page. We have a tendency to reward the firefighter for pulling an all-nighter fighting an outage but don&amp;rsquo;t value those that avoid the fire in the first place. It&amp;rsquo;s natural to throw accolades at the firefighter since it&amp;rsquo;s very easy to see the impact in their work; it&amp;rsquo;s more difficult to see the value in the person that prevents issues from even happening. The counterfactual is difficult to prove so we diminish its significance but that&amp;rsquo;s where long term value is actually built.&lt;/p></description></item><item><title>Browser zero</title><link>/2020/12/22/browser-zero/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/22/browser-zero/</guid><description>&lt;p>Inbox Zero is a productivity philosophy that&amp;rsquo;s centered around getting to an empty email inbox. That means you treat your inbox as a todo list and once you&amp;rsquo;ve handled every email you&amp;rsquo;re good to go. I&amp;rsquo;m somewhat successful dealing with my inbox but my browser tabs are another story. Browser Zero is a corollary to Inbox Zero. What good is getting to an empty inbox if you still have dozens of browser tabs open awaiting your attention?&lt;/p></description></item><item><title>2020 Goal review</title><link>/2020/12/21/2020-goal-review/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/21/2020-goal-review/</guid><description>&lt;p>At the beginning of the year I gave myself &lt;a href="http://dangoldin.com/2020/01/28/2020-goals/">4 goals for 2020&lt;/a> and now that the year is wrapping up it&amp;rsquo;s time to see how I did: not well. The year was a tough one for most people but all things considered I&amp;rsquo;m in a privileged position and can&amp;rsquo;t blame COVID too much. For sure there were periods of ennui but the root was that I just lost interest and motivation in some of these. So how did I do?&lt;/p></description></item><item><title>Stop charging for support plans</title><link>/2020/12/21/stop-charging-for-support-plans/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/21/stop-charging-for-support-plans/</guid><description>&lt;p>My latest rant is against support plans for enterprise software. It&amp;rsquo;s frustrating to be spending hundreds of thousands of dollars on a contract and yet be forced to pay an additional fee for support. Even worse, this additional fee is usually a function of your existing spend.&lt;/p>
&lt;p>I understand the rationale - there needs to be a cost to support otherwise everyone has an incentive to escalate and spend is likely correlated with complexity and volume of services used but it just feels as if I&amp;rsquo;m being nickel and dimed.&lt;/p></description></item><item><title>Blame the algorithm</title><link>/2020/12/19/blame-the-algorithm/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/19/blame-the-algorithm/</guid><description>&lt;p>Stanford Medicine is in the &lt;a href="https://www.npr.org/sections/coronavirus-live-updates/2020/12/18/948176807/stanford-apologizes-after-vaccine-allocation-leaves-out-nearly-all-medical-resid">news&lt;/a> for using an &amp;ldquo;algorithm&amp;rdquo; and allocating minimal COVID vaccines to those actually on the frontlines. Instead it supposedly prioritized senior administrators who are working from home. I&amp;rsquo;m sure there&amp;rsquo;s some truth to the algorithm biasing allocation to those who are older but whenever I see someone blame an algorithm my spidey-sense goes off. The algorithm in this case was likely some extremely simple calculation in Excel that could have been easily verified by a human - both before it was run as well as a quick analysis of the results.&lt;/p></description></item><item><title>Removing duplicate files in S3</title><link>/2020/12/18/removing-duplicate-files-in-s3/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/18/removing-duplicate-files-in-s3/</guid><description>&lt;p>I&amp;rsquo;m a digital hoarder and whenever I had to switch computers, I was always worried about losing files. These days it&amp;rsquo;s both lower risk since so much is scattered across the cloud but with the ascent of AWS I&amp;rsquo;ve resorted to just backing up my computers onto S3.&lt;/p>
&lt;p>I simply do a recursive copy of my home folder to S3 and call it a day. One problem this exposes is that there are duplicate files scattered all over the place. For example I&amp;rsquo;d have something both in my Downloads folder as well as in a Photos and maybe even a Dropbox folder. Or I would just have the same file duplicated in the same directory. At the end of the day it&amp;rsquo;s not a huge deal but at the same time it feels dirty so I started working on a script to identify these duplicates.&lt;/p></description></item><item><title>What do adtech and BGP have in common?</title><link>/2020/12/17/what-do-adtech-and-bgp-have-in-common/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/17/what-do-adtech-and-bgp-have-in-common/</guid><description>&lt;p>Despite writing web applications many engineers are not familiar with the infrastructure side of the internet - &lt;a href="https://www.cloudflare.com/learning/dns/what-is-dns/">DNS&lt;/a>, &lt;a href="https://www.cloudflare.com/learning/security/glossary/what-is-bgp/">BGP&lt;/a>, TCP/IP - and yet depend on it all working. Over the years I&amp;rsquo;ve gotten a better understanding of the guts of the internet but only in the past few weeks have I discovered &lt;a href="https://en.wikipedia.org/wiki/Resource_Public_Key_Infrastructure">Resource Public Key Infrastructure&lt;/a> (RPKI) on top of BGP. RPKI is meant to add a layer of trust on top of the existing infrastructure which was designed many decades ago without worrying about malicious actors.&lt;/p></description></item><item><title>Substack can evolve into a marketplace</title><link>/2020/12/15/substack-can-evolve-into-a-marketplace/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/15/substack-can-evolve-into-a-marketplace/</guid><description>&lt;p>When I first heard of Substack I wasn&amp;rsquo;t too impressed. I fell into the usual engineering trick of thinking it&amp;rsquo;s something that could be &amp;ldquo;built in a weekend&amp;rdquo; but there&amp;rsquo;s a lot going on behind the scenes. Last week I listened to an &lt;a href="https://www.theverge.com/22159571/substack-ceo-chris-best-interview-newsletter-subscription-model-journalism-decoder-podcast">interview with Substack&amp;rsquo;s CEO&lt;/a>, Chris Best, which made me realize the scale and scope of Substack&amp;rsquo;s ambitions.&lt;/p>
&lt;p>At the most fundamental level it&amp;rsquo;s a product that allows writers to publish a newsletter - taking care of all the mechanics to let writers write. Yet I naively assumed this was just a text editor (think Medium), coupled with payments (Stripe), all hooked up to email (Sendgrid).&lt;/p></description></item><item><title>Do your users delete or archive your email?</title><link>/2020/12/14/do-your-users-delete-or-archive-your-email/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/14/do-your-users-delete-or-archive-your-email/</guid><description>&lt;p>If you&amp;rsquo;ve been following this blog you likely know that I subscribe to the Inbox Zero approach to email management. I treat my inbox as a todo list and strive to get it down to zero with varying success. In any case, I spend a lot of time in my email and I noticed an odd pattern to how I deal with handled messages. Some I will delete while I others I&amp;rsquo;ll archive but it happens naturally. It&amp;rsquo;s not entirely subconscious but it&amp;rsquo;s a very quick decision I make as I run through my messages. The obvious rule is that anything I expect to search or reference later on I&amp;rsquo;ll archive while the truly useless emails I&amp;rsquo;ll simply delete.&lt;/p></description></item><item><title>Impact of getting to the top of Hacker News in 2020</title><link>/2020/12/13/impact-of-getting-to-the-top-of-hacker-news-in-2020/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/13/impact-of-getting-to-the-top-of-hacker-news-in-2020/</guid><description>&lt;p>Two days ago I wrote up a quick &lt;a href="/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/">post&lt;/a> highlighting how Amazon owns more than $2B domains and it struck a nerve. It&amp;rsquo;s been shared quite a bit on Twitter and I posted it to Hacker News on Friday night before going to bed to wake up to it being near the top which drove a ton of views. It&amp;rsquo;s always a surprise seeing what takes off but there&amp;rsquo;s something about Amazon owning billions of dollars in domains that&amp;rsquo;s shocking and gets people riled up. My goal is not to write in order to get my posts to go viral but it&amp;rsquo;s empowering knowing that thousands of people are reading something you&amp;rsquo;ve written.&lt;/p></description></item><item><title>Evolution of open source licenses</title><link>/2020/12/12/evolution-of-open-source-licenses/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/12/evolution-of-open-source-licenses/</guid><description>&lt;p>Open source changed the way we write code. It&amp;rsquo;s given many of us a starting set of tools that allowed us to quickly put together new products and allowed us to specialize. These days open source is as strong as ever but there are a lot of changes happening on the licensing side. Under the older open source licenses, any company could take your code and then launch it as their own service. This wasn&amp;rsquo;t a problem in the past due to a highly fragmented market and trusting the developer to provide the gold-standard enterprise service but the rise of the cloud providers changed the dynamic. They have the scale to both build the expertise but also to lock-in customers.&lt;/p></description></item><item><title>Amazon owns more than $2B worth of IPV4 addresses</title><link>/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/</guid><description>&lt;p>While listening to a &lt;a href="https://softwareengineeringdaily.com/2020/12/02/bgp-with-andree-toonk/">podcast discussing BGP&lt;/a> I heard the fact that AWS owns more than $2B worth of IP addresses. I knew AWS was massive but this came as a big shock so I decided to do some digging around. I came across a &lt;a href="https://ipv4marketgroup.com/ipv4-pricing/">site&lt;/a> that listed the market prices of IP addresses and the range looks to be anywhere from $20 to $30 per IP depending on the block size. Now it was time to figure out the IP addresses owned by Amazon. I figured this would be difficult but lucky for us AWS actually &lt;a href="https://ip-ranges.amazonaws.com/ip-ranges.json">publishes&lt;/a> their entire set of IP addresses as JSON.&lt;/p></description></item><item><title>Pay to enjoy video games</title><link>/2020/12/08/pay-to-enjoy-video-games/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/08/pay-to-enjoy-video-games/</guid><description>&lt;p>As a kid I was incredibly addicted to video games. I had a whole cat-and-mouse routine going on with my parents. They would put a BIOS boot password on the computer and I would open up the case, remove the battery, and reset the BIOS settings. They would lock the basement, where our one and only computer was located, and I would climb in through the window. They would lock the door to the computer room and I would undo the door hinges. It was difficult stopping me but remarkably I outgrew them and am now a somewhat well-adjusted member of society.&lt;/p></description></item><item><title>Degrade functionality instead of building cross region availablity</title><link>/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>/2020/12/02/degrade-functionality-instead-of-building-cross-region-availablity/</guid><description>&lt;p>AWS had a large multi-hour Kinesis &lt;a href="https://aws.amazon.com/message/11201/">outage&lt;/a> last Wednesday that affected a variety of dependent services, including Cloudwatch, Lambda, ECS, and EKS. These systems are complicated and highlight the scale and complexity of modern cloud computing.&lt;/p>
&lt;p>It&amp;rsquo;s impossible to be perfect but it&amp;rsquo;s a reminder for all of us to think through our applications and identify their failure cases. Just because there was a major AWS failure that affected your service does not mean you should drop everything and convert your application to be highly-available with regional failover. Instead try to model the probability of a particular failure, its impact, and the effort of a fix. With the above you should be able to identify some easy wins that get you fairly far.&lt;/p></description></item><item><title>Static code analysis with DeepSource</title><link>/2020/11/26/static-code-analysis-with-deepsource/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/26/static-code-analysis-with-deepsource/</guid><description>&lt;p>While listening to the &lt;a href="https://softwareengineeringdaily.com/">Software Engineering Daily&lt;/a> podcast I came across an &lt;a href="https://softwareengineeringdaily.com/2020/11/09/deepsource-static-analysis-for-code-reviews-with-jai-pradeesh-and-sanket-saurav/">interview&lt;/a> with Jai Pradeesh and Sanket Saurav who are the founders of &lt;a href="https://deepsource.io/">DeepSource&lt;/a>, a modern code analysis tool.&lt;/p>
&lt;p>I&amp;rsquo;m a sucker for these types of tools and willing to try anything that&amp;rsquo;s low friction and promises to me more productive so I gave it a shot on two of my open source repos - &lt;a href="https://github.com/dangoldin/health-stats/">health-stats&lt;/a> and &lt;a href="https://github.com/dangoldin/blog-analytics/">blog-analytics&lt;/a>.&lt;/p></description></item><item><title>Bringing a user centric approach to the command line</title><link>/2020/11/25/bringing-a-user-centric-approach-to-the-command-line/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/25/bringing-a-user-centric-approach-to-the-command-line/</guid><description>&lt;p>I have a &lt;a href="https://github.com/dangoldin/health-stats/blob/master/get_health.py">small script&lt;/a> that takes an export of Apple Health data and then dumps it into a MySQL database that I then use to visualize my health metrics over time. My prior workflow was to open the app and then Airdrop the file over to my computer at which point I&amp;rsquo;d unzip it, copy the relevant file over into the script directory, and simply run the script.&lt;/p></description></item><item><title>Subconciously learning to type Russian</title><link>/2020/11/22/subconciously-learning-to-type-russian/</link><pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/22/subconciously-learning-to-type-russian/</guid><description>&lt;p>I was born in the Soviet Union, came over as a 5 year old, and have seen my Russian getting worse and worse due to a lack of exposure. I&amp;rsquo;ve been using quarantine as a way to practice and improve my Russian through daily lessons in the Duolingo app. It&amp;rsquo;s a well designed app and while it has some flaws I&amp;rsquo;m on a 191 day streak and am loath to give it up. One thing that&amp;rsquo;s surprised me was how I&amp;rsquo;ve subconsciously picked up the Russian keyboard layout. I started having to hunt for each letter but these days have a subconscious feel for where the letters are. I actually do a better job typing without any thought than trying to figure out what letter comes next and where it is.&lt;/p></description></item><item><title>Cursed by GDPR</title><link>/2020/11/20/cursed-by-gdpr/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/20/cursed-by-gdpr/</guid><description>&lt;p>I had a fun little experience over the past few days. Someone had ordered Domino&amp;rsquo;s in the UK and seemingly typed in my email address by accident. This gave me the privilege of receiving daily emails from Domino&amp;rsquo;s promoting their latest deal. The first few days it was novel and interesting seeing what was popular across the pond but this morning I decided I might as well unsubscribe.&lt;/p>
&lt;img src="/image/dominos-pre-vpn.png" alt="Domino's marketing page without VPN" data-width="1297" data-height="185" data-layout="responsive" />
&lt;p>Lo and behold the unsubscribe link didn&amp;rsquo;t work. Rather than a web page I got a friendly server error indicating &amp;ldquo;Access Denied&amp;rdquo; and that I did not have permission to access the unsubscribe page. My suspicion that this was a result of Domino&amp;rsquo;s blocking access to the site turned out to be true since I was able to unsubscribe using a VPN.&lt;/p></description></item><item><title>Medium is a shopping mall</title><link>/2020/11/14/medium-is-a-shopping-mall/</link><pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/14/medium-is-a-shopping-mall/</guid><description>&lt;p>I empathize with Medium&amp;rsquo;s need to make money while avoiding ads but as an author it&amp;rsquo;s a bad deal. Medium has great editing and writing tools and a built in audience but if your goal is to share your articles with as broad of an audience as possible the fact that Medium throws up a paywall is a non-starter. Sure you can avoid it by going incognito but how many readers know that?&lt;/p></description></item><item><title>Poor Intel</title><link>/2020/11/11/poor-intel/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>/2020/11/11/poor-intel/</guid><description>&lt;p>Earlier this week Apple announced the new MacBook Air, featuring the Apple-built M1 chip, and heralding the return to their own chips. I wrote the &lt;a href="/2008/04/25/on-apple-buying-pa-semiconductor/">first post&lt;/a> on this blog in 2008 covering Apple acquiring PA Semi and claimed that it made sense to design their own chips for phones but not PCs. Well it&amp;rsquo;s been 12 years and Apple has figured out how to make incredible chips for their phones so might as well expand to laptops as well.&lt;/p></description></item><item><title>Google and the Apple Watch</title><link>/2020/10/27/google-and-the-apple-watch/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>/2020/10/27/google-and-the-apple-watch/</guid><description>&lt;p>If you were to summarize my personal tech stack it would be Google software and Apple hardware. There are a few exceptions but those two companies likely cover the bulk of my technology usage. They&amp;rsquo;re great at what they do within their domains yet the intersection is lacking, especially on the Apple Watch. I suspect Apple is to blame for some of this but it doesn&amp;rsquo;t seem as if Google doesn&amp;rsquo;t even try to build for the Apple Watch.&lt;/p></description></item><item><title>Gasoline</title><link>/2020/10/21/gasoline/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>/2020/10/21/gasoline/</guid><description>&lt;p>I&amp;rsquo;m a believer in climate change and know we need to change our habits and yet every time I drive I&amp;rsquo;m reminded of how amazing gasoline is. A gallon of a liquid is not much - we can all drink a gallon of water a day and yet a gallon of gasoline is enough to move a mutli-ton car more than 20 miles. It&amp;rsquo;s so normal that we don&amp;rsquo;t appreciate how amazing that actually is.&lt;/p></description></item><item><title>Substack and paid newsletters</title><link>/2020/10/21/substack-and-paid-newsletters/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>/2020/10/21/substack-and-paid-newsletters/</guid><description>&lt;p>It&amp;rsquo;s starting to feel as if everyone I follow on Twitter is launching a Substack newsletter. I don&amp;rsquo;t know how this plays out but my inbox is already overwhelmed and having more longish-form content to go through just won&amp;rsquo;t happen. It&amp;rsquo;s an interesting space that&amp;rsquo;s changing quickly and will look very different in a few years. Many seem to think that there&amp;rsquo;s a huge opportunity here for authors to build a niche audience that&amp;rsquo;s able to sustain their writing but i&amp;rsquo;m not wholly convinced. If you&amp;rsquo;re able to get the best of the best does it make sense to go for the second tier? Some people may want that additional depth but the majority will be happy with the highly skilled writer covering a broad topic. These will be the small number of authors who amass huge audiences and capture the majority of the revenue while the rest will be focused on niche topics with enough of an audience to keep them going. Inevitably, these &amp;ldquo;tail&amp;rdquo; writers will band together and offer their newsletters as a bundle. What&amp;rsquo;s old is new and we will see the rebirth of newspapers and magazines.&lt;/p></description></item><item><title>Cold turkey and fighting temptations</title><link>/2020/09/22/cold-turkey-and-fighting-temptations/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>/2020/09/22/cold-turkey-and-fighting-temptations/</guid><description>&lt;p>Some people are able to control urges but I&amp;rsquo;m much better off just removing the objects of desire entirely. In high school this manifested in me spending way too much time on video games. These days it&amp;rsquo;s buying a bag of potato chips and then being unable to stop once I crack it up. I tried giving TikTok a go to see what all the fuss was about but had to drop it cold turkey after realizing that I was spending too much time on it.&lt;/p></description></item><item><title>Get Google Group membership using Calendar</title><link>/2020/09/05/get-google-group-membership-using-calendar/</link><pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate><guid>/2020/09/05/get-google-group-membership-using-calendar/</guid><description>&lt;p>I&amp;rsquo;m not sure where I discovered this tip but I&amp;rsquo;ll often want to know the members of a Google group. The ways to get this are constantly changing - from double clicking the group within the Gmail compose window for a summary modal to going deep into the Groups UI. Somewhere in between is the group view provided by Google Calendar. I&amp;rsquo;m often in there and it turns out to be a pretty quick way of getting the list of all the users in a group without having to have anything open in another tab. One of the best parts is that it gives you the ability to expand and dedup group memberships which is great for dealing with nested groups - such as sub teams.&lt;/p></description></item><item><title>iOS 14 and local network access</title><link>/2020/09/03/ios-14-and-local-network-access/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>/2020/09/03/ios-14-and-local-network-access/</guid><description>&lt;p>Last week I installed the iOS 14 beta since I&amp;rsquo;m curious about what&amp;rsquo;s coming - even if it ends up bricking my phone. I expected it to come with more privacy features but one that surprised me was an alert that apps wanted to &amp;ldquo;find and connect to devices on your local network.&amp;rdquo; This is an obvious one Apple covered this functionality during &lt;a href="https://developer.apple.com/videos/play/wwdc2020/10110/">WWDC&lt;/a>. It&amp;rsquo;s interesting seeing which apps request this permission and speculate why.&lt;/p></description></item><item><title>Working your way up the engineering management stack</title><link>/2020/08/28/working-your-way-up-the-engineering-management-stack/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/28/working-your-way-up-the-engineering-management-stack/</guid><description>&lt;p>During a engineering management peer group discussion this morning we were talking about how involved each of us are in our team&amp;rsquo;s activities - for example how often do we look at code? Do we leave comments on pull requests? How involved are we in spec reviews? And a variety of questions of that ilk.&lt;/p>
&lt;p>At that point in time I wasn&amp;rsquo;t clever enough to come up with a framework but it&amp;rsquo;s very much about working your way up further from the code as you work your way up the management track.&lt;/p></description></item><item><title>Back to Mac</title><link>/2020/08/24/back-to-mac/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/24/back-to-mac/</guid><description>&lt;p>After almost 2 years of Ubuntu I&amp;rsquo;ve moved to a Mac. There were a variety of reasons for switching back but the ultimate reason was simply that I had stability issues on Ubuntu and it was rare for it to go a week without freezing up. The overall lack of polish in Ubuntu didn&amp;rsquo;t help but I could have dealt with it had it not be for the frequent freezes and crashes.&lt;/p></description></item><item><title>An era of productivity</title><link>/2020/08/21/an-era-of-productivity/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/21/an-era-of-productivity/</guid><description>&lt;p>A trap software engineers usually fall into is underestimating how long some things will take and I&amp;rsquo;m no different. Yet lately I&amp;rsquo;ve somehow been on the other end. Nearly every task I&amp;rsquo;ve had to do the past few weeks - either technical, managerial, or personal - has ended up taking a shorter amount of time than I expected. The blocker has been getting into the mindset and finding the time to do it but once I start I crank it out.&lt;/p></description></item><item><title>Apple and Mr. Burns: Indestructible</title><link>/2020/08/20/apple-and-mr.-burns-indestructible/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/20/apple-and-mr.-burns-indestructible/</guid><description>&lt;p>Apple hit a $2T market cap and yet I can&amp;rsquo;t help but think they&amp;rsquo;re in an incredibly precarious situation due to the relationship between the US and China. I find the Apple/US/China dynamic much more interesting than what&amp;rsquo;s going on with TikTok. Right now it feels as if every side wants Apple to succeed. For the US it&amp;rsquo;s a success story and a cultural export to the world. To China it&amp;rsquo;s an indicator that they can power the manufacturing, at scale, of one of the most sophisticated technical products in the world and a massive boon to their economy. Yet it only takes a bit of politics to completely disrupt that balance.&lt;/p></description></item><item><title>Yards, balls, fences, and software engineering</title><link>/2020/08/20/yards-balls-fences-and-software-engineering/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/20/yards-balls-fences-and-software-engineering/</guid><description>&lt;p>One of my strongest beliefs is that the best engineers are able to find ways to unblock themselves. Similarly, a pet peeve is when people say &amp;ldquo;that&amp;rsquo;s not my job&amp;rdquo; and expect a problem to be fixed by someone else. Sure this may occasionally happen but it&amp;rsquo;s important to have the attitude that you&amp;rsquo;re willing to get your hands dirty in order to solve your problem and move on.&lt;/p>
&lt;p>An analogy I used to represent this concept is to imagine having a house with a yard that&amp;rsquo;s next to a neighbor. Let&amp;rsquo;s say you&amp;rsquo;re throwing a ball around and it lands in your neighbor&amp;rsquo;s yard. Do you quickly hop over, grab it, and then come back? Or do you go to their front door, ring the bell, and then have them retrieve it for you? If you have a good relationship with your neighbor you&amp;rsquo;d clearly just go and get the ball.&lt;/p></description></item><item><title>The sandpaper learning method</title><link>/2020/08/18/the-sandpaper-learning-method/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/18/the-sandpaper-learning-method/</guid><description>&lt;p>When gluing two pieces of wood it&amp;rsquo;s useful to use sandpaper to improve the effectiveness of the glue. Learning follows the same pattern: you want to struggle a bit and get yourself ready so the knowledge sticks.&lt;/p>
&lt;p>Imagine you have two people who each encounter a problem they don&amp;rsquo;t know how to solve. The first immediately goes and asks for help from a peer and quickly get it resolved. The second person tries to solve it themselves, does research, tries a few different approaches, struggles throughout and then ends up getting help from a peer. The first person gets to the solution quicker but it&amp;rsquo;s the second person who actually learns. The struggle for the second person made it easier for them to both understand and retain the resolution. It&amp;rsquo;s the second person that will have that faster rate of learning and will be able to apply their knowledge more broadly than the first.&lt;/p></description></item><item><title>Yahoo fantasy football stats: 2020-2021 edition</title><link>/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/18/yahoo-fantasy-football-stats-2020-2021-edition/</guid><description>&lt;p>For the fifth year in a row I&amp;rsquo;ve updated my script to fetch the projected stats for the upcoming fantasy football season. These days I&amp;rsquo;m torn on football as a whole - both due to its politics and dangers - and don&amp;rsquo;t plan on watching too many games. Yet I enjoy the competition with my friends and the rote work of updating my scraping script to work every year.&lt;/p>
&lt;p>This time around there haven&amp;rsquo;t been too many changes: Yahoo changed the order of a few columns and introduced some minor stylistic changes but the code only needed a &lt;a href="https://github.com/dangoldin/yahoo-ffl/commit/764420d899ce26dae773470b9323875d702c9b41">few changes&lt;/a> to work - much simpler than last year which required running with an &lt;a href="https://github.com/dangoldin/yahoo-ffl/commit/82f1f14e84663d447cdb9db0b5738de4db64fe8c">adblocking extension to bypass a script blocker&lt;/a>.&lt;/p></description></item><item><title>Coupling in action: protobuf enum fields</title><link>/2020/08/17/coupling-in-action-protobuf-enum-fields/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/17/coupling-in-action-protobuf-enum-fields/</guid><description>&lt;p>The next few blog posts are all topics that have come up recently in my 1-1s and serve as good examples for how I think about code and software engineering. The first post is a discussion around coupling and the tradeoffs we considered.&lt;/p>
&lt;p>Our pipeline was covered in depth on the &lt;a href="http://highscalability.com/blog/2020/6/15/how-triplelift-built-an-adtech-data-pipeline-processing-bill.html">high scalability blog&lt;/a> but the important part for this post is our event collection piece which consists of the following components:&lt;/p></description></item><item><title>Big money, not big data</title><link>/2020/08/16/big-money-not-big-data/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/16/big-money-not-big-data/</guid><description>&lt;p>It&amp;rsquo;s common for companies to complain about the challenges of running big data and how difficult it is. The reality is that unless you&amp;rsquo;re running at massive scale (Google or Facebook) your problems are more to do with big money rather than big data. It&amp;rsquo;s expensive to store, process, and expose terabytes and the difficulty is in doing it cost effectively, not in simply doing it. There are enough modern tools out there across all the cloud providers (AWS, GCP, Azure) and vendors (Snowflake, Databricks) that it&amp;rsquo;s possible to do nearly everything you want but you&amp;rsquo;ll just have to pay for it.&lt;/p></description></item><item><title>Traveling at the speed of light through life</title><link>/2020/08/16/traveling-at-the-speed-of-light-through-life/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/16/traveling-at-the-speed-of-light-through-life/</guid><description>&lt;p>Years ago I was reading &lt;a href="https://www.amazon.com/Elegant-Universe-Superstrings-Dimensions-Ultimate/dp/039333810X">The Elegant Universe&lt;/a> and came across a mind-blowing idea - both then and every time I remember it: we&amp;rsquo;re always traveling at the speed of light through four dimensions - the three spatial dimensions as well as time. It just happens that almost all of that speed is through time. But that does mean that as you speed up in the spatial dimensions, time slows down - which has been shown to be true by &lt;a href="https://en.wikipedia.org/wiki/Hafele%E2%80%93Keating_experiment">measuring time drift&lt;/a> in airplanes.&lt;/p></description></item><item><title>Three types of advice from a manager</title><link>/2020/08/09/three-types-of-advice-from-a-manager/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/09/three-types-of-advice-from-a-manager/</guid><description>&lt;p>During one of my first meetings with a former manager he gave me an introduction to his working style. One of these was how he gave three types of advice:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>A passing thought&lt;/strong>. This is not something that was thought of too deeply and just popped into my mind. I&amp;rsquo;m sharing it in case it&amp;rsquo;s helpful but it&amp;rsquo;s just as likely to be useful.&lt;/li>
&lt;li>&lt;strong>A suggestion but trust you&lt;/strong>. I&amp;rsquo;ve run into similar scenarios in the past and learned from those experiences. This is one of those. At the same time I trust you to make the right decision since you understand the current problem and constraints better and know the modern tools. And if it turns out that the approach doesn&amp;rsquo;t work it would act as a learning opportunity.&lt;/li>
&lt;li>&lt;strong>Just do it&lt;/strong>. This is an order. While I will do my best to explain my reasoning I am your manager and don&amp;rsquo;t need to justify it.&lt;/li>
&lt;/ul>
&lt;p>The expectation is that the first two should make up at least 90% of the advice (and ideally more) since they&amp;rsquo;re the ones that actually instill ownership and empower the individual to make decisions. It&amp;rsquo;s also a failure of a manager if you end up in a situation where you have to override your team that often. It may be an indicator that you did not provide enough context and that your team is not seeing the full picture. It may also be that the team just doesn&amp;rsquo;t have the skill or desire to operate at the level you need them to. In any case the fact that you need to resort to this last level often is a symptom of a problem that you need to root out.&lt;/p></description></item><item><title>TikTok: A Prisoner's Dilemma</title><link>/2020/08/09/tiktok-a-prisoners-dilemma/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>/2020/08/09/tiktok-a-prisoners-dilemma/</guid><description>&lt;p>Everyone has an opinion here and I’m late but joining the fray. I fall into the ban TikTok app. I would not be doing it the way it&amp;rsquo;s currently being done but there&amp;rsquo;s a legitimate case to make that the US should adopt a &lt;a href="https://www.investopedia.com/terms/t/tit-for-tat.asp">tit for tat strategy&lt;/a> with China. China prevents many US companies from competing there so why should we allow it?&lt;/p>
&lt;p>There are of course arguments around us being the United States and needing to take the high road. This was a worthwhile belief decades ago when we had the belief that a richer China would be a more democratic and fair China but in fact the opposite has happened - China has gotten more and more emboldened and now has leverage and power. Why let it get any stronger?&lt;/p></description></item><item><title>Products change, humans stay the same</title><link>/2020/07/30/products-change-humans-stay-the-same/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/30/products-change-humans-stay-the-same/</guid><description>&lt;p>While on a quick driving errand a few days ago I decided to listen to the radio for the first time in years. I was quickly hit with a bit of nostalgia - the radio show that was on had one of those &amp;ldquo;First caller to answer this correctly gets Y&amp;rdquo; sessions. I&amp;rsquo;ve heard this thousands of times growing up but this was the first time in a very long time and for whatever reason I immediately thought of both how modern music streaming services no longer have these contests and that a similar need is being met by HQ Trivia.&lt;/p></description></item><item><title>Analyzing the AWS EC2 reservation options</title><link>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/27/analyzing-the-aws-ec2-reservation-options/</guid><description>&lt;p>While writing the post on AWS reservations I started thinking if there&amp;rsquo;s any arbitrage opportunity in the reservations. For example - does it make sense to do a 1 year or 3 year reservation for some instance types of upgrade an instance class to get a better reservation value?&lt;/p>
&lt;p>You can do this manually using ec2instances.info which provides a quick way to look at EC2 pricing info although forces you to pick the type of reservations you&amp;rsquo;re interested in. To speed things up I just ran the &lt;a href="https://github.com/powdahound/ec2instances.info">ec2instances.info script&lt;/a> to scrape the data into a JSON with a &lt;a href="https://github.com/dangoldin/analyze-ec2instance.info">quick script&lt;/a> to extract the relevant details into a CSV file. After that, it was as simple as loading into &lt;a href="https://docs.google.com/spreadsheets/d/1h5H3vsZluk1_TGGMGrZq-KFiha-Bfhu4OBzyp5QynA8/edit#gid=0">Google sheets&lt;/a> for some analysis and visualization.&lt;/p></description></item><item><title>Amusing Russian translations in Duolingo</title><link>/2020/07/24/amusing-russian-translations-in-duolingo/</link><pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/24/amusing-russian-translations-in-duolingo/</guid><description>&lt;p>Many are using the lockdown as a way to improve themselves. I grew up a native Russian speaker which has unfortunately eroded over the years and I&amp;rsquo;ve spent the past few months improving my Russian through Duolingo. It&amp;rsquo;s a surprisingly effective app and has enough gamification to keep me going. My approach has been to just take the tests to skip out of lessons until I master every lesson. It started off easy but I&amp;rsquo;m now failing to advance about 20% of the time. Some of these are very much due to Russian specific issues - namely the fact that very often you can change the order of words in Russian without affecting the translation which Duolingo doesn&amp;rsquo;t seem to be able to handle well. Yet to keep things fun I&amp;rsquo;ve been screenshotting amusing translations and thought I&amp;rsquo;d share the best ones here.&lt;/p></description></item><item><title>Quibi's problem</title><link>/2020/07/11/quibis-problem/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/11/quibis-problem/</guid><description>&lt;img src="/image/quibi-survey.png" alt="Quibi survey" data-width="586" data-height="997" data-layout="responsive" />
&lt;p>Everyone loves to bash on Quibi, including me, and yet I feel guilty. We should applaud those who try something new rather than mock it in hindsight. Sure, Quibi raised a ton of money and has an illustrious team but they still deserve respect for attempting to build something from scratch.&lt;/p>
&lt;p>Earlier this week I received a Quibi survey and decided to do my part to give them honest feedback. As I was going through the survey that was attempting to understand my usage (or lack of) I developed a personal theory of why it&amp;rsquo;s failing.&lt;/p></description></item><item><title>Optimizing AWS reservations</title><link>/2020/07/08/optimizing-aws-reservations/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>/2020/07/08/optimizing-aws-reservations/</guid><description>&lt;p>The Information has &lt;a href="https://www.theinformation.com/articles/uber-ceo-wants-to-shift-more-engineering-jobs-to-india-sparking-internal-debate">an article&lt;/a> making the case that Uber was better off having their own data centers versus relying on the cloud given the impact of COVID but that would depend on their reservation strategy. Sure if they reserved AWS capacity then they would be on the hook. Alternatively, if they had no reservations and were running everything on demand they would have incurred higher previous costs but would immediately be able to shut things down. This got me thinking about the optimal reservation strategy - the reality is that this depends very much on your business and the ability to forecast but this is an interesting exercise in speculation.&lt;/p></description></item><item><title>Overthinking how to load data into MySQL</title><link>/2020/06/30/overthinking-how-to-load-data-into-mysql/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/30/overthinking-how-to-load-data-into-mysql/</guid><description>&lt;p>I have two projects that generate data to dump into my &lt;a href="/2020/02/27/my-personal-grafana-dashboard/">personal dashboard&lt;/a>. One &lt;a href="https://github.com/dangoldin/health-stats">loads the health export&lt;/a> from my Apple watch into MySQL and the other &lt;a href="https://github.com/dangoldin/blog-analytics">analyzes my blog posts&lt;/a> and generates a CSV file of statistics that I then load into MySQL. The input to both is basically the same - either a file or a directory - and yet two different approaches to the processing.&lt;/p></description></item><item><title>Specify versions in technical blogging</title><link>/2020/06/28/specify-versions-in-technical-blogging/</link><pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/28/specify-versions-in-technical-blogging/</guid><description>&lt;p>I&amp;rsquo;ve been getting back into coding by hacking around on some side projects and am definitely rusty. Lately it&amp;rsquo;s been learning the modern Django stack and discovering it&amp;rsquo;s multiple major versions ahead of the last time I used it. But thanks to the wonders of the modern internet there&amp;rsquo;s a ton of content online that explains how to do certain tasks. Stack Overflow is one but there are also a ton of individual bloggers and tutorials that make it easier than ever to answer questions.&lt;/p></description></item><item><title>AWS 101</title><link>/2020/06/19/aws-101/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/19/aws-101/</guid><description>&lt;p>Earlier this week I hosted an &amp;ldquo;AWS 101&amp;rdquo; session at work. The goal was to give a small group an interactive introduction to the primary AWS services over 90 minutes. I figure I&amp;rsquo;d share our agenda and exercises in case they&amp;rsquo;re helpful to anyone else. We had trouble sticking to the times but the fact that everyone was remote did make it easier to do group-debugging with those stuck sharing their screens.&lt;/p></description></item><item><title>AWS Snowcone and my first job</title><link>/2020/06/19/aws-snowcone-and-my-first-job/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/19/aws-snowcone-and-my-first-job/</guid><description>&lt;img src="/image/ait-tapes.jpg" alt="AIT tapes" data-width="640" data-height="480" data-layout="responsive" />
&lt;p class="caption">&lt;a href="https://en.wikipedia.org/wiki/Advanced_Intelligent_Tape">AIT Tapes on Wikipedia&lt;/a>&lt;/p>
&lt;p>AWS recently launched &lt;a href="https://aws.amazon.com/snowcone/">Snowcone&lt;/a> and it immediately reminded me of my first post-college job. I was a &amp;ldquo;Business Information Specialist&amp;rdquo; at ZS Associates, a consulting company with a strong emphasis on leveraging data to support pharmaceutical companies. The industry was setup in such a way that there were these massive vendors that would aggregate pharmaceutical prescription data from pharmacies, trace them to the prescriber (ie doctor), and then sell these back to the pharmaceutical companies. In turn, the pharmaceutical companies would give us this data to run a variety of analyses - ranging from calculating sales rep commissions, to setting their quotas, to identifying off-label prescriptions.&lt;/p></description></item><item><title>Software defined photography</title><link>/2020/06/15/software-defined-photography/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/15/software-defined-photography/</guid><description>&lt;p>I was talking to a coworker earlier today about the ridiculous quality of today&amp;rsquo;s smartphone cameras. He pointed out how it&amp;rsquo;s becoming more and more difficult for professional photographers to charge what they have been. The parallel here is how the invention of photography. That completely destroyed the value of artists and yet led to the launch of the modern art movements - Impressionism, Surrealism, Data, and countless others. Without photography would these have happened?&lt;/p></description></item><item><title>Dark Mode is for when you run out of real features</title><link>/2020/06/11/dark-mode-is-for-when-you-run-out-of-real-features/</link><pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/11/dark-mode-is-for-when-you-run-out-of-real-features/</guid><description>&lt;p>I’ve seen a lot of apps launch dark mode and I just don&amp;rsquo;t get it. I&amp;rsquo;m sure part of it is that I mostly stick to the default settings but I can&amp;rsquo;t help but think that if someone is prioritizing dark mode then they must have run out of actually useful features to build. I&amp;rsquo;m being a bit unfair since dark mode can also be a fun side project that someone picks up and may only require changing a few colors around but if it&amp;rsquo;s anything more than that it&amp;rsquo;s an indicator that something is off with their prioritization. Unless a product has reached the mature stage there are incredibly valuable things that the team can be investing in that actually improve the workflow and experience for their customers - dark mode is rarely that.&lt;/p></description></item><item><title>Catch up with your former coworkers</title><link>/2020/06/10/catch-up-with-your-former-coworkers/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/10/catch-up-with-your-former-coworkers/</guid><description>&lt;p>It used to be the case that people used to work at a company for decades, some even retired at the same company they started working at. We&amp;rsquo;re no longer in that world. These days it&amp;rsquo;s rare to find someone who&amp;rsquo;s worked at a company for longer than 5 years.&lt;/p>
&lt;p>There is a benefit to this rotation though: when people change jobs they also bring their ideas and experiences, mix them with those at their new company, and converge to better and better approaching. A great way to accelerate this is by reaching out to all of your coworkers a few months after they left and asking how they&amp;rsquo;re doing. It&amp;rsquo;s a great way to stay in touch and maintain that relationship but it&amp;rsquo;s also a good opportunity to compare and contrast how they do things. They would have left recently enough where they understand your processes and embedded enough in their new roles to know how things work there. Those differences give you a lot of valuable advice on what other companies are doing and whether there&amp;rsquo;s anything you should change on your end.&lt;/p></description></item><item><title>Black Lives Matter</title><link>/2020/06/06/black-lives-matter/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>/2020/06/06/black-lives-matter/</guid><description>&lt;p>It&amp;rsquo;s difficult to think of or write about anything other than the protests going on right now. I hope that everything going on is a necessary step to get the United States to accept its sordid treatment of Black people and into a more fair and equal world. The past few weeks have made race come to the foreground as a topic that must be discussed. In the past I shied away from discussing anything race related; especially coming from my privileged background and despite my intent I was worried about the language I used and concerned about being misinterpreted. Given how pervasive and obvious Black mistreatment is it&amp;rsquo;s not something that anyone can be silent about. It&amp;rsquo;s critical for those of us that don&amp;rsquo;t know about what&amp;rsquo;s going on to educate ourselves and have the real and honest and often painful conversations. Those in turn will spur action which will lead to us finally tackling the nearly 500 year history of Black oppression.&lt;/p></description></item><item><title>Giving Obsidian a shot</title><link>/2020/05/31/giving-obsidian-a-shot/</link><pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/31/giving-obsidian-a-shot/</guid><description>&lt;p>I tried using Notion a while back but rebelled against having my data stored in the cloud in a proprietary format. For a &lt;a href="/2016/04/30/text-is-king/">variety of reasons&lt;/a> I want my notes to be available locally in a text based format. At the same time there&amp;rsquo;s a lot to like about the modern tools and I feel myself being worn down. A few weeks ago I discovered &lt;a href="https://roamresearch.com/">Roam&lt;/a> and am sold on its networked thought model. I&amp;rsquo;ve been using it for some of my note taking but was hesitant to commit given my concerns. Earlier this week I came across &lt;a href="https://obsidian.md/">Obsidian&lt;/a> - a Roam-like product that runs locally and stores everything in Markdown. This was opportune timing since I&amp;rsquo;m able to lean in on the networked thought model while still staying true to local files in an open format. I&amp;rsquo;m still getting used to Obsidian&amp;rsquo;s semantics - tags, links, and Markdown - but overall I&amp;rsquo;m bullish and glad to give it a shot.&lt;/p></description></item><item><title>Three categories of software engineering work</title><link>/2020/05/30/three-categories-of-software-engineering-work/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/30/three-categories-of-software-engineering-work/</guid><description>&lt;p>Managers love frameworks to help think through problems and I&amp;rsquo;m no different. While trying to think of a way to reason through our engineering work I started grouping it into three major categories:&lt;/p>
&lt;p>&lt;strong>Should have been done yesterday&lt;/strong>. Self explanatory but these are the items that as soon as you discover them you wish they had already been done. Critical bugs and issues fall into this but also product oversights that you wish you caught earlier.&lt;/p></description></item><item><title>Anatomy of a crypto mining hack</title><link>/2020/05/29/anatomy-of-a-crypto-mining-hack/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/29/anatomy-of-a-crypto-mining-hack/</guid><description>&lt;p>A few months ago I set up a simple ftp server to help a friend. I took a few security shortcuts which came to bite me this week when I received an alert from DigitalOcean that an instance was running hot.&lt;/p>
&lt;img src="/image/digitalocean-cpu-alert.png" alt="DigitalOcean CPU Alert" data-width="503" data-height="280" data-layout="responsive" />
&lt;p>I dug into it and noticed a series of processes being run by the ftp_user - the most impactful was a command called rsync. I&amp;rsquo;m familiar with rsync which syncs files across devices - and this was nothing like that. At this point I realized that I got hacked and quickly disabled the user and killed all its processes. The CPU usage dropped back to normal and now it was time to dig into the damage.&lt;/p></description></item><item><title>Metrics, logging, and error reporting</title><link>/2020/05/27/metrics-logging-and-error-reporting/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/27/metrics-logging-and-error-reporting/</guid><description>&lt;p>As software engineers it’s vital to understand how our applications are performing. The more information we have the better we can address problems, improve performance, and generally better solve problems for our customers. Imagine releasing a product with nothing being collected - you&amp;rsquo;d be flying blind.&lt;/p>
&lt;p>I&amp;rsquo;m a huge advocate for measuring application performance in terms of business metrics and aligning it as much with the customer experience as possible. This post does not go into that. Instead, I want to share my beliefs around measuring the technical application performance. In my mind, they&amp;rsquo;re grouped into three major categories: metrics, logging, and error reporting. There is some overlap but I find it a helpful way to think about them.&lt;/p></description></item><item><title>Stock portfolio hedging during the coronavirus pandemic</title><link>/2020/05/23/stock-portfolio-hedging-during-the-coronavirus-pandemic/</link><pubDate>Sat, 23 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/23/stock-portfolio-hedging-during-the-coronavirus-pandemic/</guid><description>&lt;p>I have had the majority of my liquid wealth invested in the stock market and I wanted to share what I&amp;rsquo;m doing these days. As a bit of a background, at the beginning of the year my portfolio primarily consisted of the tech companies I&amp;rsquo;ve held for years - Netflix, Amazon, Equinix, Google, and Facebook. After the first decline I liquidated about half of my portfolio - my thinking was that I&amp;rsquo;m still long term bullish about these companies and they will likely emerge stronger but I should hold on to some of my gains and have a bit of cash.&lt;/p></description></item><item><title>Prefer verbs to adverbs</title><link>/2020/05/21/prefer-verbs-to-adverbs/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/21/prefer-verbs-to-adverbs/</guid><description>&lt;p>Years ago I read a book that I unfortunately can&amp;rsquo;t recall that had a very insightful bit of writing advice - prefer verbs to adverbs. The premise was that many of us will spice up our writing by using adverbs to decorate our verbs but instead we should use strong and unique verbs. Your writing gains pithiness and leads to a more memorable message.&lt;/p>
&lt;p>As an example, rather than prepending &amp;ldquo;said&amp;rdquo; with yet another adverb instead think of verbs that get your point across better - maybe it&amp;rsquo;s shouted, or exclaimed, or charged, or yelled, or accused, or hundreds others. Each of them carry more weight than just decorating said. Not to mention this is a great way to expand your vocabulary!&lt;/p></description></item><item><title>Shopify for food delivery</title><link>/2020/05/21/shopify-for-food-delivery/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/21/shopify-for-food-delivery/</guid><description>&lt;p>I wrote about the idea of a service that &lt;a href="http://dangoldin.com/2018/11/29/a-delivery-aggregation-service/">aggregates delivery drivers&lt;/a> back in 2018 but given the latest wave of food delivery service bashing I couldn&amp;rsquo;t help but think of it again. This time around my thoughts are a bit more refined about how to actually build a business but do think there&amp;rsquo;s a real opportunity here, especially after seeing the challenges with the current model.&lt;/p>
&lt;p>The existing food delivery apps charge high fees because food delivery is one of the highest costs. That’s the part that needs to be improved in order for the companies to make sense. As many people note restaurants have been delivering food on their own for decades while maintaining profitability so it&amp;rsquo;s clearly possible. Yet these online services struggle. Instead of focusing on the delivery side they&amp;rsquo;re in a race to capture as much of the demand as possible.&lt;/p></description></item><item><title>Limits of our imagination</title><link>/2020/05/16/limits-of-our-imagination/</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/16/limits-of-our-imagination/</guid><description>&lt;p>While listening to a &lt;a href="https://a16z.com/2020/04/29/the-chief-security-officer-in-and-out-of-a-crisis/">podcast&lt;/a> describing the CSO role there was an interesting anecdote mentioned (~14:19 mark) that I couldn&amp;rsquo;t find anywhere else: in the 1900s the chief electrician on wall street was one of the highest paid positions. Back then, the electricity infrastructure was not as stable as it is now and it was not a certainty that it would stay up so those dedicated to keeping the systems running were appropriately rewarded. Over time, the electrical infrastructure became stable enough that it&amp;rsquo;s become expected and very few worry about it.&lt;/p></description></item><item><title>The curse of mastery</title><link>/2020/05/16/the-curse-of-mastery/</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/16/the-curse-of-mastery/</guid><description>&lt;p>One of the most valuable skills a software engineer can have is a &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Tabula_rasa">tabula rasa&lt;/a>&amp;rdquo; mindset. Tabula rasa, Latin for blank slate, is a philosophical concept dating back to Aristotle to explain how human minds are blank from the beginning and acquire knowledge through the human experience.&lt;/p>
&lt;p>I think of it as being able to approach new challenges and experiences with no prior bias. Done well, this is an incredible power since you&amp;rsquo;re able to cut through the complexity and focus on what actually matters. Imagine you&amp;rsquo;re a software engineer who joins a new company - there&amp;rsquo;s so much business context you need to ramp up in addition to the code that&amp;rsquo;s grown in complexity over the years. By the time you&amp;rsquo;ve mastered everything it all makes sense and you&amp;rsquo;ve embraced the complexity.&lt;/p></description></item><item><title>Sometimes it really is just the dirty, hard work</title><link>/2020/05/02/sometimes-it-really-is-just-the-dirty-hard-work/</link><pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate><guid>/2020/05/02/sometimes-it-really-is-just-the-dirty-hard-work/</guid><description>&lt;p>It&amp;rsquo;s easy to fall into the trap of only wanting to do the new and sexy thing and giving up as soon as something requires the dirty work. That&amp;rsquo;s rarely a strategy for success and jumping from one idea to another idea and only focusing on the interesting parts is a sure fire way to not get anything done.&lt;/p>
&lt;p>Last week I had a brief conversation with Twitter that got me looking for Apple&amp;rsquo;s S-1. Apple filed this in 1980 as part of their IPO process and on one hand it&amp;rsquo;s remarkable that you can find a &lt;a href="https://www.sec.gov/files/18-02062-FOIA.pdf">40 year old document online&lt;/a> and yet on the other hand it&amp;rsquo;s disappointing that it came as a PDF. I imagine every year there are thousands (tens of thousands?) of people that want to take a look at the IPO docs of one of the most impressive tech companies and yet end up with a poorly rendered version. What if someone took the time to modernize it and convert it to a modern S-1? A massive amount of people would benefit and they would get the credit.&lt;/p></description></item><item><title>Data analysis and visualization</title><link>/2020/04/29/data-analysis-and-visualization/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/29/data-analysis-and-visualization/</guid><description>&lt;p>Yesterday I had the rare chance to actually do some coding and realized how rusty I am at numerical analysis in Python. The task was simple - ingest a CSV that had a date column, two categorical columns, and a numerical column - and then generate a grid containing a series of line plots, each of which would be a combination of the two categorical columns.&lt;/p>
&lt;p>I did a ton of this work years ago so knew what was possible. That&amp;rsquo;s half the battle and after a bit of searches I got a working solution. At the same time I&amp;rsquo;m disappointed it took me that long, especially after seeing the brevity of the end result. Software ate the world and now every business is generating tons of data. Being able to make sense of it is an increasingly important skill set especially if you&amp;rsquo;re a leader. It&amp;rsquo;s unfortunately an area I haven&amp;rsquo;t kept up with over the years and something I plan on remedying after this experience.&lt;/p></description></item><item><title>Engineering management resources</title><link>/2020/04/25/engineering-management-resources/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/25/engineering-management-resources/</guid><description>&lt;p>At the beginning of the year I gave myself a goal to write more management posts. We&amp;rsquo;re nearly 5 months into the year and I&amp;rsquo;ve written only two posts even relating to management. A big reason is impostor syndrome - while I have only been managing a few years there are so many others that have more experience, are better managers, and are more effective writers. At the same time, the lack of experience hasn&amp;rsquo;t really stopped me from writing on other topics though so why should this be any different? This coupled with the fact that I write to organize my thoughts and clarify my thinking is all the more reason to commit to writing about new topics. As a start, I thought it would be helpful to share some resources that have helped me grow as a manager.&lt;/p></description></item><item><title>Margin of victory</title><link>/2020/04/24/margin-of-victory/</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/24/margin-of-victory/</guid><description>&lt;p>During the &lt;a href="https://www.youtube.com/watch?v=WXuK6gekU1Y">AlphaGo documentary&lt;/a> there&amp;rsquo;s a scene where the AI makes a very conservative move that the commentary notes is very different from what a human would do. The explanation is that AlphaGo doesn&amp;rsquo;t care about the margin of victory, just the victory itself. It&amp;rsquo;s trying to maximize the likelihood of winning, not the score differential - and that leads to a unique playing style.&lt;/p>
&lt;p>I found this part of the documentary fascinating as it&amp;rsquo;s one of those things that&amp;rsquo;s obvious when explained but you&amp;rsquo;d never stop to think about. Humans rarely think about margin of victory. Most of our decisions and competitions aren&amp;rsquo;t quantifiable so we always try to do the best we can. In sports, on the other hand, where there are actual scores, teams will play for the victory rather than the score. It&amp;rsquo;s interesting to think of decisions in our daily lives that would benefit from shifting from margin-of-victory thinking to the victory itself. It&amp;rsquo;s not natural and being able to challenge human nature is a valuable skill.&lt;/p></description></item><item><title>AlphaGo documentary</title><link>/2020/04/18/alphago-documentary/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/18/alphago-documentary/</guid><description>&lt;p>&lt;amp-youtube data-videoid="WXuK6gekU1Y" layout="responsive" width="1280" height="720">&lt;/amp-youtube>&lt;/p>
&lt;p>&lt;strong>Spoiler&lt;/strong>: If you haven&amp;rsquo;t seen the documentary and want to avoid any spoilers don&amp;rsquo;t read the post. At the same time you should watch it - it&amp;rsquo;s an incredible film.&lt;/p>
&lt;p>I watched the AlphaGo documentary and it left a powerful impression on me.&lt;/p>
&lt;p>The accomplishment by the AlphaGo team is incredible. At the same time I felt incredibly sad watching it and became slightly depressed. It felt very much like a man vs machine battle yet the machine was a huge team of engineers with nearly unlimited resources. You know that it&amp;rsquo;s inevitable that the machine wins and yet the way the documentary was done made it seem less of a highlight of the great work by the team and instead a defeat of a single person.&lt;/p></description></item><item><title>Google and Apple's COVID tracking project</title><link>/2020/04/18/google-and-apples-covid-tracking-project/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/18/google-and-apples-covid-tracking-project/</guid><description>&lt;p>Google and Apple recently announced a partnership to help people anonymously see if they&amp;rsquo;ve been exposed to someone with COVID. The idea itself is elegant and does a nice job balancing the functionality with the need for privacy. One of the most common takes I&amp;rsquo;ve seen in tech circles is that no one will use it due to a concern for privacy.&lt;/p>
&lt;p>I think the opposite. I believe that with enough marketing this will be a massive success. Over and over again we&amp;rsquo;ve seen that most people do not value privacy and this will be no different. In fact, for this case it may just be the opposite - most people would love to know the answer to &amp;ldquo;is there a chance I have Coronavirus.&amp;rdquo; Curiosity, one of our greatest traits, will be enough to overcome whatever privacy concerns people have.&lt;/p></description></item><item><title>Optimizing code? Think theoretical limits</title><link>/2020/04/18/optimizing-code-think-theoretical-limits/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/18/optimizing-code-think-theoretical-limits/</guid><description>&lt;p>This post was inspired from a conversation with an engineer who was tasked with optimizing the performance of a heavily used static JavaScript script. This code gets loaded billions of times a day across a variety of devices and small improvements to its load time and performance can drive significant value to our customers and us.&lt;/p>
&lt;p>When you start it&amp;rsquo;s easy to find the low hanging fruit and get the simple wins. But as you keep working on the same problem these wins become rarer and rarer and oftentimes a win in one area may be a loss in another. Using our static JavaScript code as an example we can argue that we should split it into multiple, smaller files and load them on demand. That would make the total amount of code loaded and executed lower but, on the other hand, would increase the number of network requests. And if you then layer in caching and the variety of devices it will run on it&amp;rsquo;s not at all obvious what the impact of a change will be.&lt;/p></description></item><item><title>Dumping Apple health data into MySQL</title><link>/2020/04/11/dumping-apple-health-data-into-mysql/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/11/dumping-apple-health-data-into-mysql/</guid><description>&lt;img src="/image/grafana-health-stats.png" alt="Grafana visualization of my Apple health stat" data-width="1860" data-height="944" data-layout="responsive" />
&lt;p>I apparently can&amp;rsquo;t get enough of Grafana and the latest quantified self push was to visualize the data from Apple health. Apple makes it pretty simple to export the data but it&amp;rsquo;s in XML so there&amp;rsquo;s a small bit of processing to turn into something that can be visualized. For my personal stats I&amp;rsquo;m dumping the data to MySQL and writing fairly simple queries to visualize them. Since I already did a similar export in my &lt;a href="https://github.com/dangoldin/email-stats">email-stats&lt;/a> code I was able to reuse a fair amount. The major difference was that the Apple health export is fairly large (my export for 2020 was an 80 MB file) and it would be a shame to not apply a few optimizations.&lt;/p></description></item><item><title>Gamification works</title><link>/2020/04/07/gamification-works/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/07/gamification-works/</guid><description>&lt;img src="/image/grafana-emails-avg-age.png" alt="Grafana dashboard for emails in inbox and the avg age" data-width="1840" data-height="943" data-layout="responsive" />
&lt;p>Gamification works. Last week I hacked together a Grafana dashboard to measure the number of emails in my inbox and sure enough this gave me enough motivation to actually go through them. Earlier this week I added another metric to track the average age of an email and sure enough that caused me to go through the 4 and 5 year old emails.&lt;/p></description></item><item><title>Start open sourcing government software</title><link>/2020/04/05/start-open-sourcing-government-software/</link><pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/05/start-open-sourcing-government-software/</guid><description>&lt;p>Two years ago I &lt;a href="/2017/09/22/open-sourcing-government/">wrote a post&lt;/a> describing an open source model for developing government software. The driver then was improving the user experience of government services and making them more approachable but the news this week makes this much more than a nicer UX. The state of NJ is apparently &lt;a href="https://www.youtube.com/watch?v=HSVgHlSTPYQ">looking for COBOL programmers&lt;/a> in order to update the unemployment system to handle the unprecedented volume. It&amp;rsquo;s easy to make a government joke but there are real people that are unable to get the services they need. This particular problem will be solved but another one will pop up in its place.&lt;/p></description></item><item><title>Finding the truth in a sea of misinformation</title><link>/2020/04/04/finding-the-truth-in-a-sea-of-misinformation/</link><pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate><guid>/2020/04/04/finding-the-truth-in-a-sea-of-misinformation/</guid><description>&lt;p>Two days ago I read an &lt;a href="https://stratechery.com/2020/an-interview-with-zeynep-tufekci-about-masks-media-and-information-ecology/">interview&lt;/a> between Ben Thompson and Zeynep Tufekci which highlighted the current state of information (and misinformation). That same day I came across an &lt;a href="https://marker.medium.com/what-everyones-getting-wrong-about-the-toilet-paper-shortage-c812e1358fe0">article&lt;/a> by &lt;a href="https://twitter.com/WillOremus">Will Remus&lt;/a> on the cause of the toilet paper shortage (spoiler: it has to do with differences between the corporate and personal toilet paper supply chains). Both of these highlight how little I actually know and how valuable skeptical mindset is these days. I still don&amp;rsquo;t know the truth and it&amp;rsquo;s likely something that changes constantly but being able to separate fact from fiction is a massive advantage. The obvious example is knowing how serious COVID-19 would be and preparing for it in January. The challenges and opportunities will only get larger as the amount of information (majority of which will be misinformation) being produced increases. The ability to be objective, dig into the sources, and understand biases is going to be a critical skill in the upcoming century.&lt;/p></description></item><item><title>Visualizing my journey to Inbox Zero</title><link>/2020/03/31/visualizing-my-journey-to-inbox-zero/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/31/visualizing-my-journey-to-inbox-zero/</guid><description>&lt;img src="/image/grafana-emails-in-inbox.png" alt="Number of emails in my inbox by account" data-width="1855" data-height="518" data-layout="responsive" />
&lt;p>I subscribe to the &amp;ldquo;Inbox Zero&amp;rdquo; philosophy and treat my email inbox as a todo list that I slowly work through. As part of the desire to get more and more quantitative I wrote a quick script to pull the number of emails from my Inbox and then insert the data as a row into a new table in my personal stats database. As usual, most of the work was in deciding to do it and once I got to coding the hacky solution was done within 20 minutes. The script uses Python&amp;rsquo;s built-in &lt;a href="https://docs.python.org/3/library/imaplib.html">imaplib&lt;/a> library to log in to an email provider and then a simple MySQL query to insert the resulting data. I hooked this up to run every 15 minutes via cron and put together a Grafana dashboard to plot the count over time. I&amp;rsquo;m currently not actually going through the content of the email messages themselves but there are tons of directions I can take this - for example slicing the data by sender or examining the age of the messages. For now I&amp;rsquo;m just hopeful this motivates me to keep going through that email.&lt;/p></description></item><item><title>A TikTok experience for streaming services</title><link>/2020/03/29/a-tiktok-experience-for-streaming-services/</link><pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/29/a-tiktok-experience-for-streaming-services/</guid><description>&lt;p>During an &lt;a href="https://stratechery.com/2020/an-interview-with-eugene-wei-about-the-half-life-of-information/">interview&lt;/a> with Ben Thomson, &lt;a href="https://www.eugenewei.com/">Eugene Wei&lt;/a> has the following response to a question digging into Netflix solving the &amp;ldquo;I just want something on in the background&amp;rdquo; use case:&lt;/p>
&lt;blockquote>
 &lt;p>Yeah, that’s a great point. I also do hope that this macro shock forces the streaming companies to think a little bit harder about innovating on their user interfaces. I still think it’s a little bit of a shame that most of them still present the grid of icons. Everybody has probably had that experience of just paralysis where you’re just scrolling around, you’re like, “What do I watch? I don’t know what to choose.”&lt;/p></description></item><item><title>Disabling TabNine to improve Ubuntu performance</title><link>/2020/03/29/disabling-tabnine-to-improve-ubuntu-performance/</link><pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/29/disabling-tabnine-to-improve-ubuntu-performance/</guid><description>&lt;p>At the end of 2018 I switched from using a Mac to using Ubuntu on a ThinkPad. While much of the development experience is better the one thing that&amp;rsquo;s noticeably worse is performance (usually first noticed in browsers) coupled with the occasional computer freeze. Back in the day when nearly everyone used Windows I remember the blue screens and the need to reboot frequently and I&amp;rsquo;m finding Ubuntu has developed the same pattern. I rarely had to reboot by Mac but find myself rebooting my current laptop at least once a week.&lt;/p></description></item><item><title>COVID-19</title><link>/2020/03/28/covid-19/</link><pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/28/covid-19/</guid><description>&lt;p>It&amp;rsquo;s taken me a few weeks to write this post. Partly due to lack of motivation, partly lack of time, and partly due to my fluctuating feelings towards COVID-19.&lt;/p>
&lt;p>Some days I was completely disillusioned and sad about the modern state of the world. It seems to be spreading unconstrained despite the more than one hundred years of scientific progress we&amp;rsquo;ve had since 1918. Politicians seem to care about their own ego more than they do about the people they purport to represent. If we didn&amp;rsquo;t have the resources or knowledge to fight back I&amp;rsquo;d be more accepting of our fate but it&amp;rsquo;s the fact that we can fight and defeat this thing if we just channel our energies the right way that&amp;rsquo;s frustrating.&lt;/p></description></item><item><title>Zoom's blow up and their financials</title><link>/2020/03/15/zooms-blow-up-and-their-financials/</link><pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/15/zooms-blow-up-and-their-financials/</guid><description>&lt;p>Zoom has had an incredible rise in the past few weeks as the tech world has shifted to remote work with nearly all meetings occurring over video conferencing. The stock is up 30% over the past month and it&amp;rsquo;s the top app in Apple&amp;rsquo;s app store - all great things! What I&amp;rsquo;m curious about is their current financial performance. Zoom makes money by charging a flat fee per user per month. And that fee was set based on both the value delivered but also some assumptions around usage. What happens to their margin if customers go from using Zoom for a few hours a week to multiple hours in day?&lt;/p></description></item><item><title>Paying for improved productivity</title><link>/2020/03/01/paying-for-improved-productivity/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>/2020/03/01/paying-for-improved-productivity/</guid><description>&lt;p>Years ago I used to prioritize cost when it came to developer tools. I&amp;rsquo;d resist paying for anything if I could find an open source alternative that worked reasonably well. These days I prioritize my effectiveness and am much more likely to pay for the right tool. A great example of this is my search for a database IDE. I started using &lt;a href="https://www.sequelpro.com/">Sequel Pro&lt;/a> (when I was on a mac) for MySQL and &lt;a href="http://squirrel-sql.sourceforge.net/">SQuirreL&lt;/a> for everything else. Sequel Pro was great - it was smooth, responsive, and felt optimized for MySQL. SQuirreL, on the other hand, was designed to support nearly all databases which made it worse across all of them.&lt;/p></description></item><item><title>My personal Grafana dashboard</title><link>/2020/02/27/my-personal-grafana-dashboard/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/27/my-personal-grafana-dashboard/</guid><description>&lt;img src="/image/personal-grafana-dashboard-blog-stats.png" alt="My personal Grafana dashboard for blog stats" data-width="1913" data-height="978" data-layout="responsive" />
&lt;p>Last year I wrote about the idea of a &lt;a href="/2019/07/10/personal-dashboards/">personal dashboard&lt;/a> and earlier this year I described my &lt;a href="/2020/01/28/2020-goals/">2020 goals&lt;/a> and how I&amp;rsquo;d go about measuring my progress. The past two days I was able to combine the two concepts and created a simple Grafana dashboard to measure my progress against the blogging goal. As with most tasks, the most difficult part was getting started and the actual exercise took a few hours. While it&amp;rsquo;s still fresh in my mind I want to document the step by step process in order to both provide a perspective into how I work while also giving others a guide to setting up their own.&lt;/p></description></item><item><title>Embracing smartphone productivity</title><link>/2020/02/11/embracing-smartphone-productivity/</link><pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/11/embracing-smartphone-productivity/</guid><description>&lt;p>Unlike a modern teenager, I tend to avoid using my smartphone for anything except consumption. I&amp;rsquo;d read emails but not respond, check up on Twitter but not tweet, look at my calendar but not schedule any events, and so forth. The excuse was that I&amp;rsquo;d be able to do these much more effectively in front of a real computer. Lately, I&amp;rsquo;ve realized that while the above is true I can still knock out a ton of small and simple tasks on my phone and save the meaty stuff for the computer. I&amp;rsquo;m not much more willing to respond to an email or send an invite on my phone rather than waiting to get on a computer. It also has the side benefit of reducing the amount of items on my mental todo list which will inevitably have loss.&lt;/p></description></item><item><title>Repository of configurations</title><link>/2020/02/07/repository-of-configurations/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/07/repository-of-configurations/</guid><description>&lt;p>While tuning Kafka yet another time I started thinking of how useful a repository of configuration files would be. Very often we install an open-source project and default to the standard options. This works great when you&amp;rsquo;re starting out but as you grow you realize the deployment is no longer cutting it. So you go back to the documentation and do a few searches to see how others have it tuned. This gives you a bit of breathing and lets you keep going until the next scaling bottleneck. Throughout you&amp;rsquo;re likely scaling up, out, or both and yet very few of us actually know whether we&amp;rsquo;re properly configured.&lt;/p></description></item><item><title>Micromanaging is underrated</title><link>/2020/02/04/micromanaging-is-underrated/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>/2020/02/04/micromanaging-is-underrated/</guid><description>&lt;p>A common belief is that micromanaging is a poor way to manage. I subscribe to this view - it&amp;rsquo;s demotivating to have someone second-guessing every step and eliminates agency and autonomy. I&amp;rsquo;d rather have someone make a few mistakes and learn along the way rather than impose a top-down view that disincentives growth.&lt;/p>
&lt;p>While reading &lt;a href="https://www.amazon.com/Ride-Lifetime-Lessons-Learned-Company-ebook/dp/B07PF6XTD8/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=&amp;amp;sr=">Bob Iger&amp;rsquo;s biography&lt;/a>, I came across a simple statement from his former boss and Disney CEO, Michael Eisner: &amp;ldquo;Micromanaging is underrated.&amp;rdquo; That gave me pause. So often we hear that micromanaging is terrible and no effective manager does yet here&amp;rsquo;s someone incredibly successful leaning into it. Bob Iger distances himself away from this black and white view by discussing finding the balance while admitting that in many cases the details do, in fact, matter and it&amp;rsquo;s up to the CEO to make sure the output of the team is as polished and great as possible. The passage reminded me of Steve Jobs and his reputation for being incredibly detail-oriented and obsessive. In many ways, his approach was all about micromanaging and yet Apple made some of the most incredible and breathtaking products. Clearly there&amp;rsquo;s some value in micromanaging.&lt;/p></description></item><item><title>Finding parking spots using YOLO</title><link>/2020/01/31/finding-parking-spots-using-yolo/</link><pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/31/finding-parking-spots-using-yolo/</guid><description>&lt;p>I finally managed to make a bit more progress on the Parking Spot Finder project. &lt;a href="http://dangoldin.com/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision/">Last time&lt;/a> I tested the computer vision products offered by Google and AWS to see how well they were able to detect cars. This time around I decided to actually start working on the computer vision side and found a nice &lt;a href="https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/">tutorial&lt;/a> that allowed me to quickly try out the YOLO computer vision library. Surprisingly, it did significantly better than AWS but was more mixed against Google. On the original image it did better than both AWS and Google at detecting the cars but it did worse than Google on the cropped images. The next step is to actually try training a model and see how well it can do. Depending on how that goes I may end up changing my approach up and see how far I can get using heuristics.&lt;/p></description></item><item><title>2020 Goals</title><link>/2020/01/28/2020-goals/</link><pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/28/2020-goals/</guid><description>&lt;p>What better way of improving your chances of completing a task than publicly proclaiming you&amp;rsquo;re going to do it? That&amp;rsquo;s the idea for publicly posting my goals for 2020. I unfortunately skipped this exercise in 2019 but hope to correct it for 2020. It&amp;rsquo;s the start of a new decade after all and a good opportunity to get into better habits. Another lesson I&amp;rsquo;ve learned is that to hold myself truly accountable they should be measurable so I came up with a quantitative way of measuring each of them over the course of 2020.&lt;/p></description></item><item><title>Software comparison done right</title><link>/2020/01/24/software-comparison-done-right/</link><pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/24/software-comparison-done-right/</guid><description>&lt;p>There are a variety of sites out there that exist solely to do comparisons of enterprise software - just type &amp;ldquo;product A vs product B&amp;rdquo; in Google and the first few pages will be littered with results. They all generally follow the same format: a side by side table comparing them feature by feature. Some of the values are factual - for example pricing - while others are based on user reviews.&lt;/p></description></item><item><title>Instagram targeting ads based on likes</title><link>/2020/01/21/instagram-targeting-ads-based-on-likes/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/21/instagram-targeting-ads-based-on-likes/</guid><description>&lt;img src="/image/instagram-milkbar-ad.png" alt="The Milk Bar image I liked" data-width="451" data-height="706" data-layout="responsive" />
&lt;p>I noticed a new type of ad targeting behavior on Instagram I haven&amp;rsquo;t seen before - liking someone&amp;rsquo;s shot containing a product leads to a targeted ad containing that product. In my case, a friend posted a picture of an order from the Milk Bar and I gave it a friendly like. A few days later I started seeing ads for the Milk Bar Store. I know it may seem like a manifestation of the &lt;a href="https://science.howstuffworks.com/life/inside-the-mind/human-brain/baader-meinhof-phenomenon.htm">Baader-Meinhof phenomenon&lt;/a> but I tend to notice ads and am pretty certain I have not seen any Milk Bar ad until I liked that image.&lt;/p></description></item><item><title>Visualizing my 2019</title><link>/2020/01/17/visualizing-my-2019/</link><pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/17/visualizing-my-2019/</guid><description>&lt;p>In order to better understand myself, I&amp;rsquo;ve been collecting daily stats over the past few years with the idea that tracking various metrics would show me ways to improve. This happened to some degree - seeing many of the numbers leads to a sense of shame - but there hasn&amp;rsquo;t been a huge insight that looks at the interaction of the various items I track. To do that, I suspect I need to dive deeper into the quantified self movement and start continually tracking my physiological metrics rather than the current approach of a daily check-in describing my mood and what I consumed.&lt;/p></description></item><item><title>Whatever happened to fast software?</title><link>/2020/01/14/whatever-happened-to-fast-software/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/14/whatever-happened-to-fast-software/</guid><description>&lt;p>Maybe it&amp;rsquo;s just me but it feels as if modern software has been getting slower and slower. My computer is the fastest it has ever been and yet I&amp;rsquo;m more frustrated by the actual performance more than ever. Websites seem slower than they&amp;rsquo;ve ever been and I often find myself typing faster than the letters appear on the screen. Part of the problem is that everything seems to be web-based; in fact, many &amp;ldquo;desktop&amp;rdquo; apps are powered by Electron which is a wrapper around Chromium that makes it easy to write desktop apps. Unfortunately, easy doesn&amp;rsquo;t mean fast and we get the performance of a browser tab in standalone tab.&lt;/p></description></item><item><title>Quick math calculations</title><link>/2020/01/10/quick-math-calculations/</link><pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/10/quick-math-calculations/</guid><description>&lt;amp-twitter width="609" height="274" layout="responsive" data-tweetid="1222729308935991298">
&lt;/amp-twitter>
&lt;p>I came across a tweet earlier today that received a surprisingly large reaction for a relatively simple math shortcut: &amp;ldquo;Percentages are reversible. 8% of 25 is the as 25% of 8 and one of them is much easier to do in your head.&amp;rdquo; I followed it up with one of my own: &amp;ldquo;Similar one is use factors of 10. Want to divide by 5? Multiply by 2 and shift decimal point. 24/5=24*2/10=4.8 same with 4 and 25., 8 and 125, etc&amp;rdquo;&lt;/p></description></item><item><title>Social network usage is exclusive</title><link>/2020/01/07/social-network-usage-is-exclusive/</link><pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/07/social-network-usage-is-exclusive/</guid><description>&lt;p>I remember the day when there were dozens of social networks with people hopping from one to another. These days it feels that there are only four: Facebook, Instagram (I&amp;rsquo;m explicitly separating it out from Facebook), Twitter, and Snapchat. This is a very US view and I realize there are tons of niche ones but at this point it feels as if those constitute the bulk of usage. More importantly, and this is pure speculation, it seems as if you pick a random person and look at their social network usage it&amp;rsquo;s very likely they&amp;rsquo;re spending the majority, if not all of their time, in only one of those networks.&lt;/p></description></item><item><title>Build for optionality</title><link>/2020/01/03/build-for-optionality/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>/2020/01/03/build-for-optionality/</guid><description>&lt;p>I wrote a company wide email back in September titled &amp;ldquo;Building for optionality&amp;rdquo; that describes how our engineering team maintains its impact despite the growing complexity of the business. There are a few TripleLift-specific concepts that I stripped away to make it more approachable but the theme is the same.&lt;/p>
&lt;hr/>
&lt;p>For a company that’s growing and changing as quickly as TripleLift, it’s critical to make sure our code is as impactful as possible. One way we do this is by building for optionality.&lt;/p></description></item><item><title>Advertising an Airbnb on Instagram</title><link>/2019/12/31/advertising-an-airbnb-on-instagram/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/31/advertising-an-airbnb-on-instagram/</guid><description>&lt;img src="/image/airbnb-instagram-ad.png" alt="An Airbnb ad on instagram" data-width="828" data-height="1792" data-layout="responsive" />
&lt;p>I saw an ad for a specific &lt;a href="https://www.instagram.com/hudsonvalleyairbnb/">Airbnb&lt;/a> on Instagram and just don&amp;rsquo;t understand the economics. It seems unlikely that running this sort of campaign ends up being profitable. I&amp;rsquo;d expect the overlap between the audience targeted and those interested in booking the Airbnb to be extremely low and potentially non-existent.&lt;/p>
&lt;p>This Airbnb has a nightly rate of ~$350 and a cleaning fee of $199. If we assume that the average guest books for 3 nights the host is getting ~$1,220 after Airbnb fees ($350 * 0.97 * 3 + $199). Let&amp;rsquo;s also assume that the mortgage, maintenance, and utilities are $5,000/month with an aggressive occupancy rate of 75%. That leads us to an actual monthly profit of $2,638 ($350 * 30 * 0.97 * 0.75 - $5000) across 7.5 (30 * 0.75/3) guests, or a per guest profit of $351.73 ($2,638 / 7.5).&lt;/p></description></item><item><title>Farewell 2019, hello 2020</title><link>/2019/12/31/farewell-2019-hello-2020/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/31/farewell-2019-hello-2020/</guid><description>&lt;p>Bill Gates once said &amp;ldquo;Most people overestimate what they can do in one year and underestimate what they can do in ten years&amp;rdquo; and it&amp;rsquo;s a good assessment of my decade. It was my first decade as an adult and it was very much about finding my footing, both professionally and personally. My first few years out of college were trying to figure things out and understand who I am and what I enjoy doing. I started the decade working as a quant, to attempting two startups, to joining a quickly growing &lt;a href="https://triplelift.com/">startup&lt;/a> that I&amp;rsquo;ve been at for the past 6 years.&lt;/p></description></item><item><title>Gaming virtual kitchens</title><link>/2019/12/31/gaming-virtual-kitchens/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/31/gaming-virtual-kitchens/</guid><description>&lt;p>Given how popular food delivery has gotten it&amp;rsquo;s no surprise that some restaurants are opting out of a retail location and are starting to rely solely on their delivery efforts. This is leading to a variety of companies that are building kitchens that can be rented out by these types of restaurants. In fact, one of them, &lt;a href="https://www.cloudkitchens.com/">CloudKitchens&lt;/a>, is Travis Kalanick new startup attempt.&lt;/p>
&lt;p>Given that these businesses depend on food delivery platforms and they don&amp;rsquo;t actually have a retail location I wonder if these restaurants are creating multiple listings on these platforms. This gives them another spot in the search results and allows them to crowd out their competitors. Of course, each would have a reduced number of orders and thus show up lower in the results but it still feels as if there&amp;rsquo;s potential for some gaming.&lt;/p></description></item><item><title>Power of media</title><link>/2019/12/31/power-of-media/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/31/power-of-media/</guid><description>&lt;p>Despite the fact that the Marvel news about a transgender character is not true it reinforced how significant media is to progress and growth of culture. Adults are set in their ways and are less impressionable than children. By exposing children to a more accepting world we help make it a reality. It&amp;rsquo;s unfortunate that this news turned out to be false but the fact that it was even believable is a sign that we&amp;rsquo;re making progress. I have no doubt that it will happen and it&amp;rsquo;s only a question of when.&lt;/p></description></item><item><title>Self hosted, externally managed</title><link>/2019/12/31/self-hosted-externally-managed/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/31/self-hosted-externally-managed/</guid><description>&lt;p>A couple of days ago I &lt;a href="/2019/12/27/whos-hosting-whos-managing/">wrote&lt;/a> about two questions one can ask when it comes to working with a technology vendor: who&amp;rsquo;s hosting and who&amp;rsquo;s managing?&lt;/p>
&lt;p>I find the externally managed but self hosted one interesting. You&amp;rsquo;re giving another company access to your accounts and have full visibility into what they&amp;rsquo;re doing yet you still pay them. An example of this is &lt;a href="https://databricks.com/">Databricks&lt;/a>. They offer a product akin to Spark-as-a-service along with a variety of bells and whistles that make it much easier to run and manage.&lt;/p></description></item><item><title>Zappos customer service</title><link>/2019/12/30/zappos-customer-service/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/30/zappos-customer-service/</guid><description>&lt;p>The outcome of the Zappos class action settlement was 10% of an order that needs to be made by January 2020. That&amp;rsquo;s better than nothing so I decided to take them up on it and the entire experience left me with an even deeper appreciation of the Zappos customers service. They&amp;rsquo;re one of the few e-commerce companies that built a reputation on exceptional customer service and it&amp;rsquo;s impressive that they&amp;rsquo;re still able to maintain it 20 years in.&lt;/p></description></item><item><title>Finding parked cars: AWS vs Google computer vision</title><link>/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision/</link><pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/29/finding-parked-cars-aws-vs-google-computer-vision/</guid><description>&lt;p>As part of the newly rebooted &lt;a href="http://dangoldin.com/2019/12/26/parking-spot-finder/">Parking Spot Finder&lt;/a> project I started poking around the cloud computer vision services to see how well they&amp;rsquo;d do at identifying parked cars from an &amp;ldquo;apartment view.&amp;rdquo; The image is pretty straightforward and shows two intersecting streets with a single available parking spot. I didn&amp;rsquo;t expect these services to identify the spot but my hope was that they&amp;rsquo;d be able to identify the cars themselves. For this test I tried both Google&amp;rsquo;s &lt;a href="https://cloud.google.com/vision/">Vision&lt;/a> and Amazon &lt;a href="https://aws.amazon.com/rekognition/">Rekognition&lt;/a>. Google worked much better but still required a few tweaks to get what I wanted.&lt;/p></description></item><item><title>Gboard vs iOS swipe keyboard</title><link>/2019/12/28/gboard-vs-ios-swipe-keyboard/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/28/gboard-vs-ios-swipe-keyboard/</guid><description>&lt;img src="/image/keyboard-ios.jpg" alt="Gboard keyboard" data-width="828" data-height="852" data-layout="intrinsic" />
&lt;img src="/image/keyboard-ios.jpg" alt="Gboard keyboard" data-width="828" data-height="852" data-layout="intrinsic" />
&lt;p>Now that I&amp;rsquo;ve been blogging &lt;a href="http://dangoldin.com/2019/12/24/write-mobile-edit-desktop/">left-handed on my phone&lt;/a> for the post week I got the chance to try out multiple iOS keyboards all with the hope of improving my typing speed. So far the best one had been &lt;a href="https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US">Gboard&lt;/a> using the swipe. It&amp;rsquo;s pretty similar to the native swipe keyboard offered within iOS but it&amp;rsquo;s just a tad more accurate - especially for shorter words.&lt;/p></description></item><item><title>Drop in piracy, drop in accessibility</title><link>/2019/12/27/drop-in-piracy-drop-in-accessibility/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/27/drop-in-piracy-drop-in-accessibility/</guid><description>&lt;p>A few days ago I came across a tweet that was making the rounds:&lt;/p>
&lt;amp-twitter width="609" height="274" layout="responsive" data-tweetid="1209255772250611712">
&lt;/amp-twitter>
&lt;p>I agree with the sentiment and know how much I benefited from having access to software as a kid. It got me both interested in computers and gave me a head start on writing code.&lt;/p>
&lt;p>The one point that&amp;rsquo;s missed here is how difficult it is to pirate software these days. In the early days of the internet it would take a very, very long time to download applications but nearly all were piratable - even if it did require running some obscure code and editing hex files. These days, nearly all substantial software is subscription based which makes it incredibly difficult, and potentially impossible, to pirate. This doesn&amp;rsquo;t affect the well-off but it becomes more and more inaccessible for those that just can&amp;rsquo;t afford it.&lt;/p></description></item><item><title>Who's hosting? Who's managing?</title><link>/2019/12/27/whos-hosting-whos-managing/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/27/whos-hosting-whos-managing/</guid><description>&lt;p>The end of the year is when companies get inundated with a ton of vendors trying to hit their sales goals. Given this volume comparisons start to become easier and patterns start to emerge. One that I noticed is the gamut of hosting options as well as managed options.&lt;/p>
&lt;p>For hosting options, you can choose either the self-hosted option where the software is deployed to your own infrastructure. Alternatively, you can pay the vendor to deploy the code to their infrastructure and expose it through the necessary interfaces.&lt;/p></description></item><item><title>In praise of the simple tech stack</title><link>/2019/12/26/in-praise-of-the-simple-tech-stack/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/26/in-praise-of-the-simple-tech-stack/</guid><description>&lt;p>These days it&amp;rsquo;s so easy to feel as if you&amp;rsquo;re falling behind the tech curve. There&amp;rsquo;s a new tool or technology being released every day and there&amp;rsquo;s a strong temptation to try everything lest you fall behind.&lt;/p>
&lt;p>That&amp;rsquo;s why it was so refreshing to come across &amp;ldquo;&lt;a href="https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b">The boring technology behind a one-person Internet company&lt;/a>.&amp;rdquo; It&amp;rsquo;s a post written by &lt;a href="https://broadcast.listennotes.com/@wenbinf">Wenbin Fang&lt;/a>, the founder and CEO of &lt;a href="https://www.listennotes.com/">ListenNotes.com&lt;/a>, and is exactly what you think it is.&lt;/p></description></item><item><title>Parking spot finder</title><link>/2019/12/26/parking-spot-finder/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/26/parking-spot-finder/</guid><description>&lt;img src="/image/jc-street-parking.jpg" alt="Jersey City street" data-width="1314" data-height="986" data-layout="responsive" />
&lt;p>I live in Jersey City and rely on street parking rather than paying for a lot. Jersey City also some aggressive alternate side parking rules with many streets having twice-a-week cleanings which in the worst case requires me to move my car 4 times a week.&lt;/p>
&lt;p>A few years ago I decided to build a simple tool that would be hooked up to a camera and would notify me whenever a spot became available. I unfortunately never finished that up and am hoping that by posting it publicly it will act as a motivator and hold me accountable.&lt;/p></description></item><item><title>Increasing creativity</title><link>/2019/12/25/increasing-creativity/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/25/increasing-creativity/</guid><description>&lt;p>I’ve been writing the rough drafts of my recent posts on my phone and this shift in environment has made me more creative. Maybe it’s due to the slower typing speed, the lack of distraction, the novelty, or just self-deception but it feels as if some of my previous rough drafts have had both more interesting thoughts, analogies, and wordsmithing. For example, &lt;a href="http://dangoldin.com/2019/12/23/iphone-11-battery/">comparing the poor battery life&lt;/a> of my previous phone to checking out at a car&amp;rsquo;s gas tank before a trip or titling a post &amp;ldquo;&lt;a href="http://dangoldin.com/2019/12/24/write-mobile-edit-desktop/">Write mobile, edit desktop&lt;/a>.&amp;rdquo; It&amp;rsquo;s impossible to know if I would have been able to do that if I were on my laptop but it really does seem as if the new environment had something to do with it.&lt;/p></description></item><item><title>Write mobile, edit desktop</title><link>/2019/12/24/write-mobile-edit-desktop/</link><pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/24/write-mobile-edit-desktop/</guid><description>&lt;p>Similar to last year I&amp;rsquo;m behind on my blogging and need to write a dozen more posts in the next week to hit my annual blogging goal of two a week. At the same time I don&amp;rsquo;t have the luxury of time. The approach I&amp;rsquo;ve settled on is to write up the rough posts one handed on my phone and then edit them on an actual computer when I can. I have about a half dozen rough drafts in this state and will be cleaning them up over the next few days in addition to the others I need to write.&lt;/p></description></item><item><title>iPhone 11 battery</title><link>/2019/12/23/iphone-11-battery/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/23/iphone-11-battery/</guid><description>&lt;p>When the new iPhones were announced I quickly made the decision to upgrade from my 6S. I resisted an upgrade for as long as I could but what finally got me over the hump was not any of the features, as incredible as they are, but instead the need for a longer battery life.&lt;/p>
&lt;p>My 6S couldn’t last a few hours of limited usage without running out of power. Similar to how someone will look at their gas tank before leaving on a trip I was doing the same for my phone’s battery charge any time I&amp;rsquo;d be away from a charger. It’s a surprisingly stressful and not healthy way to be and sadly highlights our dependence on always being connected.&lt;/p></description></item><item><title>Netflix's CEO transition</title><link>/2019/12/22/netflixs-ceo-transition/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/22/netflixs-ceo-transition/</guid><description>&lt;p>While browsing Quora I came across a neat answer by Marc Randolph, the founder of Netflix, to the question &amp;ldquo;What led you to step down as Netflix’s CEO?&amp;rdquo; I didn&amp;rsquo;t even know Netflix had a founder other than Reed Hastings and beyond that didn&amp;rsquo;t know that there was a CEO transition that early in its history.&lt;/p>
&lt;p>Netflix is an incredibly dominant company now but it took more than two decades to get there. Netflix is actually older (founded in 1997) than both Google (1998) and Facebook (2004). The anecdote is remarkable since it shows a glimpse of the Netflix culture and how radical honesty was a key part from the very beginning. I&amp;rsquo;m impressed by both Reed&amp;rsquo;s ability to be that directly honest but also communicate in a way that kept Marc aligned and engaged. It&amp;rsquo;s an incredibly difficult thing to do - being able to have someone accept that they&amp;rsquo;re not not effective at their job - and a skill I continue to grow and develop. You need strong trust on both sides and the ability to let go of the ego. A good leader can make that happen.&lt;/p></description></item><item><title>DeAMPifying my blog</title><link>/2019/12/21/deampifying-my-blog/</link><pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/21/deampifying-my-blog/</guid><description>&lt;p>Since &lt;a href="http://dangoldin.com/2016/09/05/ampifying-my-blog/">September of 2016&lt;/a> this blog has been exclusively (both mobile and desktop) in &lt;a href="https://amp.dev/">AMP&lt;/a>. I initially decided to go with it due to a combination of curiosity, liking the concept, and a desire to revamp the blog. I had to resolve a variety of challenges ranging from supporting Disqus comments to migrating all the img tags to speeding up the build times. Most of these issues have been resolved for a while now and yet I&amp;rsquo;m tempted to ditch AMP and go back to a simple HTML and CSS site.&lt;/p></description></item><item><title>Privacy updates and emission standards</title><link>/2019/12/20/privacy-updates-and-emission-standards/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/20/privacy-updates-and-emission-standards/</guid><description>&lt;p>It’s that special time of the year when a whole slew of companies are sending out their privacy policy emails. Last year the culprit was GDPR, a European privacy law, and this year it&amp;rsquo;s the CCPA, the privacy protection bill from California. Given the onslaught most users will ignore these updates. These regulations seem to be having the desired effect. A bold company may choose to have separate privacy policies per geography to reduce the coverage of these laws but it looks as if most companies have just updated their general privacy policies. Europe and California are driving fairly significant change across the tech industry.&lt;/p></description></item><item><title>Class action settlements</title><link>/2019/12/19/class-action-settlements/</link><pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/19/class-action-settlements/</guid><description>&lt;p>Earlier this month I received an email from Zappos with the subject &amp;ldquo;Settlement Notification re: 2012 Security Incident.&amp;rdquo; Sure enough, Zappos had a security issue in 2012 and the result of the class action lawsuit was that they&amp;rsquo;d give their affected customers a 10% off coupon on any order placed before January. As far as the typical class action settlement this was a surprisingly fair offer since it is something I&amp;rsquo;d actually use.&lt;/p></description></item><item><title>Shoulders of giants</title><link>/2019/12/18/shoulders-of-giants/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/18/shoulders-of-giants/</guid><description>&lt;p>A few days ago I came across a &lt;a href="https://www.thespacereview.com/article/3833/1">first hand account&lt;/a> from an engineer working on some early space programs and it made me realize how little hardware I know. It&amp;rsquo;s likely a case of the grass being greener but hardware feels like a whole new exciting world compared to software. Rather than dealing with the abstract world of software it&amp;rsquo;s dealing with the real world.&lt;/p>
&lt;p>The above article is an unfair one to use since anything related to space is going to be extra exciting but it highlights the depth and creativity necessary to work in hardware. Even in the 80s and 90s a software engineer needed to have a decent grasp of the underlying hardware to be productive. This is no longer the case. One can go through a career without understanding how computers actually work behind the scenes. That&amp;rsquo;s not necessarily a bad thing: the field is so wild with so much specialization that it&amp;rsquo;s difficult to have a good grasp of everything and we have to pick what we want to master.&lt;/p></description></item><item><title>2019 donations</title><link>/2019/12/17/2019-donations/</link><pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/17/2019-donations/</guid><description>&lt;p>It&amp;rsquo;s that time of the year where I make my annual donations. I&amp;rsquo;ve settled on a core set of foundations and charities that I donate to but occasionally have an outlier depending on what&amp;rsquo;s happening in the world and this year was no different.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.eff.org/">EFF&lt;/a>. Being in technology I&amp;rsquo;m more aware than most of the perils of technology and the EFF is at the forefront of defending digital privacy and rights in a constantly threatened world.&lt;/li>
&lt;li>&lt;a href="https://www.aclu.org/">ACLU&lt;/a>. Similar to the EFF but focused more broadly on all rights. Especially these days it&amp;rsquo;s critical to challenge unfair laws and fight for the bill of rights.&lt;/li>
&lt;li>&lt;a href="https://wikimediafoundation.org/">Wikipedia&lt;/a>. I can&amp;rsquo;t live without Wikipedia and use it all the time so it&amp;rsquo;s only fair that I donate some to keep it going.&lt;/li>
&lt;li>&lt;a href="https://www.plannedparenthood.org/">Planned Parenthood&lt;/a>. It&amp;rsquo;s a shame that we live in a world where Planned Parenthood even exists but until the day it&amp;rsquo;s unnecessary I&amp;rsquo;ll keep on donating. It provides incredibly valuable healthcare to remote areas and there are countless stories of them saving lives by being the only resource in a location.&lt;/li>
&lt;li>&lt;a href="https://amymcgrath.com/">Amy McGrath&lt;/a>. I try to stay away from politics but the current state makes me feel negligent not donating. Amy is running for the Kentucky senate seat against Mitch McConnell. So many of today&amp;rsquo;s problems can be attributed to McConnell and the sooner he&amp;rsquo;s out of office the sooner the United States can get back on track.&lt;/li>
&lt;/ul></description></item><item><title>Disney's hostile takeover</title><link>/2019/12/16/disneys-hostile-takeover/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/16/disneys-hostile-takeover/</guid><description>&lt;p>While watching &lt;a href="https://en.wikipedia.org/wiki/The_Imagineering_Story">The Imagineering Story&lt;/a> I discovered that Disney was the target of a hostile takeover attempt in the 1980s. This is shocking given how dominant Disney currently is and it&amp;rsquo;s incredible that it came that close to being broken up and sold as individual pieces. Luckily, Disney withstood the attempt but this was a wakeup call that led to Michael Eisner rekindling the Disney magic.&lt;/p>
&lt;p>There&amp;rsquo;s a &lt;a href="https://www.upi.com/Archives/1984/06/11/Walt-Disney-Productions-ended-financier-Saul-Steinbergs-takeover-attempt/1284455774400/">ton written&lt;/a> about the attempt by people better suited but what I find fascinating is that the entire breakup attempt goes against the entire strategy Walt Disney put together in 1957. The entire strategy is incredible and is still in play right now. The core are the movies and shows that Disney produces which then drive every other channel. The power of strong and compelling content creates adjacent opportunities that then feed the next generation of great content. It&amp;rsquo;s sad and scary to think what would have happened had Disney been broken up.&lt;/p></description></item><item><title>TabNine</title><link>/2019/12/15/tabnine/</link><pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/15/tabnine/</guid><description>&lt;p>I’m a heavy Visual Studio Code user and one of my favorite plugins has been &lt;a href="https://tabnine.com/">TabNine&lt;/a>. I use VS Code to hack around across a variety of languages in addition to note-taking and TabNine makes me significantly more productive. It&amp;rsquo;s a language-agnostic autocompleter that relies on machine learning rather than predefined heuristics and analyzes whatever you type in order to offer you a variety of suggestions. It&amp;rsquo;s similar to GMail&amp;rsquo;s new autocomplete functionality but in your editor.&lt;/p></description></item><item><title>Avoiding the mouse</title><link>/2019/12/14/avoiding-the-mouse/</link><pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/14/avoiding-the-mouse/</guid><description>&lt;p>I spent a ton of time in Excel back in my consulting and finance days. One of the most common refrains was that to be more efficient one had to do everything with the keyboard: no mouse allowed. The idea here that keyboard shortcuts are going to be much faster than navigating menus with a mouse.&lt;/p>
&lt;p>You don’t hear this too often in software engineering but it applies just as much. Nearly every tool we use can be controlled via shortcuts yet too often we&amp;rsquo;re gravitating towards the mouse. Keyboard shortcuts aren&amp;rsquo;t intuitive and it takes time to get the muscle memory down but it&amp;rsquo;s all worth it once it&amp;rsquo;s mastered. It&amp;rsquo;s incredible seeing people work who have this level of mastery - it&amp;rsquo;s akin to watching piano virtuosos performing complex symphonies. Even if we don&amp;rsquo;t get to that level nearly anyone can benefit from at least having the basic shortcuts down. Superhuman is an incredibly successful product and a big part is the focus on making users more efficient through clever and powerful shortcuts.&lt;/p></description></item><item><title>Disney+</title><link>/2019/12/13/disney-/</link><pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/13/disney-/</guid><description>&lt;p>I signed up for the Disney+ account as soon as it launched. The primary reason was for The Mandalorian but I&amp;rsquo;m also enjoying The Imagineering Story and access to The Simpsons archive. My biggest surprise was how polished it all was. It&amp;rsquo;s a brand new product and it&amp;rsquo;s impressive what the team has done. It had a few hiccups at launch has but has been extremely smooth since then.&lt;/p>
&lt;p>I use it on the Fire TV Stick so can’t speak to the other platforms but it&amp;rsquo;s simpler, and yet more polished than Netflix. It doesn’t have any of the autoplay nonsense that Netflix and Amazon Prime shove down your throat. It also keeps the order of shows consistent across sessions which makes navigation a breeze. Yet my favorite feature so far is the keyboard less login and being able to do it solely from your phone. It’s such a simple thing but makes the onboarding experience orders of magnitude better versus Netflix where you&amp;rsquo;re forced to type on a TV keyboard through a remote. I&amp;rsquo;m honestly surprised Netflix still hasn’t implemented this given how often the Netflix app decides to log me out.&lt;/p></description></item><item><title>2019 Predictions in review</title><link>/2019/12/12/2019-predictions-in-review/</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/12/2019-predictions-in-review/</guid><description>&lt;p>At the end of 2018 I made &lt;a href="http://dangoldin.com/2018/12/31/2019-predictions/">four predictions for 2019&lt;/a> and now that we&amp;rsquo;re nearing the end of the year it&amp;rsquo;s time to see how accurate they were. It&amp;rsquo;s a valuable exercise to review them - both to hold myself accountable but also to get a glimpse of how I thought about the world a year ago. It&amp;rsquo;s also a way to understand the biases I fall into and use that information to hopefully improve my predictions for 2020.&lt;/p></description></item><item><title>Search within Google search</title><link>/2019/12/11/search-within-google-search/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/11/search-within-google-search/</guid><description>&lt;p>Google Search has evolved quite a bit from a single search bar and now contains a ton of modules to give users what they want as quickly as they can. This manifests itself in sections optimized for different verticals: sports, travel, shopping, weather, Wikipedia, and really anything that can be somewhat structured.&lt;/p>
&lt;img src="/image/search-within-google-search.png" alt="Search within Google search" data-width="719" data-height="408" data-layout="responsive" />
&lt;p>I discovered a new module yesterday when doing a search for an online thesaurus. Rather than just have a link to thesaurus.com, Google showed a search field that is hooked up directly to the thesaraus.com search. I don’t know if this was done solely by Google or required some collaboration but it’s another way Google is delivering more immediate answers and further capturing user attention.&lt;/p></description></item><item><title>Strive for linear complexity</title><link>/2019/12/11/strive-for-linear-complexity/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/11/strive-for-linear-complexity/</guid><description>&lt;p>Good software is dynamic and evolves as the requirements grow. The code usually starts clean, elegant, and simple but inevitably grows in complexity. The challenge is to manage that growing complexity. My approach is to strive to make that complexity linear rather than multiplicative. A multiplicative approach is one where a new requirement forces you to think through other parts of your code and determine what needs to change. A linear approach is one where this additional requirement forces you to think through others in its class but not much beyond that. A simple way to visualize the difference is through if statements. A codebase with multiplicative complexity has a ton of nested ifs where writing new code requires you to understand each of the other if-else branches. A linear complexity codebase would still require you to think through a new behavior but the complexity would be limited to a flat if block.&lt;/p></description></item><item><title>Solve categories, not individual problems</title><link>/2019/12/10/solve-categories-not-individual-problems/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>/2019/12/10/solve-categories-not-individual-problems/</guid><description>&lt;p>One of the most valuable abilities in modern software development is the ability to solve entire categories of issues instead of the immediate problem in front of us. It&amp;rsquo;s simpler and tempting to solve what&amp;rsquo;s in front of us so and then move on. It takes discipline and thoroughness to understand the problem, diagnose its cause, and prevent similar problems from arising in the future. Yet that&amp;rsquo;s the way to make an impact.&lt;/p></description></item><item><title>My TI-89</title><link>/2019/11/29/my-ti-89/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/29/my-ti-89/</guid><description>&lt;p>A &lt;a href="https://news.ycombinator.com/item?id=21638980">Hacker News thread&lt;/a> hit a nostalgic moment for me and brought back all my memories of my TI-89 that lasted me throughout high school and college. I unfortunately have no idea where it is and have very little use for one now but it was an incredible machine.&lt;/p>
&lt;p>It came with a built in solver and an algebraic manipulation library which made solving all sorts of problems a breeze. Rather than being purely decimal based it also expressed irrational and rational numbers which came in incredibly healthy for many tests. For some unfathomable reason it was somehow allowed on the SATs which made them that much easier.&lt;/p></description></item><item><title>Thanksgiving</title><link>/2019/11/28/thanksgiving/</link><pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/28/thanksgiving/</guid><description>&lt;p>Despite Black Friday being my least favorite day of the year Thanksgiving is by far my favorite holiday. It’s completely secular and is about celebrating everything we have with the people closest to us. Rather than travel somewhere new we instead go to places we’ve already been to to see the same people we see every year.&lt;/p>
&lt;p>It’s a chance for us to slow down and really appreciate everything we have. I arrived to the United States as kid after family emigrated to seek a better life and I’m eternally grateful that they took that risk. We didn’t start with much but more than many others and Thanksgiving is my time to acknowledge the sacrifices my family has made while being thankful that the United States, despite its problems, remains a land of opportunity.&lt;/p></description></item><item><title>Improving product reviews</title><link>/2019/11/24/improving-product-reviews/</link><pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/24/improving-product-reviews/</guid><description>&lt;p>Modern ecommerce is predicated on having proper product reviews. Every site that sells online depends on them and yet they&amp;rsquo;re still far from what they can be. The reviews I&amp;rsquo;ve seen tend to be done when a product arrives rather than after years of use. Similarly, manufactures may change and shift production over time so early reviews will highlight the high quality of a product while latter reviews will bemoan the lack of quality.&lt;/p></description></item><item><title>Optimizing long running SQL queries</title><link>/2019/11/23/optimizing-long-running-sql-queries/</link><pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/23/optimizing-long-running-sql-queries/</guid><description>&lt;p>A year ago I read a &lt;a href="https://www.spinellis.gr/blog/20180805/">post&lt;/a> on how someone took a SQL query that took 380 hours to run and by offloading some of the work to shell commands was able to get the run time down to 12 hours. I&amp;rsquo;m a huge fan of the shell and believe it&amp;rsquo;s underutilized by most engineers yet I can&amp;rsquo;t get over the fact that a query still takes 12 hours to run. I have very little context around the data, the query, or the join but my approach would be to see if there&amp;rsquo;s a different toolset to solve the problem. Running a 12 hour query is dangerous - a disruption will require the query to be run from scratch and the affected tables are locked for that amount of time. It&amp;rsquo;s likely the case that this project doesn&amp;rsquo;t need anything better but even then there should be ways of improving the approach.&lt;/p></description></item><item><title>SMS based password manager</title><link>/2019/11/19/sms-based-password-manager/</link><pubDate>Tue, 19 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/19/sms-based-password-manager/</guid><description>&lt;p>Two Fridays ago I wrote about my &lt;a href="http://dangoldin.com/2019/11/08/dyi-password-manager/">home-built password manager&lt;/a> and while working well on a computer it was a pain to use on mobile. I&amp;rsquo;ve been trying to think of a more friendly way of supporting this on mobile and came up with the idea of retrieving the passwords via SMS. The workflow would be to send a text to a number with a search term and then get the password back as a response. Twilio made it surprisingly easy to build a simple proof of concept (&lt;a href="https://github.com/dangoldin/password-manager">GitHub code&lt;/a>) and the resulting code is fewer than 50 lines and supports the use case.&lt;/p></description></item><item><title>Rebirth of digital piracy?</title><link>/2019/11/17/rebirth-of-digital-piracy/</link><pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/17/rebirth-of-digital-piracy/</guid><description>&lt;p>The best jokes have an element of truth to them and earlier today I came across a comic that was a great example.&lt;/p>
&lt;img src="/image/streaming-piracy.jpg" alt="Streaming piracy comic" data-width="700" data-height="596" data-layout="responsive" />
&lt;p class="caption">Source: &lt;a href="https://www.reddit.com/r/comics/comments/bcdlbf/hello_old_friend_oc/">Hoppy_Doodle @ reddit.com&lt;/a>&lt;/p>
&lt;p>It&amp;rsquo;s surprising how true that is. Many of us have been cable TV free for years and were content paying a monthly internet bill along with Netflix and maybe one or two additional subscriptions. Now it feels as if there&amp;rsquo;s a new streaming service launching every few months with the costs adding up to be equivalent to a traditional cable TV bill.&lt;/p></description></item><item><title>DYI password manager</title><link>/2019/11/08/dyi-password-manager/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/08/dyi-password-manager/</guid><description>&lt;p>I prefer plaintext for all my note taking. Text files are extremely portable and serve as a flexible foundation for anything I&amp;rsquo;d want to do. For example, I can use grep to perform complex regex searches across thousands of files. If I want to apply a bulk operation to my notes I can write a quick script to do so. This is all possible because there&amp;rsquo;s no proprietary format backing them and I&amp;rsquo;m able to leverage the power of the command line.&lt;/p></description></item><item><title>Importing resources into terraform</title><link>/2019/11/06/importing-resources-into-terraform/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/06/importing-resources-into-terraform/</guid><description>&lt;p>You don&amp;rsquo;t appreciate &lt;a href="https://www.terraform.io">Terraform&lt;/a> until you have hundreds of AWS resources with dozens of security rules. But once you develop that appreciate it becomes an indispensable tool in managing and scaling your infrastructure. I&amp;rsquo;m a huge fan of Terraform and recently started moving a bunch of my personal project resources into Terraform.&lt;/p>
&lt;p>I imagine for most people it&amp;rsquo;s a chore but I find the entire process zen-like. I take something that was a mess and clean it up while learning something new. My approach has been to create a placeholder resource and then do a &amp;ldquo;terraform import&amp;rdquo; to fetch the properties. Then I go through a series of &amp;ldquo;terraform plan&amp;quot;s to reconcile the differences.&lt;/p></description></item><item><title>Upgrade your libraries</title><link>/2019/11/04/upgrade-your-libraries/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/04/upgrade-your-libraries/</guid><description>&lt;p>This blog is &lt;a href="https://github.com/dangoldin/blog.dangoldin.com">hosted&lt;/a> on GitHub and built using &lt;a href="https://jekyllrb.com/">Jekyll&lt;/a>. Jekyll is a simple static site generator that&amp;rsquo;s been working well for me and was flexible enough to allow me to switch the entire site over to &lt;a href="https://amp.dev/">AMP&lt;/a>. Unfortunately, the switch to AMP led to the site generation becoming significant slower due to the CSS-inlining requirement. Two years ago I started profiling and &lt;a href="http://dangoldin.com/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/">dropped the site generation time&lt;/a> down to about 15 seconds from over 4 minutes by generating the CSS once and then reusing it for all future pages.&lt;/p></description></item><item><title>QR codes in China</title><link>/2019/11/02/qr-codes-in-china/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>/2019/11/02/qr-codes-in-china/</guid><description>&lt;p>I&amp;rsquo;m fascinated by the way mobile works in China. Based on everything I read it seems China is far ahead of the US in the way technology has been absorbed in the day-to-day fabric of society. I&amp;rsquo;m not making the case that that&amp;rsquo;s actually a better way to be but more of an observation of how different daily life can be when technology is embraced in nearly all areas. This is much easier with China with more centralized control and a few companies, namely Tencent and Alibaba, that have monopolies over every device.&lt;/p></description></item><item><title>Computer history memos</title><link>/2019/10/29/computer-history-memos/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/29/computer-history-memos/</guid><description>&lt;p>I&amp;rsquo;m a fan of tech industry history and came across a neat trove of &lt;a href="https://sriramk.com/memos">memos&lt;/a> compiled by &lt;a href="https://twitter.com/sriramk">Sriram Krishnan&lt;/a>. The memos cover a wide time period and are first party accounts by the people actually involved. It&amp;rsquo;s easy to judge historical decisions with the gift of knowledge but reading these teleports you back to the past and gives you an appreciation for the challenges they faced.&lt;/p>
&lt;p>I only wish there were more of these memos floating around. Seminal ones usually make their way in the public domain one way or another but there&amp;rsquo;s a huge trove of second-tier memos, notes, and emails that would provide tech historians a trove of information if only they became accessible.&lt;/p></description></item><item><title>Deplying Docker using systemd</title><link>/2019/10/28/deplying-docker-using-systemd/</link><pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/28/deplying-docker-using-systemd/</guid><description>&lt;p>A few months ago in a burst of inspiration I converted a bunch of my side projects to run inside Docker. Unfortunately, I didn&amp;rsquo;t do the follow up work of actually creating a Kubernetes cluster and instead came up with a ridiculously hacky process to get them running. I created a simple shell script that would just build and run a Docker image and have just been running it inside a screen session.&lt;/p></description></item><item><title>GitHub's security automation</title><link>/2019/10/26/githubs-security-automation/</link><pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/26/githubs-security-automation/</guid><description>&lt;p>I have a variety of old projects up on GitHub and recently noticed pull requests being opened from &amp;ldquo;dependabot&amp;rdquo; to fix security vulnerabilities in old requirements files. Turns out it&amp;rsquo;s a &lt;a href="https://help.github.com/en/github/managing-security-vulnerabilities/configuring-automated-security-fixes">new feature&lt;/a> offered by GitHub that scans repos and submits patches to upgrade insecure versions. This is a great idea - there&amp;rsquo;s very little chance that I&amp;rsquo;d go through my old and unmaintained repos to upgrade their dependencies but I&amp;rsquo;d definitely approve and merge a PR that upgrades a few of my libraries.&lt;/p></description></item><item><title>File size histogram via the command line</title><link>/2019/10/20/file-size-histogram-via-the-command-line/</link><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/20/file-size-histogram-via-the-command-line/</guid><description>&lt;img src="/image/file-size-distribution-shell-script.png" alt="File size histogram using the command line" data-width="951" data-height="294" data-layout="responsive" />
&lt;p>I&amp;rsquo;m a sucker for a good shell command and recently discovered (via &lt;a href="https://superuser.com/questions/565443/generate-distribution-of-file-sizes-from-the-command-prompt">StackOverflow&lt;/a>) the most complex one yet - a one liner to generate a histogram of file sizes within a directory. The sizes are in powers of two but it&amp;rsquo;s a great way to get some simple summary statistics of files inside a directory. I still find awk mystifying to write but nearly every advanced shell command uses awk in some way. Most engineers thee days have a bias for a traditional scripting language but it&amp;rsquo;s still amazing what an awk one-liner can do.&lt;/p></description></item><item><title>Bypassing the incognito browser check on nytimes.com</title><link>/2019/10/13/bypassing-the-incognito-browser-check-on-nytimes.com/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/13/bypassing-the-incognito-browser-check-on-nytimes.com/</guid><description>&lt;img src="/image/nytimes-incognito-check.png" alt="The NY Times incognito check message" data-width="510" data-height="335" data-layout="responsive" />
&lt;p>For as long as the NY Times had a free article limit you were able to bypass it using incognito mode but it seems this is no longer the case. Earlier today I tried checking out &lt;a href="https://www.nytimes.com/2019/10/13/nyregion/14th-street-cars-banned.html">an article&lt;/a> in incognito mode and received a prompt to &amp;ldquo;Log in or create a free New York Times account to continue reading in private mode.&amp;rdquo; The loophole was fun while it lasted and glad they finally patched it up - despite the fact that I no longer have unlimited articles.&lt;/p></description></item><item><title>Online food delivery markups: Seamless vs UberEats vs DoorDash</title><link>/2019/10/12/online-food-delivery-markups-seamless-vs-ubereats-vs-doordash/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/12/online-food-delivery-markups-seamless-vs-ubereats-vs-doordash/</guid><description>&lt;p>There are a variety of online food ordering and delivery services out there but lately I&amp;rsquo;ve been a fan of DoorDash since the delivery time seems to be both quicker and more consistent than the others. Yesterday I ordered some &lt;a href="http://www.linglongxuannj.com/">Chinese food&lt;/a> and noticed that the restaurant had added a receipt with prices that differed from what DoorDash charged. In hindsight I&amp;rsquo;m not surprised but did feel a bit misled since DoorDash has separate service and delivery that I figured covered the actual service. After I overcame the feeling of being tricked I thought it would be interesting to see how whether other delivery services are doing the same thing and by how much. I looked at the items I ordered across DoorDash, UberEats, and Seamless/GrubHub. Solely looking at the prices it looks as if DoorDash has the highest markup, followed up UberEats, and then Seamless/GrubHub. Surprisingly, the cheapest item (Hot &amp;amp; Sour Soup) hsa no markup across any of the services and the percentage markup across all of them drops as the item price increases.&lt;/p></description></item><item><title>Combining Excel and the command line</title><link>/2019/10/11/combining-excel-and-the-command-line/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/11/combining-excel-and-the-command-line/</guid><description>&lt;p>I was lucky that my first job caused me to learn Excel. I was also lucky that subsequent jobs helped me discover the power of the command line. Combining both of these made me incredibly productive and are an amazing tool to have in your arsenal.&lt;/p>
&lt;p>As engineers, we&amp;rsquo;re paid for solving problems. While most software engineering problems require designing and building for the long term there&amp;rsquo;s a whole category of problems that are just &amp;ldquo;throwaway work&amp;rdquo; and the goal is to get to a solution as quickly as possible and move on. Excel and the command line are designed for this.&lt;/p></description></item><item><title>Visualizing my Twitter archive - 2019 edition</title><link>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/07/visualizing-my-twitter-archive-2019-edition/</guid><description>&lt;p>GitHub sent me an alert this past weekend that a bunch of my repos were using old libraries that had security vulnerabilities. Nearly all of them were due to my usage of an old version of the &lt;a href="https://pypi.org/project/requests/">requests&lt;/a> library. Updating those was as simple as updating the requirements.txt file to the new version.&lt;/p>
&lt;p>One of these repos, &lt;a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis&lt;/a>, is my most popular project on GitHub so I thought I might as well revisit it and see if I could both address the vulnerabilities and get it running again. Upgrading the packages was straightforward but there are very few things more humbling than looking at the code you&amp;rsquo;ve written years ago. Twitter changed the format of the archive from JSON to CSV since the last time I ran the code and as part of the upgrade I did a little bit of cleanup. The &lt;a href="https://github.com/dangoldin/twitter-archive-analysis/blob/master/analyze.py">code&lt;/a> is up on GitHub and I&amp;rsquo;ve included the visualizations it generated below highlighting my Twitter behavior over the years.&lt;/p></description></item><item><title>Make sure you're understood</title><link>/2019/10/06/make-sure-youre-understood/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/06/make-sure-youre-understood/</guid><description>&lt;a href="https://twitter.com/rnoweber/status/1180519525885759488">
 &lt;img src="/image/understood.jpg" alt="Thoughts vs understanding" data-width="668" data-height="680" data-layout="responsive" />
&lt;/a>
&lt;p>While browsing Twitter I saw a simple visualization that spoke to me. Communication is what makes us human and good communication is critical to strong teams. Historically, I&amp;rsquo;ve definitely fallen in assuming that when I spoke what people understood was exactly what I thought and was both surprised and disappointed when that wasn&amp;rsquo;t the case.&lt;/p></description></item><item><title>Upgrading pip packages within a Dockerfile</title><link>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/03/upgrading-pip-packages-within-a-dockerfile/</guid><description>&lt;p>I have an old project, &lt;a href="http://makersalley.com">makersalley.com&lt;/a>, that used to run on an old version of Python (2.7) and an archaic version of Django (1.4). Earlier this year I overhauled it to run on a newer version of Django (1.11) and Dockerized the entire setup which required all sorts of changes and library fixes.&lt;/p>
&lt;p>Last night, I took it one step further by upgrading it to the latest versions of both Python (3.7) and Django (2.2). The most difficult part was figuring out how to upgrade to the latest versions while being tied down to Docker. For example, changing the Python version in the Dockerfile caused many of the packages in requirements.txt to not build but because they were all executed within Docker I had to get the entire requirements.txt fixed before that step would succeed. Similarly, it wasn’t clear which versions of the packages in the requirements.txt depended on one another and upgrading all of them blindly would have been a fool&amp;rsquo;s errand. It also turned out that one package, MySQL-python, was not available in Python 3+.&lt;/p></description></item><item><title>Visualizing Kafka partition changes</title><link>/2019/10/01/visualizing-kafka-partition-changes/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>/2019/10/01/visualizing-kafka-partition-changes/</guid><description>&lt;p>Earlier this week we scaled up our Kafka cluster to take advantage of more availability zones and increase the replication for some of our key topics. After making sure the new brokers joined the existing cluster we needed to redo the partitioning to take advantage of these newly available brokers.&lt;/p>
&lt;p>I&amp;rsquo;m sure there are better and more modern tools out there but we&amp;rsquo;ve been using &lt;a href="https://github.com/SiftScience/kafka-assigner">SiftScience&amp;rsquo;s kafka-assigner&lt;/a>. Rather than being a naive partitioning it looks at the existing assignments and optimizes the new assignment to minimize the number of moves while striving to keep the partitions evenly distributed across all brokers.&lt;/p></description></item><item><title>SSO and SAAS pricing</title><link>/2019/09/28/sso-and-saas-pricing/</link><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/28/sso-and-saas-pricing/</guid><description>&lt;img src="/image/dropbox-pricing.png" alt="Dropbox business pricing" data-width="604" data-height="831" data-layout="responsive" />
&lt;p>For a very long time I wondered why SSO was always listed as a premium feature across nearly every enterprise software pricing page but no more! When you’re small it’s simple to manage permissions and the costs of screwing up are far smaller. Large companies, on the other hand, have complex organization structures with hundreds of employees that use a wide range of applications and access controls are a real concern. It’s possible to manage that through a herculean manual effort but it’s so much easier to just pay someone to solve the problem for you. Along similar lines, it&amp;rsquo;s valuable to minimize the amount of SAAS services used across the organization to reduce both the complexity and the cost although these days it feels like a fool&amp;rsquo;s errand.&lt;/p></description></item><item><title>Free speech and the First Amendment</title><link>/2019/09/23/free-speech-and-the-first-amendment/</link><pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/23/free-speech-and-the-first-amendment/</guid><description>&lt;p>I recently finished &lt;a href="https://www.amazon.com/gp/product/B00AAYF8WG/ref=ppx_yo_dt_b_d_asin_title_o06?ie=UTF8&amp;amp;psc=1">The Great Dissent&lt;/a> which describes the thought process Supreme Court Justice &lt;a href="https://en.wikipedia.org/wiki/Oliver_Wendell_Holmes_Jr.">Oliver Wendell Holmes Jr&lt;/a> went through as he changed the interpretation of the First Amendment and free speech in the United States. These days we all take the scope of free speech for granted and assume its a static part of our democracy but it went through multiple iterations and will likely continue to do so - something I didn&amp;rsquo;t even realize.&lt;/p></description></item><item><title>Goldilocks and the three implementations</title><link>/2019/09/16/goldilocks-and-the-three-implementations/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/16/goldilocks-and-the-three-implementations/</guid><description>&lt;p>While browsing Twitter I came across the following tweet that resonated:&lt;/p>
&lt;amp-twitter width="609" height="274" layout="responsive" data-tweetid="1173706283859341312">
&lt;/amp-twitter>
&lt;p>A common refrain I have is that the best engineers are not the ones that write the most code but are instead making decisions that reduce the amount of code that needs to be written in the future. This tweet speaks to that since very often an implementation will either be too simple to support a future use or be over-implemented to support a potential future use case. The truth is often somewhere in between.&lt;/p></description></item><item><title>Google Sheets explore functionality</title><link>/2019/09/15/google-sheets-explore-functionality/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/15/google-sheets-explore-functionality/</guid><description>&lt;p>For the past few years I&amp;rsquo;ve been tracking a variety of daily metrics - ranging from sleep, to what I eat and drink, to my mood - in a Google spreadsheet. I have an annual tradition of analyzing and visualizing the data but I never go beyond the simple summary statistics. I always mean to do a deeper analysis but inevitably just run a script I barely touched in the past few years.&lt;/p></description></item><item><title>Solving Num: A combinatoric math game</title><link>/2019/09/14/solving-num-a-combinatoric-math-game/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/14/solving-num-a-combinatoric-math-game/</guid><description>&lt;img src="/image/num-level-68-combined.png" alt="Num puzzle unsolved and solved" data-width="1108" data-height="985" data-layout="responsive" />
&lt;p>I don&amp;rsquo;t play games on my phone but one game I keep going back to is &amp;ldquo;&lt;a href="https://apps.apple.com/us/app/num-insanely-hard-math-game/id861791129">Num&lt;/a>&amp;rdquo; - an &amp;ldquo;insanely hard math game.&amp;rdquo; The premise is pretty simple - you have a few numbers that you need to combine, using the four basic math operations, so it computes to a specific number.&lt;/p>
&lt;p>The levels start off simple but it gets more difficult with a lot of trial and error at the higher levels. They&amp;rsquo;re no longer simple expressions that can be evaluated fairly linearly but instead are solved by using fairly complex intermediate values.&lt;/p></description></item><item><title>In praise of tcpdump</title><link>/2019/09/13/in-praise-of-tcpdump/</link><pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/13/in-praise-of-tcpdump/</guid><description>&lt;p>We switched over to Prometheus and Grafana for our monitoring but some of our older systems are still on Graphite and StatsD. One of these is an alert for disk usage that started going off a few weeks ago. Over the course of the day it kept fluctuating from 100% disk usage to ~40% and whenever we dug into it we only saw the 40% number. Since StatsD is push based we assumed it was another instance that was submitting its metrics under the same key. Unfortunately, because StatsD is push based, it wasn&amp;rsquo;t clear which instance was actually doing the conflicting metrics push.&lt;/p></description></item><item><title>Where's the new iPhone SE?</title><link>/2019/09/12/wheres-the-new-iphone-se/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>/2019/09/12/wheres-the-new-iphone-se/</guid><description>&lt;p>After the new iPhone announcement there&amp;rsquo;s been a bunch of commentary around the desire to come up with a new version of the SE, a smaller factor phone. Fred Wilson posted a &lt;a href="https://avc.com/2019/09/bring-back-the-se/">short post&lt;/a> about it and it was quickly picked up and spread across various forums, such as &lt;a href="https://news.ycombinator.com/item?id=20931512">Hacker News&lt;/a>. The discussions generally ranged around the market size and how it&amp;rsquo;s not just worth it but I suspect a big part is the hardware.&lt;/p></description></item><item><title>Getting public keys from GitHub</title><link>/2019/08/27/getting-public-keys-from-github/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/27/getting-public-keys-from-github/</guid><description>&lt;p>I use DigitalOcean to host a variety of personal projects and every once in a while go through a &amp;ldquo;DevOps&amp;rdquo; spree where I upgrade everything I can. Last time around I switched them over to Ansible to make these sorts of updates easier. I ran this playbook earlier this week and discovered that one of the instances had an outdated public key and I no longer had SSH access.&lt;/p>
&lt;p>It would have been straightforward to go through the DigitalOcean flow of stopping the instance and launching it again with an updated key but I wanted to explore and see if I could find a way to avoid the reboot. After poking around the DigitalOcean UI I found a section that allowed to me to login to the terminal of the instance. I was somehow able to guess the password here (shows how secure my passwords are) but then ran into another problem: the paste command garbled the text as I was pasting it into the spoofed terminal. I tried a few alternatives but was unable to get public key added through the UI.&lt;/p></description></item><item><title>Explain the why in code reviews</title><link>/2019/08/25/explain-the-why-in-code-reviews/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/25/explain-the-why-in-code-reviews/</guid><description>&lt;p>This may be obvious to most people but it&amp;rsquo;s still worth reiterating: if you&amp;rsquo;re leaving a comment on a code review make sure to explain your reasoning. Code reviews are a key component in writing high quality code, improving everyone&amp;rsquo;s skills and knowledge, and encouraging a strong and collaborative team.&lt;/p>
&lt;p>Code reviews can be a bit clinical with feedback being blunt and a few words and it&amp;rsquo;s important to add context to all but the simplest of feedback. Experiences and backgrounds vary across the team and what&amp;rsquo;s obvious to one person may not be obvious to others. Similarly, it softens the feedback and encourages both sides to have a conversation. The act of describing the why also forces the reviewer to slow down and actually think about their reason for the change request. This may, in fact, reveal that the change request is not actually useful or may not actually have a good reason.&lt;/p></description></item><item><title>Disney+</title><link>/2019/08/24/disney-/</link><pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/24/disney-/</guid><description>&lt;p>Yesterday the internet blew up after Disney&amp;rsquo;s announcement of a new series, &lt;a href="https://en.wikipedia.org/wiki/The_Mandalorian">The Mandalorian&lt;/a>, that would be available as a part of their new streaming service, Disney+. I watched the &lt;a href="https://www.youtube.com/watch?v=aOC8E8z_ifw">trailer&lt;/a> and as expected it hit all the right Star Wars nostalgia points. The show itself wasn&amp;rsquo;t what stood out. Instead it was the closing scene where Disney highlights what Disney+ includes: Disney, Pixar, Marvel, Star Wars, and National Geographic.&lt;/p></description></item><item><title>What do airlines and LinkedIn have in common?</title><link>/2019/08/22/what-do-airlines-and-linkedin-have-in-common/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/22/what-do-airlines-and-linkedin-have-in-common/</guid><description>&lt;p>Supposedly airlines could make the economy seats much better than they are without a significant investment but purposefully do not in order to encourage people that can afford business or first class to choose business or first class. Perversely, this makes sense. If economy is only slightly worse than business then I&amp;rsquo;d be willing to save quite a bit of money by choosing economy for a worse experience. Yet if the economy seats were significantly worse then I&amp;rsquo;d pay a premium for business. The art is in figuring out what that threshold should be to fill every seat.&lt;/p></description></item><item><title>Yahoo fantasy football stats: 2019-2020 edition</title><link>/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/05/yahoo-fantasy-football-stats-2019-2020-edition/</guid><description>&lt;p>In what has become an annual tradition I&amp;rsquo;ve updated my Yahoo Fantasy Football scraping script for the 2019-2020 NFL season. The script works by logging into the Yahoo Fantasy Football site and downloading the first 4 pages of projected stats for each week. The code is up on &lt;a href="https://github.com/dangoldin/yahoo-ffl">GitHub&lt;/a> as well as the stats in a &lt;a href="https://github.com/dangoldin/yahoo-ffl/blob/master/stats-2020.csv">CSV file&lt;/a>.&lt;/p>
&lt;p>Every year there&amp;rsquo;s been something that Yahoo did to break my script and this year was no different. The first thing I discovered was the login broke despite the HTML elements staying the same - it turns out my login &amp;ldquo;click&amp;rdquo; event was actually clicking on an ad which was opening up a new window. Changing the code from a click to an enter solved that problem.&lt;/p></description></item><item><title>Diablo on the web</title><link>/2019/08/04/diablo-on-the-web/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>/2019/08/04/diablo-on-the-web/</guid><description>&lt;img src="/image/diablo.jpg" alt="Diablo game cover" data-width="400" data-height="300" data-layout="responsive" />
&lt;p>I spent more time than I should have playing the newly released JavaScript port of Diablo. I remember saving up to buy this game and poring over the manual for hours before actually being able to play it due to a single and shared family computer. Diablo was different than anything I had played before and I got hooked. It was also one of the first games I played that had online support. I never beat it without cheating but it was an incredible game to play and world to explore. It also led me to lose a chunk of my high school life to Diablo 2 but that&amp;rsquo;s another story.&lt;/p></description></item><item><title>Sports biographies as management books</title><link>/2019/07/29/sports-biographies-as-management-books/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/29/sports-biographies-as-management-books/</guid><description>&lt;p>If you know me, you&amp;rsquo;d know that I am not into watching sports and view being a fan as a waste of time. I understand why some people do it but it&amp;rsquo;s just not for me. At the same time, I&amp;rsquo;m a sucker for a good biography and really enjoy the biographies of coaches and players. Sports has a good balance of trying to improve as an individual while also competing for something as part of a team and sports biographies do a great job of walking that line.&lt;/p></description></item><item><title>Open sourcing configurations</title><link>/2019/07/28/open-sourcing-configurations/</link><pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/28/open-sourcing-configurations/</guid><description>&lt;p>Open source has been an incredible boon to software. It is the foundation of every modern startup and has allowed small teams to outbuild their larger rivals. As critical as open source software is I wish there was a single repository containing production deployment stats and configurations. For example, if you look at the &lt;a href="https://kafka.apache.org/documentation/#configuration">Kafka documentation&lt;/a> you quickly discover there are hundreds of options that can be set. Some matter more than others but it&amp;rsquo;s a larger undertaking to properly configure a cluster and understand the impact each of these configuration options will have.&lt;/p></description></item><item><title>GitHub blocks access from Iran</title><link>/2019/07/27/github-blocks-access-from-iran/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/27/github-blocks-access-from-iran/</guid><description>&lt;p>Earlier today GitHub &lt;a href="https://www.businessinsider.com/microsoft-owned-github-reportedly-blocking-people-in-crimea-iran-sanction-2019-7">blocked access&lt;/a> from Iran citing sanctions. Sanctions have a time and place but make very little sense when it comes to GitHub. The entire idea of GitHub is to spread open source which aligns closely with democratic ideals and principles. It&amp;rsquo;s one thing to hurt a country economically to drive change but a whole other to prevent their population from accessing information.&lt;/p>
&lt;p>Totalitarian countries want to control access to information and the way to fight that is to make it as easy as possible to make it available and this move is the exact opposite of what should happen. GitHub does an incredible job of making information accessible and that should be encouraged. Major changes come from people pushing back against existing authority and they need to have hope to do so. Providing a glimpse of another world is how we do that and it&amp;rsquo;s disappointing that this avenue is being shut down.&lt;/p></description></item><item><title>PagerDuty's incident response guide</title><link>/2019/07/26/pagerdutys-incident-response-guide/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/26/pagerdutys-incident-response-guide/</guid><description>&lt;p>As companies grow they need to develop their incident response processes. The risks are greater and the systems become increasingly more complicated with significantly more specialized knowledge. We&amp;rsquo;re going through a process to redo our incident response process and while doing research I came across &lt;a href="https://response.pagerduty.com">PagerDuty&amp;rsquo;s guide&lt;/a> which has been incredibly helpful in guiding our approach.&lt;/p>
&lt;p>It&amp;rsquo;s extremely thorough and goes into painstaking detail that describes everything one needs to do to build a mature incident response process. There&amp;rsquo;s a slew of content around setting up the tools (using PagerDuty of course) but also around the cultural and organization pieces that need to be there to make it a success.&lt;/p></description></item><item><title>A more efficient Google Calendar</title><link>/2019/07/21/a-more-efficient-google-calendar/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/21/a-more-efficient-google-calendar/</guid><description>&lt;img src="/image/new-gcal-event-creation.png" alt="New Google Calendar event creation" data-width="451" data-height="394" data-layout="responsive" />
&lt;p>Google Calendar is starting to become one of my favorite Google products. They&amp;rsquo;re constantly shipping new features that keep on making it easier and more efficient to use. One of the most recent improvements is the ability to create a fully featured event directly from the calendar screen. One of my biggest use cases is to schedule a meeting with someone which requires knowing both their availability and the availability of a room. In the past, this required going into the full screen edit page which caused a bit of friction. Earlier this week while creating an event from the calendar page I was able to also enter a name and see their schedules appear on the calendar. Similarly, there&amp;rsquo;s a room field that also offers suggestions for free rooms with the desired capacity.&lt;/p></description></item><item><title>A more efficient Google Calendar backup</title><link>/2019/07/21/a-more-efficient-google-calendar-backup/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/21/a-more-efficient-google-calendar-backup/</guid><description>&lt;img src="/image/new-gcal-event-creation.png" alt="New Google Calendar event creation" data-width="451" data-height="394" data-layout="responsive" />
&lt;p>Google Calendar is starting to become one of my favorite Google products. They&amp;rsquo;re constantly shipping new features that keep on making it easier and more efficient to use. One of the most recent improvements is the ability to create a fully featured event directly from the calendar screen. One of my biggest use cases is to schedule a meeting with someone which requires knowing both their availability and the availability of a room. In the past, this required going into the full screen edit page which caused a bit of friction. Earlier this week while creating an event from the calendar page I was able to also enter a name and see their schedules appear on the calendar. Similarly, there&amp;rsquo;s a room field that also offers suggestions for free rooms with the desired capacity.&lt;/p></description></item><item><title>Why is your Confluence in Polish?</title><link>/2019/07/19/why-is-your-confluence-in-polish/</link><pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/19/why-is-your-confluence-in-polish/</guid><description>&lt;img src="/image/confluence-date-lozenge.png" alt="Confluence Date Lozenge settings" data-width="634" data-height="257" data-layout="responsive" />
&lt;p>Despite not speaking any Polish, I changed my Confluence language to Polish earlier this week to finally fix a problem that&amp;rsquo;s been bugging me for years. Confluence has the concept of a &amp;ldquo;&lt;a href="https://confluence.atlassian.com/doc/configuring-time-and-date-formats-150144.html">date lozenge&lt;/a>&amp;rdquo; which allows you to add a date via a helpful calendar and gives it a nice looking format. We decided to use these lozenges to track dates in our system since it improves consistency and they do get special treatment in Confluence and lead to a few neat features - for example being able to use them as due dates in tasks.&lt;/p></description></item><item><title>Substack vs Medium</title><link>/2019/07/17/substack-vs-medium/</link><pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/17/substack-vs-medium/</guid><description>&lt;p>I heard about Substack for the first time today while reading the &lt;a href="https://stratechery.com/2019/substack-raises-series-a-an-interview-with-substack-co-founders-christopher-best-and-hamish-mckenzie/">Stratechery newsletter&lt;/a>. For those not familiar (like me before today!), Substack is a startup that makes it easier for individual content creators to create and monetize newsletters with the goal of empowering them to build their own media empires.&lt;/p>
&lt;p>As I read the article I couldn&amp;rsquo;t help but think about the struggles Medium has had since its founding. It started as a blogging platform and had a variety of challenges with monetization over the years. At one point they had advertising but &lt;a href="https://digiday.com/media/medium-gives-ad-sales-calling-system-broken/">gave up&lt;/a> on it in 2017. Since then they&amp;rsquo;ve shifted to a subscription model where once you hit a limit some articles end up being put up behind a paywall, very similar to a traditional publisher. I don&amp;rsquo;t know how Medium is doing but I doubt this is where they wanted to be - monetizing in the same way as the NY Times.&lt;/p></description></item><item><title>Cloudflare outage postmortem</title><link>/2019/07/16/cloudflare-outage-postmortem/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/16/cloudflare-outage-postmortem/</guid><description>&lt;p>Postmortems are one of the best practices of modern software engineering. They allow engineering teams to learn from mistakes and drive changes that eliminate entire categories of problems. They&amp;rsquo;re a great way to own issues, and if shared publicly, provide transparency to customers and describe what will be done to prevent these types of issues in the future.&lt;/p>
&lt;p>As an engineer, it&amp;rsquo;s incredibly valuable and interesting to read these public postmortems. When large companies are hit with an issue that requires a postmortem it&amp;rsquo;s usually a gnarly issue that is inspiring and interesting to learn about. In addition, they offer a glimpse into how larger companies operate, the way their teams are organized, and the types of tools and systems they have to address issues. These are all very helpful to those of us who are running at a much smaller scale since they force us to compare and contrast our systems against theirs and helpfully allow us to improve the way we operate.&lt;/p></description></item><item><title>Environmental sustainability</title><link>/2019/07/15/environmental-sustainability/</link><pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/15/environmental-sustainability/</guid><description>&lt;p>Lately I&amp;rsquo;ve been much more aware of how much waste we generate and wish there was a way to not just recycle but instead to reuse. There&amp;rsquo;s so much we consume that recycling only gets us so far and it&amp;rsquo;s much better to reuse as much as possible. How much better off would we all be if we were able to reuse existing containers for takeout and food delivery? It is an inconvenience but it&amp;rsquo;s much more sustainable and as a society we&amp;rsquo;d get used to it.&lt;/p></description></item><item><title>DNS resolution gotchas</title><link>/2019/07/14/dns-resolution-gotchas/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/14/dns-resolution-gotchas/</guid><description>&lt;p>A few months ago we ran a pretty complex migration from AWS Route 53 to NS1 and ran into a few gotchas that I wanted to share. On the surface, DNS seems simple: you associate subdomains to specific records and then rely on the DNS provider to handle that resolution.&lt;/p>
&lt;p>The nuance occurs when you have a deeper structure where it takes a few steps to get to the final IP address. This is fairly common if you want to do DNS resolution based on the geography of the user. As an example, if I have a single site, &lt;a href="https://www.dangoldin.com">www.dangoldin.com&lt;/a>, I can use AWS Route 53 to create multiple records, each for a specific geographic region containing dedicated servers. In this case, I can set it up so that users from Europe end up with the record www-eu.dangoldin.com while users in North America end up with www-na.dangoldin.com. It&amp;rsquo;s still up to me to then have another resolution to map these regional records to the final locations but this allows the DNS resolution to be a bit more granular.&lt;/p></description></item><item><title>My Ubuntu experinece</title><link>/2019/07/14/my-ubuntu-experinece/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/14/my-ubuntu-experinece/</guid><description>&lt;p>At the end of 2018 I made the shift from OS X on a Macbook Pro to Ubuntu on a Thinkpad and it&amp;rsquo;s been enough time for me to get used to the new setup while still remember the old one so I thought I&amp;rsquo;d share my experiences.&lt;/p>
&lt;p>&lt;strong>The good&lt;/strong>&lt;/p>
&lt;p>Installing packages is much easier and requires no weird workarounds. The built-in package manager is a big boost and nearly every library or utility I need can be installed with a simple &amp;ldquo;apt install&amp;rdquo; call. As expected, there&amp;rsquo;s a lot more customization you can do than on OS X. Nearly everything can be tweaked and modified - ranging from shortcuts in different apps to various system settings to get the optimal configuration. Developing feels similar to OS X in terms of IDEs but it does feel a little bit snappier and the environment is much closer to where it will actually be deployed.&lt;/p></description></item><item><title>Toward a global government</title><link>/2019/07/14/toward-a-global-government/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/14/toward-a-global-government/</guid><description>&lt;p>The world isn&amp;rsquo;t in the best shape these days. There&amp;rsquo;s a rise in extreme nationalism and intolerance, climate change is leading to some wild weather, and overall it just feels we&amp;rsquo;re moving in the wrong direction. Everyone can speculate on the causes but to me it&amp;rsquo;s a function of the complicated, and often, antithetical incentives from the various global players. Each country is looking out for its own interests and despite their behavior having an impact on others. This leads to a &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">tragedy of the commons&lt;/a>&amp;rdquo; situation where the earth is being abused. The world&amp;rsquo;s institutions are just not strong enough to support the policies the world needs.&lt;/p></description></item><item><title>Keeping Ubuntu computers in sync</title><link>/2019/07/13/keeping-ubuntu-computers-in-sync/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/13/keeping-ubuntu-computers-in-sync/</guid><description>&lt;p>I switched to Ubuntu a few years ago and along with the change decided to keep as much of my environment setup in version control as possible. The motivation was to make the process of setting up a new computer as simple and repeatable as possible since I planned on shifting both my personal and work computers to Ubuntu and wanted to keep them in sync as much as possible.&lt;/p></description></item><item><title>Personal dashboards</title><link>/2019/07/10/personal-dashboards/</link><pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/10/personal-dashboards/</guid><description>&lt;p>They say you don&amp;rsquo;t improve what you don&amp;rsquo;t measure and there are a ton of things I want to improve about myself! In the office we use &lt;a href="https://prometheus.io">Prometheus&lt;/a> and &lt;a href="https://grafana.com">Grafana&lt;/a> to create dashboards to measure the health and performance of a variety of applications and services. They provide a ton of transparency and visibility into how we&amp;rsquo;re doing and it&amp;rsquo;s incredibly easy to spot patterns.&lt;/p>
&lt;p>What if we each had our own personal dashboard that measured what was important to us? There are obvious ones such as health and sleep metrics but there can also be a ton based on productivity or life in general. For productivity, some examples may be the number of emails in your inbox or the average time it takes to respond to an email. For life it can be about how much time we&amp;rsquo;re spending with our families and friends or just pursuing hobbies.&lt;/p></description></item><item><title>Ideal tech intern projects</title><link>/2019/07/09/ideal-tech-intern-projects/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/09/ideal-tech-intern-projects/</guid><description>&lt;p>Having a tech internship program is a big investment that companies should not undertake without making sure they can put in the work to make it successful. Some companies approach it as a source of extra help but without giving the interns the support they need it will end up in hurt productivity for them and the rest of the team.&lt;/p>
&lt;p>The ultimate goal of an internship is to provide useful work, determine whether there&amp;rsquo;s an opportunity for a full time role, and sell the candidate on that opportunity. A great way to achieve these ends is to give interns a chance to own a large project that allows them to get a feel for the work and showcase their abilities. The ideal project should have the following traits:&lt;/p></description></item><item><title>Power of habit</title><link>/2019/07/08/power-of-habit/</link><pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/08/power-of-habit/</guid><description>&lt;p>Last year I feel behind my writing and had to scramble starting in November to hit my goal of two blog posts a week. I unfortunately didn&amp;rsquo;t start this year strong and am already behind. In the first half of the year I should have written 52 posts but I&amp;rsquo;m currently at 24, just under half off the goal. This is not great if I want to have a somewhat normal November and December where I&amp;rsquo;m not spending nearly every free moment of the day writing so I&amp;rsquo;ve decided to write a post every day in July in order to eat into the debt.&lt;/p></description></item><item><title>Video of my Data Council NYC talk</title><link>/2019/07/07/video-of-my-data-council-nyc-talk/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/07/video-of-my-data-council-nyc-talk/</guid><description>&lt;p>Last November I gave a talk at Data Council NYC &amp;lsquo;18 titled &amp;ldquo;The Highs and Lows of Building an Adtech Data Pipeline&amp;rdquo; and finally saw that the video has been uploaded to YouTube. If you&amp;rsquo;re interested in hearing a runthrough of the different iterations our data pipeline went through over the course of 6 years definitely give it a watch and leave comments and feedback. I&amp;rsquo;m not the most natural of public speakers and there were moments I spoke much quicker than I should have but hopefully having the slides on the side make it a tad more understandable.&lt;/p></description></item><item><title>Credit card merchant fees</title><link>/2019/07/06/credit-card-merchant-fees/</link><pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/06/credit-card-merchant-fees/</guid><description>&lt;p>A &lt;a href="https://stratechery.com/2019/facebooks-cryptocurrency-the-problem-and-benefit-of-credit-cards-the-facebook-payment-network/">Stratechery post&lt;/a> covering Facebook&amp;rsquo;s cryptocurrency launch mentions that credit card merchant fees are generally a flat $0.29 fee plus 1.5~3.5% of the transaction. The argument is that the market is efficient and if there was a way to drop them one of the credit card providers would have to gain an advantage.&lt;/p>
&lt;p>I don&amp;rsquo;t buy this argument. I don&amp;rsquo;t have the history of merchant fees but my suspicion is that they changed minimally, if at all, over the past few decades despite the massive amount of data being collected and the processing capability to go with it. The rates could be much lower but there&amp;rsquo;s limited incentive to get there since credit card companies can charge on both the consumer and merchant side. There&amp;rsquo;s a big incentive to optimize on the consumer side since they&amp;rsquo;re the ones spending the money and understanding the default and fraud risk are major drivers of profitability. On the merchant side it&amp;rsquo;s a stable rate with little reason to change it.&lt;/p></description></item><item><title>Future of cloud is hybrid</title><link>/2019/07/05/future-of-cloud-is-hybrid/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/05/future-of-cloud-is-hybrid/</guid><description>&lt;p>Cloud providers are doing all they can to lock companies into their own offerings but my suspicion is that the future looks hybrid. On the surface they all offer the same base functionality but there&amp;rsquo;s a variety of specialization on the edges offered by Amazon vs Microsoft vs Google vs your own data center. My suspicion is that over time more and more companies will adopt hybrid setups that allow them to leverage the strengths of each platform. For example, using Google for their AI and ML capabilities, Microsoft for integrating with an existing data center (and not competing with your existing business!), your own data center for the well defined and scoped use case, and Amazon for everything else.&lt;/p></description></item><item><title>Increasing the bar for reelection</title><link>/2019/07/04/increasing-the-bar-for-reelection/</link><pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/04/increasing-the-bar-for-reelection/</guid><description>&lt;p>I avoid writing about politics but in honor of July 4th I decided to pen some thoughts around elections.&lt;/p>
&lt;p>Last year I wrote a &lt;a href="http://dangoldin.com/2017/11/25/make-all-laws-temporary/">post&lt;/a> proposing that all laws should be temporary and upon expiration require a higher and higher majority to be sustained. What if we adopt a similar policy for electing politicians? In the United States different offices have different term limits: a Supreme Court Justice has a lifetime appointment, a senator has 6 years, a president has 4 (and limited to two terms), and a congressperson has 2. The intuition behind these is that a Supreme Court Justice should have a long term outlook and not be influenced by any short term incentives. I don&amp;rsquo;t know why the others have such specific term limits or why a senator term is longer than a presidential term while a congressperson term is shorter than a presidential term. The intent of these is to avoid corruption by giving people a chance to course correct if a political is not working out but the exact years and the ratios between the offices feel arbitrary.&lt;/p></description></item><item><title>Revamping my online security</title><link>/2019/07/03/revamping-my-online-security/</link><pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/03/revamping-my-online-security/</guid><description>&lt;p>Out of curiosity I checked out &lt;a href="https://haveibeenpwned.com/">haveibeenpwned&lt;/a> to see if my data was exposed in any breach and it turns out my email was found in 22 breaches. I&amp;rsquo;m not surprised - I&amp;rsquo;m probably registered on hundreds, if not thousands, of sites and it&amp;rsquo;s only a matter of time before some of them get compromised. You should assume that every time you register for a site that data will get leaked and act accordingly - this means using a unique password on every site and, if possible, even obfuscating your email address. For the past year and a half I&amp;rsquo;ve embraced the approach of assuming my email address is public but investing heavily in strong, secure, and randomly generated passwords that are unique to every site. I&amp;rsquo;ve also been slowly going through my older registrations and updating the passwords.&lt;/p></description></item><item><title>Combine registration and sign in</title><link>/2019/07/02/combine-registration-and-sign-in/</link><pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/02/combine-registration-and-sign-in/</guid><description>&lt;p>The goal of nearly every site or app is to get users to engage as quickly as possible. That&amp;rsquo;s why it&amp;rsquo;s surprising that so many sites have a separate flow for registration versus sign in. There&amp;rsquo;s no good reason that these two flows shouldn&amp;rsquo;t be combined.&lt;/p>
&lt;p>A part of nearly every registration is entering an email and password so why not make them the first step of the sign up process? If that email and password are already in the system just log her in. And if not then you have the email address of a to-be-registered user. The only real risk is if someone thought they had an account and wanted to sign in but instead signed up - and that&amp;rsquo;s a good problem to have from the application side since you have the contact info of an interested user.&lt;/p></description></item><item><title>Sign In with Apple</title><link>/2019/07/01/sign-in-with-apple/</link><pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate><guid>/2019/07/01/sign-in-with-apple/</guid><description>&lt;p>Apple recently made the announcement that apps that provide third party login will also be required to offer a Sign in with Apple option. Apple will then, in turn, anonymize the email address and thus anomyze the user. The idea here being that the app will only have access to the random email generated by Apple and all email will have to be proxied through the email address generated by Apple.&lt;/p></description></item><item><title>Tightly coupled data loss</title><link>/2019/06/15/tightly-coupled-data-loss/</link><pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate><guid>/2019/06/15/tightly-coupled-data-loss/</guid><description>&lt;p>A few months while doing some Kafka maintenance we ran into an issue that caused us to lose approximately 10% of records across all our topics over the course of an hour. It was a big screw up but what made it worse was the interaction between the records. Our records represent the life cycle of an ad from the auction, to the render, to whether it was in view, and beyond. Rather than send all the information along for each on we keep the records as lightweight as possible and rely on our backend processing to join them together to come up with an auction log.&lt;/p></description></item><item><title>Video games of my youth</title><link>/2019/05/31/video-games-of-my-youth/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/2019/05/31/video-games-of-my-youth/</guid><description>&lt;p>Lately I&amp;rsquo;ve been feeling nostalgic about the video games from my youth and spent some time browsing YouTube and trying to remember my favorites. I was exclusively a PC gamer except for one month period where I had a Sega Genesis - which my parents got for me as a reward for reading a 1000 pages of Russian. Unfortunately (or fortunately), our &amp;ldquo;deal&amp;rdquo; did not include any additional games so the system was returned within a month of me getting it. I have a ton more memories from the mid-90s but these stood out as my earliest memories. I&amp;rsquo;m pretty sure I didn&amp;rsquo;t beat any of them but it was a wonderful time playing these as a kid in the early 90s.&lt;/p></description></item><item><title>Patents and Patent Law</title><link>/2019/05/04/patents-and-patent-law/</link><pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate><guid>/2019/05/04/patents-and-patent-law/</guid><description>&lt;p>Less than half of &lt;a href="https://www.amazon.com/Triumph-Genius-Edwin-Polaroid-Patent/dp/1627227695">A Triumph of Genius: Edwin Land, Polaroid, and the Kodak Patent War&lt;/a> is spent on the biography of Edwin Land. Rather, the majority of the book is spent on the details of the 7 year Polaroid/Kodak lawsuit. I didn&amp;rsquo;t have an appreciation for the complexity of patent law until reading the book. It&amp;rsquo;s both incredibly dry but also incredibly fascinating.&lt;/p>
&lt;p>Patents and patent law are critical to innovation but the entire legal process of a patent lawsuit is fascinating and dangerous. Experts spend time designing solutions that are then patented. And then it is up to lawyers and judges to determine whether these patents were legitimate. There&amp;rsquo;s so much nuance and complexity that needs to be explained that it&amp;rsquo;s not surprising that the majority of patent lawsuits end up settling - neither side wants to bet on a subjective outcome.&lt;/p></description></item><item><title>Polaroid and Dr. Edwin Land</title><link>/2019/04/30/polaroid-and-dr.-edwin-land/</link><pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate><guid>/2019/04/30/polaroid-and-dr.-edwin-land/</guid><description>&lt;img src="/image/sx70_2k_2k.jpg" alt="SX-70 Polaroid camera" data-width="2000" data-height="2000" data-layout="responsive" />
&lt;p>I recently finished &lt;a href="https://www.amazon.com/Triumph-Genius-Edwin-Polaroid-Patent/dp/1627227695">A Triumph of Genius: Edwin Land, Polaroid, and the Kodak Patent War&lt;/a> which starts with a wonderful biography of Edwin Land and ends with a ton of detail about the Polaroid/Kodak lawsuit. Polaroid was a remarkable company and it&amp;rsquo;s both amazing that it&amp;rsquo;s no longer around and remarkable that the modern tech world doesn&amp;rsquo;t seem to appreciate Polaroid and Edwin Land. We put Steve Jobs on a pedestal but Edwin Land was the precursor. He leveraged technology to create incredible consumer products and made Polaroid the Apple of its day.&lt;/p></description></item><item><title>Gerrymandering</title><link>/2019/04/27/gerrymandering/</link><pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate><guid>/2019/04/27/gerrymandering/</guid><description>&lt;p>I tend to avoid blogging about politics but I couldn&amp;rsquo;t help but rant against gerrymandering. The concept is rooted in the idea that political parties can divide voting areas in order to bias the populations in their favor. It doesn&amp;rsquo;t make any sense that in a functional democracy voting districts can be so easily manipulated.&lt;/p>
&lt;p>The fact that one sees numerous examples of the ridiculous geographic shapes that can develop should be sign-enough that it&amp;rsquo;s an irrational approach.&lt;/p></description></item><item><title>Free as in puppy</title><link>/2019/04/23/free-as-in-puppy/</link><pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate><guid>/2019/04/23/free-as-in-puppy/</guid><description>&lt;p>I&amp;rsquo;ve known about &amp;ldquo;free as in beer&amp;rdquo; and &amp;ldquo;free as in speech&amp;rdquo; for a while now but only recently have I learned about &amp;ldquo;free as in puppy.&amp;rdquo; I came across the phrase while reading a &lt;a href="https://stratechery.com/2019/a-regulatory-framework-for-the-internet/">Stratechery article&lt;/a> describing various ways of thinking about regulation. The idea is that just like getting a free puppy, you get it for free but it comes with a significant long term cost.&lt;/p>
&lt;p>I&amp;rsquo;m a big fan of the phrase and it highlights a very common tradeoff we make, especially in engineering. We often get carried away solving a problem but fail to consider the long term cost of maintenance and support. By the time we discover these long term costs the product is entrenched and we&amp;rsquo;re on the hook for maintaining it in its current form. It&amp;rsquo;s important to slow down and realize that nearly all software we write is in the &amp;ldquo;free as in puppy&amp;rdquo; camp. Writing the code to work is the easy part - writing it to work with no ongoing cost is the challenge.&lt;/p></description></item><item><title>YouTube's popularity</title><link>/2019/03/22/youtubes-popularity/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>/2019/03/22/youtubes-popularity/</guid><description>&lt;p>I know YouTube is huge but it’s amazing how small of a role it plays in my life. The statistics make it seem as if nearly every person online is addicted to YouTube and yet I barely use it. The extent of my YouTube experience is watching how-to videos whenever I need to do something around the house.&lt;/p>
&lt;p>The most recent time I watched YouTube was a directed search for a how to video on how to disassemble and unclog a bathtub drain. Beyond that I can’t recall the last time I watched a YouTube video. Every once in a while I will get nostalgic and watch some classic video game videos but other than that my YouTube watching is minimal.&lt;/p></description></item><item><title>Airbnb and HotelTonight</title><link>/2019/03/17/airbnb-and-hoteltonight/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>/2019/03/17/airbnb-and-hoteltonight/</guid><description>&lt;p>A few weeks ago Airbnb &lt;a href="https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/">announced&lt;/a> that they would be acquiring Hotel Tonight. This was a surprise - throughout its history Airbnb has made a point to compete with hotels and my gut was that hotels would not be too happy to partner with a competitor. I understand that Airbnb is trying to grow before their inevitable IPO and to do so they need to invest in growth. I also subscribe to the argument that from their standpoint they’re focused on giving people a place to stay and not necessarily that concerned with the exact medium - private rental or hotel or whatever else.&lt;/p></description></item><item><title>LinkedIn's terrible messaging experience</title><link>/2019/03/10/linkedins-terrible-messaging-experience/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>/2019/03/10/linkedins-terrible-messaging-experience/</guid><description>&lt;p>This is a bit of a ranty post but it’s incredible how poor the LinkedIn messaging experience is. If anyone sends me a LinkedIn message I get an email telling me that so-and-so sent me a message. To read the actual message I need to clickthrough to get to the LinkedIn site. I get it, they want to increase engagement and have me access the site and show me something new. It’s just a ridiculously poor user experience. They could easily include the message text and then I can decide whether it’s something I need to respond to and when. Instead I get a useless notification telling me that I have a new message. It’s as if the LinkedIn message experience was modeled after AOL’s “you’ve got mail.”&lt;/p></description></item><item><title>Dropbox should drop the freemium model</title><link>/2019/02/28/dropbox-should-drop-the-freemium-model/</link><pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate><guid>/2019/02/28/dropbox-should-drop-the-freemium-model/</guid><description>&lt;p>This is a bit heretical but I suspect Dropbox would make more money if it moved away from a freemium model. I have been a user of the free Dropbox account for multiple years now and due to their various promotions have gotten my free storage up to 6GB and yet I use just under 4GB. I don’t use it to store images but instead use it to store important documents that I want to both keep safe but also frequently access across multiple devices. It’s unlikely I’m going to upgrade to a new plan. The only reason would be for photo storage but there are a ton of free or cheaper options.&lt;/p></description></item><item><title>Netflix recommendation system failure</title><link>/2019/02/10/netflix-recommendation-system-failure/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>/2019/02/10/netflix-recommendation-system-failure/</guid><description>&lt;p>I have zero interest in horror or drama and have never watched any of them on Netflix yet Netflix decided to automatically start playing the gory intro to their new Ted Bundy series. I understand their desire to highlight their shows but autoplaying something that gory just seems like a terrible decision, especially given my viewing history.&lt;/p>
&lt;p>Many people will launch Netflix with their families around and having something so graphically violent greet you is a surefire way to lose fans and subscribers. It seems obvious that a company should not be autoplaying violence and I’m surprised that Netflix, a company that takes pride in its recommendation system, managed to do something this tone-deaf.&lt;/p></description></item><item><title>A personal DSP</title><link>/2019/02/09/a-personal-dsp/</link><pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate><guid>/2019/02/09/a-personal-dsp/</guid><description>&lt;p>A common refrain from people who use adblock is that they’d be willing to pay for the content if it were an option and only use adblock to avoid the tracking and slow site speed. The desire is to have a way to see how much a website is generating from their visit and give the publisher that money directly. That way the publisher gets the equivalent amount of money as they would otherwise and the user gets to avoid ads.&lt;/p></description></item><item><title>Finally, some price increases</title><link>/2019/02/02/finally-some-price-increases/</link><pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate><guid>/2019/02/02/finally-some-price-increases/</guid><description>&lt;p>For as long as I can remember it felt that there was this hidden rule that companies cannot raise prices or risk mass customer abandonment. Yet the past few few months we’ve seen two major companies increase prices - &lt;a href="https://www.isostech.com/blogs/atlassian/price-increase-sandwich/">Atlassian&lt;/a> and &lt;a href="https://bgr.com/2019/01/15/netflix-price-increase-2019-standard-basic-premium/">Netflix&lt;/a> - without any adverse effects. The products are improving and are offering more functionality so it’s only fair that the prices increase. This is a welcome sign and I hope it leads to companies starting to invest significantly more in improving their existing products with the expectation that they’ll be able to raise prices in line with the additional value they’re bringing. Imagine what sort of investments companies would be making if they knew they’d be able to raise the price of their products.&lt;/p></description></item><item><title>Modernizing Makers Alley and Better404</title><link>/2019/01/27/modernizing-makers-alley-and-better404/</link><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><guid>/2019/01/27/modernizing-makers-alley-and-better404/</guid><description>&lt;p>Years ago I started two companies, &lt;a href="http://makersalley.com">Makers Alley&lt;/a> and &lt;a href="https://better404.com">Better404&lt;/a>, and while they were both failures I didn’t have the heart to shut down the site. I put a lot of heart into building them and just pulling the plug felt cold. Instead I just paid the annual hosting fee and had them both running on a small AWS instance. This worked fine for years but unfortunately while I was messing around with Terraform I ended up terminating the instance entirely. Beyond the fact that they were both running incredibly old libraries with no available pip libraries the instance also contained the only copy of the database. I wa stuck with having the source code from a few years ago and not much else. Rather than give up I decided to use this as an opportunity to learn a bit and modernize them using some of the newer technologies.&lt;/p></description></item><item><title>Buying Kindle books on iOS</title><link>/2019/01/12/buying-kindle-books-on-ios/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>/2019/01/12/buying-kindle-books-on-ios/</guid><description>&lt;p>Earlier this week I finished an ebook on the train and wanted to purchase another one for the way back while waiting in the station. Remarkably, Apple, a company that emphasizes and takes pride in its user experience, does not make this easy. It turns out that the Kindle app on iOS does not allow you to actually purchase a book and isn’t even allowed to link to a page where it can be purchased. I understand the business rationale - Apple wants the 30% of the digital sale and Amazon doesn’t want to give it up - but it leads to an awful experience, especially when only the station has service and I know I will be signalless once I board the train.&lt;/p></description></item><item><title>GitHub's new plan</title><link>/2019/01/08/githubs-new-plan/</link><pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate><guid>/2019/01/08/githubs-new-plan/</guid><description>&lt;p>Earlier today GitHub announced that they would make a significant change to their plans: a free plan would now be able to have private repos. I’m likely one of the many GitHub users who host their personal projects on GitHub with many being private. Some of these are projects I’m just not ready to release while others are proprietary scripts or private DevOps code. I was on the $7/mo Pro plan and downgraded to the free plan. I lost the advanced collaboration tools which I wasn’t using for my personal projects in the first place and it feels good to be $7 richer each month.&lt;/p></description></item><item><title>Java's SimpleDateFormat: YYYY vs yyyy</title><link>/2019/01/06/javas-simpledateformat-yyyy-vs-yyyy/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>/2019/01/06/javas-simpledateformat-yyyy-vs-yyyy/</guid><description>&lt;p>This post would have been more appropriate a week ago when 2018 was coming to a close but better late than never. This is a friendly reminder that when formatting dates in Java’s SimpleDateFormat class there is a subtle difference between YYYY and yyyy. They both represent a year but yyyy represents the calendar year while YYYY represents the year of the week. That’s a subtle difference that only causes problems around a year change so your code could have been running perfectly fine all year only to cause a problem in the new year.&lt;/p></description></item><item><title>2019 Predictions</title><link>/2018/12/31/2019-predictions/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/31/2019-predictions/</guid><description>&lt;p>What’s more fitting on the last day of the year than providing predictions for the next year? I’ve never done these before but like the idea - it’s a good way to force me to think about the coming year as well as keep me honest when looking back. Our minds and memories are incredible fallible so by putting it in public writing it’s much more difficult to refute.&lt;/p>
&lt;p>And now onto my predictions:&lt;/p></description></item><item><title>Dorm rooms for adults</title><link>/2018/12/31/dorm-rooms-for-adults/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/31/dorm-rooms-for-adults/</guid><description>&lt;p>Not to keep beating a dead horse but the housing situation in San Francisco seems ridiculous. Back in March, the NY Times &lt;a href="https://www.nytimes.com/2018/03/04/technology/dorm-living-grown-ups-san-francisco.html">wrote an article&lt;/a> covering Starcity, a company that builds “dorms” for professionals. The article is positioned as a positive since it’s giving people an option to live in San Francisco who may not be able to afford a place to themselves but it just feels like as if the real world is mirroring the software world.&lt;/p></description></item><item><title>Farewell 2018, Hello 2019!</title><link>/2018/12/31/farewell-2018-hello-2019/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/31/farewell-2018-hello-2019/</guid><description>&lt;p>Despite falling behind significantly in my writing this past year I miraculously managed to achieve my target of 2 blog posts a week. It was not a fun time and definitely felt like a chore having to write multiple posts a day - including 3 yesterday and today - but I’m glad to have been able to achieve my target and keep the streak going. The length and depth of my blog posts has definitely declined over the past 2 months but it was refreshing being forced to write about whatever came to mind. I didn’t have the luxury to plan ahead and had to comb my immediate thoughts and short term memory for topics or fall back to a running list of ideas for inspiration. I wish I gave some of those topics more thought and plan on revisiting them for 2019.&lt;/p></description></item><item><title>Amazon vs Walmart</title><link>/2018/12/30/amazon-vs-walmart/</link><pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/30/amazon-vs-walmart/</guid><description>&lt;img src="/image/amazon-vs-walmart.png" alt="Amazon vs Walmart 1 year stock graph" data-width="651" data-height="706" data-layout="responsive" />
&lt;p>One of the most exciting business battles is Amazon vs Walmart and I’m interested in seeing what happens in 2019. As I’m writing this Amazon has a market cap of $723B while Walmart is at $268. At the same time, Amazon’s &lt;a href="https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/">2017 revenue&lt;/a> was $178B while Walmart’s &lt;a href="https://www.statista.com/statistics/555334/total-revenue-of-walmart-worldwide/">was&lt;/a> $486B. The market views them as two completely different businesses and that’s fair - Walmart is a traditional retailer while Amazon has its hands in a variety of businesses - ranging from retail, to a marketplace, to a cloud hosting provider. Walmart is still trying to figure out ecommerce: they acquired Jet.com in 2016 and this past year bought Flipkart. Amazon, on the other hand, seems to be playing a very different game. They acquired Whole Foods in 2017 and Ring the past year. In the first case Amazon is going deeper into traditional retail while the latter feels like an entry into the home. It looks as if Walmart is fighting yesterday’s battle while Amazon continues plowing ahead into the future. I don’t know how this will play out but I am an Amazon shareholder so can’t help but feel optimistic.&lt;/p></description></item><item><title>Mexico city thoughts</title><link>/2018/12/30/mexico-city-thoughts/</link><pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/30/mexico-city-thoughts/</guid><description>&lt;p>While my vacation memories are still fresh I wanted to share some thoughts about Mexico City. It was my first time in both Mexico and Mexico City but it was a great city with a ton of things to do and very helpful people. We’re all products of our experience and can’t help but compare the places we visit against the places we live. New York City and Mexico City have a lot in common: they’re both large modern cities with robust infrastructure and all the modern “millennial amenities” including ride sharing, bike sharing, various food delivery services, and a ton of bars and restaurants. In fact Mexico City also has electric scooters which still haven’t made it into NYC.&lt;/p></description></item><item><title>Twenty seconds makes all the difference</title><link>/2018/12/30/twenty-seconds-makes-all-the-difference/</link><pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/30/twenty-seconds-makes-all-the-difference/</guid><description>&lt;p>I recently switched from a 2015 Macbook Pro to a new ThinkPad with Ubuntu and one of the most noticeable differences is how much faster this blog builds. A few years ago I moved this blog over to &lt;a href="https://www.ampproject.org/">AMP&lt;/a> to improve its load times. I don’t want to go into the details of AMP but in order to get the performance wins all styling is inlined - that means that instead of linking to a CSS file the CSS rules are explicitly defined on each page. That means the build system is not simply generating an HTML page off of a template but also inlining the CSS rules onto every generated page.&lt;/p></description></item><item><title>Moving from OS X to Ubuntu</title><link>/2018/12/29/moving-from-os-x-to-ubuntu/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/29/moving-from-os-x-to-ubuntu/</guid><description>&lt;p>It gets easier and easier to set up a new computer. I took advantage of the Cyber Monday deals to get myself a ThinkPad that I immediately proceeded to switch over to Ubuntu. Before the cloud getting a new computer introduced a whole world of pain since you had to worry about migrating important files. These days it’s incredibly simple: most of our digital lives are replicated somewhere online and local files can be kept in Dropbox while code is kept on GitHub. I migrated from OS X to Ubuntu and the vast majority of the issues I’m running into are keyboard shortcut and workflow related and nothing to do with the software itself.&lt;/p></description></item><item><title>The Teotihuacan pyramids</title><link>/2018/12/29/the-teotihuacan-pyramids/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/29/the-teotihuacan-pyramids/</guid><description>&lt;img src="/image//teotihuacan-pyramids.jpg" alt="The Teotihuacan pyramids" data-width="6127" data-height="2037" data-layout="responsive" />
&lt;p>Last week I was on vacation in Mexico City and did a day trip to &lt;a href="https://en.wikipedia.org/wiki/Teotihuacan">Teotihuacan&lt;/a>, an ancient Mesoamerican city known for its pyramids. I find it inspiring being able to visit ruins from more than a thousand years ago. There’s something magical being able to set foot on a lost world with its own unique way of life that is impossible to recapture. So much time has passed between the actual living civilization and tourists exploring the ruins that it feels we’re completely different species and yet we’re still the same flesh, blood, and potential - just born at different times. It gives you some perspective knowing that what was once this huge civilization is now a tourist site.&lt;/p></description></item><item><title>Wired headphones</title><link>/2018/12/29/wired-headphones/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/29/wired-headphones/</guid><description>&lt;p>I left my Bluetooth headphones at home this morning and ended up borrowing a wired pair during the day. I forgot how easy it is to just be able to plug in and go - no waiting for the headphones to turn on, no waiting for the Bluetooth to connect, and no having to deal with selecting the proper audio out channel. The convenience of Bluetooth outweighs the costs but it was a stark reminder of how much more robust and simpler tools used to be. To be indistinguishable from “magic” our products need to have a lot of magic inside them. Almost always the magic works but every once in a while we end up stuck. It seems to be the natural way of things. When cars were first created they were extremely mechanical and owners had the skillset and ability to fix them. Now they’re all digital and require a licensed mechanic. Computers are the same way - I grew up being able to do some quick repairs and replacements but good luck trying to replace components in modern computers or phones: it’s doable but not very enjoyable. There is a part of me that yearns for the simpler days but I’m realistic enough to know that I don’t really want to go back.&lt;/p></description></item><item><title>Calendly</title><link>/2018/12/28/calendly/</link><pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/28/calendly/</guid><description>&lt;p>Over the years I’ve received a ton of &lt;a href="https://calendly.com">Calendly&lt;/a> links - it’s an application that integrates with your calendar and hosts a personal page that allows others to book time on your calendar. As a user it’s great since it offloads scheduling responsibility from you to others. At the same time, I was was always bothered by the fact that that effort was externalized to me despite it actually being more efficient for both sides. In any case, I had a real need to actually use it in order to help an external recruiter find time on my calendar to conduct some phone screens and the onboarding process was extremely slick. It was as simple as authenticating with my Google account, granting a few permissions, and then receiving a public link that could be shared with others. I wish more products had as simple of an onboarding - you get the basic functionality with minimal effort yet have the option to customize it as you become comfortable with the product and your use case expands.&lt;/p></description></item><item><title>Amazon's "Add to list" design</title><link>/2018/12/27/amazons-add-to-list-design/</link><pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/27/amazons-add-to-list-design/</guid><description>&lt;p>I use Amazon’s wish list as a placeholder of books I plan on buying. I’m not sure why I don’t just buy them immediately but whenever I’m done with a book I go to my wish list and purchase a new one. Earlier today while adding two books to my wish list I noticed something odd: the location of the “Add to list” button was different from one page to the next. Amazon is the premier ecommerce company and must be doing a ton of A/B testing to optimize the design of the site so it’s surprising that the button is in two completely different locations. I was in the same browser session as well so it’s not as if they’re randomizing it per session; instead it’s as if they’re just showing me two completely different designs for two different products.&lt;/p></description></item><item><title>Eerie blue NYC sky</title><link>/2018/12/27/eerie-blue-nyc-sky/</link><pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/27/eerie-blue-nyc-sky/</guid><description>&lt;p>&lt;amp-youtube data-videoid="6G4dYRcKnVE" layout="responsive" width="640" height="480">&lt;/amp-youtube>&lt;/p>
&lt;p>Earlier tonight I looked out the window to notice an eerie blue-green sky over NYC. It looked like the standard alien landing in a scifi movie and was incredibly surreal. It turns out it was a &lt;a href="https://www.nbcnewyork.com/news/local/Blue-Light-Sky-Explosion-Queens-503589291.html">transformer explosion&lt;/a> in Queens but for a second it really did feel as if something bizzare was happening. It makes you both appreciate and fear about the modern world - the technologies and cities we have are so complex and modern that an explosion led the night sky to glow blue for more than a minute. I didn’t even realize there was anything powerful enough to make the night sky glow that bright for that long but apparently we’re surrounded by that potential.&lt;/p></description></item><item><title>Secret management across computers</title><link>/2018/12/26/secret-management-across-computers/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/secret-management-across-computers/</guid><description>&lt;p>As part of the Cyber Monday nonsense I convinced myself to purchase a ThinkPad in order to run Linux. I have a separate post coming about the transition from OS X to Ubuntu but one area I’m still trying to get under control is secret management. I have a ton of code on the old computer and almost all of it is on GitHub. The biggest challenge so far has been migrating secrets across computers. For example I have a variety of scripts that talk to Slack or the Google ecosystem which require their own tokens and keys. Some are straightforward to reissue - for example AWS - while others require a fair amount of frustrated searching. I don’t have the answer here but have been thinking about the following approaches to make things easier and would love to know how others handle this.&lt;/p></description></item><item><title>Visualizing my blog: 2018 edition</title><link>/2018/12/26/visualizing-my-blog-2018-edition/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/26/visualizing-my-blog-2018-edition/</guid><description>&lt;p>Last year I came up with a set of scripts to analyze my blog and thought it would be interesting to rerun them this year to see what&amp;rsquo;s changed. There are a ton of visualizations up on &lt;a href="https://github.com/dangoldin/blog-analytics/tree/master/img/2018">GitHub&lt;/a> but most are just a fun visual without actually telling a story. I&amp;rsquo;ve included the most important ones below with a bit of analysis and commentary. It also looks as if the charts are getting too messy for multiple years of data so I&amp;rsquo;ll need to revisit the visualizations for 2019.&lt;/p></description></item><item><title>Happy holidays from Visual Studio Code</title><link>/2018/12/25/happy-holidays-from-visual-studio-code/</link><pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/25/happy-holidays-from-visual-studio-code/</guid><description>&lt;img src="/image/vs-code-santa-settings.png" alt="Visual Studio Code Santa settings" data-width="130" data-height="144" data-layout="responsive" />
&lt;div class="right10">
 &lt;img src="/image/vs-code-santa-settings.png" alt="Visual Studio Code Santa settings" data-width="130" data-height="144" data-layout="responsive" />
&lt;/div>
&lt;p>Visual Studio Code has been my go-to editor since the beginning of 2017 and I’m a huge fan. They reinforced my belief with a little Easter egg earlier today to celebrate Christmas. The settings icon in the bottom right corner got a special Santa hat and clicking on it gives you an additional menu option labeled “Happy Holidays!” which gives you a friendly holiday message along with some falling snow.&lt;/p></description></item><item><title>Top posts of 2018</title><link>/2018/12/25/top-posts-of-2018/</link><pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/25/top-posts-of-2018/</guid><description>&lt;img src="/image/2018-ga-blog-stats.png" alt="2018 Google Analytics blog stats" data-width="2594" data-height="742" data-layout="responsive" />
&lt;p>In the usual end-of-year tradition I want to share the top posts of 2018 - including both the posts that were written in 2018 as well as the posts that may have been written in prior years but viewed in 2018. Given the fact that I’ve been extremely behind in writing this year and am only catching up now it’s clear that 2018 was a weaker year than previous ones. Sorting all pages viewed in 2018 by descending pageviews the first post written in 2018 is in the 57th spot - every other post was written in prior years. That bodes well to the concept of evergreen content but it’s still disappointing that I didn’t have any noteworthy posts in 2018. The lesson for 2019 is to actually write on time and spend the time going into depth versus the more superficial and shorter posts I wrote in 2018. On a positive note it’s good to see that my blog does get a healthy flow of organic traffic despite the weak 2018 showing.&lt;/p></description></item><item><title>Calendars as streaming events</title><link>/2018/12/24/calendars-as-streaming-events/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/24/calendars-as-streaming-events/</guid><description>&lt;p>I like calendars. They appeal to both my sense of organization and efficiency but also to the actual depth of the technical implementation. It’s not easy creating a good calendar application - you have to think about all sorts of edge cases - for example rescheduling an event that was part of a recurring series or the usual time zone nonsense. A thought I keep going back to is treating calendars as event streams. Right now I have a personal calendar as well as a few others that are shared with different groups. Using code it’s possible to collect the events across these calendars and then apply various operations on top of them - find events that overlap, find people that overlap, analyze the event frequencies, etc but this has to be done through very specific logic. I love the idea of calendars just consistent of event streams. That way you can apply transformations on top of the streams to answer any questions you have. I imagine this to be a form of lazy list expression or even something like a SQL query that would allow me to take all the events I have going on and apply a map/reduce like operation to get what I need. This still feels a bit abstract and I need to flesh out the idea a bit more but I feel there’s something here - just being able to treat your calendar as a single stream and provide a simple way to act on top of them.&lt;/p></description></item><item><title>Open, Public, Electronic, and Necessary Government Data Act</title><link>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/24/open-public-electronic-and-necessary-government-data-act/</guid><description>&lt;p>The US Congress recently &lt;a href="https://e-pluribusunum.org/2018/12/21/congress-made-open-government-data-the-default-in-the-united-states/">passed&lt;/a> &lt;a href="https://www.govtrack.us/congress/bills/115/hr4174">HR-4174&lt;/a> (The Open, Public, Electronic, and Necessary Government Data Act) which is intended to make all public government data available and accessible. Over the years I’ve done my fair share of poking around various government datasets - both public and private - and while the data was generally available it was rarely accessible. More often than not the data would be available via a scanned PDF which required some heavy OCR work to extract anything useful or the slightly easier PDF parsing code. Even when the data was in CSV files I often ran into formatting issues or inconsistency between the column documentation and the data contents themselves. The most important available datasets will always have people willing to go through the grunt work of cleaning them up but it’s the fringe datasets that end up having too much friction for researchers and developers to dig into them. I’m glad the government is moving to make the data accessible as well since it is the strongest way to make it actionable.&lt;/p></description></item><item><title>Solve all web app performance problems with SQL</title><link>/2018/12/24/solve-all-web-app-performance-problems-with-sql/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/24/solve-all-web-app-performance-problems-with-sql/</guid><description>&lt;p>The title is clearly tongue in cheek but a very large number of web application issues can be solved through better SQL. Web applications performance problems are either UI or API related. And if the performance is on the API side it&amp;rsquo;s likely due to slow queries. The majority of endpoints are doing some form of database interaction and many are making multiple database calls. It only takes one of these to be slow to make the entire application feel sluggish - especially if you have lots of concurrent requests to this endpoint. This often happens when you rely on an ORM to manage your database reads and writes without understanding the queries that are actually being made. An ORM makes it much easier to get started but to get the best performance you need to go lower level and write your own queries. The way to dig into these is to look at the slow query log which gives you (at least in MySQL) both the query, the duration, and the amount of rows examined and sent. Sorting by the query duration gives you a the worst performing queries and then it’s up to you to address them. Sometimes it’s adding a few indices to a table. Other times it’s rewriting the query to be more efficient. Even more rarely it highlights a weakness in the application logic that requires rethinking a particular approach.&lt;/p></description></item><item><title>Quora's revenue model</title><link>/2018/12/23/quoras-revenue-model/</link><pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/23/quoras-revenue-model/</guid><description>&lt;p>While browsing a Hacker News thread about Quora I came across the following exchange:&lt;/p>
&lt;img src="/image/hackernews-quora-revenue-model.png" alt="Hacker News thread about Quora's revenue model" data-width="2604" data-height="676" data-layout="responsive" />
&lt;p>I had always assumed Quora made money through ads and didn’t even consider that they had another revenue stream. The Hacker News thread indicates that they have a substantial revenue stream coming from selling the Q&amp;amp;A data to AI/ML companies but I haven’t been able to find anything else that supports this. Part of the difficulty in searching for results is that just by typing in “Quora” the results are biased towards Quora itself.&lt;/p></description></item><item><title>Edge moving to Chromium</title><link>/2018/12/22/edge-moving-to-chromium/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/22/edge-moving-to-chromium/</guid><description>&lt;p>I’m a bit late to the party but it’s unbelievable that Microsoft is going to &lt;a href="https://www.windowscentral.com/microsoft-building-chromium-powered-web-browser-windows-10">move future versions&lt;/a> of Edge to be built on top of Chromium. This is shocking. I can’t separate Internet Explorer from Microsoft. In fact, Microsoft built, launched, and bundled Internet Explorer to drive Netscape into the ground which led to the famous Microsoft &lt;a href="https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp.">antitrust case&lt;/a>. It feels as if Microsoft is giving up on Internet Explorer and it’s incredibly surprising. This is bigger than any of the other moves Microsoft has done to show they are a very different company. This is more significant than the GitHub acquisition. This is more significant than supporting their applications on operating systems other than windows. This is moving away from 20 years of history and it shows how different the current Microsoft is from the one of the past. I would have loved to have been a fly on the wall when this decision was discussed - there must have been so much passion in that room. There must have been so many viewpoints: ranging from those that have been working on IE for the past 20 years, to the product team that&amp;rsquo;s responsible for the future of IE, to the executive team that needed to make the call. It really is surprising that they had the strength and conviction to be able to acknowledge that they&amp;rsquo;d be better off using Chromium.&lt;/p></description></item><item><title>HackerRank</title><link>/2018/12/22/hackerrank/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/22/hackerrank/</guid><description>&lt;p>These days I rarely code during work since there’s other work to be done but I miss it. Lately I’ve started working on some &lt;a href="https://www.hackerrank.com">HackerRank&lt;/a> problems. They send me a problem a day and nearly every time I’ve been able to knock out a solution in a few minutes. It feels as if I’m keeping my skills sharp and gives me that quick win. Every once in a while I’ll pick up an old project to hack on but that’s a very different style of coding. That’s more focused on understanding modern frameworks and web development while HackerRank problems are entirely standalone and tend to be logic based that often depend on finding the insight before coding up the solution:: they’re basically a poor man’s version of &lt;a href="https://projecteuler.net">Project Euler&lt;/a>. I know most people use them to prepare for interviews but I like using them for the challenge and the win.&lt;/p></description></item><item><title>FIxing spotty AirBnB wifi</title><link>/2018/12/21/fixing-spotty-airbnb-wifi/</link><pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/21/fixing-spotty-airbnb-wifi/</guid><description>&lt;p>Nearly every time I travel I use AirBnB but a fairly common problem is spotty wifi. I don’t know whether my expectations are higher than the typical guests but in a significant number of my stays the wifi has been spotty. Rather than complain or just accept it I’ve started doing my own simple wifi network maintenance to help improve the speed. The nice thing thing is that this is entirely doable - both because you can connect to the network but also because you have access to the physical router.&lt;/p></description></item><item><title>Global roaming</title><link>/2018/12/21/global-roaming/</link><pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/21/global-roaming/</guid><description>&lt;p>&lt;amp-youtube data-videoid="TviGil-U2HE" layout="responsive" width="640" height="480">&lt;/amp-youtube>&lt;/p>
&lt;p>It’s amazing how quickly we become entitled to new technological advances. Back in 2008 Louis C.K. had a bit on the Conan O’Brien late night show about airplane wifi and how despite the miracle of actually having wifi on an airplane we still have the gall to complain about the speed.&lt;/p>
&lt;p>My modern equivalent is global roaming. I have T-Mobile which offers free global roaming as part of their standard plan and it’s amazing being able to travel that way. So much of the modern world’s amenities depend on being online at any time that I can’t imagine traveling without it. Using Uber or Lyft would be significantly more difficult. Same for the electric scooter rental apps that require scanning a QR code. Or just being able to quickly look up your location to figure out where you are and how to get to your destination.&lt;/p></description></item><item><title>Tech startups are not the only ones taking risks</title><link>/2018/12/20/tech-startups-are-not-the-only-ones-taking-risks/</link><pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/20/tech-startups-are-not-the-only-ones-taking-risks/</guid><description>&lt;p>This post has been sitting on my to-write list for multiple years now but it’s a topic I’ve been meaning to write ever since my Makers Alley experience. When we created Makers Alley the goal was to create a marketplace for local designers, woodworkers, and metalworkers to offer their pieces for sale along with some customization. To get the marketplace started we had dozens of meetings with these artists describing what we were trying to do, explaining the product, taking photos, and generally pitching to get them signed up.&lt;/p></description></item><item><title>Facebook's latest hit</title><link>/2018/12/19/facebooks-latest-hit/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/19/facebooks-latest-hit/</guid><description>&lt;p>The Facebook hits just keep on coming. The latest is a &lt;a href="https://www.nytimes.com/2018/12/18/technology/facebook-privacy.html">NY Times expose&lt;/a> around Facebook’s lack of privacy or data protection when it came to their strategic partner integrations. Citing some examples won’t do it justice and the entire article is worth a read just to see how loose Facebook was with user data. It’s shocking how ridiculous some of these practices were. I’m not surprised by the data Facebook is collecting or how it’s being used for ad targeting but I’m amazed at how much data they allowed to leave their platform. Facebook’s entire bread and butter is user data and letting it leave the “walled garden” for some short term benefit was misguided. It’s impossible to get that data back and especially now with everyone eagerly looking for Facebook’s missteps it makes them look especially imprudent.&lt;/p></description></item><item><title>New iteration of DevOps</title><link>/2018/12/19/new-iteration-of-devops/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/19/new-iteration-of-devops/</guid><description>&lt;p>While catching up on some tech news today I discovered &lt;a href="https://opsmop.io">OpsMop&lt;/a> and &lt;a href="https://vespene.io">Vespene&lt;/a>. Both of these are new DevOps tools from Michael DeHaan, the creator of Ansible. Before we had a DevOps team I was doing the bulk of our AWS management through the AWS console as well as a few command line scripts but as soon as we had a real DevOps that introduced the modern DevOps stack (Ansible, Terraform, Packer, Kubernetes) I was hooked. Doing the work through the AWS console is quick and easy but inevitably leads to inconsistencies that get worse and worse as your stack gets more and more complicated.&lt;/p></description></item><item><title>I finally tried an electric scooter</title><link>/2018/12/18/i-finally-tried-an-electric-scooter/</link><pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/18/i-finally-tried-an-electric-scooter/</guid><description>&lt;p>I live in the NYC area which has Citi Bike but doesn’t allow any scooter companies to operate. In fact it actually disallows any electric bikes/scooters/skateboards/hoverboards and only makes an exception for pedal-assisted bikes. And since I haven’t been traveling much the closest I could get to trying an electric scooter was reading the endless series of articles about them.&lt;/p>
&lt;p>Luckily for me I went on a vacation to Mexico City and got the chance to try a Lime scooter. It was great. I use Citi Bike whenever I can in NYC so I didn’t expect the scooters to be that much different but it felt much more liberating with significantly less friction. The big difference is that there is no docking station which is always a concern in NYC: a station may be empty when I’m looking for a bike or a station is full and I need to dock a bike. The electric powered also makes it much easier to just hop on and go. I don’t mind the Citi Bike pedaling but the scooter only needed a small kick-start and then quickly accelerated to over 14 mph. The scooter was also much smaller and lighter than a Citi Bike which added to the feeling.&lt;/p></description></item><item><title>The new company town</title><link>/2018/12/17/the-new-company-town/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/17/the-new-company-town/</guid><description>&lt;p>A few months ago I read an article describing Amazon’s &lt;a href="https://www.theinformation.com/articles/amazon-considers-offering-home-insurance">potential move&lt;/a> to offer home insurance. The premise is that Amazon is incredibly strong operationally and both their integration of Echo into all things home and their acquisition of Ring gives them valuable signals that the typical insurer does not have.&lt;/p>
&lt;p>I couldn’t help but think back to the 19th century where companies were large enough to &lt;a href="https://en.wikipedia.org/wiki/Company_town">own&lt;/a> all the property in a town and have their employees live there. As one can imagine it was extremely exploitative since you had the company that paid you also collect money from you for your housing. They died down in the 20th century but this feels like a step in that direction. We’re clearly not all going to work for Amazon but the idea of having a single company vertically integrating around our lives seems a bit dystopian.&lt;/p></description></item><item><title>Stuck on a problem? Take a break</title><link>/2018/12/16/stuck-on-a-problem-take-a-break/</link><pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/16/stuck-on-a-problem-take-a-break/</guid><description>&lt;p>When struggling with a difficult problem that I just can’t figure out I’ve found that it helps to just stop focusing on it and work on something else. Oftentimes that break is enough to get the problem into the subconscious where after a bit of stewing it just clicks. It may feel as if you’re admitting defeat and giving up but that break gives you the respite that is necessary to find the passive inspiration to solve a problem. People say that they often have the best ideas in the shower - in fact Archimedes supposedly found his “Eureka” moment in the bathtub - but I get my moment of inspiration when I’m either waking up, falling asleep, or doodling. In all those cases my mind is wandering and miraculously decides that it actually wants to tackle the problem.&lt;/p></description></item><item><title>New code is not a linear increase in complexity</title><link>/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/</link><pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/15/new-code-is-not-a-linear-increase-in-complexity/</guid><description>&lt;p>When it comes to software development we often add features simply because it’s easy. And almost always they are - just add an additional optional argument or two to a function and suddenly you’ve expanded your application’s functionality. The catch is that this assumes that this new code is a linear increase in complexity but it’s not.&lt;/p>
&lt;p>Computer science has the concept of “&lt;a href="https://en.wikipedia.org/wiki/Big_O_notation">Big O notation&lt;/a>” to measure how a function behaves as a function of it’s input. A “Big O” of O(N) is linear while O(N^2) is quadratic. The implicit goal is that you should strive to write your code to minimize it’s complexity with the ultimate goal being O(1). The same approach can be applied to code complexity. How much will the new functionality affect the complexity of the code? A seemingly simple code change may change the “exponent” of your code’s complexity and a complex code change may actually reduce your code complexity. The code change is not always correlated with the complexity that is being introduced.&lt;/p></description></item><item><title>Overanalyzing Medium's tag suggestions</title><link>/2018/12/15/overanalyzing-mediums-tag-suggestions/</link><pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/15/overanalyzing-mediums-tag-suggestions/</guid><description>&lt;div class="right10">
 &lt;img src="/image//medium-suggestions-startup.png" alt="Medium's tag suggestions for startup" data-width="176" data-height="200" data-layout="fixed" />
&lt;/div>
&lt;p>While I keep my primary blog on dangoldin.com I also cross post to Medium to get some more views but also because I’m curious to compare the performance. While adding tags to one of these posts I typed in startup and saw the following suggestions and their frequencies: Startup (334K), Startup Lessons (10.8K), Startup Life (3.6K), Startup Marketing (2.3K), and Startup Ideas (1.1K).&lt;/p></description></item><item><title>State of tech in 2018</title><link>/2018/12/14/state-of-tech-in-2018/</link><pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/14/state-of-tech-in-2018/</guid><description>&lt;p>I’m a big fan of the &lt;a href="https://stratechery.com/">Stratechery&lt;/a> newsletter and eagerly read every article that comes out. This past week a post came out that gave a wonderful description of the &lt;a href="https://stratechery.com/2018/the-state-of-technology-at-the-end-of-2018/">state of tech in 2018&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>This, then, is the state of technology in 2018: the enterprise market is thriving, and the consumer market is stagnant, dominated by the “innovations” that a few large behemoths deign to develop for consumers (probably by ripping off a smaller company). Meanwhile a backlash is brewing on both sides of the political spectrum, but with no immediately viable outlet through competition or antitrust action, the politics surrounding technology simply becomes ever more rancid.&lt;/p></description></item><item><title>Counting the number of lines of code in a GitHub account</title><link>/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/</link><pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/</guid><description>&lt;p>It’s surprisingly difficult to count the number of lines of code in a GitHub account. One day I’d like to come up with a fully automated solution but in the meantime I’ve come up with a workaround that gets me what I need.&lt;/p>
&lt;ol>
&lt;li>Follow the steps in the following &lt;a href="https://stackoverflow.com/a/29012789">Stack Overflow&lt;/a> answer to create your own command, cloc-git, that fetches a repo and runs another utility, &lt;a href="https://github.com/AlDanial/cloc">cloc&lt;/a>, that counts the number of lines in a git repo.&lt;/li>
&lt;li>Get all your repos into a single file, one per line.&lt;/li>
&lt;li>Bulk edit the file to have each line be an invocation of the clock-git command and save them all to a single file. For example, a single line of the file should be of the format: cloc-git &lt;a href="mailto:git@github.com">git@github.com&lt;/a>:dangoldin/dangoldin-blog.git&lt;/li>
&lt;li>In the command line simply execute the file and pipe into an output file, for example sh loc.sh &amp;gt; lines-of-code&lt;/li>
&lt;li>Once the previous step succeeds you’ll have a single text file with the output of the cloc-git command for every specified repo but the formatting is not the easiest to follow.&lt;/li>
&lt;li>Run a simple grep command to get every line containing the SUM line: grep “SUM” lines-of-code and save this to the clipboard&lt;/li>
&lt;li>Unfortunately the spacing is all off so you can’t use the cut command to do a split via the shell so you have to use a simple programming language. I used python and just dumped the contents into a single variable and ran the following command to split it into lines and then retrieve the last value when splitting by a space.&lt;/li>
&lt;li>Once you have these values just do a simple sum to get the total number of lines.&lt;/li>
&lt;/ol>
&lt;p>It’s not very simple and forces you to use a variety of tools to get to the final result, ranging from reading Stack Overflow documentation to some shell commands to some Python scripting. It’s a good example of where having enough breadth of knowledge and experience with a variety of tools turns a hairy problem into one that can be solved relatively quickly. It’s not a perfect solution but for a one-off I’m happy with the results.&lt;/p></description></item><item><title>Scenepeek</title><link>/2018/12/12/scenepeek/</link><pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/12/scenepeek/</guid><description>&lt;p>Way back when as I was just leaving the finance world to go into tech I attempted a startup with a friend called Scenepeek. The goal was to constantly aggregate all of a city’s events and make them easily searchable and discoverable by people who were looking to do something. Since both of us were in NYC and there are always a ton of events we decided that it would be the perfect candidate. We built a ton of scrapers optimized for all sorts of different sites, created a data model that was able to support nearly any type of event, and ended up launching a pretty crude but functional site. It never succeeded but we learned a ton throughout the process.&lt;/p></description></item><item><title>The price of AWS vs GitHub</title><link>/2018/12/11/the-price-of-aws-vs-github/</link><pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/11/the-price-of-aws-vs-github/</guid><description>&lt;p>This is a bit of an odd comparison since they offer two very different services with very different cost models but it’s just incredible how much more expensive AWS is than GitHub. It makes sense that GitHub is significantly cheaper since it’s fundamentally just git hosting and it has virtually zero marginal costs to support new customers. AWS on the other hand is bills entirely based on usage and has to allocate the additional hardware for every customer and is definitely not zero marginal cost. Yet they’re both integral in the modern tech ecosystem and are used extensively by companies and startups.&lt;/p></description></item><item><title>EMR vs Databricks costs</title><link>/2018/12/10/emr-vs-databricks-costs/</link><pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/10/emr-vs-databricks-costs/</guid><description>&lt;p>It’s frustrating when vendors introduce their own currency in what seems to be a way to obfuscate pricing. The most recent example is Databricks which offers a slick Spark hosting solution on top of AWS and Azure. Unfortunately, instead of being explicit about the prices they introduced a Databricks Unit (DBU) currency type that then translates into dollars based on the type of usage - ranging from a simple Spark cluster with limited optimizations (Basic Plan) to an interactive one with all sorts of behind the scenes performance tweaks (Data Analytics Plan).&lt;/p></description></item><item><title>Avoiding content overload</title><link>/2018/12/09/avoiding-content-overload/</link><pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/09/avoiding-content-overload/</guid><description>&lt;p>While going through my list of blog post ideas I found one from a few years ago titled “Content Overload” which was meant to highlight how much content I was bombarded with across a variety of channels. I had a bunch of thoughts bemoaning how difficult it was to keep up but these days it’s just not an issue for me. I’m no longer trying to consume everything I can and instead get a lot of my news from conversations with friends and coworkers as well as Twitter. I try to check in on Hacker News every day but occasionally go on stretches where I forget about it for a few days and no one’s the wiser. I sadly gave up on my RSS feed and stopped checking into the other aggregators. Important news will inevitably make its way to me and the less important news I enjoy finding serendipitously. I wouldn’t be surprised if my content consumption actually dropped over the past few years. I’m reading many more books now and am okay with just not keeping up with everything. The one digital content channel that has gotten stronger is email: I’ll subscribe to interesting newsletters and prune these aggressively to make sure my email stays clean and high signal. I may be unique here but it seems I’ve managed to cope with the challenge of content overload.&lt;/p></description></item><item><title>The modern economy relies on information</title><link>/2018/12/09/the-modern-economy-relies-on-information/</link><pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/09/the-modern-economy-relies-on-information/</guid><description>&lt;p>In many ways modern capitalism is about bigger and bigger companies owning more and more industries. We’re seeing with the major tech companies moving into new verticals to keep growing. We’re seeing it with the media and telecom companies going on acquisition sprees aimed at both vertical integration and horizontal scale.&lt;/p>
&lt;p>Yet there are also ton of small business being launched across a variety of industries - especially in consumer products. The world is much more open now and if you offer something unique and compelling it’s easier to find customers than at any other time in history. You don’t need to invest in a ton of marketing or advertising so long as you know your audience and figure out the best channels to reach them.&lt;/p></description></item><item><title>Automatic login</title><link>/2018/12/08/automatic-login/</link><pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/08/automatic-login/</guid><description>&lt;p>This is somewhat heretical but whenever I receive an email that has a link behind a login I wish clicking the link would automatically log me in. I’ve spent more time than I’d like to admit typing my username and password on various sites and too often have had to do it multiple times to get the password right. It would be amazing if clicking a link on an email would automatically log me in and navigate to the linked page.&lt;/p></description></item><item><title>Speech recognition and a bunch of APIs</title><link>/2018/12/07/speech-recognition-and-a-bunch-of-apis/</link><pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/07/speech-recognition-and-a-bunch-of-apis/</guid><description>&lt;p>Google’s Duplex is clearly impressive but what I actually want to see is the frontend of Duplex hooked up to the various booking APIs. For example, instead of actually making a call to a restaurant on my behalf it would just use the various booking sites already out there, such as OpenTable, to make the reservation. From the caller’s experience the two are indistinguishable since you’re just using your voice but on the restaurant side there’s no need to talk to someone to do something that can be done through a series of API calls. The calling approach is incredibly impressive and makes sense when a business doesn’t offer any online booking functionality but many do and involving a person introduces overhead.&lt;/p></description></item><item><title>Conference call echoes</title><link>/2018/12/06/conference-call-echoes/</link><pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/06/conference-call-echoes/</guid><description>&lt;p>I’m sure conference calls have improved significantly over the past decade yet they still feel incredibly behind. They’re generally stable but it’s boggling that we still haven’t been able to figure out how to get rid of the echo. Whenever there are two unmuted people in a single room dialed in to the same meeting you hear the effect. As soon as one speaks you hear loud and whiny static that only goes away when one of the participants mutes themselves. The solution is incredibly simple and it’s shocking that there’s no automated solution.&lt;/p></description></item><item><title>The golden age of browsers</title><link>/2018/12/05/the-golden-age-of-browsers/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/05/the-golden-age-of-browsers/</guid><description>&lt;img src="/image/browser-share.png" alt="Browser market share June 2018" data-width="1248" data-height="1112" data-layout="responsive" />
&lt;p class="caption">&lt;a href="https://en.wikipedia.org/wiki/Usage_share_of_web_browsers">Browser market share June 2018&lt;/a>&lt;/p>
&lt;p>In June 2018 the top 5 browsers had an estimated 94.98% share of the browser market. This makes sense since most people either use their OS’s default browser or find a mainstream alternative, such as Chrome or Firefox. Yet there’s a lot of interesting and novel browser work happening on the fringes. &lt;a href="https://brave.com">Brave&lt;/a>, started by Brendan Eich (creator of JavaScript and cofounder of the Mozilla project), is designed for privacy and comes with built-in adblocking; &lt;a href="https://vivaldi.com">Vivaldi&lt;/a> is all about customization and tab management; &lt;a href="http://www.ucweb.com">UC Browser&lt;/a> is all about mobile performance - and is, in fact, the third most popular mobile browser.&lt;/p></description></item><item><title>How many wifi devices do we have?</title><link>/2018/12/04/how-many-wifi-devices-do-we-have/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/04/how-many-wifi-devices-do-we-have/</guid><description>&lt;p>Benedict Evans wrote a &lt;a href="https://www.ben-evans.com/benedictevans/2014/5/26/the-internet-of-things">great post&lt;/a> back in 2014 that started with the observation that while our grandparents knew how many motors they owned we have no idea and that while we know how many of our devices connect to the internet our children will not.&lt;/p>
&lt;p>It sounded nice at the time but I lived it today. Verizon sent me a new router and rather than taking the small amount of time to change the wifi network name and password back to the previous version I decided it wouldn’t be that hard to just update the wifi settings in the various devices. After I updated my phone, computer, and Fire TV Stick I realized I still had many more to go and decided, albeit too late, that it would be easier to just revert the settings. What makes changing the settings difficult is that updating the wifi settings on devices without a keyboard is just incredibly difficult. For some, such as the Fire TV Stick, you have to type using a remote and an on-screen keyboard. For others, such as the Echo, you have to switch back and forth between wifi networks to get everything set up.&lt;/p></description></item><item><title>Overcoming writing rustiness</title><link>/2018/12/03/overcoming-writing-rustiness/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/03/overcoming-writing-rustiness/</guid><description>&lt;p>Now that I’ve forced myself to write every day in order to catch up on my blogging goal it’s been much easier. The challenge was always getting started but as soon as I start the thoughts and sentences come out pretty quickly. They nearly always require a bit of editing and cleanup at the end but the initial dump is usually a pretty good basis for the rest of the post. Knowing that I need to write more than a post a day is enough to motivate me to put the proverbial pen to paper and get over that initial hump.&lt;/p></description></item><item><title>Mobile payments and messaging apps</title><link>/2018/12/02/mobile-payments-and-messaging-apps/</link><pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/02/mobile-payments-and-messaging-apps/</guid><description>&lt;p>I wrote a &lt;a href="/2018/11/14/what-messaging-war/">post&lt;/a> almost a month ago describing my relationship with messaging apps: despite the fact that are dozens of messaging apps competing against one another I treat them all as a commodity. The same situation exists with mobile payments. It feels less fragmented than the messaging space and yet there are quite a few players here: Apple Wallet, Google Pay, PayPal, Venmo (owned of PayPal), Zelle, Cash App, in addition to the region specific companies, such as AliPay and WeChat in China to PayTM in India.&lt;/p></description></item><item><title>Fat specs, light stories</title><link>/2018/12/01/fat-specs-light-stories/</link><pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate><guid>/2018/12/01/fat-specs-light-stories/</guid><description>&lt;p>Most modern software companies use some form of agile as their software development process. There are a variety of different approaches and forms out there and each company ends up with a style that works for them. One of the core beliefs I’ve developed is that stories should be kept light and that time and effort should be spent on the product requirement documents and specs. Investing in the PRDs and specs encourages everyone to understand the high level problem being solved and how the various pieces of the solutions fit together. Otherwise you run the risk of mistaking the forest for the trees where each ticket is done as written but when combined they don’t actually work to make the proper whole.&lt;/p></description></item><item><title>Tragedy of the commons: Apartment edition</title><link>/2018/11/30/tragedy-of-the-commons-apartment-edition/</link><pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/30/tragedy-of-the-commons-apartment-edition/</guid><description>&lt;p>‘Tis the season where mailrooms get filled with delivery boxes and it’s getting busier every year. Mary Meeker mentioned the trend of building lobbies becoming &lt;a href="/image/mm-landlord-storage.png">ecommerce storage&lt;/a> facilities and it’s more true than ever. My building has a small package room which has been filled solid this week after Black Friday and Cyber Monday.&lt;/p>
&lt;p>It’s so inaccessible that it’s easier for me to just wait for others to get their packages and free some space before I go in to search for my own. Unfortunately, if everyone feels that way then no one gets their package and it becomes a true &lt;a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">tragedy of the commons&lt;/a>. Instead of helping each other out we’re waiting for others to improve the situation while it constantly gets worse with newly arriving packages.&lt;/p></description></item><item><title>A delivery aggregation service</title><link>/2018/11/29/a-delivery-aggregation-service/</link><pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/29/a-delivery-aggregation-service/</guid><description>&lt;p>Full disclosure: I know very little about the food delivery space so this is more speculative than anything rooted in reality.&lt;/p>
&lt;p>Given how many companies are doing food delivery it seems there should be a single delivery aggregation service that combines all of them to act as the perfect dispatcher for drivers. If I’m delivering food on behalf of Uber Eats, DoorDash, Postmates, Grubhub, and the countless others I missed it would make sense for me to be able to pick up from as many nearby restaurants as possible and then deliver to as many colocated locations as possible. The ideal situation would be that I’m able to pick up all my deliveries from a single restaurant and then deliver them all to the same floor of an apartment building.&lt;/p></description></item><item><title>Shell history: 2018 edition</title><link>/2018/11/28/shell-history-2018-edition/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/28/shell-history-2018-edition/</guid><description>&lt;p>In what has become an annual tradition I have a very simple shell script that generates a frequency of my most commonly run shell commands. This year saw a pretty big change from 2017. The most obvious difference is that I use “git” more frequently than in the past. This is a tough one to analyze by looking at the data since my usage of &lt;a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh&lt;/a> skews the data. It provides a variety of git aliases - for example gp for git push and gco for git checkout - that appear elsewhere in the results so my pure use of “git” is almost isolated to the cases where I do a commit.&lt;/p></description></item><item><title>Privacy in a face detection world</title><link>/2018/11/27/privacy-in-a-face-detection-world/</link><pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/27/privacy-in-a-face-detection-world/</guid><description>&lt;p>In a bit of dystopian news a &lt;a href="https://www.caixinglobal.com/2018-11-22/ai-mistakes-bus-side-ad-for-famous-ceo-charges-her-with-jaywalkingdo-101350772.html">CEO was charged for jaywalking&lt;/a> when a face detection algorithm saw her face on a bus ad. While simultaneously amusing and dystopian it does make one think of a world in which everyone is constantly monitored. At some level we’re already in this world digitally - our browsing behavior is constantly tracked and we all have ad behavior profiles that are constantly being tweaked and updated. Some get around this by using adblocking while a small fraction take the opposite approach and have their browser search and navigate to random pages in order to clutter and confuse their digital footprints.&lt;/p></description></item><item><title>An ad on the Google search homepage</title><link>/2018/11/26/an-ad-on-the-google-search-homepage/</link><pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/26/an-ad-on-the-google-search-homepage/</guid><description>&lt;img src="/image/google-search-store-ad.png" alt="Google Search Homepage Store Ad" data-width="2266" data-height="1209" data-layout="responsive" />
&lt;p>Turns out not even Google is immune from the pressures of Cyber Monday. As minimal as the Google search homepage is they still made the effort to have a callout, however slight, to promote their Cyber Monday deals. A while back I read an article that calculated how much additional revenue Google would be able to generate if they put ads on their homepage. I don’t recall the exact number or the details but I imagine it’s a massive amount. Yet Google never took the step of putting ads on the search homepage so it’s interesting to see them doing it for their own store. The revenue here must be significantly less than what they’d see through traditional advertising but at the same time it’s a very minimal hit to the user experience. I can only imagine how many people needed to be involved in this decision though.&lt;/p></description></item><item><title>Aggressive code deprecation</title><link>/2018/11/25/aggressive-code-deprecation/</link><pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/25/aggressive-code-deprecation/</guid><description>&lt;p>Part of writing high quality code quickly is deprecating no longer used features and functionality. It sounds simple but more often than not there’s an abundance of references throughout - some tightly coupled and others loosely coupled - that make a full deprecation difficult. In some cases it’s is as simple as an isolated code change while in other cases it’s removing code along with some database migrations and in the extreme case it may be removing an entire service. It’s crucial to be exhaustive in your deprecation or you’ll end up in a situation months or years later where the team has changed enough that no one can tell what the code is meant to do and whether it’s still used.&lt;/p></description></item><item><title>Exploring my backlinks</title><link>/2018/11/25/exploring-my-backlinks/</link><pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/25/exploring-my-backlinks/</guid><description>&lt;p>I enjoy &lt;a href="https://hackernoon.com">Hacker Noon&lt;/a> and often find myself coming across an interesting article on the site. Yesterday I got a pleasant surprise from an old coworker who sent me an &lt;a href="https://hackernoon.com/how-to-create-a-slack-bot-that-messages-all-members-of-a-workspace-in-8-minutes-32a5b52838be">article&lt;/a> about building a Slack bot that mentioned of one of my &lt;a href="http://dangoldin.com/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/">blog posts&lt;/a>. A few months earlier the author and I had a conversation in the comment thread and I’m glad she found the conversation useful enough to write a much more thorough post.&lt;/p></description></item><item><title>Adding optionality to products</title><link>/2018/11/24/adding-optionality-to-products/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/24/adding-optionality-to-products/</guid><description>&lt;p>Building products involves making countless decisions. One of the biggest is defining the functionality and how it should be exposed to the users. In my mind, answers to this question like on a spectrum. On one extreme you have the “take what you can get” approach where the functionality is exposed with no customization and no advanced features but the experience is optimized for one specific use case. As an example of this think of the original Google search - a single search field, minimal search functionality, and two buttons. On the other extreme you have the “customize everything” approach where you think of all possible use cases and provide options to allow users to do what they want. An example of this is Microsoft Word - most people use a fraction of all the functionality yet there’s a ton hidden away behind some menu.&lt;/p></description></item><item><title>Code without online help</title><link>/2018/11/24/code-without-online-help/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/24/code-without-online-help/</guid><description>&lt;p>Whenever I need some coding help my first step is to do an online search which usually leads me to either the library documentation or a StackOverflow page. This is a poor habit and something I’m trying to move away from. While I’m almost always online it’s dangerous to rely on the internet to code - both because there will be times you may not have internet access but also because you lose the ability to do your own investigation, discovery, and critical thinking.&lt;/p></description></item><item><title>Electronic goods are cheaper than ever</title><link>/2018/11/23/electronic-goods-are-cheaper-than-ever/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/23/electronic-goods-are-cheaper-than-ever/</guid><description>&lt;p>While stumbling across yet another Black Friday deal it hit me that electronics are ridiculously cheap. One can get state of the art computers for about a thousand dollars and huge flat panel TVs for a few hundred dollars. I grew up in the 90s and it never felt that comparable gadgets were that cheap. It may be that I didn’t have a job and nearly everything was out of reach but I suspect electronic goods really are that much cheaper now. Even if a product launches at a higher price it doesn’t stay that way for long and drops much quicker than before. No price has staying power and there are always more efficient competitors catching up.&lt;/p></description></item><item><title>History's largest empires</title><link>/2018/11/22/historys-largest-empires/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/22/historys-largest-empires/</guid><description>&lt;p>I got a bit distracted today and ended up coming across the Wikipedia page listing &lt;a href="https://en.wikipedia.org/wiki/List_of_largest_empires">history’s largest empires&lt;/a>. The page came with a list of the top 140 by land area and just by looking at them you can see there’s a huge range. The British Empire was the largest at 35.5 million square kilometers while the Sumer was the smallest at 0.05. That’s a huge difference - over 700 times - and I thought it would be interesting to plot them to visualize the distribution. As expected, there’s a very steep drop and a long tail. If you add up the land areas of all the empires listed you get just over 455 million square kilometers. That metric itself doesn’t mean anything but it helps to normalize the land areas. The British Empire, for example, is 8% of the total and if you keep going down the list in descending order by size and sum up the percentages you get that the first 55 empires add up to 80% of the total land area. Once again, the total land area is useless metric but it allows us to see how close we are to the &lt;a href="https://en.wikipedia.org/wiki/Pareto_principle">80-20 rule&lt;/a>. It turns out not too close - 55 countries out of 140 are just over 39%. The top 20% empires add up to 52% of the land area. If you’re interested in playing around with the data it’s up on the Wikipedia page as well as an &lt;a href="/assets/static/data/largest-empires-land-area.xlsx">Excel version&lt;/a> with cleaned up data.&lt;/p></description></item><item><title>Superhuman review</title><link>/2018/11/21/superhuman-review/</link><pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/21/superhuman-review/</guid><description>&lt;p>I’m all about productivity so when I heard about &lt;a href="https://superhuman.com">Superhuman&lt;/a> I decided to give it a shot. I’ve been using it for the past few weeks and while it’s a solid product and well built it didn’t suit me. The onboarding experience is great and there’s a ton of functionality that Superhuman provides that makes it that much easier to go through email. This ranges from a variety of shortcuts (that can all be quickly found using Cmd+K) to functionality that you wish existed in email, such as typing a date and immediately seeing that day’s calendar to being able to copy a whole email message - attachments and all.&lt;/p></description></item><item><title>My DataEngConf 2018 talk</title><link>/2018/11/20/my-dataengconf-2018-talk/</link><pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/20/my-dataengconf-2018-talk/</guid><description>&lt;p>On November 9th I had the privilege of speaking at DataEngConf under the “Hero Engineering” track. My talk was titled “The Highs and Lows of Building an AdTech Data Pipeline” and I covered our evolution from a dead simple, sampled approach that had nothing to do with big data to the latest version which is leveraging a variety of modern open source data technologies.&lt;/p>
&lt;p>I spoke about the motivation, challenges, and lessons learned during each iteration and ended the talk with the top 3 lessons learned across the various iterations of the pipeline. If you’re interested in the details you can grab the slides as either &lt;a href="https://docs.google.com/presentation/d/1XmOPgsbxoah2Pulw3eRvzjClOM5A5Dq-B2MWfot0guo/edit#slide=id.p">Google Slides&lt;/a> or as PowerPoint from the &lt;a href="https://www.dataengconf.com/speaker/the-highs-and-lows-of-building-an-adtech-data-pipeline">DataEngConf site&lt;/a>. Note that there was also a recording made but I’m still waiting for it to be processed and uploaded it to YouTube and will share that when it’s available.&lt;/p></description></item><item><title>Computer history books</title><link>/2018/11/19/computer-history-books/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/19/computer-history-books/</guid><description>&lt;p>I’m fascinated by computer history and love reading computer history books. We live in such a digital heavy world that it’s difficult to imagine life without it. Yet it really is fairly recent. Personal computers only started becoming popular in the 1980s and the internet was only introduced in the 1990s. And it took a whole decade before the internet started resembling what we currently see. If we take the least restrictive definition of a computer we still get that computers have existed for less than a hundred years. That’s a blink of an eye in our history and it’s incredibly rewarding to read about the origin of the industry I’m a part of. Part of me wishes that I was around in the formative years so this is my way of feeling a little bit of that spark and discovery.&lt;/p></description></item><item><title>Falling behind my 2018 blogging goal</title><link>/2018/11/18/falling-behind-my-2018-blogging-goal/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/18/falling-behind-my-2018-blogging-goal/</guid><description>&lt;p>Since 2013 I’ve been writing 2 blog posts a week. This stemmed from a conscious effort to improve my writing, clarify my thought process, and grow my brand. It’s been quite a ride and while difficult I’ve been able to do keep it going for 5 consecutive years. This year I’m significantly behind but am still committed to catching up and hitting 104 posts. As I write this I’m only at 42 posts for 2018; I have 42 days to write 62 posts which means I need to write nearly 1.5 posts a day to make up for my prior sloth. I’m going to do as much as I can to hit that goal and will be extremely disappointed if I’m unable to do it. That may mean that some of the posts will be on the shorter side and half baked but there will be some that do offer something valuable and insightful. My blogging history tells me that it’s impossible (for me at least) to actually tell which posts will be popular and the best I can do is keep writing and share more of my thoughts. The one element I will not compromise on is editing - poor writing is a pet peeve and I’ll continue to edit my posts after they’re written.&lt;/p></description></item><item><title>Random quotes</title><link>/2018/11/17/random-quotes/</link><pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/17/random-quotes/</guid><description>&lt;p>Besides blogging I’m a big reader and normally get through read a book a week. Most of my reading is done on Kindle and whenever I come across any interesting passage or quote I highlight them. Then every few weeks I go through my highlights, add them to a single &lt;a href="https://github.com/dangoldin/quotes/blob/master/quotes.txt">file&lt;/a>, and dump it into a GitHub &lt;a href="https://github.com/dangoldin/quotes">repo&lt;/a>. Earlier today I decided to do something fun and wrote a simple &lt;a href="https://bots.dangoldin.com/quoteme">endpoint&lt;/a> to fetch a random quote. The majority of my reading is nonfiction and this is reflected in my quotes. In addition, I’m a sucker for biographies and computer history books so they’re an even larger share of the quotes. I also like to throw a few idioms into the mix. So if you’re ever in the mood for a random quote take a look at &lt;a href="https://bots.dangoldin.com/quoteme">https://bots.dangoldin.com/quoteme&lt;/a>.&lt;/p></description></item><item><title>Python 3 and aiohttp</title><link>/2018/11/16/python-3-and-aiohttp/</link><pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/16/python-3-and-aiohttp/</guid><description>&lt;p>A few months back I read about &lt;a href="https://aiohttp.readthedocs.io/en/stable/">aiohttp&lt;/a> and asyncio and finally got the chance to play around with it a few weeks back. The project was a quick one-off scrape of a few thousand domains to see what percentage had implemented the &lt;a href="https://github.com/InteractiveAdvertisingBureau/GDPR-Transparency-and-Consent-Framework/blob/master/pubvendors.json%20v1.0%20Draft%20for%20Public%20Comment.md">pubvendors.json&lt;/a> spec, an extension of GDPR that allows publishers to specify the vendors they’re working with.&lt;/p>
&lt;p>My initial reaction was to do it in the way I’ve done it countless times before: the requests library in a for loop. Instead I decided to actually try something new and use the aiohttp library, a new asynchronous library for Python 3. It took me a little bit of time to figure out how to structure the code and use Python’s new async functionality (which by the way is very similar to modern JavaScript) but the end result is simple for what it does and runs incredibly quickly.&lt;/p></description></item><item><title>Limiting tracking in email</title><link>/2018/11/15/limiting-tracking-in-email/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/15/limiting-tracking-in-email/</guid><description>&lt;p>It’s tough to protect your digital privacy with the modern web but I try my best. One of the tricks I’ve been using is to disable all images and avoid clicking any links that don’t go directly to the desired location. The reason for disabling images is that it prevents your opening of the email from being tracked since the image request (which contains some unique identifier) does not get made. The reason for not clicking on any of the links is similar - they’re rarely the final page you’re trying to get to but instead go through an intermediary that’s able to track your click. You can identify these by hovering over the link and seeing what the destination actually shows in the status bar. If it’s the page you want, great, otherwise I’ll go to the desired page directly or do an incognito search to get where I need to go.&lt;/p></description></item><item><title>What messaging war?</title><link>/2018/11/14/what-messaging-war/</link><pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/14/what-messaging-war/</guid><description>&lt;p>Apparently there’s a “messaging war” going on among the dozens of apps and social networks, all competing to be the dominant messaging app. There really are a ton of these. I have a folder on my phone dedicated to messaging and it contains Apple Messages, Google Hangouts, WhatsApp, WeChat, Telegram, Facebook Messenger, and Signal. In addition, Twitter, Instagram, and Snapchat all have messaging functionality. Adding these up I have 10 messaging apps on my phone.&lt;/p></description></item><item><title>A UX gem in Google Slides</title><link>/2018/11/13/a-ux-gem-in-google-slides/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/13/a-ux-gem-in-google-slides/</guid><description>&lt;p>I’m a sucker for subtle UX gems and Google continues to amaze me. While working on a presentation using the Slides product I had to modify the colors of a few objects. The UX for this is usually pretty normal - you select the object, hit the icon to modify the color, and choose a new color. Usually it’s slightly better and you get to see the existing color. What surprised me with the Google Slides experience is that I didn’t just see the existing color but also saw the previously picked color. Since I was modifying a series of objects to the same color this was a pleasant experience, especially since I was working off of a series of color shades that looked too similar. It would have been even better if I was able to bulk change the objects but a win’s a win.&lt;/p></description></item><item><title>Social Security Administration spoofing scam</title><link>/2018/11/12/social-security-administration-spoofing-scam/</link><pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate><guid>/2018/11/12/social-security-administration-spoofing-scam/</guid><description>&lt;p>I use Google Voice as my voicemail due to the built in transcription which lets me tell at a glance whether a call is spam or in the rare case, worth returning. This past Friday I had my phone on silent for most of the day and was pleasantly surprised with the following transcription greeting me after I checked in:&lt;/p>
&lt;img src="/image/ssa-scam.png" alt="Scam voicemail message" data-width="1110" data-height="298" data-layout="responsive" />
&lt;p class="caption">Scam voicemail message&lt;/p></description></item><item><title>Gmail's autocomplete</title><link>/2018/10/29/gmails-autocomplete/</link><pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate><guid>/2018/10/29/gmails-autocomplete/</guid><description>&lt;p>Ever since I’ve been managing I’ve spent an inordinate amount of time in my inbox. What’s both sad and amazing is how accurate the new Gmail autocompletions (officially called &lt;a href="https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">Smart Compose&lt;/a>) have gotten. They’re incredibly accurate for common phrases and are likely leveraging very similar technology to what Google is using for their autocomplete search. It’s great for finishing off that last sentence and even acts as a real time phrase thesaurus that helps suggest alternate phrasing.&lt;/p></description></item><item><title>Just ship it</title><link>/2018/10/25/just-ship-it/</link><pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate><guid>/2018/10/25/just-ship-it/</guid><description>&lt;p>I don’t know whether it’s the pace of modern life or something else but I no longer have a good intuition for recent events. Things that happened a few months ago feel as if they happened a year ago and things that happened a year ago feel as if they happened multiple years ago. An example of this was Twitter increasing a tweet’s character limit to 280. Try to guess when they increased the limit?&lt;/p></description></item><item><title>Equity in the gig economy</title><link>/2018/10/13/equity-in-the-gig-economy/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><guid>/2018/10/13/equity-in-the-gig-economy/</guid><description>&lt;p>Recently both &lt;a href="https://techcrunch.com/2018/09/23/airbnb-wants-to-give-its-hosts-equity-in-its-business/">AirBnB&lt;/a> and &lt;a href="https://gizmodo.com/uber-just-asked-the-sec-permission-to-give-its-drivers-1829708257">Uber&lt;/a> have asked the SEC to allow them to give equity to their contractors - hosts in the case of AirBnB and drivers in the case of Uber. The gig economy is clearly here to stay and it’s encouraging that we’re seeing companies trying to adapt to the changes. I’m sure it’s partially for the marketing spin - both AirBnB and Uber have been facing significant criticism lately - but even then this is a good example to set for future companies. We are moving towards the gig economy and being able to give equity to participants is a great way of sharing the wealth.&lt;/p></description></item><item><title>Incognito mode: Chrome vs Safari</title><link>/2018/08/21/incognito-mode-chrome-vs-safari/</link><pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate><guid>/2018/08/21/incognito-mode-chrome-vs-safari/</guid><description>&lt;p>To maintain some semblance of privacy I’ve been doing the majority of my searches in incognito mode. I normally use Chrome but a few days ago I had to do some browser testing in Safari and discovered that the two browsers handle incognito mode differently. Chrome’s incognito mode will share cookies across all incognito tabs - equivalent to running another instance of Chrome. Safari, on the other hand, will give each tab as its own cookie store.&lt;/p></description></item><item><title>Yahoo fantasy football stats: 2018-2019 edition</title><link>/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/</link><pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate><guid>/2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/</guid><description>&lt;p>This is much later than in previous years but hopefully that just makes the data more accurate. I updated my Yahoo fantasy football stats scraper to account for the slightly different design for the upcoming season. It still works as before and uses Selenium to open up Chrome and scrape the projected stats by week. The change this year involved shifting the columns around a tiny bit as Yahoo changed the order but other than that there were no changes. Maybe by next year I’ll update the script to be able to actually determine the column indices for each stat automatically. As usual, the script is up on &lt;a href="https://github.com/dangoldin/yahoo-ffl">GitHub&lt;/a> and the scraped data can just be downloaded &lt;a href="/assets/static/data/stats-2019.csv">here&lt;/a>.&lt;/p></description></item><item><title>Google Calendar: Constantly shipping</title><link>/2018/08/15/google-calendar-constantly-shipping/</link><pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate><guid>/2018/08/15/google-calendar-constantly-shipping/</guid><description>&lt;p>It’s impressive when companies constantly churn out products and features and it’s even more impressive the larger the company. In my mind this used to be Amazon but lately I’ve been surprised by how often Google Calendar is updated. It feels as if every month there’s at least one update that I accidentally discover and get a pleasant surprise.&lt;/p>
&lt;p>Google Calendar underwent a major redesign earlier this year and it seemed to have been the foundation for the recent improvements. I’m sure I’m not capturing everything but just in the past few months the following features were launched:&lt;/p></description></item><item><title>Privacy vs user experience</title><link>/2018/07/29/privacy-vs-user-experience/</link><pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/29/privacy-vs-user-experience/</guid><description>&lt;p>These days it feels as if every tech behemoth is competing with every other tech behemoth but one of the more interesting battles has been between Google and Apple. Google is the accepted leader in ML and AI and they leverage it to offer a better and constantly improving user experience. Apple, on the other hand, has been stressing that unlike Google it doesn’t make money off of data mining your data and is instead focused on privacy.&lt;/p></description></item><item><title>Bulk discounts hurt competition</title><link>/2018/07/22/bulk-discounts-hurt-competition/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/22/bulk-discounts-hurt-competition/</guid><description>&lt;p>While reading yet another &lt;a href="https://www.theinformation.com/articles/how-segway-ninebot-became-the-go-to-scooter-maker-for-rental-startups">article&lt;/a> about scooter startups I came across an obvious quote by Gao Lufeng, CEO of Segway-Ninebot, the leading scooter manufacturer: “As one of the biggest battery buyers, we have the bargaining power to get the lowest price in the market.”&lt;/p>
&lt;p>This is obvious and we see it everywhere: buying in bulk gets you a discount. As a consumer I can go to Amazon and look at any item and the per unit price when buying a single item is going to be higher than when buying a pack. And the more I buy the more the discount on a per unit basis. The business world is no different and it’s not surprising since both sides benefit in this situation: the buyer is able to get a cheaper product and the seller is able to get guaranteed sales while hopefully getting more efficiencies of scale and which further reduces the manufacturing cost.&lt;/p></description></item><item><title>Class action settlement emails</title><link>/2018/07/21/class-action-settlement-emails/</link><pubDate>Sat, 21 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/21/class-action-settlement-emails/</guid><description>&lt;p>I enjoy receiving the occasional class action emails. Other than being a customer during a particular period I didn’t do anything to encourage this and it’s always a surprise that usually comes with a free gift. Sometimes it’s a few dollars, other times it’s a free service, while other times it’s something I’m not qualified nor interested in.&lt;/p>
&lt;p>Last week I received an email from Optimum around a class action settlement. Optimum was my ISP years ago and I never had any problems so this email was a pleasant surprise. At the same time I can’t recall the last time I needed a wifi pass when traveling since so many shops and public spaces are already offering it for free.&lt;/p></description></item><item><title>In a software world, humanity comes first</title><link>/2018/07/10/in-a-software-world-humanity-comes-first/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/10/in-a-software-world-humanity-comes-first/</guid><description>&lt;p>Google Duplex was announced earlier this year as a way to automate tasks that require a phone call - such as booking an appointment. Rather than calling yourself google has an assistant that understands human speech and speaks convincingly enough to do these simple tasks. Relatedly, a few days I read &lt;a href="https://gizmodo.com/google-is-reportedly-looking-to-take-over-call-centers-1827379911">an article&lt;/a> describing how Google is also pushing Duplex to be used in call centers. A bit surprising but makes a ton of sense since the vast majority of the calls are relatively simple tasks that companies are already trying to automate as much as possible using Interactive Voice Response (IVR) systems.&lt;/p></description></item><item><title>MySQL foreign keys</title><link>/2018/07/07/mysql-foreign-keys/</link><pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate><guid>/2018/07/07/mysql-foreign-keys/</guid><description>&lt;p>Databases are the last layer of defense against corrupt data and the more restrictive you can make them the better. No matter how much validation you may have missed in your code having a strong and restrictive database schema will protect your data. One of the best approaches to building a restrictive schema is using foreign keys which specify how fields from one table relate to the fields of another table. There are a few options here and make it possible for you to specify anything from automatically removing rows when a row they’re referencing is removed to recursively updating rows when their references have changed.&lt;/p></description></item><item><title>Using personal AWS credentials in production</title><link>/2018/06/28/using-personal-aws-credentials-in-production/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>/2018/06/28/using-personal-aws-credentials-in-production/</guid><description>&lt;p>Earlier this week in a fit of security I went into AWS and revoked my old AWS credentials. I assumed that all would be well but unfortunately didn’t realize that my AWS credentials were being used on a production system that wrote data to S3. Before I revoked them I did see that the recent activity contained S3 but assumed it was just me playing around with the AWS CLI. Of course I shouldn’t have had my AWS credentials used on a live system and of course we updated the application to use its own account. At the same time the experienced taught me a few valuable lessons besides not using personal keys on deployed systems:&lt;/p></description></item><item><title>ALB and ELB access log schemas for Redshift</title><link>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</link><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid>/2018/06/05/alb-and-elb-access-log-schemas-for-redshift/</guid><description>&lt;p>Back in February I &lt;a href="/2018/02/20/analyzing-aws-elb-logs/">wrote&lt;/a> about using Redshift to quickly analyze ELB access logs. This worked great until we switched from using ELBs to using ALBs. Unsurprisingly in hindsight but frustrating at the time the ALBs have a different log schema. Both the &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html">Classic&lt;/a> and &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">Application&lt;/a> Load Balancer logs are well documented on the AWS site but unfortunately the code to create the appropriate Redshift schema is not. In the hope of helping others and passing it forward I wanted to share the Redshift schemas for both types of access logs.&lt;/p></description></item><item><title>Type 1 and Type 2 tech specs</title><link>/2018/06/01/type-1-and-type-2-tech-specs/</link><pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate><guid>/2018/06/01/type-1-and-type-2-tech-specs/</guid><description>&lt;p>Every year since Amazon went public, Jeff Bezos writes an insightful and penetrating shareholder letter that covers a variety of business topics driving Amazon’s success. In 2015 he wrote about Type 1 and Type 2 decisions:&lt;/p>
&lt;blockquote>
&lt;p>We want to be a large company that’s also an invention machine. We want to combine the extraordinary customer-serving capabilities that are enabled by size with the speed of movement, nimbleness, and risk-acceptance mentality normally associated with entrepreneurial start-ups.&lt;/p></description></item><item><title>Power of shell commands</title><link>/2018/05/26/power-of-shell-commands/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>/2018/05/26/power-of-shell-commands/</guid><description>&lt;p>It’s surprising how unappreciated shell commands are. They’re incredibly powerful and once understood are able to handle small one-off tasks much quicker than writing even simple scripts. Earlier this week I ran into a small task that highlights the power and ability of the shell.&lt;/p>
&lt;p>A few of our applications use the same configuration file which contains a variety of URLs, secrets, and passwords. If any of these applications require a field the it gets added to this growing configuration file. This is clearly not good for security and as part of a larger security revamp we’re moving to application-specific config files. Long term we want to revamp the way we do deploys such that the configuration is kept in environment variables and handled by the build system but as a short term solution we want to split this single configuration file into a file per application.&lt;/p></description></item><item><title>Memory as a stack</title><link>/2018/05/14/memory-as-a-stack/</link><pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate><guid>/2018/05/14/memory-as-a-stack/</guid><description>&lt;p>While the title is using two technical terms the post is actually about human memory - my memory. I’m not sure if it’s just me but lately my short term memory has been behaving like a stack. I’m working on something and then an interruption new comes along which suddenly gets pushed to the top of the stack. Now I’m working on this intrusion and when that’s done I hopefully remember my previous task so I can resume until it’s either done or the next task comes along and gets pushed to the top again.&lt;/p></description></item><item><title>Curse of the early adopter</title><link>/2018/05/12/curse-of-the-early-adopter/</link><pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate><guid>/2018/05/12/curse-of-the-early-adopter/</guid><description>&lt;p>Yesterday I spent a bit of time getting Ansible setup for my various instances that host my various projects. There are a handful scattered across AWS and Digital Ocean and I go through a round of maintenance every few months where I upgrade everything that needs upgrading. This was rare enough that I never bothered automating it but had half an hour to spare and thought this was an opportunity to learn Ansible - something that the TripleLift DevOps team has been using.&lt;/p></description></item><item><title>Rise of microbrands</title><link>/2018/04/28/rise-of-microbrands/</link><pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/28/rise-of-microbrands/</guid><description>&lt;img src="/image/instagram-optimum-ad.png" alt="Optimum Instagram ad" data-width="750" data-height="1334" data-layout="responsive" />
&lt;p>While browsing Instagram the other day I saw an ad for Optimum. Despite it being for Optimum the ad showed a sports clip that could have just as easily been on a highlight real or a a trailer for a sports movie. In fact, if it weren’t associated with Optimum maybe I would have even clicked on it.&lt;/p>
&lt;p>Having a strong brand is incredibly valuable - think Coca Cola. A weak brand (think amy ISP), on the other hand, is incredibly damaging. If a company has a strong brand they should definitely flaunt it but I’m surprised that in this day and age poorly branded companies still operate under their primary brand. There’s nothing stopping them from launching dozens of unique brands each designed to appeal to a particular audience.&lt;/p></description></item><item><title>Connect Four bot competition</title><link>/2018/04/25/connect-four-bot-competition/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/25/connect-four-bot-competition/</guid><description>&lt;p>Years ago when I worked at Yodle the engineering team held a Connect Four bot competition. The goal was for each person to write a Connect Four playing bot and then let them loose to determine the winner. We had either a few days or a few weeks to do this and my failed approach was to use genetic programming to evolve a bot. The best it did was beat a completely random bot 80% of the time while the winning entry leveraged Minimax with Alpha Beta Pruning.&lt;/p></description></item><item><title>Secure at the network level</title><link>/2018/04/17/secure-at-the-network-level/</link><pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/17/secure-at-the-network-level/</guid><description>&lt;p>Two weeks ago Travis CI published a &lt;a href="https://blog.travis-ci.com/2018-04-03-incident-post-mortem">postmortem&lt;/a> describing an outage that was caused by a script that truncated all tables on a production database. The script was designed to run against a test database but instead ended up wiping the production one. The remediation steps highlighted are a great start but I’m surprised they didn’t pick the most obvious one - protect systems at the network level.&lt;/p>
&lt;p>Relying on confirmation steps, user permissions, and unique credentials per environment are great steps and should be best practices but they don’t actually stop malicious or accidental behavior. They reduce the risk by adding more friction but it’s still possible to circumvent these blocks.&lt;/p></description></item><item><title>Load testing</title><link>/2018/04/12/load-testing/</link><pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/12/load-testing/</guid><description>&lt;p>I started writing this post about using Siege to do load testing but got carried away and ended up discovering how much I don’t know. In particular, I ended up stumbling unto Gil Tene’s talk on measuring latency and how nearly every tool gets it wrong due to the bias in the tools themselves. The general idea is that most tools measure service time rather than request time. Service time is how long it takes your application to handle a request while request time is the time it takes for the user to receive a response. The former is from the perspective of the application but the latter is from the perspective of the caller.&lt;/p></description></item><item><title>Open sourcing self driving car data</title><link>/2018/04/03/open-sourcing-self-driving-car-data/</link><pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate><guid>/2018/04/03/open-sourcing-self-driving-car-data/</guid><description>&lt;p>There’s a rush by the world’s largest companies to develop the first fully self driving car. The investment so far has been insane and it’s only the start. It’s one of those problems that get more and more difficult to solve the closer they are to the finish. The upside is so large that it also leads to some perverse behavior in order to get any advantage - the Uber/Waymo lawsuit comes to mind here.&lt;/p></description></item><item><title>Protecting data ouside of a Terms of Service</title><link>/2018/03/25/protecting-data-ouside-of-a-terms-of-service/</link><pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/25/protecting-data-ouside-of-a-terms-of-service/</guid><description>&lt;p>Since the Facebook/Cambridge Analytica news broke I’ve been thinking about how a company can make private data available without depending on a terms of service to enforce its usage or retention. As we’ve seen, terms of service are easily ignored and it may take years to notice that your data has been compromised.&lt;/p>
&lt;p>The only way of securely sharing data with third parties is to not actually give it to them. That seems like a contradiction but there is a way out. Rather than shipping your data to them you instead have them provide their code to you. Their code can then run within your walls and you’re able to audit it to make sure it’s working as promised. This means giving third party developers a limited set of methods that can be used and preventing any but the most minimal data from leaving the system. That means as a developer you’re working in somewhat of a black box since even debugging gets difficult. In addition, as the platform you’re incurring the additional cost of hosting and executing these third party applications.&lt;/p></description></item><item><title>Facebook's "breach"</title><link>/2018/03/18/facebooks-breach/</link><pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/18/facebooks-breach/</guid><description>&lt;p>The big news this weekend was that Facebook suspended Cambridge Analytica, a company that leveraged behavioral data to come up with very focused and accurate political ads, for using data that they were not supposed to have as well as not deleting it when caught. Everyone seems to be surprised by this revelation but I’m honestly surprised it took this long and I wouldn’t be surprised if there are still hundreds, or even thousands, of companies in the same situation as Cambridge Analytica, albeit at a smaller scale.&lt;/p></description></item><item><title>Crowdsourced data</title><link>/2018/03/11/crowdsourced-data/</link><pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/11/crowdsourced-data/</guid><description>&lt;p>Open source has become a critical part of modern software development that allows small teams to move quickly and do in months what used to take years. This has been driven by massive platforms, such as GitHub, that make it extremely easy to find useful code, contribute back, and provide feedback, comments, and requests.&lt;/p>
&lt;p>Unfortunately, data hasn’t seen as strong of an open sourcing trend. There are a few sites - ranging from &lt;a href="https://www.data.gov/developers/open-source">data.gov&lt;/a> for government data to various &lt;a href="https://github.com/awesomedata/awesome-public-datasets">aggregators&lt;/a> that offer various datasets for download but the formats are inconsistent and some even come in PDF. There just hasn’t been a single open data standard that’s been globally adopted. Instead we have cities offering PDF and CSV files for download and companies offering throttled APIs to their proprietary data.&lt;/p></description></item><item><title>Hunting for my old GeoCities site</title><link>/2018/03/03/hunting-for-my-old-geocities-site/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><guid>/2018/03/03/hunting-for-my-old-geocities-site/</guid><description>&lt;p>One of my first exposures to the web was geocities and I recall creating a site on GeoCities under the “Cape Canaveral” space. And I only remembered that when I stumbled across &lt;a href="http://www.oocities.org/">OoCities.org&lt;/a> and saw the name. Being a kid I was really into space and astronomy and created my GeoCities within that space. I keep trying to find my page but am constantly disappointed when every attempt ends in failure - although you do run into some amazing sites from the early years of the web.&lt;/p></description></item><item><title>Retrieving Kindle highlights</title><link>/2018/02/25/retrieving-kindle-highlights/</link><pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/25/retrieving-kindle-highlights/</guid><description>&lt;p>At this point I’ve moved most the majority of my reading to a Kindle. If I want to read a book I’ll only get a physical version if the Kindle version is unavailable. It’s often cheaper, immediately available, takes up no space, and comes with a variety of annotation abilities. My biggest frustration has been how difficult it’s been to export my highlights. I will often highlight interesting passages and quotes and every few weeks will dump them into a &lt;a href="https://github.com/dangoldin/quotes/blob/master/quotes.txt">giant text file&lt;/a> that I will occasionally reference or search.&lt;/p></description></item><item><title>Analyzing AWS ELB logs</title><link>/2018/02/20/analyzing-aws-elb-logs/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/20/analyzing-aws-elb-logs/</guid><description>&lt;p>Logging HTTP requests should be enabled for every application you run. When things go wrong, and they will, it’s often the first step to understand the problem. Unfortunately, logging isn’t always top of mind and is often forgotten. Luckily, if you use the Elastic Load Balancer (ELB) functionality within AWS you’re able to set up ELB logs that track every request and write it to an S3 bucket. The documentation is up on the &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html">Amazon site&lt;/a> but there’s a surprising amount of information that’s hidden away in the logs. Since it’s Amazon and they want to make it as easy for you to use their various services together it’s simple to load the logs into Redshift and start digging into them via some basic queries.&lt;/p></description></item><item><title>Optimize for keyboard shortcuts</title><link>/2018/02/10/optimize-for-keyboard-shortcuts/</link><pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/10/optimize-for-keyboard-shortcuts/</guid><description>&lt;p>I’m a big believer of offering keyboard-only experiences to your power users. In fact, I don’t think it’s possible to build a loved productivity application without keyboard shortcuts. Productivity applications are all about productivity and keyboard shortcuts are what give your users power and speed. The mouse and menus are easier for newcomers but those that stick with the application inevitably need the speed that comes from shortcuts.&lt;/p>
&lt;p>The ultimate example of this is Excel. If you watch any power user of Excel they rarely, if ever, use the mouse. Instead, they’re able to do everything they need using the keyboard and significantly quicker than if they were force the mouse. There’s some functionality that’s not even possible to do with a mouse alone - for example array formulas.&lt;/p></description></item><item><title>Making the most of the subway commute</title><link>/2018/02/09/making-the-most-of-the-subway-commute/</link><pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/09/making-the-most-of-the-subway-commute/</guid><description>&lt;p>Every day I take the subway to and from work and can’t help but think how that time could be better spent. Some people are browsing through photos, others are playing games, while others are just staring off into space listening to their headphones, while the last few are just asleep. All these are individual actions and can be done at any time but the subway is a forced melting pot and feels like a missed opportunity.&lt;/p></description></item><item><title>My follower factory</title><link>/2018/02/03/my-follower-factory/</link><pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate><guid>/2018/02/03/my-follower-factory/</guid><description>&lt;p>Last week, the New York Times &lt;a href="https://www.nytimes.com/interactive/2018/01/27/technology/social-media-bots.html">ran an expose&lt;/a> on the massive amount of follower fraud happening on Twitter. Unsurprisingly, when you can buy tens of thousands of followers for a few thousand dollars it’s not very likely that they’re going to be real. Anyone who has used Twitter for even a nominal amount of time would have quickly discovered that there’s a rampant amount of bots. Some leave cryptic comments, others like and retweet, while others follow; most do all of the above.&lt;/p></description></item><item><title>MoviePass: A fascinating business model</title><link>/2018/01/28/moviepass-a-fascinating-business-model/</link><pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/28/moviepass-a-fascinating-business-model/</guid><description>&lt;p>Given my lack of cultural knowledge the fact that I’ve heard of MoviePass should be a sign that it’s a big deal. I came across a &lt;a href="https://www.wired.com/story/moviepass-second-act/">fascinating article&lt;/a> today about MoviePass’s revenue model - they charge just under $10/month and in return you get unlimited access to movies in theaters. What’s remarkable is that it seems MoviePass doesn’t actually have a subsidized relationship with theaters and just pays theaters the full ticket price.&lt;/p></description></item><item><title>Design anti pattern: Tab switching autosave</title><link>/2018/01/23/design-anti-pattern-tab-switching-autosave/</link><pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/23/design-anti-pattern-tab-switching-autosave/</guid><description>&lt;p>Every once in awhile I get frustrated by a product experience and turn it into a design anti pattern rant. This time it was updating a bit of information in JIRA, switching to a different tab to look something up, and then going back to realize that my change was saved. Sure it was simple to edit and update the field but it seems the field should have just stayed in edit-mode until I was explicitly done. Since then I’ve been keeping an eye on how many products fall into this trap and it’s a fair amount. In the pursuit of improving efficiency they’re actually hurting it. Many tasks require referencing something else and I suspect there’s a bit of local optimization happening here with them focusing solely on their own product and not where it fits in to someone’s overall workflow.&lt;/p></description></item><item><title>Phonetic distance</title><link>/2018/01/16/phonetic-distance/</link><pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/16/phonetic-distance/</guid><description>&lt;p>Last year I &lt;a href="/2017/03/04/automating-admin-work-spreadsheets-to-slack/">wrote&lt;/a> a simple script to automate posting our On-Call schedule. It worked by reading the schedule from a Google Spreadsheet, looking up the names in Slack, and then sharing these usernames on Slack. A tiny problem I ran into was the fact that since I was using an exact match the names in the spreadsheet had to match the names in Slack. This is a trivial problem to solve since we have a finite number of engineers but it still felt a bit too sensitive. While lying in bed last night I got to thinking of ways to measure similarity between the names in order to make it a bit more fuzzy. I&amp;rsquo;ve used the &lt;a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance&lt;/a> in the past but it felt a bit too clinical for what I was trying to do and I wondered whether it was possible to do a phonetic match.&lt;/p></description></item><item><title>Calendar query language</title><link>/2018/01/13/calendar-query-language/</link><pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/13/calendar-query-language/</guid><description>&lt;p>I’m a power user of Google Calendar and use it to organize meetings, tasks, and important dates. The one thing I wish it had was a more powerful query language. I often wish I could run SQL-like queries on top of my calendar. For example being able to get a count of the number of people taking a vacation by day by team or looking at the intersection between multiple calendars. The goal would be to automate much of the work I’m doing now when looking at managing my calendar but also uncover insights that are currently constrained by a lack of easy access.&lt;/p></description></item><item><title>Learning Docker</title><link>/2018/01/03/learning-docker/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>/2018/01/03/learning-docker/</guid><description>&lt;p>I’m a bit embarrassed to admit this but I’ve been a bit behind the Docker craze. Sure I’ve done the tutorials when it came out but never really applied it to any of my actual projects. Given that nearly everyone is using Docker in some shape or form, I decided it was finally time to give it an honest effort.&lt;/p>
&lt;p>I had a small Python script that I’ve been running weekly off of my laptop and wanted to come up with a better solution. My old approach would have been to just run it as a cronjob on a VPS but the problem was that it had a variety of third party libraries that needed to be installed via pip and if I ever needed to move it elsewhere I’d have to set it up again. After speaking with a few people it seemed that this could be a use case for Docker and then running it either within a build server (Jenkins) or on top of a distributed system (DC/OS).&lt;/p></description></item><item><title>Favorite books of 2017</title><link>/2017/12/31/favorite-books-of-2017/</link><pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/31/favorite-books-of-2017/</guid><description>&lt;p>For the past few years I’ve been writing two posts a week and my end of the year ritual is to catch up on my writing and finish off the year right. Last year I did a filler post for my favorite books of 2016 and thought it would be interesting to share the 2017 version as well. I’m a big reader and rarely dislike anything I read yet sharing every book I’ve read is a bit of a cop-out so I’ve tried to focus this on the books that I just couldn’t put down and couldn’t wait to resume reading.&lt;/p></description></item><item><title>AR Adblocking</title><link>/2017/12/28/ar-adblocking/</link><pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/28/ar-adblocking/</guid><description>&lt;p>Augmented reality is still in its infancy but when it grows up it will change modern life. We’ll have a world of information at our fingertips and will end up with tools we can’t even imagine. An idea I’ve been thinking about is an AR based adblock. The way I envision these AR headsets working is that everyone will be constantly wearing them, akin to glasses, and the headsets are always processing, monitoring and altering what we see. Now imagine that there’s an adblocking application that intercepts everything visible and replaces it with blanks. Web adblockers work by blocking ad requests as well as removing ad HTML elements. This would work at an entirely different level by being in between the world and our eyes, acting as a sensory input filter.&lt;/p></description></item><item><title>Visualizing my 2017 stats</title><link>/2017/12/27/visualizing-my-2017-stats/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/27/visualizing-my-2017-stats/</guid><description>&lt;p>Over the past year I’ve been collecting a bunch of statistics for each of my days in the hope that I’ll have time to dig into them and discover some interesting patterns. Unfortunately I haven’t had a chance to do anything other than some simple visualizations but even these provide some insight into my 2017. This isn’t a wholehearted adoption of the quantified self movement but it’s something I am interested in and hoping to expand in 2018. A goal has always been to move beyond visualization and into actual analysis and actionable insights that can help me improve my lifestyle and behavior. I did the same set of &lt;a href="/2017/01/02/year-in-review-2016/">visualizations in 2016&lt;/a> so it&amp;rsquo;s useful to compare them year over and year and see how, and if, my habits have changed.&lt;/p></description></item><item><title>People do what you inspect, not what you expect</title><link>/2017/12/26/people-do-what-you-inspect-not-what-you-expect/</link><pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/26/people-do-what-you-inspect-not-what-you-expect/</guid><description>&lt;p>While reading &lt;a href="https://www.amazon.com/Who-Says-Elephants-Cant-Dance/dp/0060523808">Who Says Elephants Can’t Dance&lt;/a> about the revival of IBM in the 90s I came across a simple, yet profound statement by Louis Gerstner: “People do what you inspect, not what you expect.” We hear variations of this constantly and it’s true - if you want to drive behavior change you need to make sure that’s what you’re actually measuring and holding people accountable for. Otherwise we all run the risk of preaching what we don’t practice.&lt;/p></description></item><item><title>Send a shirt, get a shirt</title><link>/2017/12/25/send-a-shirt-get-a-shirt/</link><pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/25/send-a-shirt-get-a-shirt/</guid><description>&lt;p>Despite being an early tech adopter I avoid buying clothes online. At first the justification was that I didn’t want to deal with the cost of returns but these days nearly every retailer offers free returns. At this point it’s more habit than anything else and given how infrequently I buy clothes I’d rather just do it in person when necessary.&lt;/p>
&lt;p>The one service that I wish existed, and I’m sure it does somewhere, is a place where I can send some existing clothes that I know fit me perfectly and then have them used as the basis for new, custom made clothes. I have a great dress shirt that I’ve had for many years and it shows - there’s a giant rip on the elbow and the colors are starting to fade yet I love the fit. I refuse to throw it out and instead keep rolling up the sleeves to hide the tear. If I could get the same shirt in a variety of styles and colors but with the same perfect fit I’d be glad to throw it out.&lt;/p></description></item><item><title>Top posts of 2017</title><link>/2017/12/24/top-posts-of-2017/</link><pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/24/top-posts-of-2017/</guid><description>&lt;img src="/image/2017-pageviews.png" alt="Pageviews of my posts in 2017" data-width="2720" data-height="384" data-layout="responsive" />
&lt;p>I use Google Analytics on my blog and now that the year is almost over it’s time for the annual tradition of sharing the top posts of the year. The total number of pageviews in 2017 was a remarkable 36,410, around the same I received in 2015 and 2016 but below those of 2013 and 2014 when I was both more lucky in the popularity of my posts while and more aggressive in promoting my writing on Hacker News. While I feel my writing has improved over the years I feel my content has atrophied and is something I’d like to correct in 2018. What’s interesting is that out of all the views this year, 17% were from posts written in 2017, 74% were from posts written in previous years, and the remaining 9% were from non post pages. One argument is that as I build up a larger stable of content it will keep growing in percentage every year. At the same time I should start seeing an overall growth in views which hasn’t happened over the past few years. A worthy goal is to keep growing the total number of views of previous years’ posts while also making sure the current year’s are driving even more views and become the evergreen content for the future.&lt;/p></description></item><item><title>My Medium experiment</title><link>/2017/12/22/my-medium-experiment/</link><pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/22/my-medium-experiment/</guid><description>&lt;p>Last May I &lt;a href="/2016/05/15/experimenting-with-medium/">decided&lt;/a> to start cross posting to Medium to see what impact that would have on pageviews. My approach was to publish first on this blog and then post the same piece a few days later to Medium to get some additional views. Once concern was that the duplicate content would hurt SEO but luckily Medium allowed me to specify the &lt;a href="https://support.google.com/webmasters/answer/139066?hl=en">canonical URL&lt;/a> which I pointed back to the original post.&lt;/p></description></item><item><title>Analyzing my blog: 2017 edition</title><link>/2017/12/21/analyzing-my-blog-2017-edition/</link><pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/21/analyzing-my-blog-2017-edition/</guid><description>&lt;p>I have a set of &lt;a href="https://github.com/dangoldin/blog-analytics">scripts&lt;/a> I wrote in 2016 that aimed to &lt;a href="/2016/06/12/analyzing-my-blog/">analyze my posts&lt;/a> over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.&lt;/p>
&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/wordcloud_2017.png" alt="2017 word cloud" data-width="600" data-height="600" data-layout="responsive" />
 &lt;p>The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.&lt;/p></description></item><item><title>More intelligent credit card merchant rates</title><link>/2017/12/14/more-intelligent-credit-card-merchant-rates/</link><pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/14/more-intelligent-credit-card-merchant-rates/</guid><description>&lt;p>While reading today’s &lt;a href="https://stratechery.com/2017/patreon-reverses-itself-the-patreon-backstory-and-the-reality-of-fees-patreons-mistake/">Stratechery update&lt;/a> on Patreon changing their pricing model I came across the following passage:&lt;/p>
&lt;blockquote>
&lt;p>Of course this wasn’t great for creators: payments made for a creation released on November 2 wouldn’t land in creators’ bank accounts until some time after December 1; the reasoning, though, was clear — credit card fees. Here I can certainly bring personal experience to bear: credit card fees are really expensive (they are by far Stratechery’s largest expense)! While the exact amount varies by network (that is, American Express is the most expensive), most credit card fees are ~$0.30/charge plus anywhere from 1.5% to 3.5% of the total amount charged.&lt;/p></description></item><item><title>Google's news problem</title><link>/2017/12/10/googles-news-problem/</link><pubDate>Sun, 10 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/10/googles-news-problem/</guid><description>&lt;p>Last week, I wanted to get a bit more information about the vote in Alabama and have always had good success by starting with a Google search so did a simple search for &amp;ldquo;alabama vote.&amp;rdquo; I saw three suggested results but surprisingly the first one was a link to Breitbart.&lt;/p>
&lt;img src="/image/google-news-alabama-vote-2017-12-10.png" alt="Unsubscribe and wait 10 days" data-width="1580" data-height="1054" data-layout="responsive" />
&lt;p class="caption">Search for alabama vote on 2017-12-05&lt;/p>
&lt;p>I ran the same search a few days ago and saw that the first link was to yet another Breitbart article.&lt;/p></description></item><item><title>Jira analysis script</title><link>/2017/12/07/jira-analysis-script/</link><pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/07/jira-analysis-script/</guid><description>&lt;p>A few days ago I &lt;a href="/2017/12/01/measuring-sprint-efficiency/">wrote&lt;/a> about using average number of sprints to complete a story as a way to measure a team’s sprint efficiency. Unfortunately at that time I had a pretty hacky &lt;a href="https://github.com/dangoldin/automating-management/blob/master/jira-analysis.py">Jira analysis script&lt;/a> that I was too ashamed to share but it has been cleaned up enough for me to not feel too much guilt. It’s available on GitHub and comes with a few additional bells and whistles. One is specific to the way we work where we label relevant stories with a priority (priority:1, priority:2, etc) based on our planning process so we can hold ourselves accountable to spending the appropriate time on our initiatives. The other is a simple way that calculates the story points done by assignee. It’s a dangerous metric to use since story points are variable and not all work is measured via story points but it’s yet another metric that can help highlight or sharpen a potential issue.&lt;/p></description></item><item><title>Measuring sprint efficiency</title><link>/2017/12/01/measuring-sprint-efficiency/</link><pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate><guid>/2017/12/01/measuring-sprint-efficiency/</guid><description>&lt;p>Most startups employ some form of &lt;a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile software development&lt;/a> and one of the most common approaches is &lt;a href="https://en.wikipedia.org/wiki/Scrum_(software_development)">Scrum&lt;/a> which breaks down units of work into time based periods. I don’t want to spend too much time describing either Agile or Scrum since I suspect most are familiar with the concepts but a big challenge is measuring productivity. There are tons of different ways of doing this but the vast majority are different ways of looking at the relationship between story points and sprints. Most of these are focused on some form of velocity - measuring the amount of story points per sprint but an interesting metric I’ve started tracking is average number of sprints per story point. On the surface this is similar to taking the inverse but it turns out it tells different story.&lt;/p></description></item><item><title>Google Voice transcription fails</title><link>/2017/11/30/google-voice-transcription-fails/</link><pubDate>Thu, 30 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/30/google-voice-transcription-fails/</guid><description>&lt;p>A fun post today. I went through Google Voice’s attempts at transcribing my grandmother’s Russian voicemails into English. As expected they don’t make any sense but they do provide some comedic relief. and as expected they make no sense but there’s some gold . Google has one of, if not the, best data science teams around and it shouldn’t be too difficult to train Google Voice to detect the language before providing the transcripts. I &lt;a href="http://dangoldin.com/2016/01/17/poor-neglected-google-voice/">wrote&lt;/a> about this last January and it’s somewhat shocking that they did a redesign of Google Voice and yet didn’t address the actual functionality. But hey, at least we get to laugh.&lt;/p></description></item><item><title>Make all laws temporary</title><link>/2017/11/25/make-all-laws-temporary/</link><pubDate>Sat, 25 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/25/make-all-laws-temporary/</guid><description>&lt;p>I see the debate around net neutrality going on everywhere around me and I can’t help but think of the law making process and how it can be improved. I’m a huge proponent of net neutrality but at the same time I realize that maybe I don’t know the whole story and maybe we would all really be better if the FCC rolls it back. I don’t believe it’s likely but I’m not 100% convinced that it’s not the case either. Unlike a video game, you can’t save life at any point and restart it if something goes wrong or you want to try a different approach: we’re stuck with the decisions we’ve made. At the same time it would be nice to bring some more experimentation into the world and our laws so we can keep iterating to a more ideal state.&lt;/p></description></item><item><title>Improving Jekyll generation speed for AMP pages</title><link>/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/</link><pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/23/improving-jekyll-generation-speed-for-amp-pages/</guid><description>&lt;p>Last September I migrated my blog over to AMP which entailed a variety of challenges ranging from converting every img tag to an amp-img tag with some additional metadata to figuring out how to support Disqus. I tackled the critical ones but the one I never got to was speeding up the build time since it had no impact on the actual reader experience and just slowed down my build and commit process. During this Thanksgiving break I finally decided to do something about it after discovering that jekyll has a profiling feature. It’s expected that the bulk of the time is spent generating the post pages but running the profiler highlighted that the majority of the work wasn’t in the actual content block but in generating the head element - something that shold similar from page to page.&lt;/p></description></item><item><title>Ride sharing ride bailing</title><link>/2017/11/20/ride-sharing-ride-bailing/</link><pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/20/ride-sharing-ride-bailing/</guid><description>&lt;p>One of the more frustrating modern, first-world problems is booking a Lyft or an Uber ride and having the driver cancel the ride a soon as they find out where you’re going. While against policy it is a rational decision by the driver. Why agree to a trip somewhere where it&amp;rsquo;ll be difficult to pick up another passenger and then have to return to where you started and incur a toll? Of course you run the risk of a complaint but I suspect most people will silently accept the misjustice so the expected value of canceling the ride is in your favor.&lt;/p></description></item><item><title>Archiving large MySQL tables</title><link>/2017/11/18/archiving-large-mysql-tables/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/18/archiving-large-mysql-tables/</guid><description>&lt;p>One of the major changes we made when building the latest iteration of our data pipeline was moving our key agg tables over from MySQL to Redshift. Despite the migration we thought it would be prudent to archive these tables. The challenge was that some of these tables were hundreds of gigabytes so doing a simple mysqldump wouldn’t work. The reason these tables were so large is because they included a date dimension which led to our &lt;a href="https://github.com/dangoldin/python-tools/blob/master/archive_tables.py">archive script&lt;/a>. The script works by generating a sequence of shell commands that slice the table into chunks by date, gzip each chunk, and upload it to an S3 bucket. This keeps each individual chunk small enough to archive while making sure all the data is captured. It’s not the most elegant solution but it’s obvious and it’s quick. The one piece that’s missing is the table schema which can be fetched separately.&lt;/p></description></item><item><title>DevOps and Core Engineering application gap</title><link>/2017/11/15/devops-and-core-engineering-application-gap/</link><pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/15/devops-and-core-engineering-application-gap/</guid><description>&lt;p>Lately I’ve been pondering about applications that exist between core engineering and devops. Applications that have existed for years and have widespread adoption have best practices that many devops engineers have mastered - think of Apache or nginx. On the other extreme you have single page apps that exist solely in the browser and don’t need any support from the devops team after they’re deployed. Yet there’s a whole range of applications in the middle that don’t fall neatly into either of the camps. They have a gamut of configuration options that are heavily dependent on the workload which makes it difficult for either side to manage individually. In my limited experience these tend to be common in the big data ecosystem - Spark, Druid, and Kafka are great examples. Go through any of the documentation and you discover how complex the configuration can get and how tough it is to get it right. Without having any prior experience it’s difficult to see how one can get it immediately right. It requires understanding the use case, the expected volume, and a fair amount of experimentation to get right. This is not something that can be determined by devops or core engineering alone and is ideally a group effort.&lt;/p></description></item><item><title>Modern world of massive competiting corporations</title><link>/2017/11/11/modern-world-of-massive-competiting-corporations/</link><pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/11/modern-world-of-massive-competiting-corporations/</guid><description>&lt;p>I’m both fascinated and extremely unqualified to discuss China. Every time I read an article about technology and business in China I discover it’s a completely different world than the one I’m used to. Just today I read an &lt;a href="https://www.theinformation.com/chinas-jd-com-fights-alibaba-with-robots-drone">article&lt;/a> about two competing e-commerce companies - JD and Alibaba - and how they’re tackling ecommerce in China via antithetical approaches. And the market is so big and so nascent that it feels as if every company is trying to be involved in every industry. In the US we have Amazon which seemingly is getting involved in every industry but in China it feels as if every major technology company is doing the same. In this case JD is allying itself with Tencent - a competitor to Alibaba. Maybe it’s the natural order to have massive corporations competing with each other in every industry via partnerships with hundreds of competing startups.&lt;/p></description></item><item><title>Spark's read.jdbc</title><link>/2017/11/07/sparks-read.jdbc/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>/2017/11/07/sparks-read.jdbc/</guid><description>&lt;p>Yesterday I spent a bit of time investigating one of our Spark jobs that had suddenly shot up in run time. The purpose of our job is to collect all the events we see in an hour and generate a variety of aggregate tables and files that can then be loaded into various systems. When we first wrote the job it took about 45 minutes to run but as we&amp;rsquo;ve started seeing much higher data volume the job time has crept up to to 90 minutes. And for some reason yesterday the jobs were not completing even after 2 hours. There was clearly something odd happening.&lt;/p></description></item><item><title>Slides from my talk at DataEngConf</title><link>/2017/10/30/slides-from-my-talk-at-dataengconf/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/30/slides-from-my-talk-at-dataengconf/</guid><description>&lt;p>I had the privilege of giving a talk today at DataEngConf. Unfortunately the talk was not recorded but you can grab the slides [here]({{DATA_PATH }}/DataEngCof_NYC_Data_Startups_Dan_Goldin-Scaling-a-Data-Pipeline-Mystery-to-Mastery.pdf). The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture. Take a look at the slides and if you have any questions I’d be glad to answer them in the comments.&lt;/p></description></item><item><title>Philosophy of code</title><link>/2017/10/29/philosophy-of-code/</link><pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/29/philosophy-of-code/</guid><description>&lt;p>After writing my post on the code review pyramid I realized that I had many more thoughts about the highest level, code philosophy, and wanted to dedicate a full post to dig into it. The general idea is that a highly functional engineering team is way past the point of arguing over style and syntax and has reached the point where they share the same code philosophy. At this point all members of the team have an instinctive sense of how and where new code should be written - even if they can’t necessarily explain it.&lt;/p></description></item><item><title>The code review pyramid</title><link>/2017/10/27/the-code-review-pyramid/</link><pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/27/the-code-review-pyramid/</guid><description>&lt;p>Every modern software development process contains some form of code reviews. They ensure that all code is looked at by someone other than the author. This improves context, increases code quality, and generally leads to a stronger team and product. Yet there’s a world of difference in code reviews and I I’ve started to think of the different types as a pyramid. The peak is at the highest level but it’s not possible to get to that without going through the lower tiers. Another observation is that the lower levels can done by an individual but the penultimate ones require a team effort.&lt;/p></description></item><item><title>GPS: The foundation of them all</title><link>/2017/10/21/gps-the-foundation-of-them-all/</link><pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/21/gps-the-foundation-of-them-all/</guid><description>&lt;p>I spent a few hours driving today and couldn’t stop but think how different driving is now compared to the pre-GPS, pre-smartphone era. Before them I would be extremely wary of deviating from the preplanned path in any way. That meant avoiding all sorts of detours and prioritizing rest stops over exits. It also meant traveling with a road atlas and planning your exact route before setting off. And always questioning whether you missed a particular exit or turn and need to turn around.&lt;/p></description></item><item><title>Schedule automation using Google spreadsheets and Slack</title><link>/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/</link><pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/</guid><description>&lt;p>Back in March I &lt;a href="http://dangoldin.com/2017/03/04/automating-admin-work-spreadsheets-to-slack/">wrote a script&lt;/a> that would go through an on-call calendar kept in a Google spreadsheet and then post the current week’s schedule to a Slack channel. This worked surprisingly well and I thought of doing something similar for the other engineering team calendars. In addition to the on call rotation, we have a dedicated time for internal tech talks as well as a session to cover the news in the industry. To make them easier to manage we keep them all in that same spreadsheet.&lt;/p></description></item><item><title>Carrier specific iPhone ads</title><link>/2017/10/15/carrier-specific-iphone-ads/</link><pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/15/carrier-specific-iphone-ads/</guid><description>&lt;p>I had a bit of a lazy Sunday and spent some time watching some football games. During the commercial breaks I saw iPhone 8 ads sponsored by nearly every carrier with the format being nearly identical: a highlight of the features followed by a mention of the carrier. There was nothing there nudging me towards one carrier or another and they all felt like iPhone ads. I’m not sure if Apple is even contributing anything to these but even if they are I suspect the branding outweighs the investment - especially since an ad viewer may decide to get the new iPhone regardless of their existing carrier.&lt;/p></description></item><item><title>Scale challenges with AWS Athena</title><link>/2017/10/14/scale-challenges-with-aws-athena/</link><pubDate>Sat, 14 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/14/scale-challenges-with-aws-athena/</guid><description>&lt;p>Almost a year ago, AWS &lt;a href="https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/">launched Athena&lt;/a> which allowed you to query data directly off of S3. I loved the idea since it would allow us to simplify our workflow by reducing the need for Spark and Redshift while also cutting our costs. In theory queries that were being run via Spark or Redshift could just be run on top of data stored in S3 without having to load it into any system.&lt;/p></description></item><item><title>Downloading your AIM buddy list</title><link>/2017/10/09/downloading-your-aim-buddy-list/</link><pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/09/downloading-your-aim-buddy-list/</guid><description>&lt;p>While writing the &lt;a href="/2017/10/07/farewell-aim/">most recent post&lt;/a> about the impending AIM shut down I became curious and logged in to see what what I’ve been missing. The application felt worse but seeing my buddy list made me nostalgic and going through the usernames brought back some fond memories as I tried to remember who each screen name belonged to.&lt;/p>
&lt;p>I’m a bit of a hoarded, across both the physical and digital worlds, so didn’t want to lose my buddy list after the shut down. Unfortunately, AIM doesn’t make it very easy to download a copy of your buddy list so I came up with a crude but effective approach. Normally I’d look at the source code, identify the HTML elements containing what I wanted, and write a little bit of code in the JavaScript console to extract what I needed. I tried this approach in the AIM web client but it turns out that they update the HTML code to only show the screen names that are actually in view and it wasn’t obvious where the full list of screen names was being stored.&lt;/p></description></item><item><title>Farewell, AIM</title><link>/2017/10/07/farewell-aim/</link><pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/07/farewell-aim/</guid><description>&lt;p>Yesterday’s &lt;a href="https://www.theverge.com/2017/10/6/16435690/aim-shutting-down-after-20-years-aol-instant-messenger">big news&lt;/a> was that AIM will be shutting down in December after 20 years of service. This is not surprising - we’ve all moved on from AIM, first to Google Chat and now to the variety of smartphone messaging apps. Yet it’s incredible to think about what AIM was at its peak. I first got an account in middle school and remember it becoming the defacto way to communicate with friends throughout high school and college.&lt;/p></description></item><item><title>Generating a series of commands covering a date range</title><link>/2017/10/05/generating-a-series-of-commands-covering-a-date-range/</link><pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/05/generating-a-series-of-commands-covering-a-date-range/</guid><description>&lt;p>I know the title of the post is terrible but I found it difficult to describe the content in another way.&lt;/p>
&lt;p>Lately I’ve been spending a decent amount of my time in SQL-land and running some pretty repetitive queries where only some of the arguments are changed. These run the gamut from exporting some data for a date range by day to adding a series of date partitions while messing around with &lt;a href="http://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html">Spectrum&lt;/a>. Depending on the amount of these queries I needed to write I’d either just do it manually with a bunch of copy and pastes or use Excel to generate the queries I needed.&lt;/p></description></item><item><title>Writer's block and code</title><link>/2017/10/02/writers-block-and-code/</link><pubDate>Mon, 02 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/02/writers-block-and-code/</guid><description>&lt;p>The past few months I’ve been finding it more difficult to stick to my two post a week schedule. As I write this I’m 5 posts behind that I need to make up for by the end of the week in order to stick with my commitment. I suspect it’s common for true writers to have periods of writer’s block but I’m a casual blogger so it shouldn’t be that hard for me to just sit down and knock something out. I’ve also developed the habit, or maybe even the skill, of going through my day and being able to identify topics and themes that would be interesting to blog out. I have close to 150 of these ideas and yet lately I’ve found it difficult to even start.&lt;/p></description></item><item><title>Lessons from a Yes concert</title><link>/2017/10/01/lessons-from-a-yes-concert/</link><pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate><guid>/2017/10/01/lessons-from-a-yes-concert/</guid><description>&lt;img src="/image/yes-concert.jpg" alt="Yes concert" data-width="3024" data-height="4032" data-layout="responsive" />
&lt;p>Last week I attended a &lt;a href="https://en.wikipedia.org/wiki/Yes_(band)">Yes&lt;/a> concert. For the poor souls that don’t know - Yes was an extremely popular progressive rock band in the 70s and 80s. The implication is that the band members are currently in their 60s and 70s yet are still going on tour and performing nearly two hour long shows. And being a rock band from the 70s they ran into the typical challenges with members leaving and rejoining throughout its history.&lt;/p></description></item><item><title>Open sourcing government</title><link>/2017/09/22/open-sourcing-government/</link><pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/22/open-sourcing-government/</guid><description>&lt;p>This past week I had two stereotypical government experiences that got me thinking about ways the open source community can help improve government products and services. There’s a huge different in quality and joy when you compare consumer apps with government services. This is understandable: there’s more competition, fewer restraints, and more money in private enterprise so naturally government offerings won’t be as exciting. At the same time government services affect us much more than a generic app.&lt;/p></description></item><item><title>Examining my shell command history</title><link>/2017/09/21/examining-my-shell-command-history/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/21/examining-my-shell-command-history/</guid><description>&lt;p>A few years ago I &lt;a href="http://dangoldin.com/2014/05/12/most-commonly-used-shell-commands/">wrote&lt;/a> a simple script to analyze my shell history in order to examine my most frequently run shell commands. Being in dire need of a new blog post and suffering from a pretty heavy bout of writer’s block I thought it would be interesting to rerun the analysis and see how it compared to results from over 3 years ago.&lt;/p>
&lt;p>It’s tough to say whether my usage has changed significantly. My adoption of zsh with the &lt;a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh plugin&lt;/a> have made my usage a bit more efficient - especially when using git. The other obvious change is that I’m running python code half as much as I used to and have also reduced my usage of text editors. I used to use fabric a ton to automate some deploys but have moved completely off of that. What has remained consistent is my blogging - my little alias (cdblog) to move to my blog directory and jekyll have stayed roughly the same as well as some other administrative commands.&lt;/p></description></item><item><title>Apartment rental arbitrage</title><link>/2017/09/09/apartment-rental-arbitrage/</link><pubDate>Sat, 09 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/09/apartment-rental-arbitrage/</guid><description>&lt;p>Housing is one of the largest expenses and it’s worth trying to get the best deal you can. When it comes to finding an apartment rental there are a few tricks I’ve picked up that help find the best value. Generally, the approach is more suited for apartments that are being rented out by an owner rather than a development being rented out by a single company since there’s just more information out there.&lt;/p></description></item><item><title>Scaling meal kit distribution</title><link>/2017/09/06/scaling-meal-kit-distribution/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/06/scaling-meal-kit-distribution/</guid><description>&lt;p>Earlier today I read a neat &lt;a href="https://www.theinformation.com/blue-apron-competitors-explore-sales-to-grocers-food-manufacturers">article&lt;/a> covering the meal-kit delivery space and how give Blue Apron’s lackluster performance on the public market the still-private competitors are exploring alternative options, including partnering with grocery stores and general CPG companies. I hadn’t thought much about the space but it’s a great idea.&lt;/p>
&lt;p>Fresh food is a difficult business that benefits heavily from scale. Buying in bulk gives you significant price discounts and being able to have high throughput reduces the amount of food that spoils. One of the simplest ways to increase your scale is to increase the amount of distribution channels you have. These meal-kit delivery companies started by doing delivery to the home but there’s nothing stopping them from offering the same meal kits at grocery stores. In fact, maybe it makes sense to not even have an exclusive partnership with any single chain but try to get them into as many stores as possible. They’d have to do an accurate job modeling the demand but they’d likely be able to drop the price enough to make it attractive to customers. The biggest price reduction would come from not having to ship individual orders but instead deliver them in bulk to a single store. A more interesting version of this would be to actually allow the grocery store to contribute some of the ingredients - benefiting both.&lt;/p></description></item><item><title>Cross app AI</title><link>/2017/09/04/cross-app-ai/</link><pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate><guid>/2017/09/04/cross-app-ai/</guid><description>&lt;p>More and more apps are starting to leverage some form of AI to improve the user experience. These range from inferring user preferences when surfacing new information to notifications that come just at the right time to be helpful. Google has been leading the pack here and nearly every product has some AI-based functionality built in: Photos is doing an unbelievable job of identifying faces and objects, Maps is all about pathfinding and is constantly improving as new data is collected, Assistant is probably my favorite new product and serves as a generic catch all for keeping track of my schedule and answering my ad hoc trivia questions.&lt;/p></description></item><item><title>Amazon's real leverage</title><link>/2017/08/31/amazons-real-leverage/</link><pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/31/amazons-real-leverage/</guid><description>&lt;img src="/image/largest-employers-usa.png" alt="Largest employers in USA" data-width="950" data-height="1432" data-layout="responsive" />
&lt;p class="caption">Source: &lt;a href="https://en.wikipedia.org/wiki/List_of_largest_employers_in_the_United_States">Wikipedia&lt;/a>&lt;/p>
&lt;p>Amazon’s an incredibly competitive company with a ton of defensive moats but one I haven’t seen mentioned much is also the one I think is one of the most powerful: its sheer number of employees. And rather than having them isolated to a few key offices they’re spread out across a variety of cities, states, and countries. Amazon has over 340,000 &lt;a href="https://en.wikipedia.org/wiki/List_of_largest_employers_in_the_United_States">employees&lt;/a> which is almost 5 times the size of Google’s 72,000.&lt;/p></description></item><item><title>Focus on the story</title><link>/2017/08/28/focus-on-the-story/</link><pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/28/focus-on-the-story/</guid><description>&lt;p>Last week we hosted a meetup describing the evolution of our data pipeline over the past 5 years - starting with a simple script to aggregate log files to a fully fledged big data system relying on a slew of technologies ranging from Kafka to Spark to Redshift. I find most meetups and presentations more focused on the present which is valuable but does a disservice to the audience by not describing the history, motivations, or thought process. No system lives in a vacuum and it’s much more interesting to look at the evolution of a system rather than focusing on its current state.&lt;/p></description></item><item><title>Why live sports?</title><link>/2017/08/27/why-live-sports/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/27/why-live-sports/</guid><description>&lt;p>Yesterday the world was so abuzz with the Mayweather vs McGregor boxing match that even I couldn’t escape from hearing about it. One small tidbit I discovered was that it would cost you $100 just to watch it in the comfort of your own home. And while I definitely don’t consider myself a sports fan that still feels ridiculous.&lt;/p>
&lt;p>Everyone believes that the live nature of sports makes them completely different than other media and yet that doesn’t strike a chord with me. I’ve been to some live events and when you’re with a group the real time element and environment is critical. This also goes for the case where you’re a huge fan and need to know everything as soon as possible. But for the casual fan having a couple of hour delay doesn’t seem that critical. Sure you have to avoid hearing or reading about the outcome but without that knowledge the two situations seem equivalent - at least rationally, if not emotionally.&lt;/p></description></item><item><title>A unified Lambda architecture</title><link>/2017/08/25/a-unified-lambda-architecture/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/25/a-unified-lambda-architecture/</guid><description>&lt;p>Lately I’ve been thinking about the &lt;a href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda architecture&lt;/a> used in modern data pipelines. Lambda architectures are designed for systems that contain massive amounts of streaming data that needs to be processed and exposed quickly. The architecture consists of two different systems. One is a real time pipeline that’s not perfectly accurate but is able to handle large volumes while providing a solid estimate quickly. The other is a batch process that is accurate but runs on a delay. By combining the two you get the best of both worlds - accurate historical data and reasonably correct recent data that will be corrected by the batch job when it runs.&lt;/p></description></item><item><title>Death of retail</title><link>/2017/08/22/death-of-retail/</link><pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/22/death-of-retail/</guid><description>&lt;p>I keep hearing that retail is dying and while I do believe that to be true it’s been difficult to see that from living in the NYC area. The city streets are always bustling and while there are always some empty commercial spaces they seem to always get rented within a month or two. Yet this morning on my way to the office I came across a legitimate sign, literal and figurative, that brick and mortar retail is on its way out.&lt;/p></description></item><item><title>JSON to CSV</title><link>/2017/08/20/json-to-csv/</link><pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/20/json-to-csv/</guid><description>&lt;p>A while back I needed to dump some some EC2 instance information into a CSV file for a quick analysis. Just to get it done I took the immediate approach of using the AWS API to pull the details and then just navigating the massively deep structure. This approach required code designed for that exact structure so it got me thinking of a more generic approach that would be able to extract CSV data from an arbitrary JSON structure. It’s a surprisingly tricky problem since JSON consists of both lists and dictionaries and can have a pretty hairy nesting structure. Just to get the EC2 instances one has to go through a list of reservations each containing a list of instances with the various fields at different hierarchy levels - and some depending on another value within the same structure.&lt;/p></description></item><item><title>A Snapcode in the mail</title><link>/2017/08/12/a-snapcode-in-the-mail/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/12/a-snapcode-in-the-mail/</guid><description>&lt;img src="/image/snapcode-jet.jpg" alt="Snapcode for Jet.com" data-width="1512" data-height="2016" data-layout="responsive" />
&lt;p>Earlier today I received one of those typical promotions in the mail from Jet.com - $20 off for a $100 purchase. Nothing special about the offer itself but something I found fascinating was that there was a Snapcode that when scanned via Snapchat took you to a mobile optimized Jet.com page within Snapchat. Most mail I’ve seen has the callouts to encourage people to follow the brand on Facebook and Twitter but this promotion didn’t have either of those - just the Snapcode.&lt;/p></description></item><item><title>Start with pen and paper</title><link>/2017/08/10/start-with-pen-and-paper/</link><pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/10/start-with-pen-and-paper/</guid><description>&lt;p>Oftentimes when starting a new project I have a tendency to just dive in. It makes me feel immediately productive and I know I can just go back and tweak whatever needs tweaking. Yet almost always I suspect I would have been better of if I took the time to take a step back, get a sheet of paper and a pen, and think through the various steps and flows, even it was just a series of doodles.&lt;/p></description></item><item><title>Google Docs vs Confluence</title><link>/2017/08/08/google-docs-vs-confluence/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/08/google-docs-vs-confluence/</guid><description>&lt;p>I’ve been a happy user of Google Docs for years now and have yet to find another product that makes collaboration that easy or simple. It’s a well designed product with a ton of shortcuts that make it incredibly easy to be exceptionally productive. I can dive in and quickly leave a few comments as well as assign some todos knowing that the relevant folk will be notified. And for a very long time I’ve been using it for the bulk of my writing - including specs, design documents, and general note taking.&lt;/p></description></item><item><title>Ephemeral data</title><link>/2017/08/07/ephemeral-data/</link><pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate><guid>/2017/08/07/ephemeral-data/</guid><description>&lt;p>A thought experiment that&amp;rsquo;s been on my mind lately is this idea of ephemeral data. Imagine a computer sending a message to another computer and then immediately deleting it. This message then gets sent from computer to computer without actually getting saved down anywhere. It doesn’t have a permanent home and just hops from machine to machine. If that computer shuts down before the message is passed on then it’s lost forever. Only by being on a machine when it receives a message do you get to see it - otherwise it keeps going along on its infinite journey. I like to think of this as information that only exists in wires and is constantly crossing the world at the speed of light.&lt;/p></description></item><item><title>Visualizing my meetings over time</title><link>/2017/07/28/visualizing-my-meetings-over-time/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/28/visualizing-my-meetings-over-time/</guid><description>&lt;p>As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would &lt;a href="http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs/">identify meeting room hogs&lt;/a> and &lt;a href="https://github.com/dangoldin/gcal-shaming/blob/master/meeting_duration_growth.py">repurposed it&lt;/a> to just download every one of my calendar events from when I joined TripleLift and another small script to &lt;a href="https://github.com/dangoldin/gcal-shaming/blob/master/analyze.py">analyze&lt;/a> this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos. Unsurprisingly, there was a pretty large increase over time as we grew from a scrappy startup of 15 people to one with over 150 and as my role evolved from an individual contributor to a manager and then to the head of the engineering team.&lt;/p></description></item><item><title>A Google Docs efficiency win</title><link>/2017/07/24/a-google-docs-efficiency-win/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/24/a-google-docs-efficiency-win/</guid><description>&lt;div class="right10">
 &lt;img src="/image/gdocs-new.png" alt="Old Google Docs behavior" data-width="450" data-height="571" data-layout="responsive" />
 &lt;p class="caption">Old Google Docs behavior&lt;/p>
&lt;/div>
&lt;p>I’m an efficiency fiend and love seeing my heavily used products updated with any and all functionality that make them a tad easier to use. A few weeks ago, Google Docs, one of my most heavily used applications - in fact where I’m writing this very post - made a minor change to the “Move to” functionality that created one of these small efficiency wins. Before the change, clicking the small folder icon while in a document would show a menu with the item’s current location and a “Move this item” option. Clicking that behavior would enable move mode and would require the user to navigate to the appropriate folder and then confirming the move.&lt;/p></description></item><item><title>The wild world of online trackers</title><link>/2017/07/23/the-wild-world-of-online-trackers/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/23/the-wild-world-of-online-trackers/</guid><description>&lt;div class="right10">
 &lt;img src="/image/freshdirect-ghostery.png" alt="Ghostery on FreshDirect's home page" data-width="154" data-height="660" data-layout="responsive" />
&lt;/div>
&lt;p>Working in AdTech I’m slightly more aware of how modern digital advertising works compared to the average person and get curious when I see an for a brand that I have never seen before. Just yesterday I was on Twitter and saw a Whole Foods ad for what I think was the first time. This got me thinking about what I must have done to get into their targeting list. Of course it may have just been a new campaign but I know enough to suspect it had something to do with my recent browsing behavior.&lt;/p></description></item><item><title>Cities and outsourced infrastructure</title><link>/2017/07/13/cities-and-outsourced-infrastructure/</link><pubDate>Thu, 13 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/13/cities-and-outsourced-infrastructure/</guid><description>&lt;p>Earlier today I read an interesting piece about the &lt;a href="https://www.theinformation.com/chinas-bike-share-upstarts-face-tough-road-in-u-s">difficulty facing Chinese bike share companies&lt;/a> trying to enter the US market. The primary challenge is that many cities have already signed exclusive agreements with ride sharing companies and in many cases subsidized the initial investment. I find this fascinating since it highlights how even in relatively new industries it’s very easy to encounter established players that are difficult to dislodge - especially when they have the backing of the local government.&lt;/p></description></item><item><title>Increasing software engineer specialization</title><link>/2017/07/09/increasing-software-engineer-specialization/</link><pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/09/increasing-software-engineer-specialization/</guid><description>&lt;p>I’m not sure if it’s always been this way and I just never realized it but it seems there’s a lot of specialization happening within the software engineering industry. For a long time it felt that the majority of the software industry field was shallow enough that being a strong developer was enough to get into anything of interest. Now it does feel as if the industry is turning a corner and there’s significantly more specialization. There have always been exceptions in the past but it does feel specialization was along the edges - now there’s enough depth in the industry that it’s difficult to be a general software engineer, especially further in a career.&lt;/p></description></item><item><title>Yahoo fantasy football stats: 2017-2018 edition</title><link>/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/</link><pubDate>Sat, 08 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/</guid><description>&lt;p>In what has become an annual tradition I updated my Yahoo Fantasy Football scraping bot for the 2017-2018 season. Every year Yahoo makes a few changes to their page and this year was no different. It’s always fun to cross my fingers, run the script, and see what breaks. This year the changes were surprisingly minor. For some reason Yahoo changed the name attribute of the password field from “passwd” to “password” and made a few tweaks to the table structure which required updating the XPath selectors. Other than that everything worked as expected and the 2017-2018 data is available &lt;a href="/assets/static/data/stats-2018.csv">here&lt;/a> with the code up on &lt;a href="https://github.com/dangoldin/yahoo-ffl">GitHub&lt;/a>.&lt;/p></description></item><item><title>Thoughtful code</title><link>/2017/07/04/thoughtful-code/</link><pubDate>Tue, 04 Jul 2017 00:00:00 +0000</pubDate><guid>/2017/07/04/thoughtful-code/</guid><description>&lt;p>Lately I’ve found myself thinking more deeply about the code I’m writing. No matter how small the task or script I’ll think through the implications of my approach and whether I should be doing anything differently. This doesn’t mean I’ll always pick the more correct and flexible approach and more often than not I’ll choose the quick and dirty one to save time but the thought process itself is valuable since it gets me in the habit of questioning and constantly improving my code. The following is an example that illustrates this approach.&lt;/p></description></item><item><title>Send private messages to all members of a Slack channel</title><link>/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/</link><pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/</guid><description>&lt;p>One of my more recent “management automation” tricks was to write a simple script that gets all active members of a Slack channel and then sends them a direct message. I’ll often want to poll the entire team and ask them to fill out a survey or submit a questionnaire but the response rates tend to be poor. But if I send a message to people directly I end up with a much better response rate. It turns out that in my case I was able to get a greater than 100% improvement in response rate by using this approach. In a group channel there’s a lot going on so it’s likely that some people don’t see the message or decide they’ll do it later but inevitably forget. But by messaging them directly it sends a pretty strong signal that I care about the response and prompts people to just get it done. Despite the success I am hesitant to overuse it since it may lead to people ignoring these direct messages as well. As they say, with great power comes great responsibility. As usual, the code is up on &lt;a href="https://github.com/dangoldin/automating-management/blob/master/spam_channel_members.py">GitHub&lt;/a> and suggestions and pull requests are welcome.&lt;/p></description></item><item><title>RSS finally fixed</title><link>/2017/06/26/rss-finally-fixed/</link><pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/26/rss-finally-fixed/</guid><description>&lt;p>Last week’s post highlighting my victory getting AMP working in RSS was a bit premature since it turns out my solution only worked locally. While being powered by Jekyll, GitHub Pages doesn’t support custom plugins which I was using to replace the “amp-img” tags with “img” when generating the XML feed. So while my approached worked when generating the blog locally it silently ignored my custom template tags when pushed to GitHub.&lt;/p></description></item><item><title>Unsubscribe and wait 10 days</title><link>/2017/06/24/unsubscribe-and-wait-10-days/</link><pubDate>Sat, 24 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/24/unsubscribe-and-wait-10-days/</guid><description>&lt;img src="/image/unsubscribe-10-days.png" alt="Unsubscribe and wait 10 days" data-width="788" data-height="216" data-layout="responsive" />
&lt;p>We live in a world of miraculous technology and yet it “may take up to 10 days” for me to unsubscribe from a mailing list. I have no clue what needs to happen to remove my email from a mailing list but it feels as if they’re using the &lt;a href="https://en.wikipedia.org/wiki/Pony_Express">Pony Express&lt;/a>. If someone built a system that takes 10 days to deactivate an email address whoever built that system should not be writing any code. Modern systems are able to deal with billions of events a second and support millions of concurrent users but somehow updating a field in a database takes up to 10 days?&lt;/p></description></item><item><title>Apple's ARKit</title><link>/2017/06/22/apples-arkit/</link><pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/22/apples-arkit/</guid><description>&lt;p>During this year’s WWDC Apple announced &lt;a href="https://developer.apple.com/arkit/">ARKit&lt;/a>, a development framework that simplifies the ability of incorporating augmented reality into your app. I’ve been fascinated by this for a while and took a stab at getting their code running. I’ve only dabbled in iOS development so the bulk of the effort involved getting the latest Xcode beta and then discovering to run the full example I needed to upgrade my phone to run the iOS 11 beta. After this the included ARKit example worked perfectly and left me pleasantly surprised. It worked remarkably well and highlighted how powerful ARKit can be; I&amp;rsquo;ve included some sample screenshots below. Snapchat had to invest the time and effort to develop their filters but ARKit will make that significantly easier for future developers. This is an extremely powerful move by Apple since it’s all about the apps and this gives app developers a tool unique to iOS. I have a lot to learn here but I’m excited to get deeper into the augmented reality world. I remember playing with VRML in the 90s and I consider this an extension of my childhood interests.&lt;/p></description></item><item><title>Getting AMP into RSS</title><link>/2017/06/20/getting-amp-into-rss/</link><pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/20/getting-amp-into-rss/</guid><description>&lt;p>A little less than a year ago I &lt;a href="http://dangoldin.com/2016/09/05/ampifying-my-blog/">migrated&lt;/a> this blog over to &lt;a href="https://www.ampproject.org/">AMP&lt;/a> which required a lot of small tweaks - ranging from automating the markup changes to getting the Disqus plugin to work. One thing I didn’t get a chance to finish until earlier this week was supporting the RSS feed. This blog is hosted on GitHub pages which is powered by Jekyll and comes with a pretty powerful templating engine. One of the predefined templates was the ability to generate an RSS atom feed. It worked by taking the content of each post, escaping it, and concatenating them together into a massive XML file.&lt;/p></description></item><item><title>Investigating application issues</title><link>/2017/06/17/investigating-application-issues/</link><pubDate>Sat, 17 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/17/investigating-application-issues/</guid><description>&lt;p>A skill that seems lacking is the ability to debug large scale applications. Most people are comfortable looking at exceptions or log files and working their way back to an issue in the code but given the complexity of modern applications that’s not enough. These days applications are hosted across dozens of cloud instances while utilizing a ton of cloud services. This makes it easier to ship applications but also makes it more difficult to isolate and identify issues since they’re no longer isolated to a single service or application. When there are dozens of instances and dozens of services talking to one another an issue in one system may manifest itself as a phantom issues in others which can lead to a significant amount of wasted investigative effort.&lt;/p></description></item><item><title>The Amazon juggernaut</title><link>/2017/06/16/the-amazon-juggernaut/</link><pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/16/the-amazon-juggernaut/</guid><description>&lt;p>Earlier today Amazon announced that it is acquiring Whole Foods. I can’t count how many different lines of business Amazon has but this is yet another step in its quest to capture every consumer dollar spent. Unsurprisingly, people spend the most on &lt;a href="https://www.bls.gov/news.release/cesan.nr0.htm">necessities&lt;/a> which is basically food and housing followed by clothing and transportation. In 2015, a whopping $600B was &lt;a href="https://www.statista.com/topics/1660/food-retail/">spent at grocery stores&lt;/a> in the US - and this doesn’t include eating or drinking out. Amazon has massive ambitions and getting into these categories is a necessity. Amazon has already been getting into food and clothing but this is a huge step into the brick and mortar world which will open up a ton of possibilities.&lt;/p></description></item><item><title>Mary Meeker's Internet Trends 2017</title><link>/2017/06/04/mary-meekers-internet-trends-2017/</link><pubDate>Sun, 04 Jun 2017 00:00:00 +0000</pubDate><guid>/2017/06/04/mary-meekers-internet-trends-2017/</guid><description>&lt;p>In what has become annual tradition, Mary Meeker has just published the &lt;a href="http://dq756f9pzlyr3.cloudfront.net/file/Internet+Trends+2017+Report.pdf">2017 Internet Trends report&lt;/a> and it’s a whopper. Over 350 pages describing the state of the modern digital world. I can’t imagine how long it must have taken to pull the data and put it together but I’m incredibly grateful that it’s been done and continues to be done every year. There’s so much great stuff in there that it’s worth going through it on your own but I wanted to highlight the slides that stood out to me.&lt;/p></description></item><item><title>Learn from failure, not success</title><link>/2017/05/30/learn-from-failure-not-success/</link><pubDate>Tue, 30 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/30/learn-from-failure-not-success/</guid><description>&lt;p>Maybe it’s due to all the Sherlock Holmes I read as a kid but I enjoy the process of debugging and exploring technical outages and failures. The more cryptic and challenging the problem the more fun it is to sink my teeth into and the more rewarding it is when I finally get to the root cause. The real benefit of going through failures is that it is one of the best ways to get familiar with a tool or a technology. When things work we often only get a superficial understanding of the technology since we have no reason to go beyond the surface. When things fail, on the other hand, we’re forced to dig deep until we understand the cause of the failure and can make the necessary modifications to prevent the same issues in the future.&lt;/p></description></item><item><title>Engineering, product, and design</title><link>/2017/05/27/engineering-product-and-design/</link><pubDate>Sat, 27 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/27/engineering-product-and-design/</guid><description>&lt;p>The best products are built when engineering, product, and designers work together. When things are running smoothly each brings skill, knowledge, and experience the others cannot. When things are running poorly the teams do not respect or trust each other and question the others’ approach and decision making. Inevitably this leads to a flawed product that doesn’t solve customer needs and gets replaced by one that does. It’s possible to ride previous success through inertia, marketing, and price but to grow in the modern era you need to focus on product.&lt;/p></description></item><item><title>IVR to SMS</title><link>/2017/05/25/ivr-to-sms/</link><pubDate>Thu, 25 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/25/ivr-to-sms/</guid><description>&lt;p>Seems it’s all about messaging now and yet we somehow still have the ridiculous automated call systems which the industry calls interactive voice response. Every single person I know hates having to call a customer support number only to hear a robotic voice that requires a series of frustrated enunciations to get what you wanted. I understand that it’s a huge win in terms of cost savings since you’re replacing people with software but moving over to SMS and asynchronous messaging would make it a better experience at an even lower cost.&lt;/p></description></item><item><title>Type dependent databases</title><link>/2017/05/23/type-dependent-databases/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/23/type-dependent-databases/</guid><description>&lt;p>I’m a huge proponent of strong types when it comes to coding. Unless it’s a throwaway project it’s always worth spending the extra time to define your objects and the way they will be exposed in the code. This investment makes it more likely that you’ve thought through the way the code will need to evolve and the various edge cases you need to handle.&lt;/p>
&lt;p>This philosophy is even more important when thinking about your database structure since that’s going to be even more difficult to change than your code. Changing the code requires a deploy while changing a database schema will require a migration and a series of corresponding code changes.&lt;/p></description></item><item><title>Copying production SQL data to other environments</title><link>/2017/05/20/copying-production-sql-data-to-other-environments/</link><pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/20/copying-production-sql-data-to-other-environments/</guid><description>&lt;p>I suspect most developers have encountered this problem at least once: how do I copy some production data to my test or development environment? This can stem from needing to fix a bug that only manifests in production or just getting a more complicated, real-world dataset that doesn’t yet exist in the test environment. In an ideal world we’d have everything we need in fixtures and properly tested but in the cases we don’t it seems simpler to just copy the data over from the production environment.&lt;/p></description></item><item><title>The different flavors of engineering interviews</title><link>/2017/05/17/the-different-flavors-of-engineering-interviews/</link><pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/17/the-different-flavors-of-engineering-interviews/</guid><description>&lt;p>Technical interviews often come in a variety of flavors and I thought it would be interesting to list as many as I can think of and my thoughts on each one. In general I think there’s value in each type of approach but some are going to be more appropriate than others depending on the person’s experience and role.&lt;/p>
&lt;ul>
&lt;li>Brainteaser. These are seemingly simple problems that require a trick or insight to get them right. If you’ve heard it before or are familiar you can blow through these otherwise you’ll need to rely on a series of hints to get to the final answer. Probably not the best way to judge someone’s ability since it’s likely removed from the work they’ll actually be doing.&lt;/li>
&lt;li>Data structures. A bit more legitimate than the brainteaser approach this question digs into your knowledge of data structures. These usually start with some sense of complexity and then end up in implementing some type of traversal or tree search. The theory and knowledge of these is important but it’s pretty rare to have to implement a low level data structure.&lt;/li>
&lt;li>Architecture. This moves higher level and asks you to think about designing a larger application. How would the various components look? If it’s a service what endpoints would be exposed? What are the arguments and results for each of the calls? How would you scale this? What if you needed to make changes? These are a useful way to see how someone thinks and whether they have some familiarity thinking through the architectures of large and complex systems.&lt;/li>
&lt;li>Technical. Depending on the domain this gets into the nuances of a language or technology. These range from a rapid fire style that’s asking for descriptions of various HTTP status codes to a deeper dive into the TCP/IP protocol to discussing high level networking or the nuances of particular language or application versions. The goal here is to quickly get a sense if the person knows what they’re talking about or whether there’s only a superficial knowledge. Used alongside some of the other approaches this is a solid way of gauging the accuracy of a resume.&lt;/li>
&lt;li>Code test. This is the typical code test where you’re given a problem, a computer, and a time limit. Hopefully the problem is simple enough and offers a variety of implementation options that allow you to see the thought and decision process. The most successful ones involve introducing the problem and making sure everyone is on the same page and an occasional check-in to answer and address any questions. The difficulty here is that not everyone can code in a high pressure environment and you may be missing out on a lot of great people that are strong coders but don’t do well on code tests.&lt;/li>
&lt;li>Pair programming. A variation of the code test that tries to make it a bit less stressful is to do a pairing exercise where you’re both working on a problem and bouncing ideas off of one another. The goal is to have the candidate do most of the coding and you act as a sounding board since you do want to get a sense of the person’s coding ability. This also serves the benefit of showing you whether you’d get along since it’s a quick glimpse into how you’d work together on the same project.&lt;/li>
&lt;li>Feature addition. This is working with someone on the production code to build out a simple feature. Pair programming is usually done on a predefined problem but this takes an actual problem you’re working on and turns it into a pairing exercise. I haven’t seen this done much since it usually requires a ton of context to ramp someone up to your codebase and every person ends up with a different experience. Nice in theory but I don’t know how well it works in practice.&lt;/li>
&lt;li>Unit test fixing. I’ve seen this done a few times and it’s usually set up as a fully written project with a few broken unit tests. It’s your job to go through the underlying code and fix it to get the unit tests to pass. I like this one since it tests something everyone needs to do - go through someone else’s code, understand what it’s trying to do, and make some enhancements without breaking the existing functionality. This requires a fair amount of setup work, especially if you need to support multiple languages, but it’s a great way of testing the skills any developer should have.&lt;/li>
&lt;/ul>
&lt;p>The goal of every interview is to make sure the person you’re interviewing will be successful at their job. The best way is to give them something that’s as close to the job itself as possible. If they succeed it’s likely they’ll be able to do the job itself and if they fail it’s likely they wouldn’t be able to cut it. The implication is that you need to structure your process to optimize for this. No single one of these will tell you everything you need to know so it’s important to mix and match to find the combination that gives you the most confidence in your process.&lt;/p></description></item><item><title>In praise of the polymath</title><link>/2017/05/13/in-praise-of-the-polymath/</link><pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/13/in-praise-of-the-polymath/</guid><description>&lt;p>These days it’s incredibly easy to keep learning and find information on any topic but it’s much rarer to find people that are interested in everything they come across. More often you find people passionate about a few domains that are the most relevant to them and don’t bother pursuing knowledge of anything new. This mindset is both sad and irritating - why would someone consciously limit their knowledge?&lt;/p>
&lt;p>Beyond knowledge for knowledge’s sake it’s valuable to train your mind to learn and absorb information; you never know when it can come in handy. Beyond making you a generally more interesting person you’ll be able to connect with nearly anyone - an immensely useful ability regardless of what you do. It also adds to your own character and gives you the ability to think from a variety of perspectives - something that seems to be lacking in the modern world.&lt;/p></description></item><item><title>Using options to play Snapchat's quarterly results</title><link>/2017/05/10/using-options-to-play-snapchats-quarterly-results/</link><pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/10/using-options-to-play-snapchats-quarterly-results/</guid><description>&lt;p>I rarely write about finance but a decade ago I did a stint in finance and picked up a few things. One of these was the idea of options which are an interesting and powerful way to participate in the market. There’s a ton of information online describing how they work but a simple explanation is that they give you the “option,” or the right, to buy or sell shares of the underlying stock at a particular price by a future expiration date. This particular price, referred to as the strike price, and the expiration date, are the significant drivers of the price of the option. But generally, buying options on unlikely scenarios (ie far away from the current value) in the very short term ends up being extremely cheap while buying very likely scenarios with a long horizon can get very expensive. I’m not the best equipped to get into the specifics of pricing options but it’s incredibly intricate and involves some deep math that one can get lost in and is worth exploring for the mathematically minded or curious.&lt;/p></description></item><item><title>Tools over languages</title><link>/2017/05/07/tools-over-languages/</link><pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/07/tools-over-languages/</guid><description>&lt;p>The more I code the more I’m convinced that the quality of tools determines the value of a programming language. Some languages, such as Haskell, have wonderful and elegant design and are a joy to write but mostly remain hobby languages. Other languages, on the other hand, such as Java, may be rote and uninspired but have a massive amount of supported tools that allow them to keep growing. Unless we’re exploring or playing around the ultimate goal of every line of code we write is for it to be deployed to production and solid tools allow that to happen. Unfortunately, this also leads to a “rich get richer” scenario where a popular language continues to grow with an evolving and more compelling toolset rather than on any individual merits. And while the result is a better experience than it was before these new tools other languages may not have even gotten the chance to succeed.&lt;/p></description></item><item><title>Security across multiple AWS regions</title><link>/2017/05/04/security-across-multiple-aws-regions/</link><pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate><guid>/2017/05/04/security-across-multiple-aws-regions/</guid><description>&lt;p>As great as AWS is there’s still a major gap in the way cross-region support are handled. It’s boggling that there’s no single screen to see every one of your instances and you’re forced to do it a region at a time. Beyond the cosmetic it’s not-obvious how to get instances from multiple regions to communicate securely with one another. On one hand Amazon has the neat concept of a Virtual Private Cloud (VPC) that allows you to create a group of machines that act as if they’re on the same network. This makes it simple come up with some pretty neat security rules - for example only allowing for an instance to communicate with the outside world via port 80 but with its network on other ports. Using a combination of VPCs and security groups one can come up with a pretty intricate security system.&lt;/p></description></item><item><title>Having some fun with the RGB color model</title><link>/2017/04/30/having-some-fun-with-the-rgb-color-model/</link><pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/30/having-some-fun-with-the-rgb-color-model/</guid><description>&lt;img src="/image/color-fun.png" alt="Color fun" data-width="2312" data-height="574" data-layout="responsive" />
&lt;p>The best way to learn a new technology is to play with it so to learn React I started a simple project I termed “&lt;a href="https://dangoldin.github.io/color-fun/">color-fun&lt;/a>&amp;quot; (&lt;a href="https://github.com/dangoldin/color-fun">GitHub&lt;/a>). The general idea is to let you specify a starting color along with a step size for each of the digital primary colors and see the color progression. By messing around with various combinations one can get a pretty good sense of the way the RGB color scheme works. To make it a bit less boring there’s also a “random” option to generate a new value combination and a new color row.&lt;/p></description></item><item><title>The golden age of big data tools</title><link>/2017/04/23/the-golden-age-of-big-data-tools/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/23/the-golden-age-of-big-data-tools/</guid><description>&lt;p>I really dislike using the phrase “big data” but it is catchy so I’m going with it. It really does feel we’re in the golden age of big data tools. The rise of cloud computing, distributed storage, and the proliferation of open source have led to multiple orders of magnitude more data generated now than a decade ago. It’s an impossible number to calculate but some project that between 2010 and 2020 there will be a 50 fold increase in the amount of data collected. And the rate is only increasing as more and more people around the world get smartphones and the internet of things starts becoming a part of daily life.&lt;/p></description></item><item><title>Subscription all the things</title><link>/2017/04/21/subscription-all-the-things/</link><pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/21/subscription-all-the-things/</guid><description>&lt;p>Reading modern technology and business news it seems that every single thing is moving to the subscription model. It’s no longer just the obvious stuff that’s being turned into a subscription with the help of technology. Software is becoming a critical part of many systems - ranging from powering cars to tractors to juicers - and would be hilarious if it weren’t so real. Software has transformed the world and now we’re trying to find the remaining places that software can be jammed into. And once the software sets in everything can be turned into a subscription model.&lt;/p></description></item><item><title>AMP and subscription paywalls</title><link>/2017/04/16/amp-and-subscription-paywalls/</link><pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/16/amp-and-subscription-paywalls/</guid><description>&lt;p>A variety of publishers are adopting &lt;a href="https://www.ampproject.org/">Accelerated Mobile Pages&lt;/a> (AMP) to speed up the performance of their sites on mobile. In fact, I’m using AMP to power my entire blog on both desktop and mobile and it’s significantly faster than my old, heavyweight site. But I’m a small time blogger and to get real publishers on board AMP needs to support a variety of monetization options - including ads and subscriptions - that are able to generate the same revenue they’re getting without AMP.&lt;/p></description></item><item><title>SQL is the perfect interface</title><link>/2017/04/11/sql-is-the-perfect-interface/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/11/sql-is-the-perfect-interface/</guid><description>&lt;p>The more I code the more I’m exposed to SQL. It started with the usual relational suspects - MySQL, PostgreSQL, and even SQL Server (back in the finance days), but has since then expanded to columnar database such as Redshift, Vertica, and MonetDB. And now I’m starting to use SQL to query data on S3 using SparkSQL, Athena, and Hive.&lt;/p>
&lt;p>SQL was &lt;a href="https://en.wikipedia.org/wiki/SQL">introduced in the 70s&lt;/a> and became an official standard in 1986 and it’s just incredible to see how dominant and dispersed it’s become. It’s still used for the original RDBMS use cases but it’s expanded significantly since then for a whole slew of new use cases. At the same time the underlying query syntax remained surprisingly similar. We talk about designing good interfaces that allow us to write reusable and clean code but SQL is an interface that’s existed, evolved, and expanded for more than 40 years.&lt;/p></description></item><item><title>Optical illusions and self driving cars</title><link>/2017/04/09/optical-illusions-and-self-driving-cars/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/09/optical-illusions-and-self-driving-cars/</guid><description>&lt;p>While catching up on the latest self driving car news and digging into the way neural networks work I started thinking of the ways self driving cars would navigate optical illusions or seemingly impossible physical scenarios. I’m by no means an expert but current machine learning and artificial techniques don’t build a relational representation of the world but instead focus on statistical ways of classifying the information they see and then making decisions off of that information.&lt;/p></description></item><item><title>Quality over quantity: NextDoor vs Craigslist</title><link>/2017/04/08/quality-over-quantity-nextdoor-vs-craigslist/</link><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/08/quality-over-quantity-nextdoor-vs-craigslist/</guid><description>&lt;p>Last weekend my wife and decided that we needed to upgrade our couch and in our eagerness decided that we wanted it delivered as soon as possible which happened to be Friday (yesterday). This led us to the question of what to do with our existing couch. The pragmatist in me decided that we should list it on &lt;a href="http://craigslist.org">Craigslist&lt;/a> and &lt;a href="https://nextdoor.com/">NextDoor&lt;/a> for $300 and see who would bite but after a few days without any response it became clear that if we wanted it gone soon it would need to be free. So this past Wednesday I relisted as a free and started receiving bites - just under a dozen on Craigslist and one via NextDoor.&lt;/p></description></item><item><title>Changing good code is easy</title><link>/2017/04/04/changing-good-code-is-easy/</link><pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/04/changing-good-code-is-easy/</guid><description>&lt;p>I’ve done my fair share of code reviews and one of the best indicators of great code is the locality of changes. Nearly all code is taking some data, transforming it, and passing it along somewhere else. This implies that modifications change or add to this flow. It may be passing an additional variable to a function, changing the behavior of a function, or adding another step in our execution. It turns out that if the code is poorly written a seemingly minor change may require changing a series of functions since for some reason each function in the flow needs to be modified. Great code, on the other hand, is written in such a way where making a change to a single function or behavior doesn’t cause any changes upstream or downstream of the code.&lt;/p></description></item><item><title>Slack's channel exit anti pattern</title><link>/2017/04/02/slacks-channel-exit-anti-pattern/</link><pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate><guid>/2017/04/02/slacks-channel-exit-anti-pattern/</guid><description>&lt;p>Slack has grown incredibly quickly and solves a difficult problem but I can’t help but notice that some design antipatterns that increase Slack usage but don’t benefit the user. I came across one of these designs on Friday when I saw someone leave a channel. They no longer found the channel useful or were only there to answer a few questions but as soon as they left everyone in the channel saw the message “so and so has left the channel.”&lt;/p></description></item><item><title>Fulfillment by Amazon counterfeiting</title><link>/2017/03/26/fulfillment-by-amazon-counterfeiting/</link><pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/26/fulfillment-by-amazon-counterfeiting/</guid><description>&lt;p>Recently I’ve come across a few articles describing the supposedly massive amount of counterfeiting happening on Amazon. The way it works is that Amazon offers a &lt;a href="https://services.amazon.com/fulfillment-by-amazon/how-it-works.htm/ref=asus_fba_snav_how">Fulfillment by Amazon&lt;/a> (called FBA) option where a merchant sends their items to Amazon’s warehouse which is then eligible for Prime shipping since it’s just going to be shipped by Amazon. The way Amazon implements this is by commingling the items - so if two merchants send Amazon the same item Amazon will treat it as the same item when it comes to consumers. A merchant is able to opt out of this commingling but only with a higher fee.&lt;/p></description></item><item><title>The effects of marketing on price</title><link>/2017/03/25/the-effects-of-marketing-on-price/</link><pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/25/the-effects-of-marketing-on-price/</guid><description>&lt;p>I like to think of myself as extremely relational and efficient and something that’s always bothered me is marketing. I hated the idea that companies had to market their products since the cost of the marketing would just be absorbed into the price I would be paying for the final product. Of course it’s not that simple and marketing isn’t just a cost and can bring value to everyone involved. In fact, George Akerlof wrote a paper in 1970, &lt;a href="https://en.wikipedia.org/wiki/The_Market_for_Lemons">The Market for Lemons&lt;/a>, which describes how markets with asymmetric information end up full of “lemons” and led to a Noble Prize. One of the solutions to this problem is to do marketing since that will signal that you’re a legitimate company offering a legitimate product.&lt;/p></description></item><item><title>Embrace experimentation</title><link>/2017/03/22/embrace-experimentation/</link><pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/22/embrace-experimentation/</guid><description>&lt;p>Rather than debating various approaches the best way to determine the ideal option is to run an experiment and look at the outcome. The tech industry has embraced this and is constantly running all sorts of A/B tests to optimize any and all metrics. Unfortunately, this approach hasn’t spread to the rest of society where decisions are based on abstract theories and perceptions. Imagine how much society could improve if we expanded experimentation into policies affecting our cities, states, and countries.&lt;/p></description></item><item><title>Refactor driven development</title><link>/2017/03/19/refactor-driven-development/</link><pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/19/refactor-driven-development/</guid><description>&lt;p>There are a variety of software development methodologies and I’d like to throw another one into the fray - refactor driven development. Rather than focusing on tests or models or functions the focus should be on expressive and maintainable code. Imagine spending 80% of your development time on refactoring old code and laying a solid foundation for all future work. Then the remaining 20% of the time can be spent on writing new features and functionality that drive the product forward. Once this work is done it may lead to more refactoring work to get the code back into a pristine state.&lt;/p></description></item><item><title>In praise of long running code</title><link>/2017/03/12/in-praise-of-long-running-code/</link><pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/12/in-praise-of-long-running-code/</guid><description>&lt;p>There’s something spectacular in checking in on a project you worked on years ago and discovering it&amp;rsquo;s still running years later. This past Friday I got an HTTPS alert from &lt;a href="https://letsencrypt.org/">Let’s Encrypt&lt;/a> reminding me that my SSL certificate for &lt;a href="https://yahnr.dangoldin.com/">https://yahnr.dangoldin.com&lt;/a> was set to expire. I checked it out and remarkably it&amp;rsquo;s still up and running. I built that in March of 2013 as a proof of concept of what I termed a &lt;a href="http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/">“pseudo-static site.”&lt;/a> The idea was to have a site hosted on statically on S3 but powered by a dynamic job that refreshes the underlying data.&lt;/p></description></item><item><title>Stop procrastinating</title><link>/2017/03/09/stop-procrastinating/</link><pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/09/stop-procrastinating/</guid><description>&lt;p>I often find myself wanting to postpone something but after forcing myself to actually do it I discover that most of the difficulty was getting over the postponement hump. Especially these days it’s very easy to get distracted - whether it’s checking your email, responding to a few tweets, or clearing notifications on Slack - but it’s important to just focus on the most important task at hand. After starting you discover that the task wasn’t worth delaying and get an energy boost from actually finishing something.&lt;/p></description></item><item><title>Automating admin work: Spreadsheets to Slack</title><link>/2017/03/04/automating-admin-work-spreadsheets-to-slack/</link><pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate><guid>/2017/03/04/automating-admin-work-spreadsheets-to-slack/</guid><description>&lt;p>Recently we adopted the concept of owning your own up time for our engineering teams. The goal is to encourage a stronger sense of ownership and actually give the teams the autonomy to approach their development and release process the way they’re comfortable with. Before this we relied on a single on call every week that would be responsible for monitoring all issues and escalating them to the appropriate team. One minor side effect of this change was that I now had to manage the on call calendar and post the new rotation on Slack every week. Since this was a good opportunity to mess around with the Google Spreadsheet and Slack APIs I decided it good be a fun little project.&lt;/p></description></item><item><title>Lessons learned from today's S3 failure</title><link>/2017/02/28/lessons-learned-from-todays-s3-failure/</link><pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/28/lessons-learned-from-todays-s3-failure/</guid><description>&lt;p>Today was quite a day. S3, the most resilient of Amazon’s services went down for a few hours in the US-EAST-1 zone and led to a series of failures across a variety of services. There are a ton of lessons one should take away from this - ranging from running across multiple availability zones to being integrated with a variety of cloud providers. The challenge is that it’s not easy; especially when you’re small. At that point you have to prioritize building support for a 0.01% chance of massive failure versus a variety of features and product enhancements to drive your business forward.&lt;/p></description></item><item><title>My Snapchat investment strategy</title><link>/2017/02/26/my-snapchat-investment-strategy/</link><pubDate>Sun, 26 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/26/my-snapchat-investment-strategy/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/fb-stock-chart.png" alt="Facebook stock chart" data-width="897" data-height="230" data-layout="responsive" />
 &lt;p class="caption">Facebook stock price since IPO&lt;/p>
&lt;/div>
&lt;p>Snapchat is expected to IPO &lt;a href="http://fortune.com/2017/02/17/snapchat-ipo-what-time-when-stock/">March 2nd&lt;/a> and I’m torn as to whether to invest. I think it’s an innovative product that provides a compelling experience but there’s a series of red flags - from the weird ownership structure to the potential slowdown in user growth to the growing per user costs. At the same time it may be the next social network. Facebook IPOed at just over $100B and is now worth almost $400B. I hate to compare Snapchat to Facebook but the bull case is that it can be the next Facebook. There’s been a history of startups growing to surpass the prior generation of companies and outside of Snapchat there’s nothing in the social media space. Snapchat has its work cut out but they do offer a unique product with a strong user experience that’s constantly improving.&lt;/p></description></item><item><title>Advice for coding bootcamp graduates</title><link>/2017/02/21/advice-for-coding-bootcamp-graduates/</link><pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/21/advice-for-coding-bootcamp-graduates/</guid><description>&lt;p>Coding bootcamps are increasingly popular and I’ve seen a large number of resumes come across my desk so wanted to share my perspective and offer some advice. I think it’s great that more people are learning to code. At the same time there’s a lot of volume and based on a few months it’s difficult to stand out, especially as more and more bootcamps spring up. First off, I respect the hell out of people taking the leap. It takes a lot of effort to stop what you were doing and pursue a completely different career track. It’s not easy and already sends a signal that you’re motivated and willing to grow your skills. Below are a few other ways to help improve your odds of getting hired. Most of these are relevant even if you’re not coming from a bootcamp so read on if interested.&lt;/p></description></item><item><title>Math is incredible</title><link>/2017/02/19/math-is-incredible/</link><pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/19/math-is-incredible/</guid><description>&lt;p>Maybe I never learned this or maybe I forgot but while reading &lt;a href="https://www.amazon.com/Prime-Obsession-Bernhard-Greatest-Mathematics/dp/0452285259">Prime Obsession&lt;/a> I came across a concept that blew my mind. We all learn about infinite series and how some converge (think 1 + ½ + ¼ + .. = 2) and some diverge (1 + ½ + ⅓ + ¼ + ..) but it turns out that not all convergent series are the same. The same numbers, added in a different order, can lead to a different resulting sum. These series are called &lt;a href="http://mathworld.wolfram.com/ConditionalConvergence.html">conditionally convergent&lt;/a>. This is incredible since I always assumed that addition was commutative but it turns out even fundamental ideas are violated when dealing with infinite sums. I still can’t wrap my mind around how this makes any sense but the math doesn’t lie. The example below is from the book but I’d love to see others so I can continue wrapping my head around it. I’ve been out of school for a while now but this discovery brings me back.&lt;/p></description></item><item><title>Read books, not blog posts</title><link>/2017/02/16/read-books-not-blog-posts/</link><pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/16/read-books-not-blog-posts/</guid><description>&lt;p>Right now I have three tabs open for books I plan on reading: &lt;a href="http://dataintensive.net/">Designing Data-Intensive Applications&lt;/a>, Google’s &lt;a href="https://landing.google.com/sre/book.html">Site Reliability Engineering&lt;/a>, and &lt;a href="http://www.deeplearningbook.org/">Deep Learning&lt;/a>. Unfortunately these tabs have been open for over a week and yet I haven’t deeply committed to any one of them. Yet during this time I spent a bunch of time reading a variety of blog posts and articles that provide bite size information. This is terrible. I could have instead spent the same amount of time actually diving deeper and gaining a much better understanding of a new topic but instead I distracted myself and resorted to the easy reward.&lt;/p></description></item><item><title>Mancur Olson and the changing economy</title><link>/2017/02/13/mancur-olson-and-the-changing-economy/</link><pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/13/mancur-olson-and-the-changing-economy/</guid><description>&lt;p>I studied economics in college but only encountered Mancur Olson years later when I picked up “&lt;a href="https://www.amazon.com/Rise-Decline-Nations-Stagflation-Rigidities/dp/0300030797">The Rise and Decline of Nations&lt;/a>” at a bookstore years later. It proposes a simple theory that explains why some countries succeed while others fail - as a country maintains growth and stability it ends up developing more and entrenched interests that hobble future growth. Olson uses a variety of historical economic examples to prove this point - ranging from the British empire to Germany and Japan post World War II - that make a compelling case. This also ties in nicely with the sharing economy since it ties into the idea of regulatory capture - where government agencies are lobbied by special interest groups to introduce laws that defend their market position. This is currently playing out with the sharing economy - the taxi and limo lobbies are fighting Uber while hotels are challenging AirBnB. Both Uber and to a slightly lesser degree bring significant value to society but spend money on lawyers and lobbyists that could be spent on pushing the economy and world forward.&lt;/p></description></item><item><title>Traffic efficiency</title><link>/2017/02/09/traffic-efficiency/</link><pubDate>Thu, 09 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/09/traffic-efficiency/</guid><description>&lt;div class="right10">
 &lt;img src="/image/traffic-efficiency.png" alt="traffic efficiency" data-width="389" data-height="367" data-layout="responsive" />
&lt;/div>
&lt;p>While using Google Maps to pull some directions I stumbled unto a case where every single option gave the same time estimate. I like harboring the idea enough people use Google Maps that they’re able to optimize each route to reduce the total time across the entire system. By having enough data from the tens of thousands of drivers on these roads Google can predict how busy the roads will be due to the directions they’re providing. In turn, they’re able to use this information to predict what the traffic patterns will look like on those roads in the future and use that knowledge now to change it.&lt;/p></description></item><item><title>New law? Let the opposition name it</title><link>/2017/02/06/new-law-let-the-opposition-name-it/</link><pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/06/new-law-let-the-opposition-name-it/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/bills-passed-by-congress.jpeg" alt="Team point density" data-width="1024" data-height="768" data-layout="responsive" />
 &lt;p class="caption">&lt;a href="https://www.washingtonpost.com/news/the-fix/wp/2014/04/10/president-obama-said-the-113th-congress-is-the-least-productive-ever-is-he-right/">The Washington Post&lt;/a>&lt;/p>
&lt;/div>
&lt;p>Whenever a new law is passed it’s incredibly difficult to have it repealed since there are enough people benefiting who will fight against the removal. This leads to a situation where instead of replacing old laws with new laws we end up with a massive system of laws with new ones that just get thrown on. This leads to a whole slew of inefficiencies and requires experts to help navigate the landscape who themselves benefit from the complexity and want to maintain it as much as they can.&lt;/p></description></item><item><title>Identifying product weaknesses using Google autocomplete</title><link>/2017/02/04/identifying-product-weaknesses-using-google-autocomplete/</link><pubDate>Sat, 04 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/04/identifying-product-weaknesses-using-google-autocomplete/</guid><description>&lt;p>A simple trick I’ve started using when learning about new tools or technologies is to just type in “{name} vs” in Google search and see what pops up. This relies on the wisdom of the crowds and Google’s prediction to give me insight into other items in the same space. Oftentimes going to the corporate site gives me standard marketing copy but looking at comparisons provides the actual details: What do people use it for? What are the competitors? How does it compare against them? What are some success stories? What are some failure cases?&lt;/p></description></item><item><title>Learning modern frontend development</title><link>/2017/02/01/learning-modern-frontend-development/</link><pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate><guid>/2017/02/01/learning-modern-frontend-development/</guid><description>&lt;p>Until a few weeks ago my frontend programming experience ended with jQuery so I decided to do something about it and start getting up to speed with modern frontend development. This ranged from starting to mess around with React, to using ES6, to integrating webpack and Babel in these projects. I’ve been using Sublime Text for the past 6 years but am switching to &lt;a href="https://code.visualstudio.com/">Visual Code Studio&lt;/a> as my primary editor.&lt;/p></description></item><item><title>The freedom to move</title><link>/2017/01/28/the-freedom-to-move/</link><pubDate>Sat, 28 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/28/the-freedom-to-move/</guid><description>&lt;p>I&amp;rsquo;ve had this thought for a while now but Trump&amp;rsquo;s latest executive order banning Muslim immigration is forcing me to put it in writing. The freedom to move should be a human right and can act as the foundation of all others. If you don&amp;rsquo;t like the policies of your city? Move to another one. Your state? Move to another one. Your country? Same thing.&lt;/p>
&lt;p>By giving people the freedom to choose where they live policies will arise that benefit the greatest number of people. If you&amp;rsquo;re keen on having a strong religious state you should be able to move there. If you want to live in a big city you should be able to move to one. If it turns out that a particular location is losing residents it will act as a signal that they need to do something. Over time the best policies will rise to the top and the world will have an rapid pace of policy innovation.&lt;/p></description></item><item><title>Shame on United and Bank of America</title><link>/2017/01/26/shame-on-united-and-bank-of-america/</link><pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/26/shame-on-united-and-bank-of-america/</guid><description>&lt;p>Over the past month I had two experiences that seem too coincidental to be true and highlight how desperate some companies are to chase every penny. These are massive corporations that seem intent on sneaking in false fees and charges hoping that their customers don&amp;rsquo;t notice.&lt;/p>
&lt;p>The first involved United Airlines. The airline industry already suffers from poor reputation and it seems United is doubling down on it. I had a flight back from a vacation and thought I&amp;rsquo;d treat ourselves to an upgrade. United had an option to use miles and money which would charge you immediately, add you to the upgrade list, and refund both if you weren&amp;rsquo;t picked. We didn&amp;rsquo;t get the upgrade but we miraculously received a refund of the miles but only half of the money. It took a few phone calls but in the end I got the refund.&lt;/p></description></item><item><title>Powering our devices using the human body</title><link>/2017/01/16/powering-our-devices-using-the-human-body/</link><pubDate>Mon, 16 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/16/powering-our-devices-using-the-human-body/</guid><description>&lt;p>Lately I’ve been thinking of how we’re moving more and more to a world where humans will start merging with technology. We’re already carrying our phones around nearly every minute of every day but at least we keep them in our pockets. Watches are always on our wrists and with augmented reality the permanently attached technologies will only grow.&lt;/p>
&lt;p>At some point they’ll just become part of our bodies and I was curious what additional energy we’d need to consume in order to power these devices. A naive calculation makes it seem that our devices are incredibly cheap to power. An iPhone 6S &lt;a href="https://www.apple.com/legal/more-resources/docs/apple-product-information-sheet.pdf">comes with&lt;/a> 6.61 Watt hour battery which is 23,796 Watt seconds (6.61 * 60 * 60). And since a single calorie is equivalent to 4.1868 Watt seconds with some simple math we get that an iPhone 6s battery is equivalent to 5683 calories. But these are true calories and not what’s actually listed on food labels. Food labels list kilocalories so remarkably the capacity of an iPhone 6s is fewer than 6 “food” calories. For context 6 calories is what one gets from a large strawberry. I charge my phone once a day and I’m shocked that the energy in a strawberry would be able power my phone for a single day.&lt;/p></description></item><item><title>Find the arbitrage opportunities in hiring</title><link>/2017/01/14/find-the-arbitrage-opportunities-in-hiring/</link><pubDate>Sat, 14 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/14/find-the-arbitrage-opportunities-in-hiring/</guid><description>&lt;p>With the football playoffs upon us I’ve been thinking of why some teams are more successful than others. They all have approximately the same budget to allocate across a 53 person roster yet some teams are consistently dominant while others predictably fail. What’s so special about the teams that succeed? There’s obviously the coaching but I believe the bulk of the credit should go to the players. Imagine if a single team had a budget that was an order of magnitude more than the next best team: that team would be stacked with the best players and would dominate in competitions.&lt;/p></description></item><item><title>Information bandwidth of audio and display ads</title><link>/2017/01/11/information-bandwidth-of-audio-and-display-ads/</link><pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/11/information-bandwidth-of-audio-and-display-ads/</guid><description>&lt;p>I’ve been reading a lot of articles lately on how the shift to audio interactions at home, namely Echo, have the potential to disintermediate Google. The gist is that Google makes the bulk of its revenue from search ads that will show up alongside the organic results. At that point the user can either click on an organic result powered by Google search or an ad powered by an auction. This approach works for the web where we can process the page at a glance but doesn’t translate neatly in an audio context. Imagine asking the Echo to give you the local weather but instead of that getting a list of weather related options first. That would be an utter flop but we tolerate it on the web where our eyes have multiple of orders more information bandwidth than our ears.&lt;/p></description></item><item><title>Top posts of 2016</title><link>/2017/01/04/top-posts-of-2016/</link><pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/04/top-posts-of-2016/</guid><description>&lt;p>I’ve been writing two posts a week since the beginning of 2013 and at this point I have quite a few articles - some are one hit wonders, others are completely forgotten, while a select few are “evergreen” and are able to continually attract readers. With the start of 2017 I’ve seen a ton of bloggers doing a post highlighting their top posts from 2016 so I figured it would be fun to do the same. The data was all fetched using Google Analytics but there was a week long gap in early September when I deliberately broke the tracking while upgrading the blog to AMP. While I was hoping to get some increased growth in 2016 the total traffic ended up being pretty close to that 2015. A goal for 2017 is to focus more on evergreen posts with the goal being to build a larger audience with posts that are less responsive to the news and more about providing real value. And now on to the data:&lt;/p></description></item><item><title>Year in review: 2016</title><link>/2017/01/02/year-in-review-2016/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>/2017/01/02/year-in-review-2016/</guid><description>&lt;p>A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data. The goal is to identify healthy and unhealthy trends over time and use that information to make changes in my life. At the moment the stats are mostly high level summaries but what I want to do is use this data in order to identify hidden relationships in order to improve my physical state and mental moods. This is a work in progress but I hope to do more of that this coming year as well as improve the way I’m gathering this data. The analysis &lt;a href="https://github.com/dangoldin/annual-stats-analysis">code is up on GitHub&lt;/a> with a guide and a sample file that can be analyzed. And now on to the data:&lt;/p></description></item><item><title>Amazon's peer to peer marketplace</title><link>/2016/12/31/amazons-peer-to-peer-marketplace/</link><pubDate>Sat, 31 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/31/amazons-peer-to-peer-marketplace/</guid><description>&lt;div class="right10">
 &lt;img src="/image/craigslist-disrupted.png" alt="Andrew Parker's Spawn of craigslist" data-width="1208" data-height="906" data-layout="responsive" />
&lt;/div>
&lt;p>It looks as if Amazon is entering yet another market - &lt;a href="https://www.amazon.in/p2p">the local peer to peer marketplace&lt;/a>. They just launched a peer to peer marketplace in Bangalore. The pitch is incredibly simple - you create a listing and it’s listed on Amazon. As soon as it’s sold Amazon will pick up the item and deliver it to the buyer. I’ve used Craigslist a ton in the past and coordinating the delivery and handoff was always a frustrating experience. There doesn’t seem to be much information on how quality is measured - what happens if the time you get isn’t in the shape it was listed or just doesn’t work?&lt;/p></description></item><item><title>Favorite books of 2016</title><link>/2016/12/31/favorite-books-of-2016/</link><pubDate>Sat, 31 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/31/favorite-books-of-2016/</guid><description>&lt;p>I have a longer post planned taking a quantified self approach to my 2016 but I wanted to share my favorite books of 2016. Looking at this list the primary themes were rediscovering my love for science fiction, digging deeper into society and culture, getting back into history - both focused on technology but also the world.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B0052FF7YM/ref=oh_aui_d_detailpage_o05_?ie=UTF8&amp;amp;psc=1">The Righteous Mind: Why Good People Are Divided by Politics and Religion&lt;/a> (Jonathan Haidt): Heidt highlights morality framework that explains why there&amp;rsquo;s such a disconnect between the right and the left in politics. I ended up reading alongside the election since it just seemed that the world is getting more and more polarized.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B01HWKSBDI/ref=oh_aui_d_detailpage_o07_?ie=UTF8&amp;amp;psc=1">The Death and Life of Great American Cities&lt;/a> (Jane Jacobs): This has been on my list for a while and finally got the chance to dig into it. I love cities and this is foundational to understand what makes cities, and neighborhoods, great.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Between-World-Me-Ta-Nehisi-Coates-ebook/dp/B00SEFAIRI/ref=sr_1_1?s=digital-text&amp;amp;ie=UTF8&amp;amp;qid=1483245443&amp;amp;sr=1-1&amp;amp;keywords=between+the+world+and+me">Between the World and Me&lt;/a> (Ta-Nehisi Coates): Really personal book on what it&amp;rsquo;s like to be black in the United States and having to deal with the never-ending struggles.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B0166ISAS8/ref=oh_aui_d_detailpage_o02_?ie=UTF8&amp;amp;psc=1">Hillbilly Elegy: A Memoir of a Family and Culture in Crisis&lt;/a> (J. D. Vance): After reading Between the World and Me this takes a very similar approach but focuses on the story of growing up in the rural Midwest and making his way out.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B00QPHKR0K/ref=oh_aui_d_detailpage_o00_?ie=UTF8&amp;amp;psc=1">Deep South: Four Seasons on Back Roads&lt;/a> (Paul Theroux): Just an engaging read of the author driving through the rural South. I was inspired to read this after Hillbilly Elegy since I just wanted to understand the world outside of the coasts better.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/How-Paris-Became-Invention-Modern-ebook/dp/B00GC53AEA/ref=sr_1_4?s=books&amp;amp;ie=UTF8&amp;amp;qid=1483062652&amp;amp;sr=1-4&amp;amp;keywords=paris+city">How Paris Became Paris: The Invention of the Modern City&lt;/a> (Joan DeJean): Picked this up when I was traveling in Paris and it&amp;rsquo;s an extremely immersive and engaging read highlighting how Paris was the first city to introduce what we claim are modern necessities: including sidewalks, public transport, and street lighting.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B0029PBVCA/ref=oh_aui_d_detailpage_o01_?ie=UTF8&amp;amp;psc=1">Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age&lt;/a> (Michael A. Hiltzik): Just a really neat look at Xerox PARC, their growth, as well as the failure. So many key technologies were developed here and it&amp;rsquo;s great to see how it all played out.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B019MMUAAQ/ref=oh_aui_d_detailpage_o07_?ie=UTF8&amp;amp;psc=1">Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley&lt;/a> (Antonio Garcia Martinez): A very personal and open look at how Silicon Valley works with a strong focus on adtech and Facebook.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B00BWQW73E/ref=oh_aui_d_detailpage_o00_?ie=UTF8&amp;amp;psc=1">The Everything Store: Jeff Bezos and the Age of Amazon&lt;/a> (Brad Stone): Amazon is such a dominant company and this gives a history and talks about the strategy Amazon has taken to get where it is.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Seveneves-Novel-Neal-Stephenson-ebook/dp/B00LZWV8JO/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1483245954&amp;amp;sr=1-1&amp;amp;keywords=seveneves">Seveneves: A Novel&lt;/a> (Neal Stephenson): What a great and novel plot. The moon is destroyed and this talks about humanity escaping and trying to rebuild. I don&amp;rsquo;t want to spoil any more but it&amp;rsquo;s a great read.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Nexus-Trilogy-Book-1-ebook/dp/B00TOZI7FM/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1483246162&amp;amp;sr=1-1&amp;amp;keywords=ramez+naam">The Nexus series&lt;/a> (Ramez Naam): Really engaging series that focuses on bionanotechnology and both the risks and benefits it exposes. This is a three part series and each book&amp;rsquo;s great.&lt;/li>
&lt;/ul></description></item><item><title>Human-AI cooperation</title><link>/2016/12/30/human-ai-cooperation/</link><pubDate>Fri, 30 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/30/human-ai-cooperation/</guid><description>&lt;p>There’s been a lot written about the rise of AI and the impending automation which will lead to a mass reduction in industries requiring a human touch. The counterargument is that every time there has been a technological improvement new, higher value jobs have been created that more than offset the loss. Farming used to grossly inefficient when done by man but now massive harvesters are able to replace dozens of human workers. Similarly, technology has allowed entirely new industries to be created that just wouldn’t have been possible before - every company is embracing technology to succeed and with that comes improved efficiency and lower costs.&lt;/p></description></item><item><title>Traveling? Buy a local book</title><link>/2016/12/29/traveling-buy-a-local-book/</link><pubDate>Thu, 29 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/29/traveling-buy-a-local-book/</guid><description>&lt;p>While traveling I&amp;rsquo;ve developed the habit of visiting a local bookstore and buying a book that dives into the history of that area, sometimes it’s the city and other times the country. This gives me something relaxing to do in the evenings after a day spent running around but also adds another dimension to my trip. The history heightens my experience when exploring the city since I’m able to relate what I read to the real world - be it landmarks, sculptures, or the culture.&lt;/p></description></item><item><title>Comparing public transit systems: New York vs London</title><link>/2016/12/24/comparing-public-transit-systems-new-york-vs-london/</link><pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/24/comparing-public-transit-systems-new-york-vs-london/</guid><description>&lt;p>I&amp;rsquo;m behind on my 2 blog posts per week goal since I&amp;rsquo;m traveling and want see as much as I can but plan on catching up next week when I&amp;rsquo;m back. Right now I&amp;rsquo;m on a high speed train going from Porto to Lisbon and have 2.5 hours to do a bit of writing. And what&amp;rsquo;s more appropriate than comparing the public transit system of New York with that of London, and generally those of the United with those of Europe.&lt;/p></description></item><item><title>Programming languages and developer tools</title><link>/2016/12/17/programming-languages-and-developer-tools/</link><pubDate>Sat, 17 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/17/programming-languages-and-developer-tools/</guid><description>&lt;p>As developers we want to be as productive as possible. This encourages us to improve our tools and languages to accommodate new patterns and challenges. Many of these improvements have come due to better hardware since we&amp;rsquo;re better able to cope with slight inefficiencies at the expense of higher productivity. This, coupled with the constant advancement of compilers and interpreters, has led to a massive adoption of scripting languages.&lt;/p>
&lt;p>Lately I&amp;rsquo;ve been thinking about the trade-offs we accept when we use scripting languages. They&amp;rsquo;re easier to dive into and make it easy to build a prototype. This is especially easier with the growth of open source tools and frameworks. Unfortunately, these prototypes are difficult to scale as as they and the team working on them get larger in scope. The lack of strong and static types makes it more difficult to undertake large scale refactors and encourages type-related bugs.&lt;/p></description></item><item><title>Automatically taking screenshots of HTML elements</title><link>/2016/12/13/automatically-taking-screenshots-of-html-elements/</link><pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/13/automatically-taking-screenshots-of-html-elements/</guid><description>&lt;p>I’ve worked on a variety of scraping projects that required spinning up a browser (via selenium) and having it browse a variety of pages unattended in order to capture some data. The two most recents ones &lt;a href="https://github.com/dangoldin/turo-automation">scraping my account data&lt;/a> from Turo and the &lt;a href="https://github.com/dangoldin/yahoo-ffl">fantasy football stats&lt;/a> from Yahoo. These were relatively straightforward since the browser was used purely to navigate from page to page with the actual data capture done by parsing the underlying HTML.&lt;/p></description></item><item><title>Word clouds and text similarity</title><link>/2016/12/10/word-clouds-and-text-similarity/</link><pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/10/word-clouds-and-text-similarity/</guid><description>&lt;p>I’m a sucker for data visualizations so when I came across a simple word cloud-generating &lt;a href="https://github.com/amueller/word_cloud">Python script&lt;/a> I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year. This didn’t reveal too much but got me thinking about how my writing has changed over the years. This led my discovery of a &lt;a href="http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents">script on StackOverflow&lt;/a> that works by translating each block of text into an tf-idf (term frequency - inverse document frequency) vector and then calculating the cosine distance between them. This intuitively makes sense. The tf-idf vector is used to highlight and quantify the unique words in a given document as a vector and the cosine distance is used to compare the similarities between them - if they vectors are equivalent the angle between them is 0 which has a cosine of 1. Turns out that high school math is incredibly useful.&lt;/p></description></item><item><title>It's donation season</title><link>/2016/12/09/its-donation-season/</link><pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/09/its-donation-season/</guid><description>&lt;p>It’s that time of the year when many organizations are ramping up their donation efforts and I wanted to share the organizations I donate money to. I feel incredibly lucky to be where I am and being able to donate to worthy causes is a great way to pay it forward. Everyone is passionate about something and donating to that cause is incredibly worthwhile and valuable.&lt;/p>
&lt;p>&lt;a href="https://www.wikipedia.org/">Wikipedia&lt;/a>. The need for education is critical to a functioning society and unfortunately this has been magnified recently by the explosion of fake news. Wikipedia is incredible at providing factual information and I find myself visiting it multiple times a day. It’s both education and entertainment since it’s just so easy to get lost in its labyrinth. Out of all the tools and services I pay for Wikipedia offers by far the highest return.&lt;/p></description></item><item><title>Efficiency vs expressiveness</title><link>/2016/12/06/efficiency-vs-expressiveness/</link><pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/06/efficiency-vs-expressiveness/</guid><description>&lt;p>The ideal code is both efficient and expressive but they’re often at odds with one another. Last week I was working on a &lt;a href="https://github.com/dangoldin/aws-billing-details-analysis/blob/master/analyze_aws_details.py">simple script&lt;/a> to parse and visualize a detailed AWS bill across a variety of dimensions and came across a clear example. The script loads a CSV file into a Pandas dataframe and adds a few columns based on the values of some others. The challenge is that the CSV file can be millions of rows so minor improvements can lead to significant efficiency gains. Given this quick overview the code below should make sense but there are two functions that each iterate through the values of the same column in order to generate two additional columns.&lt;/p></description></item><item><title>Read the release notes</title><link>/2016/12/03/read-the-release-notes/</link><pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate><guid>/2016/12/03/read-the-release-notes/</guid><description>&lt;p>I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade. During my career I’ve run into a variety of issues that could have been avoided by a thorough reading of the release notes. There’s still a chance you’ll miss something and that’s why you should always have a sandbox environment and try to containerize as much as you can. Below are a few examples of issues I’ve run into upgrading various applications over the past few months:&lt;/p></description></item><item><title>Becoming a better developer</title><link>/2016/11/30/becoming-a-better-developer/</link><pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/30/becoming-a-better-developer/</guid><description>&lt;p>Yesterday I had the privilege of giving a talk at &lt;a href="http://www.hackreactor.com/">HackReactor&lt;/a> titled “Things I wish I knew” which was an amalgam of the various themes and topics I’ve been blogging and thinking about. While going through the blog I came up with two themes for the topic - the first was tactics that would make someone a better programmer immediately and the second was how to improve as a developer over time.&lt;/p></description></item><item><title>Visualizing your AWS costs</title><link>/2016/11/27/visualizing-your-aws-costs/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/27/visualizing-your-aws-costs/</guid><description>&lt;p>There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, &lt;a href="https://aws.amazon.com/premiumsupport/trustedadvisor/">Trusted Advisor&lt;/a>, that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: &lt;a href="https://www.cloudability.com">Cloudability&lt;/a> starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.&lt;/p></description></item><item><title>Joy of old code</title><link>/2016/11/19/joy-of-old-code/</link><pubDate>Sat, 19 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/19/joy-of-old-code/</guid><description>&lt;p>I have over &lt;a href="https://github.com/dangoldin?tab=repositories">50 repositories&lt;/a> on GitHub with the majority being one time projects that were either me exploring a new technology, writing a small script, or doing a quick data analysis and visualization project. Every once in awhile when I’m a bit nostalgic I’ll go through these old projects and mend some of the code.&lt;/p>
&lt;p>What’s surprising is discovering old projects and scripts that work as is without me having to do anything to update the underlying code. I’m used to so working with so many open source libraries and cryptic documentation that it’s rare to find a public library that works exactly as you expect. Of course my projects are much simpler than the typical open source library but I find it remarkable that I can get code up and running within a few minutes of a checkout.&lt;/p></description></item><item><title>Comparing the web requests made by the top sites: 2014 vs 2016</title><link>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</link><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/18/comparing-the-web-requests-made-by-the-top-sites-2014-vs-2016/</guid><description>&lt;p>A &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">few years ago&lt;/a> I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they&amp;rsquo;re making.&lt;/p></description></item><item><title>Recursive redirects with AWS Lambda</title><link>/2016/11/13/recursive-redirects-with-aws-lambda/</link><pubDate>Sun, 13 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/13/recursive-redirects-with-aws-lambda/</guid><description>&lt;p>Two years ago I &lt;a href="http://dangoldin.com/2014/12/31/redirect-recursion/">toyed around&lt;/a> with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS &lt;a href="https://aws.amazon.com/lambda/">Lambda&lt;/a> binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda. Turns out it’s incredibly easy.&lt;/p></description></item><item><title>A poor man's data pipeline</title><link>/2016/11/12/a-poor-mans-data-pipeline/</link><pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/12/a-poor-mans-data-pipeline/</guid><description>&lt;p>Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.&lt;/p></description></item><item><title>Restricted highlighting on the Amazon Kindle</title><link>/2016/11/06/restricted-highlighting-on-the-amazon-kindle/</link><pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate><guid>/2016/11/06/restricted-highlighting-on-the-amazon-kindle/</guid><description>&lt;p>I love to read so it took me a surprisingly long time to get a Kindle. Before then I felt fine either just grabbing a physical book or using a tablet or a phone. LCD displays never bothered me so I figured I might as well get the responsiveness and additional functionality of a tablet rather than a single-use device. But earlier this year I spent some time using my wife’s Kindle and loved the form factor as well as the battery life. I also started to buy a lot more ebooks so finally took the plunge and got myself a Kindle.&lt;/p></description></item><item><title>The NFL abroad</title><link>/2016/10/30/the-nfl-abroad/</link><pubDate>Sun, 30 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/30/the-nfl-abroad/</guid><description>&lt;p>NFL viewership is &lt;a href="http://www.wsj.com/articles/ratings-fumble-for-nfl-surprises-networks-advertisers-1475764108">down 10%&lt;/a> this season and I understand the desire to grow the brand and the sport &lt;a href="https://en.wikipedia.org/wiki/NFL_International_Series#Long-term_deals_and_the_NFL.27s_return_to_Mexico:_2016.E2.80.93present">abroad&lt;/a>. It seems misguided to take a product that’s declining in popularity and rather than fixing the core problems to try to grow it as is. This is akin to a tech startup marketing the hell out of a product that’s unable to retain its existing customers. The proper approach is to nail the product before trying to push it into the market.&lt;/p></description></item><item><title>Simple data visualizations from the command line</title><link>/2016/10/26/simple-data-visualizations-from-the-command-line/</link><pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/26/simple-data-visualizations-from-the-command-line/</guid><description>&lt;p>Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.&lt;/p></description></item><item><title>Preventing future DDOS attacks</title><link>/2016/10/23/preventing-future-ddos-attacks/</link><pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/23/preventing-future-ddos-attacks/</guid><description>&lt;p>After Friday’s DNS DDOS attack I’ve been thinking of approaches that could prevent this from happening in the future. In a perfect world every device would be up to date with the latest updates and it would be difficult to compromise anything that’s connected to the internet. Unfortunately, this is not the case and there’s an ever growing number of devices that are quickly hacked together and sold without any focus placed on security. Akamai did a &lt;a href="https://www.wired.com/2016/10/akamai-finds-longtime-security-flaw-2-million-devices/">study that shows&lt;/a> over 2 million internet connected devices have been compromised which allows them to be used to run DDOS attacks, very similar to the one that took down a big chunk of the internet on Friday. The challenge is that most owners both don’t know and don’t bother to do any security audits when setting up these devices and very likely never upgrade the firmware nor the software to make them more secure.&lt;/p></description></item><item><title>Revisiting my Twitter activity</title><link>/2016/10/19/revisiting-my-twitter-activity/</link><pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/19/revisiting-my-twitter-activity/</guid><description>&lt;p>While going through my old GitHub repos I discovered that the most starred repo was &lt;a href="https://github.com/dangoldin/twitter-archive-analysis">twitter-archive-analysis&lt;/a>, a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.&lt;/p></description></item><item><title>My new blogging setup</title><link>/2016/10/13/my-new-blogging-setup/</link><pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/13/my-new-blogging-setup/</guid><description>&lt;div class="right10">
 &lt;img src="/image/lapdesk-blogging.jpg" alt="My lapdesk, keyboard, and phone setup" data-width="350" data-height="467" data-layout="responsive" />
&lt;/div>
&lt;p>The past couple of weeks I’ve had a big case of writer’s block. I haven’t been able to motivate myself to write as much as I used to and when I did get to write it felt more like a chore than a joy. I didn’t know how to break out of it but this past weekend I kicked off the OS X upgrade without realizing how much time it would take.&lt;/p></description></item><item><title>Setting up secor for Kafka 0.10</title><link>/2016/10/10/setting-up-secor-for-kafka-0.10/</link><pubDate>Mon, 10 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/10/setting-up-secor-for-kafka-0.10/</guid><description>&lt;p>Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that &lt;a href="https://github.com/pinterest/secor">secor&lt;/a>, a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.&lt;/p></description></item><item><title>Friction and mass surveillance</title><link>/2016/10/09/friction-and-mass-surveillance/</link><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/09/friction-and-mass-surveillance/</guid><description>&lt;p>One of the best arguments I’ve heard against mass surveillance is that the marginal cost has dropped to nearly zero which warps the system. Since so much of our world is digital it costs the government nothing extra to collect each additional data point. Given these incentives it’s no surprise that the government was able to get the major companies to provide a dedicated feed of the data they were collecting - modern technology has enabled both the collection and analysis of massive amounts of data.&lt;/p></description></item><item><title>iOS wifi security recommendation</title><link>/2016/10/02/ios-wifi-security-recommendation/</link><pubDate>Sun, 02 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/02/ios-wifi-security-recommendation/</guid><description>&lt;div class="right10">
 &lt;img src="/image/ios-security-recommendation.png" alt="iOS security recommendation" data-width="300" data-height="534" data-layout="responsive" />
&lt;/div>
&lt;p>While exploring the city earlier today I ended up wandering too close to the Google building and somehow got connected to their guest wifi network, GoogleGuest, and noticed that iOS 10 gave me an “Security Recommendation” notification. My first reaction was that this was an Apple jab at Google but It turns out that iOS 10 introduced a &lt;a href="https://www.engadget.com/2016/07/22/ios-10-unsecured-networks/">new feature&lt;/a> to let people know that they were connecting to an open network. The intent seems to be to warn users that they may not be on a secure connection but it’s a bit hidden away and didn’t actually prevent me from connecting: it was more of an FYI.&lt;/p></description></item><item><title>Shaming meeting room hogs</title><link>/2016/10/01/shaming-meeting-room-hogs/</link><pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate><guid>/2016/10/01/shaming-meeting-room-hogs/</guid><description>&lt;p>One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.&lt;/p></description></item><item><title>NFL Thursdays on Twitter</title><link>/2016/09/25/nfl-thursdays-on-twitter/</link><pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/25/nfl-thursdays-on-twitter/</guid><description>&lt;p>In April, Twitter &lt;a href="http://www.bloomberg.com/news/articles/2016-04-05/twitter-said-to-win-nfl-deal-for-thursday-night-streaming-rights">announced a deal&lt;/a> with the NFL to broadcast Thursday night games and I gave it a shot this past Thursday via the Twitter app on my FireTV. The primary motivation was to watch the game but I was also curious to see Twitter’s implementation. I was pleasantly surprised by how smooth and clean the overall experience was: you could watch the entire game without knowing it was via Twitter but the tweets added a please, yet optional, touch. The only real difference between the Twitter app and any other FireTV streaming app was that Twitter augmented the experience with twitter content - tweets, images, and scopes.&lt;/p></description></item><item><title>A smarter Touch ID</title><link>/2016/09/23/a-smarter-touch-id/</link><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/23/a-smarter-touch-id/</guid><description>&lt;p>Apple’s Touch ID is great but one thing it doesn&amp;rsquo;t handle well is wet fingers. Even if my hands are a little bit sweaty or not completely dry it&amp;rsquo;s difficult to unlock the phone. Yet as soon as they’re dry the phone immediately unlocks. What’s surprising, especially given Apple&amp;rsquo;s focus on delivering the perfect user experience, is that this is still a problem. I&amp;rsquo;m not familiar with the hardware behind Touch ID but even if there&amp;rsquo;s some sort of warped fingerprint it should be good enough. The fact that there are a few unsuccessful attempts with the wet thumb followed by successful attempt should be enough to develop a profile for the wet version which can be used on future attempts. Modern products succeed by delivering optimized experiences; future products will need to adapt and grow along with us until they become eerily predictive.&lt;/p></description></item><item><title>Automating management</title><link>/2016/09/18/automating-management/</link><pubDate>Sun, 18 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/18/automating-management/</guid><description>&lt;p>One of the biggest lessons I learned when I became an engineering manager was how important the basic operational elements. These are all the things that need to get done outside of code and allow the whole team to be as productive as possible and range from reminding people to do code reviews to creating dashboards to highlight key metrics to enforcing an on-call process. These tasks are important yet repetitive so being a good engineer I’ve spent some time automating them. There’s still a long way to go but strong engineers have a mindset that they want to automate as much repetitive work as possible in order to focus on unique and novel challenges.&lt;/p></description></item><item><title>Supporting Disqus in AMP</title><link>/2016/09/13/supporting-disqus-in-amp/</link><pubDate>Tue, 13 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/13/supporting-disqus-in-amp/</guid><description>&lt;p>After migrating my blog to AMP the last task was getting &lt;a href="https://disqus.com/">Disqus&lt;/a> working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos. As a generic solution, AMP provides the amp-iframe tag which allows you to include a restricted iframe.&lt;/p></description></item><item><title>Comparing my top sites: 2012 vs now</title><link>/2016/09/10/comparing-my-top-sites-2012-vs-now/</link><pubDate>Sat, 10 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/10/comparing-my-top-sites-2012-vs-now/</guid><description>&lt;p>While going through and making sure each of my old posts was AMP compatible I came across a &lt;a href="/2012/06/07/achieving-browser-autocomplete/">post from 2012&lt;/a> where I tried to list the first autocomplete suggestion for each letter. This naturally made me think of what the results would be if I did the same exercise now. Comparing the information 4 years apart is an interesting way to see how my habits have changed but also provide a glimpse into the evolution of companies, products, and technology. The biggest surprise is how much of the list is work related - it’s somewhat expected given how much time we spend working and how many more cloud services there are but it’s still shocking that almost half the list is work related. The other major realization is that much of my consumption has shifted to mobile - many of the sites that are no longer on the list I actively use on my phone; I may actually use Instapaper, Google Maps, and Twitter more frequently now but it’s mostly on mobile via an app. Given how interesting this exercise was I plan on doing this annually and encourage others to do the same - it’s an extremely simple way to see how technology and our relationship to it changes over time.&lt;/p></description></item><item><title>AMP migration scripts</title><link>/2016/09/08/amp-migration-scripts/</link><pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/08/amp-migration-scripts/</guid><description>&lt;p>Over Labor Day weekend I migrated my blog to use &lt;a href="https://www.ampproject.org/">AMP&lt;/a> but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &amp;lt;img&amp;gt; tag with &amp;lt;amp-img&amp;gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &amp;lt;img&amp;gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version. Unfortunately, I ran the script without too much testing and forgot to add closing tags which caused some of the content to go missing.&lt;/p></description></item><item><title>AMPifying my blog</title><link>/2016/09/05/ampifying-my-blog/</link><pubDate>Mon, 05 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/05/ampifying-my-blog/</guid><description>&lt;p>Ever since AMP was announced I’ve been meaning to migrate my blog but hesitated due to the fear that it would take an inordinate amount of time and would be laden with edge cases. But over the Labor Day weekend I decided to give it a shot and see how far i could get. A quick GitHub search showed two promising repos - &lt;a href="https://github.com/juusaw/amp-jekyll">amp-jekyll&lt;/a> and &lt;a href="https://github.com/ageitgey/amplify">amplify&lt;/a> - and I gave them both a shot. They approach AMP integration in two different ways - amp-jekyll creates an AMP version of every post and has it live in a separate folder structure while amplify is a comprehensive theme. This made the amp-jekyll integration much easier since it’s designed to work parallel to the existing blog but I wanted to do a full rewrite.&lt;/p></description></item><item><title>Visualizing fantasy football stats</title><link>/2016/09/05/visualizing-fantasy-football-stats/</link><pubDate>Mon, 05 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/05/visualizing-fantasy-football-stats/</guid><description>&lt;p>In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on &lt;a href="https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R">GitHub&lt;/a> and I’ll keep updating it as I keep adding newer visualizations and analyses.&lt;/p></description></item><item><title>Violating the norms of a social network</title><link>/2016/09/01/violating-the-norms-of-a-social-network/</link><pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate><guid>/2016/09/01/violating-the-norms-of-a-social-network/</guid><description>&lt;p>Social networks carry extreme network effects and have massive winner-take-all dynamics. This makes it impossible for two social networks that have the same pitch to co-exist and leads to pretty strong differentiation. Facebook owns relationships. Twitter owns interests. Instagram owns lifestyle. Snapchat is starting to own experience. This is why I find it fascinating when the content from one social network or medium bleeds into another. Twitter doesn’t allow for tweets longer than 140 characters so people overcome that by sharing screengrabs of long form text. I’ve seen the same on Imgur - it’s primarily used for images but often you’ll see someone posting an image of a long story. We have our own unique relationships across each of these networks so it’s not surprising that we’ll sometimes want to communicate something that’s best expressed with a specific medium yet it’s still fascinating seeing it in action. I get the feeling that they’re publicly exploiting a loophole and adding a tiny bit of chaos to the universe.&lt;/p></description></item><item><title>Food identification with Google's Cloud Vision</title><link>/2016/08/29/food-identification-with-googles-cloud-vision/</link><pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/29/food-identification-with-googles-cloud-vision/</guid><description>&lt;p>Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last. I’m sure refrigerators in 10 years will have this built in but I wanted to see what I could cobble together in an evening.&lt;/p></description></item><item><title>Giving wallabag a shot</title><link>/2016/08/28/giving-wallabag-a-shot/</link><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/28/giving-wallabag-a-shot/</guid><description>&lt;p>I’ve been a happy Instapaper user for years but the news that it was being acquired by Pinterest got me thinking about some alternatives. Not because I have anything against Pinterest; in fact I think this is a great fit and they’ll be able to complement each other but because it’s a reminder that no third party product is guaranteed to last and I wanted to see what open source alternatives are out there.&lt;/p></description></item><item><title>Writing scrapers as APIs</title><link>/2016/08/24/writing-scrapers-as-apis/</link><pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/24/writing-scrapers-as-apis/</guid><description>&lt;p>While building the &lt;a href="http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/">Turo scraper&lt;/a> I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API. In that case I can architect the code so that the retrieval and manipulation of the ride data is completely separate from the scraping code. Then if and when Turo does decide to release an official API all I’d need to do is swap my unofficial implementation out for the official one.&lt;/p></description></item><item><title>Downloading your Turo ride history</title><link>/2016/08/21/downloading-your-turo-ride-history/</link><pubDate>Sun, 21 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/21/downloading-your-turo-ride-history/</guid><description>&lt;p>I&amp;rsquo;ve been using &lt;a href="https://turo.com/">Turo&lt;/a> to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: &lt;a href="https://github.com/dangoldin/turo-automation">scraping&lt;/a>. Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.&lt;/p></description></item><item><title>Integrating poorly documented Open Source libraries</title><link>/2016/08/14/integrating-poorly-documented-open-source-libraries/</link><pubDate>Sun, 14 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/14/integrating-poorly-documented-open-source-libraries/</guid><description>&lt;p>Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected. This is proportional to the obscurity of the library - popular libraries will have most of their kinks worked out but esoteric ones that are likely maintained by one person won’t have the polish.&lt;/p></description></item><item><title>Fantasy football stats: 2016-2017 edition</title><link>/2016/08/13/fantasy-football-stats-2016-2017-edition/</link><pubDate>Sat, 13 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/13/fantasy-football-stats-2016-2017-edition/</guid><description>&lt;p>This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on &lt;a href="https://github.com/dangoldin/yahoo-ffl">GitHub&lt;/a>. If all you care about is the raw data you can just download the &lt;a href="https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2017.csv">CSV&lt;/a>.&lt;/p></description></item><item><title>Consumer tech leads to cyborgs</title><link>/2016/08/08/consumer-tech-leads-to-cyborgs/</link><pubDate>Mon, 08 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/08/consumer-tech-leads-to-cyborgs/</guid><description>&lt;p>The iPhone is the top selling consumer product of all time and a variety of podcasts and articles makes it seem that this is the peak of consumer technology and we’ll never see anything as popular. This is shortsighted. Every new technology achieved wider and wider adoption and eclipsed the previous generation - &lt;a href="http://ben-evans.com/benedictevans/2014/4/25/ipad-growth">laptops eclipsed desktops and smartphones eclipsed laptops&lt;/a>. One thing that’s clear is that each generation of tech gets closer and closer to us. Initially we were exposed to computers when we went into the office. Soon we started buying desktops for our homes. After that we decided we wanted laptops that we could carry around with us. Smartphones gave us the ability to carry computers around in our pockets with a full day’s worth of charge.&lt;/p></description></item><item><title>Engineering management: Measuring the unmeasurable</title><link>/2016/08/07/engineering-management-measuring-the-unmeasurable/</link><pubDate>Sun, 07 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/07/engineering-management-measuring-the-unmeasurable/</guid><description>&lt;p>A key part of management is getting out of the way and building out processes that help your team be as productive as possible. At the same time, you can’t change what you can’t measure. Combining these two makes it clear that to improve, whether people or process, you need to start measuring and tracking the appropriate metrics.&lt;/p>
&lt;p>In software engineering, some things are easy to track: how many bugs there are, how quickly they’re resolved, how much code are written - but rarely tell the whole story and may lead to perverse incentives. The common example is measuring developer productivity through number of lines of code written: a smart developer would purposefully write verbose and long winded code to get their metric up.&lt;/p></description></item><item><title>One of my favorite programs</title><link>/2016/08/06/one-of-my-favorite-programs/</link><pubDate>Sat, 06 Aug 2016 00:00:00 +0000</pubDate><guid>/2016/08/06/one-of-my-favorite-programs/</guid><description>&lt;p>While working on a &lt;a href="/2016/07/17/coding-puzzle-word-transformation-through-valid-words/">small programming puzzle&lt;/a> I remembered Peter Norvig’s &lt;a href="http://norvig.com/spell-correct.html">spell checker&lt;/a> and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.&lt;/p></description></item><item><title>Smarter geographic ad targeting</title><link>/2016/07/26/smarter-geographic-ad-targeting/</link><pubDate>Tue, 26 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/26/smarter-geographic-ad-targeting/</guid><description>&lt;p>Targeting is one of the best ways to improve the return on an advertising campaign. By identifying potential customers you&amp;rsquo;re able to focus your advertising on them rather than someone random. And one of the best simplest ways is to set up your advertising campaigns to focus on a specific geography. Maybe your product is only sold in the United States and advertising it elsewhere is a waste. Or maybe your product is sold everywhere but the messaging and copy needs to vary by region. Or maybe it&amp;rsquo;s sold everywhere with the same exact copy but the price varies by region. Being able to change your campaigns by geography is a simple way to improve the performance of any campaign.&lt;/p></description></item><item><title>Double down on your brand and IP</title><link>/2016/07/22/double-down-on-your-brand-and-ip/</link><pubDate>Fri, 22 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/22/double-down-on-your-brand-and-ip/</guid><description>&lt;p>Pokemon Go is huge right now. People across all age ranges, demographics, and geographies are getting involved and it’s hard to imagine this sort of adoption for any other game. What I find fascinating is that Pokemon Go was based on the same augmented reality mechanics as Ingress, another game developed by Niantic Labs. Ingress has a loyal following but pales in comparison against Pokemon Go when looking at the user numbers, despite Pokemon Go being less than 2 weeks old while Ingress has been around for almost 4 years.&lt;/p></description></item><item><title>Coding puzzle: Word transformation through valid words</title><link>/2016/07/17/coding-puzzle-word-transformation-through-valid-words/</link><pubDate>Sun, 17 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/17/coding-puzzle-word-transformation-through-valid-words/</guid><description>&lt;p>A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.&lt;/p></description></item><item><title>Whatever happened to automatic login?</title><link>/2016/07/16/whatever-happened-to-automatic-login/</link><pubDate>Sat, 16 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/16/whatever-happened-to-automatic-login/</guid><description>&lt;p>When I started building sites one of the accepted principles was to give customers what they want as soon as you can. This manifested itself by taking users to the logged in view whenever they navigated to the site’s homepage. This makes sense - if you know a user’s logged in why waste their time by showing them a homepage that’s designed to sell the product?&lt;/p>
&lt;p>Yet recently I encountered two sites, &lt;a href="https://www.greenhouse.io/">Greenhouse&lt;/a> and &lt;a href="https://tallie.com/">Tallie&lt;/a>, that will default to the homepage and only load the logged in view when I click the sign in link. One argument is that they both have separate domains for the logged in experience - app.greenhouse.io rather than &lt;a href="https://www.greenhouse.io">www.greenhouse.io&lt;/a> and usetallie.com rather than tallie.com - but there’s nothing stopping them from redirecting to those as soon as they recognize that a user is logged in.&lt;/p></description></item><item><title>A Twitter Moments fail</title><link>/2016/07/10/a-twitter-moments-fail/</link><pubDate>Sun, 10 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/10/a-twitter-moments-fail/</guid><description>&lt;div class="right10">
 &lt;img src="/image/twitter-moments-fail.png" alt="Content not available in your country" data-width="750" data-height="1334" data-layout="responsive" />
&lt;/div>
&lt;p>I’m a huge Twitter fan so it’s especially frustrating when I encounter issues. The latest one was discovering a &amp;ldquo;This content is not available in your country&amp;rdquo; message when trying to catch up on some Euro Cup highlights in a moment. I understand that in today’s digital rights world there’s always a chance for some content to be unavailable but there’s no reason it should have been included in Twitter’s flagship product that’s supposed to attract and engage new users. The fact that it’s manually curated makes it even worse - how could this have slipped through? One explanation is that the curator was not based in the US and had access to the video. The other is that the video was available initially but was pulled later on. In both cases Twitter should have had the appropriate safeguards to identify this was happening and amend the moment. An even better approach would have been to have different versions of the moment depending on the user&amp;rsquo;s location. The current implementation just feels sloppy and I can’t stand to see it in a product I love using.&lt;/p></description></item><item><title>Consumer protection for UX</title><link>/2016/07/09/consumer-protection-for-ux/</link><pubDate>Sat, 09 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/09/consumer-protection-for-ux/</guid><description>&lt;p>I haven’t seen much written about how consumer protection relates to a product’s user experience but it’s a topic that’s worth exploring. I was reminded of this when my mortgage loan was sold to a new servicer. I came home to find a letter in the mail notifying me that my loan has been sold and that going forward I’d have to use a different payment portal and system. It was simple enough to register but the payment process became less efficient and there was no support for a Mint integration.&lt;/p></description></item><item><title>Maximize the potential energy of your code</title><link>/2016/07/04/maximize-the-potential-energy-of-your-code/</link><pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/04/maximize-the-potential-energy-of-your-code/</guid><description>&lt;blockquote>
&lt;p>
 &lt;strong>Potential energy&lt;/strong>: the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system.&lt;br/>
 &lt;a href="http://www.dictionary.com/browse/potential-energy">Dictionary.com&lt;/a>
&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>
 &lt;strong>Kinetic energy&lt;/strong>: the energy of a body or a system with respect to the motion of the body or of the particles in the system.&lt;br/>
 &lt;a href="http://www.dictionary.com/browse/kinetic-energy">Dictionary.com&lt;/a>
&lt;/p></description></item><item><title>Snapchat's massive potential</title><link>/2016/07/03/snapchats-massive-potential/</link><pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate><guid>/2016/07/03/snapchats-massive-potential/</guid><description>&lt;p>The more I use Snapchat the more obvious the potential. The way the product has evolved reminds me of Facebook’s history. Facebook started simply as a profile page for Ivy League college students but due to strong execution and brilliant product decisions has grown into the current behemoth. Snapchat is on a similar path - the initial version was a simple ephemeral photo sharing app but the recent updates seem frequent and massively impactful.&lt;/p></description></item><item><title>Low cost at small scale</title><link>/2016/06/26/low-cost-at-small-scale/</link><pubDate>Sun, 26 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/26/low-cost-at-small-scale/</guid><description>&lt;p>The Wall Street Journal had a &lt;a href="http://www.wsj.com/amp/articles/why-fruits-and-veggies-are-so-crazy-cheap-in-chinatown-1466762400">great piece&lt;/a> on why produce is so cheap in Chinatown. The conclusion:&lt;/p>
&lt;blockquote>
 &lt;p>Her discovery: Chinatown’s 80-plus produce markets are cheap because they are connected to a web of small farms and wholesalers that operate independently of the network supplying most mainstream supermarkets.&lt;/p>
 &lt;p>Most of the city’s fruits and vegetables come from wholesalers at the Hunts Point Produce Market, the South Bronx distribution hub boasting all the color and accessibility of La Guardia Airport. Chinatown’s green grocers, in contrast, buy their stock from a handful of small wholesalers operating from tiny warehouses right in the neighborhood.&lt;/p></description></item><item><title>The Brave browser</title><link>/2016/06/23/the-brave-browser/</link><pubDate>Thu, 23 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/23/the-brave-browser/</guid><description>&lt;p>Trying to launch a new browser seems like a fool’s errand and yet if there’s anyone that can do it it’s &lt;a href="https://en.wikipedia.org/wiki/Brendan_Eich">Brendan Eich&lt;/a>, who in addition to creating JavaScript also ran Mozilla. Given his pedigree I decided to give his new browser, &lt;a href="https://brave.com/">Brave&lt;/a>, a shot. It’s definitely a bit on the rough side compared to the mainstream browsers but it’s surprisingly fast. The speed improvement comes from a built in adblocker rather than having it implemented via slower browser extensions. At the same time Brave wants to pay publishers for their content by partnering with higher quality advertisers in order to serve benevolent ads that should also be priced at a premium.&lt;/p></description></item><item><title>Messaging app fragmentation</title><link>/2016/06/22/messaging-app-fragmentation/</link><pubDate>Wed, 22 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/22/messaging-app-fragmentation/</guid><description>&lt;p>The messaging space is fascinating. There are probably hundreds of apps available with pretty massive fragmentation. Onavo collected the following data to indicate the reach of the various messaging apps by country and while WhatsApp (owned by Facebook) is clearly dominant there are some countries that WhatsApp is a fringe player, especially among Asian countries.&lt;/p>
&lt;img src="/image/onavo-insights-global-messaging-reach.png" alt="Onavo Insights global messaging app reach" data-width="593" data-height="863" data-layout="responsive" />
&lt;p class="caption">&lt;a href="https://techcrunch.com/2013/06/13/messaging-apps/">via TechCrunch&lt;/a>&lt;/p></description></item><item><title>Blogging from my phone</title><link>/2016/06/19/blogging-from-my-phone/</link><pubDate>Sun, 19 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/19/blogging-from-my-phone/</guid><description>&lt;p>A few weeks ago I had to run some errands at the mall and ended up having some free time. I was also a few blog posts behind so decided to see how much I could actually do via phone. Surprisingly, I got a fair amount done. The posts still required a fair amount of editing when I was back on my computer but for getting the bulk of the content and structure down on my phone was nearly as good as via a real keyboard. What it lacked in speed it made up for by not having real multitasking which made it more difficult to get distracted. It wouldn’t work for posts that require search or significant research but for quick blurbs or jotting down thoughts it works remarkably well and I suspect it will only improve with time. Years ago I viewed phones and tablets as being purely designed for consumption rather than creation so this has been a pleasant surprise and I’m coming around to the idea that one can be productive without an actual computer. Next is to try attempting to write a blog post via voice dictation.&lt;/p></description></item><item><title>AWS, Stripe, and WeWork</title><link>/2016/06/18/aws-stripe-and-wework/</link><pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/18/aws-stripe-and-wework/</guid><description>&lt;p>I have been subscribed to &lt;a href="https://stratechery.com/">Stratechery&lt;/a> for almost a year now but have recently started listening to the &lt;a href="http://exponent.fm/">Exponent podcasts&lt;/a>. One of them, titled &lt;a href="http://exponent.fm/episode-071-pickaxe-retailers-2/">Pickaxe Retailers&lt;/a>, makes the case that WeWork has an appropriate valuation due to their ability to leverage their strong brand and become the utility layer for real estate as well as provide a slew of products to their tenants. Similar to the way AWS has eliminated the need to run your own data center and Stripe has eliminated the need to acquire merchant accounts and negotiate with vendors, WeWork may do the same for physical space - both commercial and &lt;a href="http://www.fastcompany.com/3055325/from-wework-to-welive-company-moves-members-into-its-first-residential-building">residential&lt;/a>.&lt;/p></description></item><item><title>Analyzing my blog</title><link>/2016/06/12/analyzing-my-blog/</link><pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/12/analyzing-my-blog/</guid><description>&lt;p>I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple &lt;a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.py">Python script&lt;/a> that went through each post and dumped it into a CSV file with a series of columns that would be easy to &lt;a href="https://github.com/dangoldin/blog-analytics/blob/master/analyze.R">analyze via R&lt;/a>. The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.&lt;/p></description></item><item><title>Following up on a website optimization offer</title><link>/2016/06/11/following-up-on-a-website-optimization-offer/</link><pubDate>Sat, 11 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/11/following-up-on-a-website-optimization-offer/</guid><description>&lt;p>I’ve been getting a stream of offers to help “optimize” my site and decided to follow through with one and see where it went. The general pitch is to call out existing errors and problems and offer a service to help fix the variety of errors and improve my search ranking. Here’s the text of the most recent email:&lt;/p>
&lt;blockquote>
&lt;p>Dear business owner of dangoldin.com,&lt;/p>
&lt;p>How is it possible that your website is having so many errors? Yes, most of the people share their anger and frustration once they get my email.&lt;/p></description></item><item><title>Word clouds in R</title><link>/2016/06/06/word-clouds-in-r/</link><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/06/word-clouds-in-r/</guid><description>&lt;p>Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a &lt;a href="http://www.r-bloggers.com/building-wordclouds-in-r/">quick script&lt;/a> I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.&lt;/p></description></item><item><title>Blogging: The small wins</title><link>/2016/06/05/blogging-the-small-wins/</link><pubDate>Sun, 05 Jun 2016 00:00:00 +0000</pubDate><guid>/2016/06/05/blogging-the-small-wins/</guid><description>&lt;p>I started working on a project to investigate my blog posts and see how my writing has evolved over time. I’m still working on it and will definitely write up the results but the entire process got me thinking about my blog and some of the highlights. I started blogging to improve my writing, improve my thinking, and grow my personal brand. Despite being a large time commitment I enjoy doing it and there have been a variety of small episodes that have made it even better:&lt;/p></description></item><item><title>Uber and self driving cars</title><link>/2016/05/28/uber-and-self-driving-cars/</link><pubDate>Sat, 28 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/28/uber-and-self-driving-cars/</guid><description>&lt;p>Self driving cars are inevitable and yet I’m surprised by how aggressive Uber is in contributing to the space. Uber is winning right now due to massive network effects. For most drivers and passengers Uber is the primary option and they only switch when Uber is either in surge if you’re a passenger or if you’re a driver when no passengers are available. Self driving cars eliminate half of the market. They won’t need to balance multiple apps on their phones and won’t need to go back and forth trying to find a passenger. It will all happen behind the scenes and do a much better job than any human would. They’d be as likely to work with Uber as any of their competitors. In fact, the entire protocol may evolve to be open with owners setting up their cars to start picking up and dropping off passengers when they’re not in use. The equivalent of how you can sell electricity back into the grid without having to do a ton of extra work. Imagine being able to own a car and just let it roam so it starts earning.&lt;/p></description></item><item><title>Analyzing IMDB data: Actors vs actresses</title><link>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</link><pubDate>Sun, 22 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/22/analyzing-imdb-data-actors-vs-actresses/</guid><description>&lt;p>After getting the &lt;a href="http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/">IMDB data loaded&lt;/a> it was time to dive in and start looking at the data. In 2012, I did an &lt;a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">analysis&lt;/a> to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the &lt;a href="https://www.curiousgnu.com/imdb-age-distribution">help of CuriousGnu&lt;/a>, I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on &lt;a href="https://github.com/dangoldin/imdb">GitHub&lt;/a> and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.&lt;/p></description></item><item><title>Analyzing IMDB data: Step 1 - Cleaning and QA</title><link>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/</guid><description>&lt;p>In 2012 I did a &lt;a href="http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/">simple analysis of IMDB&lt;/a> to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great &lt;a href="https://www.curiousgnu.com/imdb-age-distribution">post by CuriousGnu&lt;/a> that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The &lt;a href="http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0">NY Times interviewed&lt;/a> a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.&lt;/p></description></item><item><title>Experimenting with Medium</title><link>/2016/05/15/experimenting-with-medium/</link><pubDate>Sun, 15 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/15/experimenting-with-medium/</guid><description>&lt;p>Despite my &lt;a href="http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/">aversion&lt;/a> to walled gardens and platforms I’ve seen a ton of people make the switch to &lt;a href="https://medium.com/">Medium&lt;/a>. Within the past month I’ve seen a variety of bloggers move over to Medium, both big and small: &lt;a href="https://bothsidesofthetable.com/finding-a-new-medium-aa0f882815d#.s4y1c45ky">Mark Suster&lt;/a>, &lt;a href="http://blog.semilshah.com/2016/04/30/medium-rare/">Semil Shah&lt;/a>, &lt;a href="http://thegongshow.tumblr.com/post/143602596745/corporate-governance-dictatorships-vs-democracy">Andrew Parker&lt;/a>, and a former coworker, &lt;a href="https://medium.com/@dillonforrest">Dillon Forrest&lt;/a>. I’m still not convinced that Medium is for me but it definitely feels as if it’s at that inflection point with more and more people moving to Medium. And from what I’ve heard it does wonders for reach and promotion - something that I’ve been relying on Google search and Twitter for.&lt;/p></description></item><item><title>Expiration date selection design anti pattern</title><link>/2016/05/14/expiration-date-selection-design-anti-pattern/</link><pubDate>Sat, 14 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/14/expiration-date-selection-design-anti-pattern/</guid><description>&lt;div class="right10">
&lt;img src="/image/chipotle-app-cc-expiration.png" alt="Chipotle app credit card expiration" data-width="750" data-height="1334" data-layout="responsive" />
&lt;/div>
&lt;p>Wanting to avoid a busy lunch rush but hankering for Chipotle I decided to download their app to order ahead. It’s a straightforward app and everything went as expected until I had to enter the expiration date for my credit card. The way the app is set up is that you’re expected to choose the month first followed by the year. Unfortunately it prevents you from picking a month in the past. One can probably guess what problem this leads to: if the expiration date is in the future but the expiration month is before today’s month the app rejects the month change until you change the year. The screenshot illustrates the design.&lt;/p></description></item><item><title>Identifying unused database tables</title><link>/2016/05/11/identifying-unused-database-tables/</link><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/11/identifying-unused-database-tables/</guid><description>&lt;p>When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.&lt;/p>
&lt;p>Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them. But there are a simple ways to identify tables that are no longer updated.&lt;/p></description></item><item><title>Google's photo search is eerily incredible</title><link>/2016/05/08/googles-photo-search-is-eerily-incredible/</link><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid>/2016/05/08/googles-photo-search-is-eerily-incredible/</guid><description>&lt;p>Last Friday, Fred Wilson &lt;a href="http://avc.com/2016/05/feature-friday-photo-search/">wrote a post&lt;/a> lauding Google’s photo search. I’ve had the same experiences. In the past couple of months I’ve made numerous searches without expecting a useful result but in nearly every case I was pleasantly surprised. Just in the past week I wanted to search for a short story I wrote while in middle school that I digitized at some point over the past few years. My first attempt was to search for “paper” which got me too many results to parse through. But for my second attempt I tried “essay” and was able to find a photo of one of the hand-written pages. It was simple to look at the date I uploaded that one page to find the others. A couple of days ago I was out of town but needed my passport information to fill out an online government form. Turns out that I have a photo of my passport on my Google account - I backed it up years ago as I was traveling so I had proof in case anything happened to it.&lt;/p></description></item><item><title>Text is king</title><link>/2016/04/30/text-is-king/</link><pubDate>Sat, 30 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/30/text-is-king/</guid><description>&lt;p>As many people know despite being bullish on tech I’m spartan and utilitarian with my technology usage. This expresses itself as a strong bias for text above nearly another format. There are tons of apps that try to help me organize my tasks and todos but I prefer simple text files and an intelligent folder structure. This is true when it comes to blogging as well - rather than using a fancy CMS or hosted application I rely on Jekyll which exposes my content in Markdown.&lt;/p></description></item><item><title>Avoid full body code transplants</title><link>/2016/04/27/avoid-full-body-code-transplants/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/27/avoid-full-body-code-transplants/</guid><description>&lt;p>When working on new features it’s easy to keep increasing scope until you end up doing a full rewrite of your code. Don’t. It’s healthy to refactor code as you go but you need to be wary of how many things you’re changing and the risks those changes carry. Code will get stale unless it’s constantly maintained and updated as the rest of the product evolves but trying to change too much at once will make it difficult to diagnose issues and increase the odds of bugs in production.&lt;/p></description></item><item><title>A Telegram blog bot</title><link>/2016/04/23/a-telegram-blog-bot/</link><pubDate>Sat, 23 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/23/a-telegram-blog-bot/</guid><description>&lt;p>A combination of bots being in vogue and Telegram offering $1M in &lt;a href="https://telegram.org/blog/botprize">bot prizes&lt;/a> got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup. A nice feature that Telegram has is the ability to write a bot that can respond to both polling and webhooks. The polling approach is a much trivial to get started with since you don’t need to worry about any of the devops work and can work on the core interaction. The cons are that it won’t respond immediately and you need a way to track messages your bot has already replied to. Changing it to a webhook provided real time responses but made it a bit more difficult to test and wrapping everything inside a minimal web framework. The biggest hiccup was the requirement of HTTPS for a webhook integration but &lt;a href="https://letsencrypt.org/">Let’s Encrypt&lt;/a> made it simple to get up and running. A year ago I wouldn’t have bothered prototyping anything that required HTTPS but these days it’s incredibly easy to set up. The &lt;a href="https://github.com/dangoldin/bots">code is up&lt;/a> on GitHub and if you’re interested in bots definitely take a look. And if you have Telegram installed try messaging “danblog” with /blogme to get a random blog post.&lt;/p></description></item><item><title>Irrationality of the markets: Amazon up, Netflix down</title><link>/2016/04/18/irrationality-of-the-markets-amazon-up-netflix-down/</link><pubDate>Mon, 18 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/18/irrationality-of-the-markets-amazon-up-netflix-down/</guid><description>&lt;img src="/image/amazon-prime-monthly.png" alt="Amazon monthly pricing" data-width="938" data-height="481" data-layout="responsive" />
&lt;p>Last night, Amazon &lt;a href="http://www.nytimes.com/2016/04/18/business/amazon-challenges-netflix-by-opening-prime-to-monthly-subscribers.html">announced&lt;/a> that in addition to the annual plan they’re going to start offering Prime as a monthly service. Sure enough, investors interpreted this as good move by Amazon (up 1.51% at end of day) while hurting Netflix (down 2.79% at end of day and even more post earnings). These percentages translate into a $1.34B decrease to the Netflix valuation and a $4.49B increase in valuation for Amazon. As a shareholder of both I find this behavior interesting for its irrationality.&lt;/p></description></item><item><title>The corporate email signup design pattern</title><link>/2016/04/13/the-corporate-email-signup-design-pattern/</link><pubDate>Wed, 13 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/13/the-corporate-email-signup-design-pattern/</guid><description>&lt;p>One of the latest trends I’ve noticed is B2B companies is allowing you to sign up with a company email address and automatically linking you with the rest of the organization. This is a definite no-brainer and a really simple way of getting new users setup without having to be bottlenecked by a burdensome administrative process. No one on the HR team has to enter employees into the system nor send anyone their username or account info. Instead they just provide a link to the service and have people sign up with their company email address. Once this is done they immediately have access to whatever the base employee account should have. Only later one does an admin need to grant additional permissions and privileges.&lt;/p></description></item><item><title>Unintended consequences</title><link>/2016/04/10/unintended-consequences/</link><pubDate>Sun, 10 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/10/unintended-consequences/</guid><description>&lt;p>Earlier today I read an &lt;a href="http://fusion.net/story/287592/internet-mapping-glitch-kansas-farm/">article about MaxMind&lt;/a>, a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations. Lo and behold these locations are now seeing tons of harassment from law enforcement and various strangers online.&lt;/p></description></item><item><title>Generalize at n=3</title><link>/2016/04/07/generalize-at-n3/</link><pubDate>Thu, 07 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/07/generalize-at-n3/</guid><description>&lt;p>Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.&lt;/p>
&lt;p>If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios. In this case writing minimal code would have served you better since it would have been much easier to adapt or throw away to support the new case.&lt;/p></description></item><item><title>The best code is no code</title><link>/2016/04/05/the-best-code-is-no-code/</link><pubDate>Tue, 05 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/05/the-best-code-is-no-code/</guid><description>&lt;p>The best code isn’t code that’s elegant or code that’s brilliant it’s code that doesn’t need to be written. One of the best feelings is when you can take a new problem and turn it into an existing problem that already has a solution. Sometimes that requires making a few tweaks and compromises to the problem or the code but the time and effort saved can be massive. This requires a deep understanding of the problem being solved as well as the existing code. Someone knowing the code but not the problem won’t be able to transform the problem into something applicable. And someone having a deep knowledge of the problem but not the code won’t be able to see how the code can be adapted to solve this scenario. The optimal result comes from someone who can strip away the cruft from both of them while still maintaining the spirit of both in order to combine them.&lt;/p></description></item><item><title>Ben Thompson's “laddering up” and building bigger moats</title><link>/2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats/</link><pubDate>Sun, 03 Apr 2016 00:00:00 +0000</pubDate><guid>/2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats/</guid><description>&lt;p>Last weekend I finished the &lt;a href="http://www.amazon.com/The-Everything-Store-Bezos-Amazon-ebook/dp/B00BWQW73E">The Everything Store&lt;/a>, which details the rise of Amazon from a fledgling online book retailer to its current form. One pattern that stood out for me was how Amazon was able to continuously push into new business areas due to the infrastructure that they had in place based on previous decisions and commitments.&lt;/p>
&lt;p>They started with books but were able to grow into other smaller products once they figured out the logistics behind shipping smaller items. Once Amazon had that in place they kept tweaking their distribution system to expand the variety of products offered while improving the speed of delivery. This allowed them to keep amassing a list of products which they used to open up their platform to third party sellers. And as Amazon improved their infrastructure they were able to open that up to these third party sellers as well. In parallel, they built AWS to provide computer services to internal Amazon teams but were able to turn it into a brand new line of business that powers the majority of new startups. And now their are rumors of Amazon building out a shipping service to bypass FedEx and UPS.&lt;/p></description></item><item><title>Unbundling politics</title><link>/2016/03/27/unbundling-politics/</link><pubDate>Sun, 27 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/27/unbundling-politics/</guid><description>&lt;p>Yesterday I made the case that the current political system consists of a &lt;a href="/2016/03/26/political-parties-are-product-bundles/">series of product bundles&lt;/a> and I’ve been thinking of ways unbundling would work. And what better way than to look at existing products and industries that have been unbundled.&lt;/p>
&lt;p>As numerous people have pointed out, the music industry is a clear example. Initially music was sold on CDs and there was no idea of buying solo songs. But with the launch of the iPod, iTunes, and internet proliferation it became possible to buy individual songs. Lately we’ve been back in the bundling phase with the various monthly music subscription services, such as Spotify and Apple Music.&lt;/p></description></item><item><title>Political parties are product bundles</title><link>/2016/03/26/political-parties-are-product-bundles/</link><pubDate>Sat, 26 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/26/political-parties-are-product-bundles/</guid><description>&lt;p>I rarely write about politics but it’s an election year and I had an interesting realization. Political parties are just like product bundles. We each have our own issues and policies we’re passionate about but it’s impossible to find a politician, less a party, that has the same views we do. Instead we have political parties that take a few issues and policies and try to wrap them up in a bundle hoping to appeal to enough people to win an election.&lt;/p></description></item><item><title>Data analysis needs to be fun</title><link>/2016/03/21/data-analysis-needs-to-be-fun/</link><pubDate>Mon, 21 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/21/data-analysis-needs-to-be-fun/</guid><description>&lt;p>In order to do any meaningful data analysis you need to have fun doing it. Otherwise it becomes a chore that’s extended by each additional analysis you run and each additional failed attempt at an insight. This requires a positive attitude and enjoying the slow, methodical process of discovery and appreciating each iteration while getting closer to the end goal. The vast majority of analyses lead to no new insight, especially when all the easy stuff has already been figured out, and it’s critical to remain the optimist while appreciating the present.&lt;/p></description></item><item><title>Passing the torch: IBM to Google</title><link>/2016/03/20/passing-the-torch-ibm-to-google/</link><pubDate>Sun, 20 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/20/passing-the-torch-ibm-to-google/</guid><description>&lt;p>Last week the big news was that Google’s AlphaGo was able to win 4 of the 5 games against Lee Sedol in Go. As we’ve gotten better and better hardware it’s not surprising that an AI was finally able to win in a well defined environment. AIs will continue to improve and we’ll start seeing more and more of this behavior across a wide range of problems and not just games. The most significant part for me was that this was achieved by Google and not by IBM. IBM had two recent notable achievements in AI - one was building Deep Blue in 1997 which beat Gary Kasparov in chess and the other was building Watson in 2001 which dominated at Jeopardy. Yet just five years later Google has claimed the AI victory with AlphaGo.&lt;/p></description></item><item><title>Approach work like the gym</title><link>/2016/03/13/approach-work-like-the-gym/</link><pubDate>Sun, 13 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/13/approach-work-like-the-gym/</guid><description>&lt;p>To get the most benefit from working out it’s important to have a plan and consistently measure yourself and keep pushing your goals and yet it’s surprising how rarely that’s done in a professional setting. We spend over 40 hours a week working but the majority of us view it as a chore and something that we just have to do. Imagine if everyone approached work the same way they approach the gym. People would have much clearer ideas of what they want to do and what challenges they face. They would be able to measure how well they’re doing and understanding what they need to start doing to get to the next level. Instead most approach it as something that they need to do rather than something they want to do.&lt;/p></description></item><item><title>The MySQL enum type</title><link>/2016/03/10/the-mysql-enum-type/</link><pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/10/the-mysql-enum-type/</guid><description>&lt;p>The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected. Doing a bit of research I discovered how MySQL represents the enum type. Rather than storing the values in a specific order MySQL supposedly creates a map-like structure to relate the integer values with their enum counterparts. This allows you to change the order of the enum definition without changing the underlying map or any of the stored values. I still wouldn’t use enums for anything that would require a join but for storing small and simple sets of data it works great.&lt;/p></description></item><item><title>Airbnb, Uber, and local laws</title><link>/2016/03/06/airbnb-uber-and-local-laws/</link><pubDate>Sun, 06 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/06/airbnb-uber-and-local-laws/</guid><description>&lt;p>Yesterday I read an &lt;a href="http://www.nytimes.com/2016/03/06/business/airbnb-pits-neighbor-against-neighbor-in-tourist-friendly-new-orleans.html">interesting piece&lt;/a> on Airbnb in New Orleans. The content itself isn&amp;rsquo;t new - it highlights the typical conflict between those that view Airbnb as violating local ordinances and ruining the city and others who believe that Airbnb brings value and is helping New Orleans rebuild after Katrina.&lt;/p>
&lt;p>But what was interesting was the repeated claims of Airbnb and the other rental marketplaces that it’s just not scalable to follow local policies for every city and it’s up to the users to know their local regulations and follow them. I understand it’s difficult to localize complex products but these responses just feel like an excuse.&lt;/p></description></item><item><title>AWS EC2 instance arbitrage</title><link>/2016/03/05/aws-ec2-instance-arbitrage/</link><pubDate>Sat, 05 Mar 2016 00:00:00 +0000</pubDate><guid>/2016/03/05/aws-ec2-instance-arbitrage/</guid><description>&lt;p>While reserving some EC2 instances earlier this week I discovered that Amazon allows you to &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-buying-guide.html">sell&lt;/a> reserved instances you’re no longer using. Usually the prices the third parties are offering are very close to the fair market value but I wondered if there was an arbitrage opportunity by reserving a longer term instance and selling it for a series of shorter term leases. The &lt;a href="https://aws.amazon.com/ec2/pricing/">typical discount&lt;/a> for buying a 1 year reserved instance is 30% while buying one for 3 years can get over 60%. The idea being that if you can get an instance for a 60% discount over 3 years and then sell it for 3 one year terms at a 25% discount you end up coming out ahead. Of course the challenge is that Amazon constantly drops prices so a 60% discount now may be equivalent to something much smaller three years later. There’s also the risk of no one purchasing your instances but that seems unlikely since you can always undercut Amazon’s official price. The other factor is the discount rate since you’re paying up front for 3 years worth of an instance. During that time you could have taken that money and invested it elsewhere which could have led to a better return but which would have been unlikely when you’re getting a 30% discount over the course of a year.&lt;/p></description></item><item><title>Tool specialization and growing companies</title><link>/2016/02/28/tool-specialization-and-growing-companies/</link><pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/28/tool-specialization-and-growing-companies/</guid><description>&lt;p>It’s obvious in hindsight but incredible when you experience it but every successful company has to iterate through a variety of tools as it, and its problems, grow. A typical modern tech startup starts by identifying a problem and using a common web framework to quickly come up with the first pass. But as this company grows new problems and situations arise that the initial solution no longer supports. They may end up having a series of asynchronous tasks and need to start using RabbitMQ with that use case. MySQL may no longer be enough and they start offloading their data to Redshift. That off the shelf web framework is no longer performant enough so they have to split it into multiple components and start embracing strong, statically typed languages.&lt;/p></description></item><item><title>Aural information density</title><link>/2016/02/27/aural-information-density/</link><pubDate>Sat, 27 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/27/aural-information-density/</guid><description>&lt;p>Whenever I watch some online lectures or listen to a podcast one of the first things I do is change the speed to either 1.5x or 2x the original. Sometimes I’ll have to skip back or reduce it back to the normal speed but for the most part this approach saves me tons of time and I like to think that I absorb the same amount of information. But the fact that I can absorb and process information at twice the speed makes me wonder how much more productive I’d be if every conversation I had occured at twice the speed. Is there some physiological reason we don’t speak at twice the speed? Is there a cultural factor? Does this information density vary based on language?&lt;/p></description></item><item><title>Platform partnerships</title><link>/2016/02/22/platform-partnerships/</link><pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/22/platform-partnerships/</guid><description>&lt;p>I &lt;a href="http://dangoldin.com/2016/02/21/amazon-echo">set up&lt;/a> the Amazon Echo over the weekend and have been an active user of my wife’s Spotify account which comes integrated with the Echo. I would have preferred to use my Apple Music account but the Echo currently only supports Spotify. I suspect the biggest reasons are competitive - Amazon and Apple are competing for the home and it’s likely that either Amazon doesn’t want to integrate Apple or Apple is preventing Amazon from getting the integration done. At the same time Amazon has a music offering yet they specifically call out the Spotify integration. Is this because Spotify is only a competitor for music and the value of an Echo trumps this? Is it because Spotify has more reach and this is a necessary integration? I’m sure the answer is a bit of both but it’s fascinating to see how these partnerships develop.&lt;/p></description></item><item><title>Amazon Echo</title><link>/2016/02/21/amazon-echo/</link><pubDate>Sun, 21 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/21/amazon-echo/</guid><description>&lt;p>After reading the positive reviews I got past the gimmick factor and jumped aboard the Amazon Echo train and got it set up yesterday. After going through the obvious examples (what’s the weather, tell me a joke, add x to my shopping list, play song y) and playing around with it I’m past the gimmick stage. The always on listening is really a different way to interact with our devices. Conceptually it’s no different than using Siri or Google Now but in practice it’s a world of difference. I don’t always have my phone with me and for some things it just feels more natural to start speaking and see an immediate effect. Whether that’s playing some specific songs or playlists, changing the volume, or adding items to a shopping list it feels more natural than having to go through a phone. One of my favorite use cases so far has been using the Echo to keep track of my shopping list. In the past I’d be in the kitchen and realize we needed something and would forget as soon as I switch tasks. With the Echo I can immediately call out what to add and have the list readily available next time I go to buy something.&lt;/p></description></item><item><title>Let's Encrypt</title><link>/2016/02/20/lets-encrypt/</link><pubDate>Sat, 20 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/20/lets-encrypt/</guid><description>&lt;p>I’ve been meaning to mess around with &lt;a href="https://letsencrypt.org/">Let’s Encrypt&lt;/a> since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.&lt;/p>
&lt;p>Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy. It was smart enough to go through each of my Apache configuration files and prompted me to see which domains I wanted to switch over to HTTPS. After selecting a few and continuing to the next step it generated new configuration files with the appropriate setting to enable SSL support.&lt;/p></description></item><item><title>The impact of self driving cars</title><link>/2016/02/17/the-impact-of-self-driving-cars/</link><pubDate>Wed, 17 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/17/the-impact-of-self-driving-cars/</guid><description>&lt;p>Nearly every major tech company is pursuing a self driving car future and it’s inevitable that at some point most cars on the road will be completely autonomous. Cheap and easy transportation is the immediate change but there will be massive secondary effects to the shapes of cities and society.&lt;/p>
&lt;p>A &lt;a href="https://www.johnson.cornell.edu/Faculty-And-Research/Profile?id=lvo2">college professor&lt;/a> used the example of the invention of the car to highlight these sort of effects - if told that cars would be successful most people could have guessed that they’d replace horses and clean up cities. But very few would have been able to predict the rise of highways which led to the development of suburbs and the current structure of the United States.&lt;/p></description></item><item><title>Design your database for flexibility</title><link>/2016/02/15/design-your-database-for-flexibility/</link><pubDate>Mon, 15 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/15/design-your-database-for-flexibility/</guid><description>&lt;p>One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.&lt;/p>
&lt;p>With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have. If this is a large table you have to figure out how to migrate the data to a new schema. The simple way is to deal with the downtime and hope the migration works. The more complex way is to support two database schemas at once with your code while the migration occurs. Neither of these would be necessary if you think through the database design choices you’re making. It’s going to be impossible to address every future need but there’s incredible value in at least thinking through potential changes and how they’d be supported.&lt;/p></description></item><item><title>Longer terms in government?</title><link>/2016/02/13/longer-terms-in-government/</link><pubDate>Sat, 13 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/13/longer-terms-in-government/</guid><description>&lt;p>Many have written about society’s inability to enact laws quickly enough to deal with the current pace of technological innovation. Governments are still trying to figure out how to regulate the sharing economy with both AirBnB and Uber being reacted to rather than being effectively regulated. This leads to different treatment in different locations and causes confusion for consumers, the businesses, and regulators.&lt;/p>
&lt;p>A potential way to rectify this is to actually increase term limits for people in government. With politicians focusing on reelection every few years and constantly moving in and out of office it’s tough to develop a consistent regulatory approach. This worked well a hundred years ago when new industries would take a decade to develop and you could regulate them as they grew. Now it can take a year for entirely new businesses to be created before governments can react to what’s happening. By then the new consumer behavior has become entrenched and becomes difficult to change. Rather than worrying about reelection and undoing prior policies politicians should be focused on the future and how to adapt government for an increasingly changing society.&lt;/p></description></item><item><title>Retargeting gone wrong</title><link>/2016/02/10/retargeting-gone-wrong/</link><pubDate>Wed, 10 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/10/retargeting-gone-wrong/</guid><description>&lt;p>Retargeting ads work by checking to see a product you’ve looked at and then showing you that product over and over again with the hope that at some point you buy it. There are entire companies dedicated to this with extremely sophisticated algorithms so I’m surprised when I see inefficient behavior. In my case it was an Amazon ad that kept following me around even after I already purchased the product, a precision cooker. Given that Amazon knows my purchase history and sees that I’ve already bought the cooker it makes no sense to keep showing it to me. It seems that their algorithm figured this out as well and started showing me the same product in different packages and at different price points. The fact that they have logic that’s smart enough to show me different variations of the same product but not take into account my purchase history shocks me. What makes this even worse is that I own some Amazon stock and realize that this inefficiency has an impact, albeit a tiny one, on my shares.&lt;/p></description></item><item><title>Don't build a Homer</title><link>/2016/02/07/dont-build-a-homer/</link><pubDate>Sun, 07 Feb 2016 00:00:00 +0000</pubDate><guid>/2016/02/07/dont-build-a-homer/</guid><description>&lt;p>Years ago, one of my projects at Yodle involved building out an automated reporting system that would consolidate all the existing reports being run via SQL queries and consolidate them into a unified application that would take care of the execution and the delivery. During the design process I spoke with existing users to see what else they’d like and it quickly morphed from a cron-job like application that just emailed CSV files based on SQL queries into a full fledged business intelligence tool that users could use to pull arbitrary data formatted in a multitude of ways. While thinking through the design of this application I spoke with the CTO and he gave me a phrase I keep going back to: “To get the expressiveness of SQL you have to write SQL.”&lt;/p></description></item><item><title>Describe what your code won't do</title><link>/2016/01/31/describe-what-your-code-wont-do/</link><pubDate>Sun, 31 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/31/describe-what-your-code-wont-do/</guid><description>&lt;p>When starting to spec out a new feature a good habit is to think about what it won’t do. This forces you to focus on the problems that aren’t being solved and makes you aware of the tradeoffs you’re making. Rather than focusing on the problems being solved it’s equally important to know what you’re not doing as well as what your implementation will preclude you from doing in the future. To be effective we need to make tradeoffs or we’d never be able to launch anything but we shouldn’t make them blindly. We need to be aware of the tradeoffs we’re making and understand the paths that will be closed off by a given implementation. By thinking of the negatives of a particular approach we’re able to surface many of these dormant issues. This helps avoids surprises later on and ensures the code has been dissected and thought through in a variety of ways.&lt;/p></description></item><item><title>Unification in tech and a new Gilded Age</title><link>/2016/01/29/unification-in-tech-and-a-new-gilded-age/</link><pubDate>Fri, 29 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/29/unification-in-tech-and-a-new-gilded-age/</guid><description>&lt;p>I just saw that Apple has &lt;a href="http://techcrunch.com/2015/11/24/apple-faceshift/">acquired&lt;/a> Faceshift, a VR based startup that makes it easier to create realistic animated characters. Two years ago Facebook &lt;a href="https://www.facebook.com/zuck/posts/10101319050523971">acquired&lt;/a> Oculus VR and Google soon followed by an &lt;a href="http://venturebeat.com/2014/10/13/google-counters-facebooks-oculus-buy-with-500m-investment-in-vr-startup-magic-leap/">investment&lt;/a> in Magic Leap. Apple is rumored to working on a self driving car and we all know Google is doing the same thing. And around the time that Facebook acquired Oculus Uber was &lt;a href="http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0">poaching&lt;/a> a good chunk of the robotics department at Carnegie Mellon.&lt;/p></description></item><item><title>The famed 10x developer</title><link>/2016/01/24/the-famed-10x-developer/</link><pubDate>Sun, 24 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/24/the-famed-10x-developer/</guid><description>&lt;p>If you’re in the software engineering world you’ve probably heard of the 10x developer. They’re an order of magnitude more productive than everyone else and can make all sorts of problems go away. The 10x number is completely arbitrary but I’ve worked with numerous developers who were notably more productive than others. A big part of it is just being able to write more code - a combination of knowing the right tools for the job and moving quickly while avoiding mistakes. But a bigger part in the productivity comes from making the appropriate decisions that are able to stand the test of time. If your code needs rewriting every time a new feature comes out it’s going to be tough to be as productive as someone whose code can be easily expanded and maintained as the product evolves. Great developers make design decisions that are able to solve the immediate problem but also leave a clear path for the improvements that will inevitably come. If you know what’s coming in a couple of months or in a year it’s simple to account for it in the current design but the real skill comes in being able to think of the unanticipated cases and be able to support them with minimal effort. Beyond that some choices end up unlocking opportunities that would have been difficult to fathom in the first place. Imagine coming up with an elegant implementation that solves an urgent problem and a couple of months later you realize that with minimal tweaking that implementation can turn into something that is transformative to the product. It’s impossible to think through every decision since you’ll end up stuck in a world of “analysis paralysis” but great engineers either have a gut feel or enough experience to make these high leverage decisions more frequently than others.&lt;/p></description></item><item><title>The ownership hierarchy</title><link>/2016/01/23/the-ownership-hierarchy/</link><pubDate>Sat, 23 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/23/the-ownership-hierarchy/</guid><description>&lt;p>Great engineers assume end to end ownership of their products. Rather than focusing on one feature at a time they understand how it fits in with the rest of the product and think about the impact it will have on users and the business. This leads to code that scales with the product while being able to be maintained and developed by a small team. But you can only have this with everyone embracing full ownership over a product.&lt;/p></description></item><item><title>Poor, neglected Google Voice</title><link>/2016/01/17/poor-neglected-google-voice/</link><pubDate>Sun, 17 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/17/poor-neglected-google-voice/</guid><description>&lt;p>Nearly all the conversations with my family is in Russian and phone calls are no different. The fun happens when I miss a call and it goes to voicemail. Turns out that despite the amazing job Google does in transcribing English calls it fails terribly at Russian. Instead of realizing that it’s not English it ends up with transcriptions such as “douche nozzle booster.”&lt;/p>
&lt;img src="/image/google-voice.png" alt="Google Voice Russian transcription" data-width="744" data-height="162" data-layout="responsive" />
&lt;p>Given Google’s expertise in machine learning and their massive data sets I’d expect them to at least be able to identify a non-English language. My guess is that Google Voice is no longer a priority and may not even be under development at all. I had a little over a hundred unread messages I needed to mark as read. With Gmail you get the option of applying an action to the entire selection - not just what’s visible - but with Google Voice you have to go through it page by page. And there’s no way to include more items per page. A tiny bit of modern web functionality did make it through though and I was able to use shortcuts to get the job done relatively quickly. I realize self driving cars are both more exciting and have more potential but I wish there was something being done to improve Google Voice - there’s a ton of us still using it.&lt;/p></description></item><item><title>2015 Stats: Part 1</title><link>/2016/01/12/2015-stats-part-1/</link><pubDate>Tue, 12 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/12/2015-stats-part-1/</guid><description>&lt;p>Over the past year I’ve been collecting personal stats nearly every day in order to see if I can spot any patterns and just understand myself better. These ranged from the time I spent sleeping to my mood (both physical and mental) to what I ate and drank. Over the weekend I hope to dive deeper into them and work out some relationships and patterns but for now I wanted to share just some basic summary stats. The script to analyze the data is up on &lt;a href="https://github.com/dangoldin/annual-stats-analysis">GitHub&lt;/a> but note that it’s designed for my file format.&lt;/p></description></item><item><title>Cleanest way to read a CSV file with Python</title><link>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</link><pubDate>Sun, 10 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/10/cleanest-way-to-read-a-csv-file-with-python/</guid><description>&lt;p>Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They&amp;rsquo;re related but not synonymous.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/usr/bin/python&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> csv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> collections &lt;span style="color:#f92672">import&lt;/span> namedtuple
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Can add whatever columns you want to parse here&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Can also generate this via the header (skipped in this example)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Row &lt;span style="color:#f92672">=&lt;/span> namedtuple(&lt;span style="color:#e6db74">&amp;#39;Row&amp;#39;&lt;/span>, (&lt;span style="color:#e6db74">&amp;#39;ymd&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;state&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;size&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;count&amp;#39;&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">with&lt;/span> open(&lt;span style="color:#e6db74">&amp;#39;file.csv&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;r&amp;#39;&lt;/span>) &lt;span style="color:#66d9ef">as&lt;/span> f:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> r &lt;span style="color:#f92672">=&lt;/span> csv&lt;span style="color:#f92672">.&lt;/span>reader(f, delimiter&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;,&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> r&lt;span style="color:#f92672">.&lt;/span>next() &lt;span style="color:#75715e"># Skip header&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rows &lt;span style="color:#f92672">=&lt;/span> [Row(&lt;span style="color:#f92672">*&lt;/span>l) &lt;span style="color:#66d9ef">for&lt;/span> l &lt;span style="color:#f92672">in&lt;/span> r]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Do whatever you want with rows&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>Have consistent development environments</title><link>/2016/01/09/have-consistent-development-environments/</link><pubDate>Sat, 09 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/09/have-consistent-development-environments/</guid><description>&lt;p>An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries. I’ve run into numerous issues where small version difference led to weird behaviors that ended up taking a long time to debug. Consistent tools help as well - using emacs on one machine but vim on another slows you down when you have to context switch and figure out which one you’re using. By committing to one you become more efficient as you develop the shortcuts and flows that are possible. Using virtual environments and containers helps get at this point - they’re both ways to ensure that the code you’re writing and testing is going to be the same code that’s running on production. Without this every time you deploy new code you’re risking failure. More often than not it will work as expected but it’s those rare cases that will be problematic and anything that can be done to avoid them should be done. One of the simplest ways is to align your development environments with your production ones.&lt;/p></description></item><item><title>Paris versus New York City</title><link>/2016/01/03/paris-versus-new-york-city/</link><pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/03/paris-versus-new-york-city/</guid><description>&lt;p>I just got back from a 10 day vacation in Paris and couldn&amp;rsquo;t help but compare it against New York. That&amp;rsquo;s what traveling does - forces you to compare what you&amp;rsquo;re comfortable with the novelty you&amp;rsquo;re exposed to. Some make you appreciate what you have while others make you want more. In any case I wanted to share my thoughts while they&amp;rsquo;re still fresh.&lt;/p>
&lt;ul>
&lt;li>Public transit: One of the first things you notice after living in New York are the public transit systems in other cities. New York has a reputation for having one of the best (one of the best?) in the world and I was curious to see how Paris handled it. The first thing I noticed was how short the platform was - rather than the multiple block stops in New York the Paris platform is enough for a 5 car train - and sure enough that&amp;rsquo;s the size of the Paris trains. Each station I&amp;rsquo;ve been to had accurate time estimates and it felt as if the trains ran frequently and I never had to wait longer than 6 minutes although I&amp;rsquo;ve only taken it during the day. One thing that&amp;rsquo;s struck me as odd was that it seemed as if every train had their own method of opening the door. In New York the doors open automatically but in Paris you need to either hit a button or pull some sort of level to get the doors to open. The way the stations were labeled felt friendly to tourists as well - each time you had to decide on an uptown or downtown train it would list each of the stops along with the potential transfers along each route which made it very easy to orient ourselves. The last thing I want to mention is price: the NYC subway costs $2.75 right now and you have to pay a fee for the metrocard itself. In Paris the fee is €1.80 which is just under $2 at current rates and you can buy 10 at a time for €1.40 each - significantly cheaper than the NYC subway.&lt;/li>
&lt;li>Bike and car share programs: New York has Citibike and Paris has an equivalent version called &lt;a href="http://en.velib.paris.fr/">Vélib&lt;/a>. I didn&amp;rsquo;t get a chance to use it so don&amp;rsquo;t have much of an opinion but the rates they offered were significantly lower than a non-annual Citibike pass. A daily Citibike pass is close to $10 whereas you can get day of Vélib for €1.70 and a week for €8. In addition to a bike share program, Paris has an electric car share program with stations prevalent across Paris. I didn&amp;rsquo;t get a chance to use these but it seemed like a really neat idea that reminded me of ZipCar without the burden of needing to return the car to the original destination.&lt;/li>
&lt;li>Neighborhoods, not districts: This might be entirely due to where I stayed and wandered but each neighborhood felt like it&amp;rsquo;s own little city. We&amp;rsquo;d walk around in a neighborhood and it would have everything one would need - a bakery, a cafe, some grocery stores, a few restaurants and bars, a dry cleaning place, and a few boutique shops. It made it seem that one only needs to walk a few blocks to have everything they need. In New York it feels as if there are districts - the flower district on 28th, the diamond district in midtown, the theater district near Time Square, the rug district on 31st, the lighting stores in chinatown - but it didn&amp;rsquo;t feel as if Paris was structured the same way. Paris of course is known for the shopping on Champ-Elysees but that&amp;rsquo;s more the exception than the rule. The only other area that felt like a district was a series of falafel shops in the Marais. Of course this may be completely wrong and only visible through my tourist lens.&lt;/li>
&lt;li>Architecture: Compared to New York Paris is ancient and its architecture and layout reflects that. Due to &lt;a href="https://en.wikipedia.org/wiki/Georges-Eug%C3%A8ne_Haussmann">Baron Haussmann&lt;/a>’s work during the 19th century Paris has a consistent look and feel which adds to the beauty. Paris barely has any skyscrapers since the majority of the buildings were construct before the elevator era. I was also struck by how mixed use the buildings were - many of them were businesses on the ground floor while the higher floors were residential. New York definitely has a bit of that but still feels as if it has some areas that are resident focused while others are commercially focused.&lt;/li>
&lt;li>Price: Based on my conversions and research I expected Paris to be a lot more expensive than it actually was. The biggest reason was that the exchange rate was hugely in my favor ($1.1 per €1) but even then the cost felt offset by the listed price including tips and taxes. For example, if I go to a restaurant in NYC and have a $14 dish it’ll end up costing me close to $18 due to the tax (8.875%) and tip (~15-20%). At an exchange rate of 1.1 dollars per euro that’s equivalent to a €16.37 dish. We went to a few grocery stores and the prices for fresh food felt reasonable and only a tad bit higher than what we were used to. We also got a chance to look at some posted real estate listings and they seemed cheaper than NYC - but the apartments are generally smaller. This is a pretty biased view since we spent it as tourists and didn’t have to buy clothes or any real house items but I suspect all in all it would be pretty comparable, if not cheaper, than New York.&lt;/li>
&lt;li>Panhandlers: In NYC it’s typical for people to look away and rush by someone panhandling but what struck me about Paris was that people would stop and have conversations with them. Even more, people were stopping with their children to chat and seemed to be engaging in meaningful conversations. My French wasn’t good enough to pick up the contents but the fact that people actually stopped and had conversations struck a chord with me. We talk about treating poverty and homelessness but unless we treat them as people and provide proper respect it will be for naught.&lt;/li>
&lt;li>Restaurants: Not too much here but one thing I wanted to point out was how diverse the streets of Paris were compared to the “front” of the restaurants. The host and waitresses at nearly every restaurant we ate at had the “classically French” look - I’m not sure whether this was intentional but it struck me as odd given how much diversity we have in NYC.&lt;/li>
&lt;li>Public restrooms: I haven’t seen this anywhere yet but Paris has free, public, self cleaning restrooms. It’s a bit slow since you have to wait through the washing cycle for each person but the fact that it’s publically available and free amazes me.&lt;/li>
&lt;li>Cabs: For the most part we used the subway but we had an interesting experience when we used a cab. The driver suggested an alternate route to the one provided by his GPS and it took us a bit longer than expected to get where we were headed. Instead of charging us what the meter showed he admitted fault and told us to pay a lower amount. Despite our protests he stuck to the lower amount. I’m not sure if this is a common experience but I’m extremely doubtful something like this would ever happen in NY.&lt;/li>
&lt;/ul>
&lt;p>Combined, these make it seem that I prefer Paris to New York but I honestly haven’t figured that out. Paris seems to have more progressive policies than New York but I’m basing that purely on my 10 day trip and actually living and working there may be entirely different. It feels as if Paris takes public services more seriously - the public transit is cheaper, more frequent, and more robust since I didn’t experience a single stall or failure which is sadly a common occurrence in New York. I’m also aware that I’ve only spent 10 tourist days in Paris and may be approaching it through rose-colored classes. I’d love to get thoughts from people that have lived for significant periods in both.&lt;/p></description></item><item><title>Lessons from 2015</title><link>/2016/01/01/lessons-from-2015/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>/2016/01/01/lessons-from-2015/</guid><description>&lt;p>Part of my 2015 goals was to have a weekly retrospective where I’d be distraction free and force myself to just sit and think. I usually did this on a Sunday morning by going outside and sitting on a bench overlooking the river or inside a quiet park. At the end of each of the retrospectives I’d sit down and jot down my thoughts in order to consistently revisit the list in order to keep improving. Below are the lessons of 2015 that I’m adopting going into 2016.&lt;/p></description></item><item><title>Smartphone usage by generation</title><link>/2015/12/28/smartphone-usage-by-generation/</link><pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/28/smartphone-usage-by-generation/</guid><description>&lt;p>By examining my family’s phone usage you get an interesting representations of how different generations use their phones. The five of us - parents, younger brother, and younger sister - are all on the same plan and T-Mobile breaks down the usage into phone minutes used, messages sent, and total amount of data used. I played a game with my friends to see whether they’d be able to decipher who’s who but it turned out to be surprisingly difficult and unintuitive. Turns out that my teenage sister uses least talking minutes and data but consumers average number of texts. At the same time, my mom runs a business and has close to 2000 minutes of talk time with the most number of texts sent while only using a moderate amount of data. My brother and I have a similar usage pattern - low minutes and messages but the highest data usage out of the entire family. The biggest surprise is my sister’s data usage that reinforces how much time teenagers are spending via apps and on separate social networks. That stereotype I grew up with of teenagers being constantly on their phones is still true - texting and talking have just bee replaced by siloed apps.&lt;/p></description></item><item><title>The true cost of low quality</title><link>/2015/12/25/the-true-cost-of-low-quality/</link><pubDate>Fri, 25 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/25/the-true-cost-of-low-quality/</guid><description>&lt;p>Recently I’ve found myself have similar conversations with various members of the engineering team regarding the tradeoff between speed and quality. Every situation is different but without going into project details I&amp;rsquo;ve found that quality come first, speed second. Not because I think speed is unimportant but because I think quality is underrated. In the desire to push the next feature and launch the next product quality tends to be sacrificed. This is fine as long as we understand the tradeoffs but in most cases those are externalized to others. For example, if an engineering team ships a buggy feature, the engineering team only incurs the cost of fixing it, and even then only if they end up fixing it. Simultaneously, the cost is passed on to the users who are powerless to fix it. And then it goes through multiple tiers - first the end user who becomes inefficient and may lose work, then the support person responsible for dealing with these issues, the product manager who has to context switch to both understand and prioritize the issue, and finally the engineer. During each step time is lost but most importantly is the interruption of &lt;a href="https://en.wikipedia.org/wiki/Flow_%28psychology%29">flow&lt;/a> for multiple people, each of whom gets distracted from what they’re doing in order to deal with a problem that could have been prevented in the first place.&lt;/p></description></item><item><title>Have a work “workout” plan</title><link>/2015/12/20/have-a-work-workout-plan/</link><pubDate>Sun, 20 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/20/have-a-work-workout-plan/</guid><description>&lt;p>If you go to the gym you end up getting a lot more out of it if you approach your workout with a plan in mind. The same thing happens with work. If you go every day you will inevitably get better but if you come in with concrete goals and ways to push yourself you’ll be in a much better position. It’s not as easy to measure your performance at work compared to the gym but just taking the first step and realizing that you want to improve is already beyond how most people approach work. Just by thinking about your performance you improve your ability to identify your strengths and weaknesses. Doing this on a consistent basis gets you into the habit of being introspective and improves your self-awareness, which is necessary to improve.&lt;/p></description></item><item><title>Automatically generating APIs</title><link>/2015/12/19/automatically-generating-apis/</link><pubDate>Sat, 19 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/19/automatically-generating-apis/</guid><description>&lt;p>A large part of modern software engineering is working with external APIs and services. Whether you want to automate a deployment on AWS, collect payments via Stripe, or track various behaviors using MixPanel, the process is the same - go through their documentation to figure out the available endpoints, the request requirements, and what the response will be. The next step is writing a simple API wrapper around the relevant endpoints that can then be accessed by the rest of the application. Given all the investment in AI research I’d love to see an application that’s able to generate API wrappers in any language for an API based solely on the documentation. Amazon has taken the first steps by &lt;a href="https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/">developing a data model&lt;/a> to represent their API which is then used to generate the actual libraries in a variety of languages. By changing something in the definition they can quickly rebuild the libraries in every language. One can also imagine using this data model to generate the actual documentation. This documentation can then be used to go back to the data model which can then be used to go back to the documentation.&lt;/p></description></item><item><title>Know what's mission critical</title><link>/2015/12/15/know-whats-mission-critical/</link><pubDate>Tue, 15 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/15/know-whats-mission-critical/</guid><description>&lt;p>The accepted belief is that startups should move quickly and err on the side of speed rather than quality. This makes sense. Startups are so risky that they won’t fail due to making a few mistakes but will fail if they get out maneuvered and out innovated. The big advantage startups have is speed and that needs be leveraged.&lt;/p>
&lt;p>The one caveat I’d make is that every company, big and small, should have mission critical elements that need to be maintained when pushing new features and updates. I was reminded of this last week when an unnamed corporate feedback startup sent out the private one-on-one notes people jotted down in preparation for their meeting to everyone within the company. This was a huge betrayal of trust and ruined the good will people had for the company and the product. If they weren’t able to get this basic piece right how are they expected to do the rest? Every company has these mission critical components that everyone needs to be aware of and great care must be taken to ensure they work before every deploying or change. In the adtech case it’s serving ads - if ads aren’t working then publishers aren’t making any money and losing money during each impression. For cloud productivity applications it’s critical that they don’t lose your data - downtime is annoying but at least you can switch to another task while they get back up. If you lose your data and documents you have to figure out exactly what you lost and decide whether it’s worth recreating. Everyone in the company should know what these these mission critical components are and it’s everyone’s job to make sure they’re working as expected since failure carries existential risk for the customer relationship. It’s unlikely that a single bad event will ruin things but as soon as it becomes a pattern it’s likely that that customer will be lost forever and never return due to the faulty first impression.&lt;/p></description></item><item><title>Jersey City garbage truck routes</title><link>/2015/12/12/jersey-city-garbage-truck-routes/</link><pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/12/jersey-city-garbage-truck-routes/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-garbage-trucks.png" alt="Jersey City garbage trucks" data-width="718" data-height="811" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>A couple of months ago I took a stab at plotting the Jersey City &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/">parking zones&lt;/a> after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome &lt;a href="http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530">comment&lt;/a> pointing out that Jersey City has a bunch of open data available, including a near-real time feed of &lt;a href="http://www.jciaonline.org/gpsMap.php?view=map">garbage truck locations&lt;/a>, a general &lt;a href="http://data.jerseycitynj.gov/">open data portal&lt;/a>, as well as the ability to &lt;a href="https://jerseycitynj.seamlessdocs.com/w/records_request">request custom data&lt;/a>. As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and the code is up on &lt;a href="https://github.com/dangoldin/jersey-city-open-data">GitHub&lt;/a>.&lt;/p></description></item><item><title>Optional museum fees and corporate sponsorships</title><link>/2015/12/07/optional-museum-fees-and-corporate-sponsorships/</link><pubDate>Mon, 07 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/07/optional-museum-fees-and-corporate-sponsorships/</guid><description>&lt;p>Yesterday I attended a concert at the Newark Museum and ran into a fairly common situation when lining up to get in. They had a suggested donation amount, which is entirely optional, while at the same time they provided free admission to anyone with a Bank of America card due to Bank of America’s sponsorship. I’ve seen the same sort of setup at museums in New York and I suspect it’s common elsewhere in the United States as well as abroad but the entire concept strikes me as odd.&lt;/p></description></item><item><title>Setting up a new computer - then and now</title><link>/2015/12/06/setting-up-a-new-computer-then-and-now/</link><pubDate>Sun, 06 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/06/setting-up-a-new-computer-then-and-now/</guid><description>&lt;p>In the past I’d be wary of setting up a new computer knowing that every time I’d need at least a couple of hours to get everything into a workable state. These days I actually look forward to setting up a new computer. Nearly every file I care about is hosted online and a large chunk of my productivity apps are online as well. The only tools I need to run locally are the various IDEs as well as a variety of open source tools and libraries that my code depends on. Even then I’d bet it takes less than an hour to get things to an 80% state at which point I’ll only discover what’s missing by just going through my day.&lt;/p></description></item><item><title>Think interfaces, not implementation</title><link>/2015/12/02/think-interfaces-not-implementation/</link><pubDate>Wed, 02 Dec 2015 00:00:00 +0000</pubDate><guid>/2015/12/02/think-interfaces-not-implementation/</guid><description>&lt;p>An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future. As engineers there’s a strong desire to obsess over the perfect code which can lead to a significant amount of refactors and rewrites without translating into actual business value. Thinking in terms of interfaces and components forces you to get the design and architecture right and leaving the implementation details for later. A side benefit for me has been being able to take pride in the design and flow and not worry about the code itself - allowing me to write code at a much faster place and sprinkle a series of todos for the parts of the code that I know need improving.&lt;/p></description></item><item><title>Analyzing large networks</title><link>/2015/11/26/analyzing-large-networks/</link><pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/26/analyzing-large-networks/</guid><description>&lt;p>While going through some old repos I came across an old &lt;a href="https://github.com/dangoldin/meerkat-crawl">project&lt;/a> I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including &lt;a href="http://gephi.github.io/">Gephi&lt;/a>, &lt;a href="http://www.cytoscape.org/">Cytoscape&lt;/a>, and &lt;a href="https://networkx.github.io/">NetworkX&lt;/a> - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.&lt;/p></description></item><item><title>Why are Netflix and Spotify so different?</title><link>/2015/11/22/why-are-netflix-and-spotify-so-different/</link><pubDate>Sun, 22 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/22/why-are-netflix-and-spotify-so-different/</guid><description>&lt;p>The news that Adele was not going to put her new album on the streaming services got me thinking about the differences between the way music and video are consumed. Just last week Rdio announced that it’s selling its assets to Pandora which is a reminder of how hard it is to start a music company - music labels wield all the control and are able to dictate the terms they want. Even Spotify is not yet profitable despite having millions of subscribers.&lt;/p></description></item><item><title>More MySQL fun</title><link>/2015/11/21/more-mysql-fun/</link><pubDate>Sat, 21 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/21/more-mysql-fun/</guid><description>&lt;p>I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">drop&lt;/span> &lt;span style="color:#66d9ef">table&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#66d9ef">exists&lt;/span> dan_test;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">create&lt;/span> &lt;span style="color:#66d9ef">table&lt;/span> dan_test (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id int &lt;span style="color:#66d9ef">not&lt;/span> &lt;span style="color:#66d9ef">null&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id2 int &lt;span style="color:#66d9ef">not&lt;/span> &lt;span style="color:#66d9ef">null&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> dan_test (id, id2) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>), (&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>), (&lt;span style="color:#ae81ff">3&lt;/span>,&lt;span style="color:#ae81ff">3&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">select&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#66d9ef">from&lt;/span> dan_test;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">select&lt;/span> id, &lt;span style="color:#66d9ef">case&lt;/span> &lt;span style="color:#66d9ef">when&lt;/span> id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">then&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">else&lt;/span> id &lt;span style="color:#66d9ef">end&lt;/span> &lt;span style="color:#66d9ef">as&lt;/span> id, id2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">from&lt;/span> dan_test;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">select&lt;/span> id, &lt;span style="color:#66d9ef">sum&lt;/span>(id2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">from&lt;/span> dan_test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">group&lt;/span> &lt;span style="color:#66d9ef">by&lt;/span> id;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">select&lt;/span> &lt;span style="color:#66d9ef">case&lt;/span> &lt;span style="color:#66d9ef">when&lt;/span> id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">then&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">else&lt;/span> id &lt;span style="color:#66d9ef">end&lt;/span> &lt;span style="color:#66d9ef">as&lt;/span> id, &lt;span style="color:#66d9ef">sum&lt;/span>(id2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">from&lt;/span> dan_test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">group&lt;/span> &lt;span style="color:#66d9ef">by&lt;/span> id;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">select&lt;/span> &lt;span style="color:#66d9ef">case&lt;/span> &lt;span style="color:#66d9ef">when&lt;/span> id &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">then&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">else&lt;/span> id &lt;span style="color:#66d9ef">end&lt;/span> &lt;span style="color:#66d9ef">as&lt;/span> new_id, &lt;span style="color:#66d9ef">sum&lt;/span>(id2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">from&lt;/span> dan_test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">group&lt;/span> &lt;span style="color:#66d9ef">by&lt;/span> new_id;&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>Adhoc task management</title><link>/2015/11/19/adhoc-task-management/</link><pubDate>Thu, 19 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/19/adhoc-task-management/</guid><description>&lt;p>A recent trick I’ve picked up to manage my time a bit better is to take all the adhoc tasks I have to do and scatter them into my calendar for the next few days. This allows me to actually get to working on the tasks and I can make sure none of them are forgotten. Using a calendar also forces me to think about the time I expect these tasks to take and plan around that. I’m nearly always running behind and am constantly shuffling tasks around but it’s much better than my previous system of a text file with a constantly growing list of todos. A side benefit of this approach is that I can split my day into &lt;a href="http://www.paulgraham.com/makersschedule.html">maker versus manager&lt;/a> chunks rather than be at the whim of meeting invites.&lt;/p></description></item><item><title>My old projects</title><link>/2015/11/12/my-old-projects/</link><pubDate>Thu, 12 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/12/my-old-projects/</guid><description>&lt;p>Writing up my old projects got me browsing through my GitHub account to see what else I&amp;rsquo;ve worked on. Some I&amp;rsquo;ll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don&amp;rsquo;t require any setup or configuration to start using. The other is how many third party libraries or APIs I&amp;rsquo;ve used and how much more difficult everything would have been had I had to build everything from scratch. If anyone is interested in forking and ressurecting some of these I&amp;rsquo;ll be glad to polish it up.&lt;/p></description></item><item><title>Apps on LTE, mobile web on wifi</title><link>/2015/11/09/apps-on-lte-mobile-web-on-wifi/</link><pubDate>Mon, 09 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/09/apps-on-lte-mobile-web-on-wifi/</guid><description>&lt;p>Lately, I’ve noticed an interesting trend with my smartphone usage. When I’m on wifi I’m much more likely to use the mobile web, click links, and read various articles whereas if I’m on LTE I’ll stick to dedicated apps. I noticed this at my apartment which has a narrow layout with my living room having wifi and my bedroom stuck on LTE. Despite me being in the same mindset regardless of which room I’m in my behavior changes dramatically.&lt;/p></description></item><item><title>A company marches on its data</title><link>/2015/11/08/a-company-marches-on-its-data/</link><pubDate>Sun, 08 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/08/a-company-marches-on-its-data/</guid><description>&lt;p>A couple of hundred years ago nearly every European country was engaged in some sort of military conflict which led either Napoleon or Frederick the Great to state that “&lt;a href="http://www.oxfordreference.com/view/10.1093/oi/authority.20110803095425331" target="_blank">an army marches on its stomach&lt;/a>.” The point being that logistics are the most important when it comes to having a successful army. These days the corporate equivalent would be that a company marches on its data.&lt;/p>
&lt;p>Every company claims to be data driven and there’s a slew of data collected about us each day. The most successful companies are able to leverage this data and use it to derive insights that drive direction. Unsuccessful companies may collect the same data but don&amp;rsquo;t leverage in an impactful way. It’s easy to collect information but it’s a huge challenge to turn into action. There are many options just for storing the data: one approach may make it easy to store tons of data while making it hard to run large scale analyses while another allows for a distributed computation approach that&amp;rsquo;s too slow. Beyond data storage there’s the actual analysis piece: what’s the appropriate model to use that can represent the relationships between the variables while being true to life? All these are questions that will become increasingly critical and separate the winners from the losers. Data itself has potential for massive &lt;a href="http://dangoldin.com/2013/07/21/beware-the-data-monopoly/">monopoly feedback loops&lt;/a> - companies that succeed are able to collect more and more which improves their product which collects more data. Right now it may only seem as if larger companies should care but I suspect within the next 10 years we’ll see more and more small and local businesses adopt a truly data-driven approach, whether through internal tools or through external services&lt;/p></description></item><item><title>Zoning Airbnb</title><link>/2015/11/05/zoning-airbnb/</link><pubDate>Thu, 05 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/05/zoning-airbnb/</guid><description>&lt;p>I’ve been meaning to share some thoughts on regulating Airbnb for a couple of months now but kept putting it off. The &lt;a href="http://www.engadget.com/2015/10/21/Airbnb-ads/" target="_blank">recent news&lt;/a> was motivating enough for me to finish it off.&lt;/p>
&lt;p>I’m a huge fan of Airbnb and it’s my first step whenever I’m traveling. Nearly all my experiences have been great and I’m contemplating getting rid of my Starwood card since it’s just not as useful anymore given that I gravitate towards Airbnb first. At the same time I understand the impact renting a place Airbnb has on the neighbors and can imagine myself hating it if my neighbors were listing their places.&lt;/p></description></item><item><title>Good code is easy to build and deploy</title><link>/2015/11/01/good-code-is-easy-to-build-and-deploy/</link><pubDate>Sun, 01 Nov 2015 00:00:00 +0000</pubDate><guid>/2015/11/01/good-code-is-easy-to-build-and-deploy/</guid><description>&lt;p>A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.txt”, updating the configuration file, creating the database, and running the database migration script. This is a massive improvement when compared to my initial projects where there was no documentation and my setup involved a ton of adhoc, undocumented work directly on the production server that’s now lost.&lt;/p></description></item><item><title>Some nostalgia</title><link>/2015/10/29/some-nostalgia/</link><pubDate>Thu, 29 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/29/some-nostalgia/</guid><description>&lt;p>This past weekend I was going through some old projects and got a bit nostalgic. Some were my first foray into web programming and startups while others were just me messing around and trying to learn a new framework or language. Each of them have taught me valuable lesson and I thought it would be fun to go through each one and jot down a quick background as well as the lessons learned. I’m doing a high level pass so if any of these are interesting definitely let me know and I’ll do a deeper dive.&lt;/p></description></item><item><title>Production makes fools of us all</title><link>/2015/10/25/production-makes-fools-of-us-all/</link><pubDate>Sun, 25 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/25/production-makes-fools-of-us-all/</guid><description>&lt;p>The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don&amp;rsquo;t scale on production. Ideally your development environment mirrors production and has the same load and hardware but that&amp;rsquo;s rarely the case. For the other cases cases I’d go through the following items to make sure your code is ready for production:&lt;/p></description></item><item><title>Jsonify.me 2.0</title><link>/2015/10/21/jsonify.me-2.0/</link><pubDate>Wed, 21 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/21/jsonify.me-2.0/</guid><description>&lt;p>A couple of weeks ago I wrote about the idea of having a &lt;a href="/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/">“go to” project&lt;/a> that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of &lt;a href="http://jsonify.me" target="_blank">jsonify.me&lt;/a>. It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.me page but in JSON. This object can then be mapped to any subdomain (mine is at json.dangoldin.com) and be referenced by any third party code. For example, you can construct your personal jsonify.me object based on the information in your various social media profiles and then make that information accessible to a variety of sites or pages that can generate it in a variety of ways. One site can turn it into a simple resume while another one can turn into a visual timeline of your history. At the moment it’s entirely open ended with the vast majority of the functionality provided solely through an API. Over time I’ll add some more bells and whistles but I’d love to see the community come up with their own unique JSON format that can then get adopted - similar to the way the hashtag system on Twitter evolved. I suspect it’s going to be significantly more complicated since there’s no 140 character limit but am still interested to see where this goes. Play around with it and let me know what you think!&lt;/p></description></item><item><title>Dates in the shell</title><link>/2015/10/19/dates-in-the-shell/</link><pubDate>Mon, 19 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/19/dates-in-the-shell/</guid><description>&lt;p>The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.&lt;/p></description></item><item><title>Writing code? Think about the deployment</title><link>/2015/10/18/writing-code-think-about-the-deployment/</link><pubDate>Sun, 18 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/18/writing-code-think-about-the-deployment/</guid><description>&lt;p>The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand. Large companies and teams have dedicated ops teams that handle deployments but small teams need to do this on their own.&lt;/p></description></item><item><title>Adblock, publishers, and content quality</title><link>/2015/10/15/adblock-publishers-and-content-quality/</link><pubDate>Thu, 15 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/15/adblock-publishers-and-content-quality/</guid><description>&lt;p>Ever since the release of iOS 9 and it’s support for adblocking apps I can’t go a day without seeing some article about adblock. Some condemn it and claim it’s stealing from publishers while others make the case that ads are so intrusive that they deserve to be blocked. I don’t want to dwell into either of these but something that’s been on my mind is that publishers aren’t doing enough to differentiate themselves based on the quality of their audience.&lt;/p></description></item><item><title>Have a 'go to' project when learning a new programming language</title><link>/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/</link><pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/</guid><description>&lt;p>At the beginning of the year I wanted to learn a bit of Node.js and decided the best way was to code up a simple project. The idea was &lt;a href="http://jsonify.me/" target="_blank">jsonify.me&lt;/a>, a simple API only app that provided people a simple way to generate their own JSON profiles that they would then be able to map to any domain name, for example &lt;a href="http://json.dangoldin.com">http://json.dangoldin.com&lt;/a>. The primary goal was to get some real experience with Node.js rather than rely on some walkthroughs and tutorials. Since then I’ve used it as the starter project to learn new languages. I’ve coded it up in Scala and have just finished up the Go version.&lt;/p></description></item><item><title>Not every big company is evil</title><link>/2015/10/10/not-every-big-company-is-evil/</link><pubDate>Sat, 10 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/10/not-every-big-company-is-evil/</guid><description>&lt;p>It&amp;rsquo;s become a popular idea that big companies are evil and we should only be supporting small and local businesses. There’s some truth to it - smaller companies are much more aligned with the incentives of the community whereas larger companies may be managed from thousands of miles away via a spreadsheet. When the only goal is to make more money it’s very likely that morality and honesty will suffer.&lt;/p></description></item><item><title>Is media consumption zero sum?</title><link>/2015/10/04/is-media-consumption-zero-sum/</link><pubDate>Sun, 04 Oct 2015 00:00:00 +0000</pubDate><guid>/2015/10/04/is-media-consumption-zero-sum/</guid><description>&lt;p>I’ve seen the point made across a variety of articles that media companies see media consumption as being zero sum - there’s only a fixed amount of consumption that people have and they’re allocating it between a variety of options. The typical example is people abandoning legacy television for YouTube. Thus, the rationale goes, the time people would have spent watching television has been replaced by them watching YouTube.&lt;/p>
&lt;p>I understand the concern. People are watching less and less standard television with only time sensitive and restricted content (ie sports) being viewed on television but there’s a world of nuance. People may be watching less and less broadcast television but the content that’s being consumed is growing. Cordcutters pride themselves on not paying for cable television yet they’re still subscribing to Netflix and Hulu. The same shows are being watched - just not on the usual devices. In addition, there’s the rise of multi-screen consumption - we’ll be on our phones or laptops while watching TV. Whether it’s a distraction from a commercial, a way to follow an event on Twitter, or just browsing Wikipedia to get a bit more information, we’re parallelizing our media consumption and increasing the size of the media pie rather than solely switching between platforms.&lt;/p></description></item><item><title>The iPad Pro and Apple's walled garden</title><link>/2015/09/26/the-ipad-pro-and-apples-walled-garden/</link><pubDate>Sat, 26 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/26/the-ipad-pro-and-apples-walled-garden/</guid><description>&lt;p>Supposedly the performance of the recently announced iPad Pro will rival that of the Macbook and encourage tons of people to buy it with productivity in mind rather than just consumption. And at a starting price of $799 it’s significantly cheaper than the Apple laptop options. Apple is known for achieving large product margins but I wonder if the iPad Pro is sold at a lower margin to get people to switch to iOS and get even more tied to Apple’s ecosystem. I currently use a Macbook Pro for my work but can switch to any Unix based environment without any hit to my productivity. I’m not sure if this is me being too cynical but I can definitely see Apple taking a long term view here and taking a much lower profit margin on the iPad Pro in order to get people to actually make the switch to the walled garden of iOS. Among the developer community Safari is already seen as the &lt;a href="http://nolanlawson.com/2015/06/30/safari-is-the-new-ie/" target="_blank">reincarnation of IE&lt;/a> given Apple’s lackluster support. These may just be coincidences but for a company as detail oriented as Apple this feels like a strategic decision to shift away from the open web and into the walled app garden.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones II</title><link>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</link><pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/24/mapping-the-jersey-city-parking-zones-ii/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span8">
 &lt;div class="thumbnail">
 &lt;img src="/image/jersey-city-parking-zones.png" alt="Jersey City parking zones" data-width="442" data-height="640" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at &lt;a href="https://dangoldin.github.io/jersey-city-open-data/">https://dangoldin.github.io/jersey-city-open-data/&lt;/a> and includes the zones that had enough geocodeable addresses to generate a valid polygon.&lt;/p></description></item><item><title>Keep on learning</title><link>/2015/09/20/keep-on-learning/</link><pubDate>Sun, 20 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/20/keep-on-learning/</guid><description>&lt;p>I&amp;rsquo;m a strong believer that one needs to keep learning and to not get content with the knowledge they have. This can come in the form of new experiences or challenges but should be seen as a learning opportunity. Throughout school we have a structure in place to help us learn but after we graduate we have to take the responsibility ourselves. Unfortunately, many people don&amp;rsquo;t and even take pride that they haven&amp;rsquo;t read a book since college.&lt;/p></description></item><item><title>Internalizing externalities</title><link>/2015/09/15/internalizing-externalities/</link><pubDate>Tue, 15 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/15/internalizing-externalities/</guid><description>&lt;p>Recently I&amp;rsquo;ve adopted the practice of having the engineering team support other team when the core technology can support it - even if hasn&amp;rsquo;t been fully built in to the product. This may require manually adding entries to a series of database tables that or manually pulling reports that aren&amp;rsquo;t yet availabe via the UI. Despite being an inefficiency for the engineering team it provides a variety of benefits that outweigh this minor inconvenience.&lt;/p></description></item><item><title>Workaround driven product development</title><link>/2015/09/13/workaround-driven-product-development/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/13/workaround-driven-product-development/</guid><description>&lt;p>As engineers, it&amp;rsquo;s easy to get focused on technical problems and lose sight of the business. We realize our code will be used externally but we have a tendency to focus on what&amp;rsquo;s close to home rather than the actual real world usage. One of the biggest eye openers for me has been seeing people interact with our products.&lt;/p>
&lt;p>We like to think of ourselves as &amp;ldquo;hackers&amp;rdquo; but it&amp;rsquo;s amazing to see the length people go to &amp;ldquo;hack&amp;rdquo; our products to do what they want. Whether it&amp;rsquo;s someone keeping multiple tabs open to be able to reference information back and forth and avoid losing data or someone registering multiple accounts to bypass a database uniquness constraint - it&amp;rsquo;s a way for people to bypass the intended design and I&amp;rsquo;d argue that these &amp;ldquo;hackers&amp;rdquo; are a sign of a useful product. In fact, I&amp;rsquo;d argue that if people aren&amp;rsquo;t hacking around a product&amp;rsquo;s limitations it&amp;rsquo;s not a good one. These workarounds are a sign that the product is so useful that people are willing to go through additional manual effort to use it for a different use case. If that&amp;rsquo;s not a sign of new functionality to support I don&amp;rsquo;t know what is.&lt;/p></description></item><item><title>Mapping the Jersey City parking zones</title><link>/2015/09/12/mapping-the-jersey-city-parking-zones/</link><pubDate>Sat, 12 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/12/mapping-the-jersey-city-parking-zones/</guid><description>&lt;p>A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there&amp;rsquo;s a &lt;a href="http://jcparking.org/PDF/ZONE%20PERMITS%20ALL%20ZONES.pdf" target="_blank">PDF&lt;/a> that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.&lt;/p></description></item><item><title>Smart OSes</title><link>/2015/09/05/smart-oses/</link><pubDate>Sat, 05 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/05/smart-oses/</guid><description>&lt;p>It feels as if Google has been getting better and better at what I call search inference. I’ll oftentimes do a search for a particular place on either Google Maps or the general site and see it automatically show up in Google Now. Or I will start with a simple search query that needs to be refined with Google able to offer perfect suggestions. Given how much data they’re collecting it’s not a surprise but it’s an easy way to realize I’m not that unique.&lt;/p></description></item><item><title>Come on Twitter</title><link>/2015/09/03/come-on-twitter/</link><pubDate>Thu, 03 Sep 2015 00:00:00 +0000</pubDate><guid>/2015/09/03/come-on-twitter/</guid><description>&lt;p>Eugene Wei &lt;a href="http://www.eugenewei.com/blog/2015/9/1/when-the-network-is-mature" target="_blank">published a great post&lt;/a> on the power of networks and how Twitter hasn’t been taking advantage of their core product - a public messaging protocol. Given this thesis, Twitter should move away from the artificial 140 character limit and innovate on top of the protocol rather than be bound by it.&lt;/p>
&lt;p>I’m not nearly as eloquent but I also have my gripes with Twitter that his post motivated me to write. It just feels they don’t care about the user experience. Cross device sync is still a problem - if I clear a notification on my phone why do I see it again on my computer? People are still complaining about the OS X app not being as functional as the other versions. Even on my phone the navigation feels inconsistent - sometimes I get taken out of the app and sometimes a screen is loaded inside. This causes me to hit the back button at the wrong time and randomly leave the app which resets my location. I’m a big fan of Twitter and it’s pretty much the only social network I actually use but I’m frustrated by how poor it is.&lt;/p></description></item><item><title>Two factor authentication hell</title><link>/2015/08/31/two-factor-authentication-hell/</link><pubDate>Mon, 31 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/31/two-factor-authentication-hell/</guid><description>&lt;p>While my phone was being repaired I ran into a predicament. The only way I could log in to my Google accounts was by authenticating via an SMS code which I wasn’t to get without an SMS code. Additionally, I never bothered to actually write down the backup codes thinking I’d never need them so I was stuck in the envious position of being Google account free for 4 days.&lt;/p></description></item><item><title>Game theory dining</title><link>/2015/08/29/game-theory-dining/</link><pubDate>Sat, 29 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/29/game-theory-dining/</guid><description>&lt;p>This past Wednesday I had dinner at Blu - a restaurant that’s &lt;a href="http://well.blogs.nytimes.com/2015/08/17/what-would-you-pay-for-this-meal/
" target="_blank">adopted&lt;/a> a “pay what you want” pricing model. Customers have an incentive to underpay the final check so I was curious to see how Blu handled it throughout dinner. I noticed three tactics they used to get people to pay fairly and am sure they utilized a bunch more that I didn’t even notice:&lt;/p>
&lt;ul>
&lt;li>Anchoring: Before sitting down to eat the waitress explained that it was pay what you want and most of the dishes are estimated to be priced between $10 and $12. This sets the expectation early so if you do decide to pay less you’re making an explicit decision to underpay.&lt;/li>
&lt;li>Reminder: At the end of the meal we were told how many dishes we ordered. This was also helpful but I can’t help but think that this is a way to give you an estimate of how much you should pay - especially when paired with the fact that the expectation is $10 per dish - a very easy number to multiply.&lt;/li>
&lt;li>Shame: I found this the most interesting one. Instead of giving you a blank receipt and allowing you to write what you want to pay you have to tell the waitress what you want them to charge. This forces you to explicitly vocalize your payment to another person rather than quickly writing something and slinking away. And no one wants to be judged as cheap face to face so we’re encouraged to pay well.&lt;/li>
&lt;/ul>
&lt;p>I’m a huge fan of behavioral psychology experiments that shed some light on the way our minds work and it was a great experience to partake in one. I only wish I could have spotted more behavioral cues that I’m sure they employed.&lt;/p></description></item><item><title>Learn the application architecture through AWS</title><link>/2015/08/23/learn-the-application-architecture-through-aws/</link><pubDate>Sun, 23 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/23/learn-the-application-architecture-through-aws/</guid><description>&lt;p>Last month I &lt;a href="http://dangoldin.com/2015/07/25/use-the-database-luke/">wrote&lt;/a> that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.&lt;/p></description></item><item><title>Development cycles across programming languages</title><link>/2015/08/20/development-cycles-across-programming-languages/</link><pubDate>Thu, 20 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/20/development-cycles-across-programming-languages/</guid><description>&lt;p>The most common way of making sure code works is by going through the “develop-run-test” loop. We write some code that we expect to have a certain behavior, we run the code and trigger that behavior, and then we confirm that the results are what we expected. And we keep iterating, hopefully making more progress with each new iteration.&lt;/p>
&lt;p>One thing I’ve noticed is that this pattern varies drastically for me depending on the language I’m working with. I’ll cycle through iterations much quicker in Python than I will with Java. Part of it is that my Java projects are larger and take a longer amount of time to start but I suspect the bigger benefit is that Java’s strong and static type system makes it easier to take larger coding steps than I’d be able to with Python. For example, if I need to write a method to extract data from a JSON object I’ll approach it very different if I’m doing it in Python than I would if I were doing it in Java. With Python I’d jump into the REPL and walk through a few examples and make sure I handle the the various edge cases whereas with Java I’d place a lot more faith in the IDE and it’s litany of warnings.&lt;/p></description></item><item><title>A phoneless day</title><link>/2015/08/16/a-phoneless-day/</link><pubDate>Sun, 16 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/16/a-phoneless-day/</guid><description>&lt;p>As they say you don&amp;rsquo;t appreciate something until it&amp;rsquo;s gone and I got to experience first hand when I cracked the screen on my phone and dropped it off for a quick repair. Unfortunately, the repair wasn&amp;rsquo;t so quick due to a screw up and I&amp;rsquo;m still phoneless more than a day later.&lt;/p>
&lt;p>I find myself reaching for it despite knowing it&amp;rsquo;s gone and even feel it vibrating in my pocket without it being there. It&amp;rsquo;s both amazing and frightening how significant my phone has become in my life and I&amp;rsquo;m am actually glad that it&amp;rsquo;s missing. In many ways I feel like an addict that missed a fix and it&amp;rsquo;s a rude awakening. For the first time in years I had to ask a stranger for the time and had to find an open barber shop without a map or an online search. I also went to bed without my usual habit of checking up on Twitter or catching up on some blog posts and I woke up without immediately reaching for my phone.&lt;/p></description></item><item><title>Generating vs filtering</title><link>/2015/08/15/generating-vs-filtering/</link><pubDate>Sat, 15 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/15/generating-vs-filtering/</guid><description>&lt;p>While playing around with Scala I rediscovered streams - a list-like structure that&amp;rsquo;s lazily evaluated - meaning that only when you access a particular value is it evaluated. This makes it possible to create infinite streams since all you need is a function that&amp;rsquo;s able to compute the next value. In such a way we can create a stream of all numbers, just the positive even numbers, or just the prime numbers. Calculating each successive prime number will become more difficult but it is possible.&lt;/p></description></item><item><title>Reprioritizing a non priority RabbitMQ queue</title><link>/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue/</link><pubDate>Wed, 12 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/12/reprioritizing-a-non-priority-rabbitmq-queue/</guid><description>&lt;p>Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.&lt;/p></description></item><item><title>Static site search</title><link>/2015/08/09/static-site-search/</link><pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/09/static-site-search/</guid><description>&lt;p>I’ve written &lt;a href="/2013/03/12/mmmm-pseudo-static-sites/" target="_blank">previously&lt;/a> about the appeal of static sites and recently came up with another example of how powerful the setup can be. The gist is that the site’s content is static HTML, CSS, and JavaScript but the relevant underlying content is refreshed on a recurring basis with a separate job. This allows you to host the entire site on S3 and avoid maintaining your own web server.&lt;/p></description></item><item><title>Debate what's necessary and no more</title><link>/2015/08/03/debate-whats-necessary-and-no-more/</link><pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/03/debate-whats-necessary-and-no-more/</guid><description>&lt;p>A critical component in communicating between various teams is knowing who has what responsibility. Especially with driven people it’s easy to have overlap between various functions - product and design; design and frontend engineering; and frontend engineering and backend engineering. This is both good - because it’s able to focus more eyes on a particular problem and provides a new perspective - and bad - because people may feel that they can’t move quickly enough and don’t want to cede decision making power. Great teams thrive in this environment while poor teams degenerate into a Dilbert cartoon.&lt;/p></description></item><item><title>It's not done until it's deployed</title><link>/2015/08/01/its-not-done-until-its-deployed/</link><pubDate>Sat, 01 Aug 2015 00:00:00 +0000</pubDate><guid>/2015/08/01/its-not-done-until-its-deployed/</guid><description>&lt;p>As a developer, it feels wonderful to commit some code and knock an item off of the ever growing to do list. Unfortunately, until that code is deployed it’s not delivering any actual benefit. It’s easy to open a pull request and move on to the next task but to create high quality products we need to only consider our code complete when it’s deployed and running issue free. So many things need to happen between writing the code and deploying it - handling conflicts with other database changes, updating database schemas, and monitoring the actual code to make sure it’s working as expected on a production system. Calling something done before it’s deployed is a lazy shortcut.&lt;/p></description></item><item><title>The Go interface</title><link>/2015/07/29/the-go-interface/</link><pubDate>Wed, 29 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/29/the-go-interface/</guid><description>&lt;p>I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-java" data-lang="java">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">interface&lt;/span> &lt;span style="color:#a6e22e">Animal&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#66d9ef">boolean&lt;/span> &lt;span style="color:#a6e22e">isFurry&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">public&lt;/span> String &lt;span style="color:#a6e22e">speak&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Dog&lt;/span> &lt;span style="color:#66d9ef">implements&lt;/span> Animal {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#66d9ef">boolean&lt;/span> &lt;span style="color:#a6e22e">isFurry&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">true&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">public&lt;/span> String &lt;span style="color:#a6e22e">speak&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Woof&amp;#34;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">aRandomFunction&lt;/span>(Animal a) { .. } &lt;span style="color:#f92672">//&lt;/span> Can take anything that &lt;span style="color:#66d9ef">implements&lt;/span> Animal&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>Use the database Luke</title><link>/2015/07/25/use-the-database-luke/</link><pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/25/use-the-database-luke/</guid><description>&lt;p>I’m convinced that the best way to ramp up as a newly hired engineer is to go through the database. Rather than relying on outdated documentation or discovering undocumented features the database is the actual source of truth and defines both the limits and the capabilities of the application. You can examine the relationships between the various objects as well as the litany of features and options that are supported. It’s definitely more difficult to get up to speed on a database rather than documentation or a demo of the UI but the knowledge gained is significantly deeper. Especially when you’re going to be working on features that depend on the database it’s incredibly useful to know how the database is laid out and set up. On its own a walk through of the UI provides a high level overview of how it works but coupling that with the database allows you to internalize the connections and actually understand how the user interactions feed the data and vice versa.&lt;/p></description></item><item><title>Tunnel vision</title><link>/2015/07/20/tunnel-vision/</link><pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/20/tunnel-vision/</guid><description>&lt;p>A common behavior when solving a coding problem is focusing too much on the solution and not enough on the general context. If this is a software problem this may manifest itself as a very quick turnaround on a task that inadvertantly breaks an existing behavior or even something that ends up causing a headache months from now when a slightly more nuanced use case needs to be supported. Experienced developers will not only solve the task at hand but will also understand the limitations of their solution and are able to identify the areas that will be adversely affected by their solution. Nearly every software decision comes with tradeoffs and strong developers can think through this maze and pick the most appropriate one given the situation.&lt;/p></description></item><item><title>Excel wins</title><link>/2015/07/19/excel-wins/</link><pubDate>Sun, 19 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/19/excel-wins/</guid><description>&lt;p>Other than the usual developer tools the only desktop based app I use is Excel and every few months I try to wean myself away. I love being able to keep all my text docs and slideshows online and have them accessible and sharable anywhere. The best part is updating the content without having to worry about bombarding people with yet another email.&lt;/p>
&lt;p>I tried doing the same with Google Sheets and it works for smaller tables but as soon as you get tables with thousands of rows it’s noticeably slower than Excel. It’s amazing for what it can do but it feels as if the browser just can’t handle the rendering nor the calculation that a large spreadsheet entails. Some of the time closing and reopening the table fixes the problem but this is too reminiscent of Windows in the 90s and ends up in a glacial pace after a few minutes of work. I continue to use Google Sheets for small, collaborative files but for anything larger or anything that will need heavy computation I’ll switch to Excel. I’ve also been using R for more repetitive analyses but for the quick and dirty analysis that comes from the result of a SQL query Excel is still king.&lt;/p></description></item><item><title>A Bloom filter in my head</title><link>/2015/07/16/a-bloom-filter-in-my-head/</link><pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/16/a-bloom-filter-in-my-head/</guid><description>&lt;p>As many at TripleLift will tell you I have a fondness for &lt;a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom filters&lt;/a> but only recently did I realize that our brains work in a similar way. We don’t always know every particular detail or have perfect recall but what we do have is the ability to realize that something is familiar and that we might have encountered it before. This triggers enough additional thoughts that we’re able to dig up the actual thought or reference. For example I can’t always recall the exact Java library I need to use for a particular problem but I know that I’ve solved similar problems before and can quickly rediscover my previous solution, whether through an online search with the appropriate keywords or even by going through some old code.&lt;/p></description></item><item><title>Comparing SQL schemas</title><link>/2015/07/12/comparing-sql-schemas/</link><pubDate>Sun, 12 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/12/comparing-sql-schemas/</guid><description>&lt;p>During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn&amp;rsquo;t necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state. In the past I would just run a simple query (select table_schema, table_name, column_name from information_schema.columns;) on each environment and then use either Excel or Google Sheets to spot the differences. This takes a bit of time so this weekend I put together a quick &lt;a href="https://dangoldin.github.io/js-tools/#tab-sql-schema-comparison" target="_blank">JavaScript tool&lt;/a> to automate the process. You simply run the schema query on each of the environments and paste the resulting rows into the two text areas. The result is a JSON based diff showing the additions, deletions, and modifications to each of the tables and fields. The next step is to modify it to also identify differences in the column types.&lt;/p></description></item><item><title>Apple and Intel</title><link>/2015/07/08/apple-and-intel/</link><pubDate>Wed, 08 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/08/apple-and-intel/</guid><description>&lt;p>I recently finished &lt;a href="http://www.amazon.com/The-Intel-Trinity-Important-Company/dp/0062226762" target="_blank">The Intel Trinity&lt;/a> which detailed the history of Intel and its rise from a small memory manufacturer to the leader in microprocessors. The entire book is worth a read if you’re interested in startups and the rise of Silicon Valley but one anecdote that immediately stood out was about the reason Apple didn’t use Intel chips &lt;a href="https://en.wikipedia.org/wiki/Apple's_transition_to_Intel_processors" target="_blank">until 2005&lt;/a>. Before then Macs relied on &lt;a href="https://en.wikipedia.org/wiki/List_of_Macintosh_models_grouped_by_CPU_type" target="_blank">MOS Technology, Motorola and PowerPC&lt;/a> chips. The Intel Trinity makes the case that the reason Apple waited so long to adopt Intel chips was due to the fact that Steve Wozniak didn’t have enough money to build the Apple I prototype using Intel and had to resort to the cheaper option - a MOS 6502/Motorola 6800. And the reason Steve Wozniak didn’t have enough money was because Steve Jobs didn’t split the Atari payment fairly between them and took the lion’s share without even telling Steve Wozniak about it.&lt;/p></description></item><item><title>Zsh and Oh My Zsh</title><link>/2015/07/06/zsh-and-oh-my-zsh/</link><pubDate>Mon, 06 Jul 2015 00:00:00 +0000</pubDate><guid>/2015/07/06/zsh-and-oh-my-zsh/</guid><description>&lt;p>I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful &lt;a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank">oh-my-zsh&lt;/a> framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts. Just a few days ago I enabled a plugin to allow for autocompletion for Python’s fabric commands. The advantage for a single command is tiny if you’re quick on the keyboard but when you’re running hundreds of commands each day it’s nice to get your typing speed to be as quick as your thought process. Zsh comes close.&lt;/p></description></item><item><title>Ambiguous SQL queries</title><link>/2015/06/27/ambiguous-sql-queries/</link><pubDate>Sat, 27 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/27/ambiguous-sql-queries/</guid><description>&lt;p>One of the best habits to develop when working with SQL is to always refer to fields through an alias. Numerous times I decided to just take a shortcut and ended up regretting it later. Even if you’ve tested your query to make sure it works there’s no guarantee that a future change to a table schema won’t break it.&lt;/p>
&lt;p>Let’s say you have the following two tables - with items.category_id corresponding to categories.id&lt;/p></description></item><item><title>Domain specific API definitions</title><link>/2015/06/23/domain-specific-api-definitions/</link><pubDate>Tue, 23 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/23/domain-specific-api-definitions/</guid><description>&lt;p>Yesterday, Amazon &lt;a href="https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/" target="_blank">announced&lt;/a> a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a &lt;a href="https://github.com/boto/boto3/tree/develop/boto3/data" target="_blank">standardized representation&lt;/a> for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I&amp;rsquo;ve included a subset of the &lt;a href="https://github.com/boto/boto3/blob/develop/boto3/data/ec2/2015-04-15/resources-1.json" target="_blank">EC2 definition&lt;/a> below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.&lt;/p></description></item><item><title>Properly handling text based two factor authentication</title><link>/2015/06/20/properly-handling-text-based-two-factor-authentication/</link><pubDate>Sat, 20 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/20/properly-handling-text-based-two-factor-authentication/</guid><description>&lt;p>The purpose of two factor authentication is to prevent unauthorized access to your accounts by requiring a device other than a password to verify that it’s actually you. Usually this is a text message to a phone or an app such as Authy or Google Authenticator. Being paranoid and despite the inconvenience I chose to do it for the vast majority of my accounts that support it but some are significantly more secure than others.&lt;/p></description></item><item><title>The Edmunds API</title><link>/2015/06/19/the-edmunds-api/</link><pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/19/the-edmunds-api/</guid><description>&lt;p>As part of the &lt;a href="/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/">RelayRides analysis&lt;/a> I needed to estimate the price of a car and stumbled across the &lt;a href="http://developer.edmunds.com/" target="_blank">Edmunds API&lt;/a>. I came in with some low expectations but was pleasantly surprised by how well it worked. I thought I’d need to go through a data cleanup process to make sure I was using the correct arguments in the HTTP requests but somewhat remarkably the Edmunds API was able to properly handle nearly every request.&lt;/p></description></item><item><title>Smartphone productivity</title><link>/2015/06/13/smartphone-productivity/</link><pubDate>Sat, 13 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/13/smartphone-productivity/</guid><description>&lt;p>Smartphones are supposed to be the next big wave but I can’t get myself to be productive on them. Every action takes an order of magnitude longer than it would on a regular computer which prevents from me from starting it in the first place. The challenge is that I’m a power user on a computer able to leverage shortcuts across a variety of programs to be extremely productive. The cost is that when I switch to a phone it’s impossible for me to attain that level of speed which is extremely frustrating.&lt;/p></description></item><item><title>A MySQL “GROUP BY” nuance</title><link>/2015/06/09/a-mysql-group-by-nuance/</link><pubDate>Tue, 09 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/09/a-mysql-group-by-nuance/</guid><description>&lt;p>I discovered a nuance with MySQL&amp;rsquo;s GROUP BY statement earlier today that I’ll share with the hope that others can learn from it. It’s fairly common to use a coalesce statement to handle null values while keeping the resulting field the same name. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">SELECT&lt;/span> coalesce(a.user_id, b.other_user_id) &lt;span style="color:#66d9ef">as&lt;/span> user_id, &lt;span style="color:#66d9ef">sum&lt;/span>(s.num) &lt;span style="color:#66d9ef">as&lt;/span> total_nums
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">FROM&lt;/span> table_a a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">LEFT&lt;/span> &lt;span style="color:#66d9ef">JOIN&lt;/span> table_b &lt;span style="color:#66d9ef">on&lt;/span> a.some_id &lt;span style="color:#f92672">=&lt;/span> b.some_other_id
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">LEFT&lt;/span> &lt;span style="color:#66d9ef">JOIN&lt;/span> stats s &lt;span style="color:#66d9ef">on&lt;/span> a.stat_id &lt;span style="color:#f92672">=&lt;/span> s.id
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">GROUP&lt;/span> &lt;span style="color:#66d9ef">BY&lt;/span> user_id;&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>Finding the optimal car to list on RelayRides</title><link>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</link><pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate><guid>/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/</guid><description>&lt;p>After discovering and browsing &lt;a href="https://relayrides.com/">RelayRides&lt;/a> I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.&lt;/p></description></item><item><title>Date range generation</title><link>/2015/05/30/date-range-generation/</link><pubDate>Sat, 30 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/30/date-range-generation/</guid><description>&lt;p>I finally had the chance to go back and add &lt;a href="https://dangoldin.github.io/js-tools/#tab-date-generation" target="_blank">another quick tool&lt;/a> to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges. By having it available via the web it makes it a lot easier to generate exactly what I need as well as provides the flexibility to keep on improving. If there are any improvements you’d like to see or if anything is unclear definitely let me know.&lt;/p></description></item><item><title>Dealing with a stripped screw</title><link>/2015/05/26/dealing-with-a-stripped-screw/</link><pubDate>Tue, 26 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/26/dealing-with-a-stripped-screw/</guid><description>&lt;p>Note that this is straying a bit far from my usual posts but I thought it would be helpful for anyone that’s had to deal with a stripped screw or a broken screw head. In my haste I used the wrong driver bit and completely stripped the screw head. It was deep enough that I wasn’t able to extract it using pliers while being so stripped that none of my screwdrivers had enough grip to finish screwing it in. After a bunch of failed ideas I finally stumbled unto a solution that worked and could have helped me over the years. The idea is to use a drill/driver but instead of using a bit in the head you tighten it around the stripped screw. Then when it’s tight around the screw you drive it in until it’s where you want it to be. The other option is to use this approach to get the screw out and replace it with a brand new one to make sure it’s able to removable in the future.&lt;/p></description></item><item><title>Google Chrome knows what's best for me</title><link>/2015/05/25/google-chrome-knows-whats-best-for-me/</link><pubDate>Mon, 25 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/25/google-chrome-knows-whats-best-for-me/</guid><description>&lt;p>Earlier today I wanted to check up on my electricity bill but ran into an issue trying to login to my PSEG account. Turns out that my nightly version of Google Chrome is preventing me from logging into their site since it has a poor HTTPS configuration. Instead of seeing the login page I get the following message: “Server has a weak ephemeral Diffie-Hellman public key”. Luckily for me this only happened in the nightly build and I was able to login using both the nightly version of Firefox and the standard version of Chrome.&lt;/p></description></item><item><title>The decline of niche tools</title><link>/2015/05/22/the-decline-of-niche-tools/</link><pubDate>Fri, 22 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/22/the-decline-of-niche-tools/</guid><description>&lt;p>I have a few sites that are “first stops” for specific use cases. I’ll go to Google Maps for directions, Foursquare for ideas of where to go, and Amazon whenever I need to buy something. They’re great most of the time but what’s interesting is what happens in the failure case. At that point my primary tool is no longer sufficient and I need to move on to secondary options. In these cases I tend to not have a well defined set of fallback options - for most of them I’ll fall back to the general case of using Google and then exploring from the search results. The only clear exception is Foursquare in which case I’ll go to Yelp before moving on to a general Google search. What’s surprising is that the fall back option usually leads to a successful outcome. Maybe I should switch my approach to start with the general search first and only move on to the specific tools when it fails. I wonder if we’re converging to a world run by fewer, smarter, and more powerful apps. &lt;a href="http://dangoldin.com/2013/07/21/beware-the-data-monopoly/" target="_blank">Data begets data&lt;/a> and as we supply more of it to the leaders we entrench their position, making it significantly harder for new companies to launch. We need regulation that enforces data mobility and allows people to export all the data that they’ve contributed and share it with whoever they’d like.&lt;/p></description></item><item><title>Experts, time, and quality</title><link>/2015/05/18/experts-time-and-quality/</link><pubDate>Mon, 18 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/18/experts-time-and-quality/</guid><description>&lt;p>We live in a world where it’s impractical to be a generalist so we specialize in a subset of skills and go to others for everything else. This works well but there’s still imperfect information - it’s tough to gauge someone’s skill level when you’re not an expert. In fact, when we lack awareness we end up using time spent as a proxy for skill when comparing across service providers. Imagine going to two barbers that charge the same amount for a haircut but one takes 10 minutes and the other takes 30 minutes. Even if we can’t tell the difference between the two haircuts we’d value the 30 minute one more due to the time difference. This seems backwards. The barber that was able to achieve the same result in 10 minutes is the more skilled one but instead we feel swindled when we back the price into an hourly rate. We should be willing to pay more for the 10 minute haircut since it gives us more time for our own pursuits. Yet when we lack knowledge we opt for the shortcut of equating time and skill. I’m trying to break this tendency by thinking about the end result rather than the effort and time involved. It’s interesting to compare this to software development. I know that just because someone spent more time on a project doesn’t mean it&amp;rsquo;s better than someone who knocked it out yet I still view other skilled professions from a “time equals quality” perspective. It makes you wonder whether professionals in other industries have a similar mindset where they realize time spent isn’t an indicator of quality in their own profession yet view it as a sign of quality in others.&lt;/p></description></item><item><title>Taking spontaneous notes</title><link>/2015/05/17/taking-spontaneous-notes/</link><pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/17/taking-spontaneous-notes/</guid><description>&lt;p>Many of my thoughts come serendipitously - whether it’s an errand I need to run, an idea for a blog post, or a feature I should build into one of my projects. But unless I’m able to jot it down soon after it slips my mind until I have another serendipitous thought to bring it back. It’s frustrating when I know I had something but can’t recall what the actual thought.&lt;/p></description></item><item><title>Mosh trumps shoddy internet</title><link>/2015/05/12/mosh-trumps-shoddy-internet/</link><pubDate>Tue, 12 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/12/mosh-trumps-shoddy-internet/</guid><description>&lt;p>Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was &lt;a href="https://mosh.mit.edu/" target="_blank">Mosh&lt;/a>. Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions. In fact, I can start a Mosh session on Friday afternoon before leaving the office for the weekend, let my computer go to sleep, and then have it automatically resume as soon as I get back to the office on Monday and wake my computer up. I’m still amazed at how well it works and only wish I discovered it sooner.&lt;/p></description></item><item><title>Driving before GPS</title><link>/2015/05/09/driving-before-gps/</link><pubDate>Sat, 09 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/09/driving-before-gps/</guid><description>&lt;p>I’ve been thinking about driving before GPS. I remember my family having an atlas in the backseat that we’d reference for long trips and actually map out our journey - which roads to take, which exits to get off of exits, and the distances involved. My clearest memory was constantly trying to figure out whether we missed an exit or not. The usual solution was to just pay attention for the next couple of minutes and try to use the signs along with the road atlas to figure out where you were on the map. Now, you just type in the destination on your smartphone as soon as you get the car and just start driving. Even if you make a mistake the directions automatically update to correct your course. The amount of time saved by GPS for every trip that no longer needs to be preplanned or adjusted enroute must be incredible. I suspect it’s also changed the type of trips we’re making - rather than going to the same old nearby spots that we know we can get to, we’re confident enough to go beyond that and discover something new, knowing that our phones will bail us out.&lt;/p></description></item><item><title>Computer show fliers from the mid 1990s</title><link>/2015/05/06/computer-show-fliers-from-the-mid-1990s/</link><pubDate>Wed, 06 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/06/computer-show-fliers-from-the-mid-1990s/</guid><description>&lt;p>While doing some spring cleaning I discovered a bunch of fliers from various computer shows I attended in the mid 90s. Bsed on the Windows 95 and Windows NT promotions I suspect this must have been in 1995 or 1996. What’s striking is how much better our computers are. It’s one thing to be abstractly aware of Moore’s Law but shocking to actually see it. The top of the line model in 1995 was $2,500 and came with a 4 GB hard drive, 64 MB of RAM, a 200 MHz processor, and a 33.6 kbps modem. Adjusting for inflation, this is equivalent to $3,700 in 2015 dollars. With that budget you can a top of the line computer with an order of magnitude more of everything and still have enough leftover for a smartphone which is also an order of magnitude more powerful than a computer from the mid 1990s.&lt;/p></description></item><item><title>A unique solution to every problem</title><link>/2015/05/02/a-unique-solution-to-every-problem/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>/2015/05/02/a-unique-solution-to-every-problem/</guid><description>&lt;p>A thought experiment I’ve had on my mind is this idea of a programming language that only has a single way of solving every programming problem. Regardless of the problem, multiple people writing code independently would end up with the same exact code. No such language actually exists (yet) but it’s fun to think about extreme cases in order to understand where we stand now. With this programming language the only differentiation between developers would be time since the end result would be the same. Beyond that, if there was always a unique solution to every problem this language would be able to write the code itself.&lt;/p></description></item><item><title>A few days with Google Contributor</title><link>/2015/04/28/a-few-days-with-google-contributor/</link><pubDate>Tue, 28 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/28/a-few-days-with-google-contributor/</guid><description>&lt;p>Near the end of last year, Google announced the &lt;a href="https://contributor.google.com" target="_blank">Contributor&lt;/a> program - a way to pay a monthly fee which would then be distributed across the websites you visit. In return, you’d start seeing fewer ads. Earlier this week I got off the waitlist and decided to give it a shot. The signup process was amazingly simple - choose a monthly dollar amount and you’re good to go. The effect is noticeable - on many sites I’ll see a blank spot where an ad should have been. The best part is being able to see how much I’ve contributed to the various sites I visit. Over the past couple of days I’ve spent a little over 60 cents removing 51 ads. An unforeseen effect is that I’m more aware of the content I consume and the sites I visit - seeing that some of my money is going towards shady sites makes me more conscious of my browsing behavior. I’m definitely curious to see where this approach goes.&lt;/p></description></item><item><title>AWS service limits</title><link>/2015/04/26/aws-service-limits/</link><pubDate>Sun, 26 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/26/aws-service-limits/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/aws-ec2-launch-failure.png" alt="AWS EC2 launch failure due to service limits" data-width="960" data-height="182" data-layout="responsive" />
&lt;/div>
&lt;p>Something I haven’t seen mentioned much is that AWS has &lt;a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html" target="_blank">service limits&lt;/a>. The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.&lt;/p></description></item><item><title>Adding columns in PostgreSQL and Redshift</title><link>/2015/04/23/adding-columns-in-postgresql-and-redshift/</link><pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/23/adding-columns-in-postgresql-and-redshift/</guid><description>&lt;p>A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.&lt;/p></description></item><item><title>Don't scrape into a Dropbox folder</title><link>/2015/04/19/dont-scrape-into-a-dropbox-folder/</link><pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/19/dont-scrape-into-a-dropbox-folder/</guid><description>&lt;p>Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.&lt;/p>
&lt;p>Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.&lt;/p></description></item><item><title>Dealing with an unresponsive Google CDN</title><link>/2015/04/12/dealing-with-an-unresponsive-google-cdn/</link><pubDate>Sun, 12 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/12/dealing-with-an-unresponsive-google-cdn/</guid><description>&lt;p>I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to &lt;a href="https://developers.google.com/speed/libraries/" target="_blank">Google&amp;rsquo;s CDN&lt;/a> were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.&lt;/p>
&lt;div class="thumbnail">
 &lt;img src="/image/slow-font-load.png" alt="Fonts taking 45 seconds to load" data-width="896" data-height="125" data-layout="responsive" />
&lt;/div>
&lt;p>After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.0.2351.3 canary (64-bit)) and even occured when visiting the URL directly. Neither standard Chrome, Firefox, Firefox nightly, nor a simple curl requested had this issue - it seemed to be a purely Chrome Canary issue.&lt;/p></description></item><item><title>The uncanny valley of advertising</title><link>/2015/04/12/the-uncanny-valley-of-advertising/</link><pubDate>Sun, 12 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/12/the-uncanny-valley-of-advertising/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/old-spice-uncanny-valley.gif" alt="Uncanny valley Old Spice ad" data-width="636" data-height="357" data-layout="responsive" />
 &lt;p>Old Spice ad mocking the uncanny valley (Credit: &lt;a href="http://gizmodo.com/new-old-spice-mandroid-ads-hilariously-embrace-the-unca-1604622781">Gizmodo&lt;/a>)&lt;/p>
&lt;/div>
&lt;p>The &lt;a href="http://en.wikipedia.org/wiki/Uncanny_valley" target="_blank">uncanny valley&lt;/a> is this idea that although we keep getting better at depicting people through technology, a few small kinks ruin everything and make people feel repulsed compared to an obvious imitation. Another way to explain it is that we’re a lot more comfortable with cartoon characters that are obviously fake than pseudo-realistic video game characters that look real but have non-human behavior or expressions.&lt;/p></description></item><item><title>Redshift meets Excel</title><link>/2015/04/06/redshift-meets-excel/</link><pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/06/redshift-meets-excel/</guid><description>&lt;p>As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it. We wrote a simple query to count the numbers of rows per day per hour in order to spot any gaps.&lt;/p></description></item><item><title>My tool setup</title><link>/2015/04/03/my-tool-setup/</link><pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate><guid>/2015/04/03/my-tool-setup/</guid><description>&lt;p>Great tools have the potential to make us significantly more productive and I wanted to share my existing setup. A huge part of that productivity is our comfort with our tools since over time we learn the shortcuts, understand the capabilities better, and develop processes to solve common problems. The challenge is that there is always a tool that might be better but the learning curve is too steep to warrant a time investment. Here&amp;rsquo;s what I have so far.&lt;/p></description></item><item><title>iOS first and username claiming</title><link>/2015/03/29/ios-first-and-username-claiming/</link><pubDate>Sun, 29 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/29/ios-first-and-username-claiming/</guid><description>&lt;p>Both Meerkat and Periscope launched on iOS first. That doesn’t bother me despite have an Android phone. They’re running a business and it’s up to them to decide where they want to invest the time they have. What bothers me is that I’ve been using a specific username across the various services, dangoldin, and now run the risk of losing it on these newer networks. A simple fix would be to at least allow me to preregister my username without requiring an iOS device. This would also encourage me, and a lot of other Android users, to download the app when it finally does make its way to Android.&lt;/p></description></item><item><title>Why login after resetting your password?</title><link>/2015/03/24/why-login-after-resetting-your-password/</link><pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/24/why-login-after-resetting-your-password/</guid><description>&lt;p>I can’t figure out why nearly every website forces you to login after resetting your password. It’s an extra step that adds nothing to security and introduces friction into the experience. The fact that I just entered my password into a form field should be enough to trigger the authentication flow and get me back into the app. The only reasons I can think of that it’s a way to confirm that the person actually remembers their new password or that the functionality just hasn’t been built. The former case doesn’t make sense - the fact that they forgot their password indicates they rarely use the site and will just forget it again by their next login attempt. It’s easier to just give them the immediate access and have them reset their password later. An even better approach would be to just have them enter the same password twice to make sure they match. The latter reason is just sloth - the engineering effort would be minimal and it would improve the experience and mood of the users who are already frustrated after multiple failed login attempts.&lt;/p></description></item><item><title>The sharing economy and human behavior</title><link>/2015/03/21/the-sharing-economy-and-human-behavior/</link><pubDate>Sat, 21 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/21/the-sharing-economy-and-human-behavior/</guid><description>&lt;p>The sharing/rental economy is getting stronger and stronger and will have a massive impact on societies - especially cities. One thing that’s been on my mind is how it will fit into human behavior and biases. We’re so used to making infrequent or one time payments and then owning something that moving to a rental or sharing model might be difficult. For example, I don’t own a car and mostly rely on a combination of public transportation and CitIBike to get around. The rare times I need a car I’ll use either Zipcar, Hertz 24/7, Lyft or Uber depending on my exact situation yet each time I make the decision I can’t help but think about the cost. I realize that in the grand scheme of things it’s much cheaper than owning a car but during the moment itself it’s draining. It’s similar to the of unbundling TV - it’s much cheaper to just pay a dollar per episode to watch a TV show than pay more than $100 each month for cable but do people actually want to be thinking about spending the dollar each time? I suspect most would rather pay the premium for the entire bundle and the option of watching anything instead of feeling as if they’re being nickeled and dimed. I have no idea whether this is innate in human behavior or something that we’ve just grown accustomed to. I suspect it’s the latter - there are countless items we pay for individually and don’t think twice about it. What will make the sharing economy universal is when we start treating the majority of our purchases as per use rather than a lifetime subscription.&lt;/p></description></item><item><title>Power of engineering standards</title><link>/2015/03/18/power-of-engineering-standards/</link><pubDate>Wed, 18 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/18/power-of-engineering-standards/</guid><description>&lt;p>When it comes to productive coding, one of the most important things to do is to impose a set of standards and conventions. As long as you stick with them your code becomes significantly easier to write and maintain. Conventions range from having a standard way of declaring variables to the way files are organized within a project to the field names in database tables. The obvious benefit is that your code becomes significantly easier to navigate, both to you as well as to others on the team, since you don’t have to run through a series of searches trying to figure out whether a variable is called myVariable, MyVariable, or my_variable. The bigger impact is how much simpler your code becomes. By using a standard structure it’s possible to write code that’s further up in the abstraction hierarchy. This is a huge win for productivity and quality since &lt;a href="http://www.coverity.com/press-releases/annual-coverity-scan-report-finds-open-source-and-proprietary-software-quality-better-than-industry-average-for-second-consecutive-year/" target="_blank">more code leads to more errors&lt;/a> and the best code is code that’s not written in the first place.&lt;/p></description></item><item><title>Learning to spell again</title><link>/2015/03/15/learning-to-spell-again/</link><pubDate>Sun, 15 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/15/learning-to-spell-again/</guid><description>&lt;p>I recently discovered that I’m a much worse speller than I used to be. The culprit is obvious - computers make it too easy to correct our mistakes. In school when making mistakes we’d have to rewrite each word until it became ingrained but these days all we do is just click on the suggested fix without a second thought. There’s nothing there to help me retain the mistake so I continue making it.&lt;/p></description></item><item><title>An ode to Pi</title><link>/2015/03/14/an-ode-to-pi/</link><pubDate>Sat, 14 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/14/an-ode-to-pi/</guid><description>&lt;p>Since it’s Pi Day (at least in the US)I decided to jump on the bandwagon and contribute my own thoughts. Pi is fascinating. It’s such a simple definition - the ratio of a circle’s circumference to it’s diameter - yet it’s both irrational and transcendental and impossible to actually express as a simple number. People have been trying to get more accurate estimates for multiple millennia with multiple great mathematicians trying to derive their own approximation.&lt;/p></description></item><item><title>Strongly typed, static language tools</title><link>/2015/03/10/strongly-typed-static-language-tools/</link><pubDate>Tue, 10 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/10/strongly-typed-static-language-tools/</guid><description>&lt;p>Good tools are essential for developer productivity. Imagine how tough it would be to write code in an editor that didn’t have any of the features we use on a daily basis - syntax highlighting, smart spacing, shortcut keys, auto completion, etc. It takes time to get used to all the tools available but once we’re familiar with them we’re orders of magnitude more productive.&lt;/p>
&lt;p>For the past year my primary language has been Java although I’ve gotten to do a fair amount of JavaScript, Python, and PHP as well. As great as Sublime is, I’m much more productive in Eclipse. It has nothing to do with the editor and everything to do with the language. Eclipse is able to provide a lot more functionality due to Java’s static, strongly type nature. Some examples are being able to quickly rename variables and methods, move packages, and quickly identify dumb mistakes in method signatures and typos. I suspect similar tools exist for weakly typed or dynamic languages but I can’t imagine them working as well as they do in Eclipse. Strongly typed and static languages are able to get rid of an entire class of errors that are common with scripting languages - typos, forgetting to add an argument to a method call, messing up a type - that the time saved typing gets replaced with the time spent debugging. For many tasks this tradeoff makes sense but larger projects that involve multiple developers and require higher performance would benefit from moving to a strongly typed, static language.&lt;/p></description></item><item><title>Computer literacy for protection and productivity</title><link>/2015/03/08/computer-literacy-for-protection-and-productivity/</link><pubDate>Sun, 08 Mar 2015 00:00:00 +0000</pubDate><guid>/2015/03/08/computer-literacy-for-protection-and-productivity/</guid><description>&lt;p>Earlier today I came across another reason why basic computer literacy is a necessary skill. It’s not just about knowing to code or understanding how computers work but getting access to a slew of tools that are orders of magnitude better than what you’d find on a sketchy site.&lt;/p>
&lt;div class="thumbnail">
 &lt;img src="/image/github-youtube-dl-search.png" alt="Github search for youtube download" data-width="1038" data-height="650" data-layout="responsive" />
&lt;/div>
&lt;p>My realization came when I was trying to download some YouTube videos that could be watched without internet access. Doing a quick Google search I found dozens of sites with each one trying to force me to download some additional software to “speed up” my experience. I’m positive most, if not all, of these would fall into the malware category so I decided instead to do a simple search on GitHub for “youtube download.” Lo and behold the first result was the wonderful &lt;a href="https://github.com/rg3/youtube-dl" target="_blank">youtube-dl&lt;/a> library. Within two minutes I kicked off a script that proceeded to download a dozen videos.&lt;/p></description></item><item><title>Getting the most out of log4j</title><link>/2015/02/28/getting-the-most-out-of-log4j/</link><pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/28/getting-the-most-out-of-log4j/</guid><description>&lt;p>Something that’s incredibly helpful when writing Java code is customizing &lt;a href="http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html" target="_blank">log4j&lt;/a>. There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.&lt;/p>
&lt;p>One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.&lt;/p></description></item><item><title>Lists and localStorage</title><link>/2015/02/26/lists-and-localstorage/</link><pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/26/lists-and-localstorage/</guid><description>&lt;p>I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.&lt;/p></description></item><item><title>In praise of the full stack developer</title><link>/2015/02/23/in-praise-of-the-full-stack-developer/</link><pubDate>Mon, 23 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/23/in-praise-of-the-full-stack-developer/</guid><description>&lt;p>I’m a pretty new engineering manager but a philosophy I’ve adopted is to try to have everyone on the team be as full stack as possible. Everyone has their strengths and weaknesses but being able to grasp the entire stack improves code quality and reduces disruption. And it goes beyond technology and into the business and user world too. Understanding how these various components fit together allow you to make smarter decisions and provide the tools to test and verify your code. The other big benefit is that you’re not waiting on anyone and avoid having your flow disrupted by others.&lt;/p></description></item><item><title>Lessons from Node</title><link>/2015/02/22/lessons-from-node/</link><pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/22/lessons-from-node/</guid><description>&lt;p>I&amp;rsquo;ve decided to move on from &lt;a href="http://nodejs.org/" target="_blank">Node&lt;/a> after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.&lt;/p>
&lt;ul>
&lt;li>Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write.&lt;/li>
&lt;li>It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code.&lt;/li>
&lt;li>JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want.&lt;/li>
&lt;li>Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests.&lt;/li>
&lt;li>Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.&lt;/li>
&lt;/ul>
&lt;p>I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.&lt;/p></description></item><item><title>Smarterphones</title><link>/2015/02/16/smarterphones/</link><pubDate>Mon, 16 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/16/smarterphones/</guid><description>&lt;div class="right10">
 &lt;img src="/image/google-where-you-at.png" alt="AWS EC2 launch failure due to service limits" data-width="894" data-height="444" data-layout="responsive" />
&lt;/div>
&lt;p>Yesterday I got a reminder of how deep smartphones and tech companies have gotten into our lives. After spending a day volunteering at the &lt;a href="http://www.c4q.nyc/">C4Q&lt;/a> office, I got a text from my wife asking me where I was. When I opened the Hangouts app I saw an option to share my current location. This is the first time I’ve seen a contextual behavior in Hangouts. I’ve seen it before in other apps - a Gmail alert telling me I forgot to attach a file when the text has “please find attached” or Google Calendar defaulting to a weekly repetition if I put “weekly” in the meeting title - but this is the first time I’ve seen it happen in Hangouts. Basic contextual behavior is relatively simple to support and can just require a simple word search but it has incredible potential as more and more data gets collected. Our smartphones are with us wherever we go collecting data each step of the way. Right now the behavior is formulaic and standardized but soon enough our phones will act as personal assistants - keeping track of everything in our calendars while understanding everything we have going on. This has the potential to drastically simplify our lives but we may be making a Faustian bargain in the process.&lt;/p></description></item><item><title>Learning Scala</title><link>/2015/02/13/learning-scala/</link><pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/13/learning-scala/</guid><description>&lt;p>Over the past week I&amp;rsquo;ve been learning Scala. The initial motivation was that our API code is in PHP and in dire need of a rewrite. And since we&amp;rsquo;ve been rewriting our other critical applications in Java we want to leverage the JVM as much as possible. At the same time, we want to keep the code simple, expressive, and maintable while being fun and easy to write. I&amp;rsquo;ve heard great things about Scala so decided to give it a shot.&lt;/p></description></item><item><title>URL redirection app</title><link>/2015/02/07/url-redirection-app/</link><pubDate>Sat, 07 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/07/url-redirection-app/</guid><description>&lt;p>At &lt;a href="http://www.triplelift.com" target="_blank">TripleLift&lt;/a>, we’re big fans of the &lt;a href="https://chrome.google.com/webstore/detail/switcheroo-redirector/cnmciclhnghalnpfhhleggldniplelbg?hl=en" target="_blank">Switcheroo&lt;/a> plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.js. Unfortunately, it’s only available for Chrome which makes it more difficult to run browser specifics tests on other browsers.&lt;/p></description></item><item><title>MySQL vs PostgreSQL sort order</title><link>/2015/02/01/mysql-vs-postgresql-sort-order/</link><pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate><guid>/2015/02/01/mysql-vs-postgresql-sort-order/</guid><description>&lt;p>At &lt;a href="http://triplelift.com" target="_blank">TripleLift&lt;/a>, we have a migrations job that copies aggregate data from Redshift to MySQL so it can be accessed along the rest of the transactional data. As part of a test, I tried comparing that the data matched exactly but ran into an issue when exporting the data to select. Namely, to make the comparison as simple as possible I wanted to run the same select query in both tables and compare the results. Unfortunately, the sort order between MySQL and PostgreSQL (what Redshift is based on) acts differently for text fields. PostgreSQL takes case into account while MySQL does not. This has an especially weird results when you have values that contain characters with an ASCII code between the lower and upper case letters: []^-`. It took some research but I discovered that MySQL provides an option to do a case sensitive sort - just add a “BINARY” option before the field name.&lt;/p></description></item><item><title>Migrating a simple HTTP application on AWS</title><link>/2015/01/29/migrating-a-simple-http-application-on-aws/</link><pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/29/migrating-a-simple-http-application-on-aws/</guid><description>&lt;p>A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.&lt;/p></description></item><item><title>IBM's rumored layoff</title><link>/2015/01/26/ibms-rumored-layoff/</link><pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/26/ibms-rumored-layoff/</guid><description>&lt;p>Given the &lt;a href="http://www.itworld.com/article/2875112/ibm-is-about-to-get-hit-with-a-massive-reorg-and-layoffs.html" target="_blank">rumor&lt;/a> of the massive IBM layoffs I decided to pull some others and see how it compared. Surprisingly, the next highest was also at IBM - but in 1993. On one hand, it&amp;rsquo;s odd to see this pattern as if they&amp;rsquo;ve learned nothing. On the other, it&amp;rsquo;s a sign that they acknowledge the problem and are willing to adapt. Since the 1993 layoff IBM&amp;rsquo;s stock price increased over 950% and this round may provide another burst.&lt;/p></description></item><item><title>The changing fidelity of the past</title><link>/2015/01/25/the-changing-fidelity-of-the-past/</link><pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/25/the-changing-fidelity-of-the-past/</guid><description>&lt;p>A fun exercise is picking a random day in the past and trying to recreate it using the various tools at our disposal. In my case the most useful ones are my calendars, both personal and work, the photos I took, and Foursquare/Swarm. As long as I was vigilant in documenting the events it’s simple to figure out what I did. We lose a bit of the mystery when we document our lives and we no longer have long discussions trying to recreate events with friends. I don’t know whether this is better or worse but we’ll probably see more and more of this happening. Our phones are already collecting our location and video is becoming increasingly popular. Now we have high fidelity versions of recent events but only vague memories of our childhood. I wonder whether kids that are growing up now will have access to accurate memories of their childhood when they grow up and what the impact will be.&lt;/p></description></item><item><title>Don't trust client side data</title><link>/2015/01/24/dont-trust-client-side-data/</link><pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/24/dont-trust-client-side-data/</guid><description>&lt;p>At &lt;a href="http://triplelift.com/" target="_blank">TripleLift&lt;/a>, we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.&lt;/p></description></item><item><title>Fun with GitHub's map tools</title><link>/2015/01/18/fun-with-githubs-map-tools/</link><pubDate>Sun, 18 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/18/fun-with-githubs-map-tools/</guid><description>&lt;p>After discovering &lt;a href="https://github.com/blog/1772-diffable-more-customizable-maps" target="_blank">GitHub&amp;rsquo;s map visualization&lt;/a> feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short &lt;a href="https://github.com/dangoldin/map-fun" target="_blank">Python script&lt;/a> later and I was able to convert the GPX data into &lt;a href="http://geojson.org/geojson-spec.html" target="_blank">geojson&lt;/a>. The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize. My 232 runs contained 162,071 latitude/longitude pairs which turned into a 4MB file - not massive but large enough for GitHub to refuse to visualize it. The simplest solution was to generate multiple files but that made it impossible to see all my runs on a single map. The other solution was to see if converting to topojson would reduce the file size. That helped but I wasn&amp;rsquo;t able to find the right balance between compression and quality and ended up with a hybrid approach - two files, one per running year, each in topojson.&lt;/p></description></item><item><title>Take the first step</title><link>/2015/01/17/take-the-first-step/</link><pubDate>Sat, 17 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/17/take-the-first-step/</guid><description>&lt;p>Numerous times I’ve procrastinated on doing something convincing myself that it would either take too long or just wasn’t worth doing but more often than not when I finally take the first step I’m able to quickly complete the task. I don’t know why our minds encourage procrastination but I suspect it’s not just me. I’ve been combating this tendency by recognizing that it’s happening and forcing myself to just do something, as simple as it is. A small task typically turns into a series of small tasks in which I’m able to make a significant amount of progress. In fact, there have been numerous times where I’ve even been able to achieve “flow” - despite being hesitant in the first place. It doesn’t seem like much but 10 minutes here and there do add up. Whether it’s coding up a simple feature, doing a quick data analysis, or just jotting down a few ideas, it’s infinitely more valuable than staring at a phone.&lt;/p></description></item><item><title>2014 stats</title><link>/2015/01/11/2014-stats/</link><pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/11/2014-stats/</guid><description>&lt;p>At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a &lt;a href="https://gist.github.com/dangoldin/14906d4f863cd83f3008" target="_blank">simple Python script&lt;/a>. For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.&lt;/p></description></item><item><title>Limited language codebases</title><link>/2015/01/09/limited-language-codebases/</link><pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/09/limited-language-codebases/</guid><description>&lt;p>I’ve discussed the pros and cons of having a codebase out of a few languages versus having the choice made per project or application with a bunch of people and opinions differ. On one hand, having many languages provides flexibility in choosing the right language for the job and allows engineers to learn and explore new tools. Better habits are encouraged since the interface between components requires a well structured and tested structure rather than relying on code similarity. On the other, it prevents code and component reuse and makes it difficult for teams to standardize around a style and codebase. Also, engineers can’t switch projects as easily as they’d be able to under a common language and prevents the depth of knowledge one gets from working on a shared codebase with others.&lt;/p></description></item><item><title>Some quick Twitter analytics analysis</title><link>/2015/01/06/some-quick-twitter-analytics-analysis/</link><pubDate>Tue, 06 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/06/some-quick-twitter-analytics-analysis/</guid><description>&lt;p>I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.&lt;/p></description></item><item><title>Introducing jsonify.me</title><link>/2015/01/04/introducing-jsonify.me/</link><pubDate>Sun, 04 Jan 2015 00:00:00 +0000</pubDate><guid>/2015/01/04/introducing-jsonify.me/</guid><description>&lt;p>Over the past few weeks I’ve been experimenting with Node.js and wanted to share the project I’ve been working on, &lt;a href="http://jsonify.me/" target="_blank">jsonify.me&lt;/a>. It’s an “API only” product without an interface other than a &lt;a href="http://jsonify.me/" target="_blank">documentation page&lt;/a>. The idea is to allow anyone to have a publicly accessible URL endpoint that can contain whatever information they want as long as it can be stored as a JSON object. In my case, I have all sorts of semi-structured data that I want to make accessible and keeping it under my domain (json.dangoldin.com) makes it easy to access for whoever is savvy enough to figure it out.&lt;/p></description></item><item><title>Redirect recursion</title><link>/2014/12/31/redirect-recursion/</link><pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/31/redirect-recursion/</guid><description>&lt;p>I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.&lt;/p></description></item><item><title>A new Turing test</title><link>/2014/12/29/a-new-turing-test/</link><pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/29/a-new-turing-test/</guid><description>&lt;p>A friend sent me &lt;a href="http://www.bloombergview.com/articles/2014-06-09/fake-victory-for-artificial-intelligence" target="_blank">an article&lt;/a> where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.&lt;/p></description></item><item><title>DevOps for the rest of us</title><link>/2014/12/26/devops-for-the-rest-of-us/</link><pubDate>Fri, 26 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/26/devops-for-the-rest-of-us/</guid><description>&lt;p>I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen. This is especially important when working at a small company where there’s immense value in having general skills that can be used to make progress independently without need to disrupt others. My first job writing code I only had to worry about my small patch of software but over time I’ve slowly picked up a variety of DevOps skills that help me write better code. Below are the skills that every software engineer should know - they may not all fall under traditional DevOps but I believe they’re essential for anyone writing code. If you have any others let me know and I’ll add them to the list.&lt;/p></description></item><item><title>Passive technology and the decline of privacy</title><link>/2014/12/20/passive-technology-and-the-decline-of-privacy/</link><pubDate>Sat, 20 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/20/passive-technology-and-the-decline-of-privacy/</guid><description>&lt;p>I just discovered that Google launched a new &lt;a href="http://searchengineland.com/google-store-visits-estimated-conversions-metric-adwords-211254" target="_blank">AdWords feature&lt;/a> to help brick and mortar store owners track the effect their online spending is having in the offline world. The way it works is that if a user sees an ad for a particular store or product on their phone and then ends up close (based on the location sharing option in iOS and Android) to the store in question, Google will use that information as a signal that the ad was the cause of the store visit. It’s not supposed to be perfectly accurate but the idea is that with enough data Google can come up with models that can estimate the actual numbers.&lt;/p></description></item><item><title>Amazon's Fire TV Stick</title><link>/2014/12/18/amazons-fire-tv-stick/</link><pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/18/amazons-fire-tv-stick/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/firestick.jpg" alt="Fire stick" data-width="475" data-height="370" data-layout="responsive" />
 &lt;p class="caption">Photo by &lt;a href="http://www.ranchodelaluna.com/views/pages/09_firestick_dancing.htm">Rancho de la Luna&lt;/a>&lt;/p>
&lt;/div>
&lt;p>Earlier this week I set up Amazon’s &lt;a href="http://www.amazon.com/Amazon-W87CUN-Fire-TV-Stick/dp/B00GDQ0RMG">Fire TV Stick&lt;/a> and wanted to jot down some thoughts while they’re still fresh. For the $20 promotion price, it’s a great deal. My alternative was an Xbox 360 along with a Raspberry Pi running XBMC. The Xbox would be used for streaming shows on Netflix and watching older DVDs while the Raspberry Pi would let me watch various files off of a USB stick. I’ve never tried a Chromecast so don’t know how the Fire Stick compares but so far it’s been much quicker to startup and navigate than either the Xbox or the Raspberry Pi. When all you want to do is watch a quick show during dinner it’s a bit frustrating when you’re done eating by the time the Netflix app is ready to use on the Xbox.&lt;/p></description></item><item><title>Why log me out?</title><link>/2014/12/16/why-log-me-out/</link><pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/16/why-log-me-out/</guid><description>&lt;p>Now that fantasy football season is over for me and I have no risk of angering the fantasy football gods I can complain about an interface decision in the Yahoo Fantasy Football Android app. Every once in a while the app will sign me out, which I suspect is a security feature, but I can log back in without having to re-enter a password. The only effect this “feature” has is getting me annoyed. The app has clean and simple visual design but that shouldn’t be prioritized over actual usability. Hopping between apps is such a common task that developers should strive to make it as painless as possible. This may involve changing the views around to make them more light weight or figuring out a way to simulate behavior without having to show a loading screen but it definitely makes the app feel snappier and more responsive.&lt;/p></description></item><item><title>Waiting for the aha moment</title><link>/2014/12/14/waiting-for-the-aha-moment/</link><pubDate>Sun, 14 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/14/waiting-for-the-aha-moment/</guid><description>&lt;p>Something I’ve encountered is being stuck on a difficult problem but then taking a break until an “aha moment” just materializes. This happened throughout college on difficult problem sets as well as countless engineering projects at work. Sometimes instead of getting hung up on a tough problem the best thing to do is to forget about it and go for a walk and let the subconscious take over. I don’t know why this works but it seems to be common with others as well.&lt;/p></description></item><item><title>Travel to appreciate technological growth</title><link>/2014/12/13/travel-to-appreciate-technological-growth/</link><pubDate>Sat, 13 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/13/travel-to-appreciate-technological-growth/</guid><description>&lt;p>If you’re constantly watching something grow it’s hard to notice the magnitude while those further away see it immediately. This is well known for parents not seeing how quickly their children are growing but obvious for distant relatives and friends who get a glimpse once every few months.&lt;/p>
&lt;p>I have the same relationship with technology. I’m surrounded by it each day that it’s hard to tell how much it’s changed but a way to combat this bias is by traveling, especially to developing countries.&lt;/p></description></item><item><title>Piketty and the power of data</title><link>/2014/12/09/piketty-and-the-power-of-data/</link><pubDate>Tue, 09 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/09/piketty-and-the-power-of-data/</guid><description>&lt;p>I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.&lt;/p></description></item><item><title>Internetless coding</title><link>/2014/12/07/internetless-coding/</link><pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/07/internetless-coding/</guid><description>&lt;p>Whenever I fly I try to be at least somewhat productive. This time it entailed finishing up an old blog post and messing around with Node.js on a side project. They say the only way to appreciate something is when it’s gone and that’s how I feel about developing without internet access. It’s such a common occurrence to need to look up the documentation for a particular function or library or search for novel error messages that my approach is completely altered without the internet. Where before a few visits to Google or Stack Overflow would have taken care of the problem now I get to rely on man pages as well as dozens small experiments to figure out what’s happening.&lt;/p></description></item><item><title>Symptom based monitoring</title><link>/2014/12/01/symptom-based-monitoring/</link><pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate><guid>/2014/12/01/symptom-based-monitoring/</guid><description>&lt;p>A month or so ago I read Rob Ewaschuk’s &lt;a href="https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/preview?sle=true" target="_blank">philosophy on alerting&lt;/a> and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level. It may take longer to diagnose what the underlying problem is but it will reduce the total number of alerts required.&lt;/p></description></item><item><title>AWS: Always shipping</title><link>/2014/11/24/aws-always-shipping/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/24/aws-always-shipping/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/aws-services-new.png" alt="AWS services" data-width="1027" data-height="694" data-layout="responsive" />
&lt;/div>
&lt;p>I never thought about this until &lt;a href="https://twitter.com/szach">Shaun&lt;/a> brought it up but now I see it all the time: AWS ships code quicker than any other company that size. Some are simple feature improvements to existing products, such as an advanced instance search, additional configuration options for ELBs, and new instance types, but others are entirely new products, such as Lambda, EC2 Container Service, and the newly announced code management suite.&lt;/p></description></item><item><title>Fab's fall from grace and the difficulty of startups</title><link>/2014/11/22/fabs-fall-from-grace-and-the-difficulty-of-startups/</link><pubDate>Sat, 22 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/22/fabs-fall-from-grace-and-the-difficulty-of-startups/</guid><description>&lt;p>The recent news that PCH is set to acquire Fab reiterates how difficult startups are. So many startups strive to get an investment and believe that once they raise a round everything will get easier. That’s when things get difficult. Instead of focusing on achieving product market fit you start worrying about market share, competition, company culture, recruiting, process, which require a completely different skillset than what you started with. And to add to that you’re now accountable to a growing list of employees, shareholders, and customers. When I was working on my first startup I really thought that being able to get funding was the measure of success. Now I realize how naive that view was and how much more there actually is. Fab raised over $330M and wasn’t able to grow into a successful business despite undertaking massive pivots. There are probably thousands of founders claiming that they’d be able to succeed with that kind of money without realizing how difficult it actually is.&lt;/p></description></item><item><title>Paying publishers without ads</title><link>/2014/11/20/paying-publishers-without-ads/</link><pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/20/paying-publishers-without-ads/</guid><description>&lt;p>Google recently launched a program, &lt;a href="https://www.google.com/contributor/welcome/" target="_blank">Contributor&lt;/a>, that offers an ad-free monetization model to publishers. The idea is that a user pays Google up to $3 a month and in return Google will not show that user any display ads on a website that’s a participant in the program. The monthly payment will then be distributed across the participating sites - most likely based on how many times you’ve visited that site.&lt;/p></description></item><item><title>Backendless applications</title><link>/2014/11/18/backendless-applications/</link><pubDate>Tue, 18 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/18/backendless-applications/</guid><description>&lt;p>I’ve been a fan of GitHub pages ever since I started using them to host my blog a couple of years ago and a thought that’s been constantly popping up is why there haven’t been any products or services that help small businesses host their sites on GitHub. GitHub’s &lt;a href="https://help.github.com/articles/github-terms-of-service/" target="_blank">terms of service&lt;/a> forbid a third party from hosting pages on behalf of customers but it doesn’t seem as if there’s anything stopping someone from building a tool or documenting the set of steps to help someone create a simple site and have it hosted on GitHub. That way the business only has to pay a domain registration fee while still getting fast and robust hosting with a fairly solid CMS.&lt;/p></description></item><item><title>Productivity optimization</title><link>/2014/11/13/productivity-optimization/</link><pubDate>Thu, 13 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/13/productivity-optimization/</guid><description>&lt;p>A pretty trivial post but something I’ve been doing for a while now is keeping my dock as a vertical bar on the right of my screen. I started doing this years ago when I was working on Windows and it was too difficult to track every single program that was running. At that point I was in finance and would have a dozen Excel workbooks open and needed to be able to quickly switch between them. The only way I could do this with a bottom toolbar was by making it extremely thick which would take up too much space. Moving it to the side solved that problem and I stuck with it as I moved to Ubuntu and now OS X.&lt;/p></description></item><item><title>Some simple AWS tools</title><link>/2014/11/09/some-simple-aws-tools/</link><pubDate>Sun, 09 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/09/some-simple-aws-tools/</guid><description>&lt;p>Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.&lt;/p></description></item><item><title>Luxury for cheap</title><link>/2014/11/05/luxury-for-cheap/</link><pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/05/luxury-for-cheap/</guid><description>&lt;p>I recently read a FiveThirtyEight article on the &lt;a href="http://fivethirtyeight.com/features/you-see-sneakers-these-guys-see-hundreds-of-millions-in-resale-profit/" target="_blank">sneaker resale market&lt;/a>. The concept is extremely foreign to me since I tend to not collect anything other than old notes and have a tendency of grossly mistreating my shoes and clothes. Nonetheless, I found it fascinating as it discusses the incentives of the various parties involved and comparing them against standard economic theory. One passage in particular was so insightful that I had to save it:&lt;/p></description></item><item><title>Tools as tools, not products</title><link>/2014/11/01/tools-as-tools-not-products/</link><pubDate>Sat, 01 Nov 2014 00:00:00 +0000</pubDate><guid>/2014/11/01/tools-as-tools-not-products/</guid><description>&lt;p>I keep on discovering new use cases for &lt;a href="http://daringfireball.net/projects/markdown/syntax" target="_blank">Markdown&lt;/a> the more I use it. My first exposure was when I migrated my blog to GitHub pages from Wordpress and Tumblr. Since then I’ve discovered GitHub flavored markdown which supports syntax highlighting which has been amazingly useful when blogging on tech topics or putting together notes for a tech talk. Just recently I wanted to include some MySQL snippets in a Keynote presentation and discovered the &lt;a href="http://remarkjs.com/#1" target="_blank">Remark.js&lt;/a> library which lets you generate in-browser slideshows in Markdown with syntax highlighting.&lt;/p></description></item><item><title>Libraries</title><link>/2014/10/28/libraries/</link><pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/28/libraries/</guid><description>&lt;img src="/image/cornell-ad-white.jpg" alt="A.D. White Reading Room @ Cornell University" data-width="700" data-height="466" data-layout="intrinsic" />
&lt;p class="caption">&lt;a href="http://www.flickr.com/photos/eflon/2874341070/in/set-72157607982619613">A.D White Library @ Cornell University by eflon&lt;/a>&lt;/p>
&lt;p>For the first time in almost a decade I checked out a book from a library. I don’t know why I ever stopped - the experience is extremely simple and you’re able to read a book for free. I had a book on my Amazon wishlist for a couple of weeks that I held off on buying but was able to read it over the past week after a quick visit to the library.&lt;/p></description></item><item><title>Lessons learned teaching a MySQL class</title><link>/2014/10/27/lessons-learned-teaching-a-mysql-class/</link><pubDate>Mon, 27 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/27/lessons-learned-teaching-a-mysql-class/</guid><description>&lt;p>Last Thursday was the last lesson of the four part Introduction to MySQL class I’ve been teaching at &lt;a href="http://www.c4q.nyc/" target="_blank">Coalition for Queens&lt;/a> and I wanted to summarize my thoughts while they’re still fresh.&lt;/p>
&lt;p>The diversity of the class was amazing and shows how useful affordable technology programs are. You get a mix of people from different backgrounds and different ages that all want to improve themselves and can all contribute in their own ways. Everyone has a unique experience and introducing technology into it may open up new opportunities.&lt;/p></description></item><item><title>Spam blog comments</title><link>/2014/10/26/spam-blog-comments/</link><pubDate>Sun, 26 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/26/spam-blog-comments/</guid><description>&lt;p>Ever since I’ve started blogging I’ve been getting around one spam blog comment a week. &lt;a href="https://disqus.com/" target="_blank">Disqus&lt;/a> does a nearly perfect job of flagging them so I don’t understand the motivation behind it. They’re obviously spam and my readers are suave enough to never click on any of the links. There’s also little, possibly none, SEO value since they’re all loaded asynchronously and every link has a rel=”nofollow” property. And if the goal is to spark a discussiong and raise awareness they&amp;rsquo;re so poorly worded that no reader will take them seriously. The only thing I can think of is that these companies pay a third party service to grace tangentially related blogs with content on their behalf and these third party services go the cheapest possible route in both effort and quality.&lt;/p></description></item><item><title>My first open source contribution</title><link>/2014/10/19/my-first-open-source-contribution/</link><pubDate>Sun, 19 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/19/my-first-open-source-contribution/</guid><description>&lt;p>Despite being a huge proponent of open source I’ve never made a contribution to a &lt;a href="https://github.com/whymarrh/jeopardy-parser" target="_blank">third party project&lt;/a> until this weekend. The project was a simple scraper that downloads each Jeopardy game from &lt;a href="http://j-archive.com/" target="_blank">j-archive.com&lt;/a>, parses the data, and loads into a SQLite database. The project had one issue open that was to make the download code threaded in order to reduce the time of downloading nearly 4700 games from over 7 hours to less than 30 minutes. I gave this a stab on Saturday and submitted a pull request that was merged in by the author on Sunday.&lt;/p></description></item><item><title>Link bait titles are a race to zero</title><link>/2014/10/18/link-bait-titles-are-a-race-to-zero/</link><pubDate>Sat, 18 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/18/link-bait-titles-are-a-race-to-zero/</guid><description>&lt;p>A couple of days ago I saw a mic.com article with the title “A European country is now offering free college education to Americans” but the only way to find out which country this was (Germany) was by clicking through to the actual page.&lt;/p>
&lt;img src="/image/mic-link-bait.png" alt="Mic link bait title" data-width="1458" data-height="868" data-layout="responsive" />
&lt;p>I understand that content sites make the bulk of their revenue through advertising but resorting to a link-bait approach seems like a terrible idea. It’s a shortsighted attempt that increases page views at the cost of insulting your audience and cheapening your effort that will not work as a sustainable strategy. Relying on headlines to generate traffic without any meaningful content is a great way to get to become a commodity. I hope that there are enough people out there that care about the content they’re producing and have a passionate audience that can be monetized based on quality of engagement rather than on quantity of page views. Otherwise we’ll all end up in a race to zero.&lt;/p></description></item><item><title>AWS Glacier</title><link>/2014/10/12/aws-glacier/</link><pubDate>Sun, 12 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/12/aws-glacier/</guid><description>&lt;p>I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click. Many of them are unique in the world and only exist on an old hard drive. I realize it’s foolish to keep them around but it’s tough to let go.&lt;/p></description></item><item><title>Clearing my head</title><link>/2014/10/08/clearing-my-head/</link><pubDate>Wed, 08 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/08/clearing-my-head/</guid><description>&lt;p>This weekend I felt discouraged after getting cold but dragged myself outside to go for a walk and clear my head. I ended up getting a cup of coffee and just sat on a bench for 30 minutes and letting random thoughts go in and out of my head. It turned into a form of personal meditation where some ideas crystallized and stuck around while others quickly disappeared. It felt great (so great that I ended up going back to my apartment and doing a bunch of chores I’ve been putting of!), especially considering that I had to drag myself away from the couch to even go for the walk and would have much preferred to just sit on the couch and watch football.&lt;/p></description></item><item><title>XKCD movie quotes by Android</title><link>/2014/10/04/xkcd-movie-quotes-by-android/</link><pubDate>Sat, 04 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/04/xkcd-movie-quotes-by-android/</guid><description>&lt;p>Earlier this week XKCD featured a &lt;a href="https://xkcd.com/1427/" target="_blank">comic&lt;/a> where oft-quoted movie quotes are autocompleted by iOS keyboard predictions. I decided to do replicate the exercise using Android and Swype. Some are similar while others are completely different. I suspect a big part of the difference is that Swype uses my history when offering the suggestions and since I’ve been travelling recently many of them tend to be airport related.&lt;/p></description></item><item><title>Normalizing a CSV file using MySQL</title><link>/2014/10/01/normalizing-a-csv-file-using-mysql/</link><pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate><guid>/2014/10/01/normalizing-a-csv-file-using-mysql/</guid><description>&lt;p>As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.&lt;/p></description></item><item><title>Mobile web browsing and Javelin</title><link>/2014/09/27/mobile-web-browsing-and-javelin/</link><pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/27/mobile-web-browsing-and-javelin/</guid><description>&lt;p>Something that’s bothered me ever since I started using a smartphone is the link opening behavior. Whenever I’m in an app and click on a web link it would immediately open up that page in a browser window. And when I’m already in a mobile browser and click on a link it would open that page up in a new tab. Compare this with the desktop environment. Clicking on a link within an app does open up a new browser window immediately but since there are shortcuts to quickly switch between programs it’s not a huge deal. And when I’m already looking at a webpage and want to open a new link it’s possible to open it in the background using command+click.&lt;/p></description></item><item><title>Dev tools matter</title><link>/2014/09/25/dev-tools-matter/</link><pubDate>Thu, 25 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/25/dev-tools-matter/</guid><description>&lt;p>It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.&lt;/p>
&lt;p>After Java, Python felt like a breath of fresh air. The code was simpler, more compact, and I was able to just dive in. Discovering pip and virtualenv made me enjoy it even more. But no language is perfect and with enough you uncover the imperfections. Performance became a bottleneck when I started working on serious code and I missed the benefits of static typing - especially when refactoring a large projects.&lt;/p></description></item><item><title>Software is good enough, why improve the hardware?</title><link>/2014/09/21/software-is-good-enough-why-improve-the-hardware/</link><pubDate>Sun, 21 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/21/software-is-good-enough-why-improve-the-hardware/</guid><description>&lt;p>&lt;amp-youtube
data-videoid="0lvMgMrNDlg"
layout="responsive"
width="640" height="480">&lt;/amp-youtube>&lt;/p>
&lt;p>Earlier this morning I watched a Steve Jobs talk from 1980 where he discusses Apple and the relationship between hardware and software. An interesting piece comes at the 12:30 mark where he addresses the question “Right now software is powerful enough, what impact will improvements in hardware have on software?” His answer is great: “[We will] start chewing up power specifically to help that one on one interaction go smoothly and specifically not to help the calculation&amp;hellip; start applying that power to remove that barrier”&lt;/p></description></item><item><title>Dealing with an RDS replication issue</title><link>/2014/09/20/dealing-with-an-rds-replication-issue/</link><pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/20/dealing-with-an-rds-replication-issue/</guid><description>&lt;p>Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn&amp;rsquo;t a huge deal since our production system relies on the master database but we did have to spend time dealing with this.&lt;/p></description></item><item><title>Web development bootcamps</title><link>/2014/09/17/web-development-bootcamps/</link><pubDate>Wed, 17 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/17/web-development-bootcamps/</guid><description>&lt;p>I recently attended two web development workshop “meet and greet” sessions where recent graduates presented their projects and chatted with potential employers. I’m honestly surprised by how polished the projects were. Sure there were a few simple ones but most were solid; they were good ideas, well designed, and had functional backends. It’s amazing what it’s possible to do in 12 weeks.&lt;/p>
&lt;p>These programs focus on a single frontend framework, such as Backbone or Angular, and a backend framework, usually Ruby on Rails. With the number of plugins and public APIs available it’s easier to get an app up and running than ever before. Of course these programs won’t provide the same level of knowledge as a degree or years of experience will but for many projects that’s not important. Being able to get something functional and private is more important than perfect and private and these bootcamps provide enough skills to do that. More importantly, they make code accessible to an entirely new group of people and provide enough skills to allow them to continue learning on their own.&lt;/p></description></item><item><title>Top down vs bottom up coding</title><link>/2014/09/16/top-down-vs-bottom-up-coding/</link><pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/16/top-down-vs-bottom-up-coding/</guid><description>&lt;p>Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.&lt;/p>
&lt;p>Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation. The other benefit is that I start thinking at a systems level and come up with implementations that tend to be more extensible.&lt;/p></description></item><item><title>Curated Twitter timelines and the tech stack</title><link>/2014/09/08/curated-twitter-timelines-and-the-tech-stack/</link><pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/08/curated-twitter-timelines-and-the-tech-stack/</guid><description>&lt;p>Apparently Twitter is considering curation user’s timelines. A perspective people haven’t really discussed is the impact on the tech side. Right now each user has a unique timeline that needs to be presented in near-real time in case they need to see it. This results in a massive storage operation using Redis where these timelines are &lt;a href="http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html" target="_blank">continuously generated and cached&lt;/a>. By moving to a model where every user can be categorized into a group that sees a particular set of tweets Twitter can drastically reduce the amount of data they need to store per user. I’m sure Twitter already has a way of categorizing users in order to support the ad product and this approach would extend it to the “stream” product. In a way it’s akin to how compression works - find repeated patterns and replace every occurrence with something shorter. Then when you want to uncompress you just reverse the process.&lt;/p></description></item><item><title>Importance of various tech services</title><link>/2014/09/07/importance-of-various-tech-services/</link><pubDate>Sun, 07 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/07/importance-of-various-tech-services/</guid><description>&lt;p>A &lt;a href="http://techcrunch.com/2014/09/05/monetization-automation-enforcement/" target="_blank">recent article&lt;/a> on our attachment to social media got me thinking about my most commonly used services and their relative importance. The goal is to answer the question of how I’d feel if various services suddenly disappeared. After going through this process it feels as if these services moved from being necessities to feeling like luxuries. They either have a substitute that will do what they do or only have value due to the network - clearly these are important but I just have no attachment to the product itself.&lt;/p></description></item><item><title>API first startups</title><link>/2014/09/01/api-first-startups/</link><pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate><guid>/2014/09/01/api-first-startups/</guid><description>&lt;p>When building a SAAS product geared towards developers the quickest way to start is to build an API. One can even make the argument that the MVP should just be the API documentation. This benefits both sides. Potential users of the API will know exactly what to expect and have a clear understanding of the functionality and limitations and you can quickly see if there are any issues or inconsistencies in what you’re building. Some non-fiction authors will share a table of contents with potential readers in order to get feedback and this extends that idea to companies and their products. Especially when your primary users are developers this is a simple way to share your idea and approach without resorting to buzzwords or even relying on a beautiful site design.&lt;/p></description></item><item><title>Managing settings files in Django projects</title><link>/2014/08/30/managing-settings-files-in-django-projects/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/30/managing-settings-files-in-django-projects/</guid><description>&lt;p>I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.&lt;/p></description></item><item><title>Yahoo fantasy football stats - 2014 edition</title><link>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</link><pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/26/yahoo-fantasy-football-stats-2014-edition/</guid><description>&lt;p>This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and &lt;a href="https://github.com/dangoldin/yahoo-ffl" target="_blank">updated it to work&lt;/a> for the 2014 season. The old version used the great &lt;a href="http://scrapy.org/" target="_blank">Scrapy&lt;/a> framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses &lt;a href="http://www.seleniumhq.org/" target="_blank">Selenium&lt;/a> to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.&lt;/p></description></item><item><title>Cofounders and their situations</title><link>/2014/08/22/cofounders-and-their-situations/</link><pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/22/cofounders-and-their-situations/</guid><description>&lt;p>Countless people have written about cofounder conflicts in a startup but I rarely see anyone talk about how important a similar situation is - financial and personal. There are no problems when things are going well and it&amp;rsquo;s only when things start going poorly, which they inevitably will, that these issues surface.&lt;/p>
&lt;p>A founder that doesn&amp;rsquo;t have a lot of savings will have a different relationship to fundraising than the founder who has enough savings to keep going. The former will push to fundraise early while the latter will want to wait and search for the best opportunity.&lt;/p></description></item><item><title>Trick your users into staying</title><link>/2014/08/18/trick-your-users-into-staying/</link><pubDate>Mon, 18 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/18/trick-your-users-into-staying/</guid><description>&lt;p>It’s just too easy to rant against LinkedIn but I can’t help it. They recently offered me a free month of business plus so I took them up on it. Little did I know (although I should have expected it) that canceling would be a maze that I still may not have escaped.&lt;/p>
&lt;img src="/image/linkedin-settings.png" alt="LinkedIn Settings Page" data-width="445" data-height="130" data-layout="responsive" />
&lt;p>The cancel screen hides the downgrade to free option and automatically chooses a paid “recommended account” with a bright clickable “Downgrade Account” button. And then, when you actually do manage to downgrade, it’s not clear from the account settings page that you downgraded since it still displays as the premium account option. Maybe when my free month is up it will downgrade or maybe I’ll get charged - how am I supposed to know? I do see a note that says canceled and I suspect I’m in the clear but there’s no way to actually confirm other than contacting support.&lt;/p></description></item><item><title>A MySQL class proposal</title><link>/2014/08/17/a-mysql-class-proposal/</link><pubDate>Sun, 17 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/17/a-mysql-class-proposal/</guid><description>&lt;p>I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with &lt;a href="http://www.tealsk12.org/" target="_blank">TEALS&lt;/a>, a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the &lt;a href="http://www.c4q.nyc/" target="_blank">Coalition 4 Queens&lt;/a> program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.&lt;/p></description></item><item><title>Books are insanely cheap</title><link>/2014/08/11/books-are-insanely-cheap/</link><pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/11/books-are-insanely-cheap/</guid><description>&lt;p>While the pricing battle between Amazon and Hachette rages on, I’ve been thinking about the relationship between price and value. A typical ebook on Amazon costs $9.99 while a movie in a theater, especially one in New York, can cost more than $10. And yet the book takes longer to experience - a movie is over within 2 hours while a book can be enjoyed for hours. Or how about a beer or coffee, they’re two to three times cheaper than an ebook but are consumed an order of magnitude faster book and only provide immediate gratification.&lt;/p></description></item><item><title>Evolution of code deployment</title><link>/2014/08/09/evolution-of-code-deployment/</link><pubDate>Sat, 09 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/09/evolution-of-code-deployment/</guid><description>&lt;p>I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I&amp;rsquo;m hoping to get.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP. This worked well for simple projects where I was the only contributor.&lt;/p></description></item><item><title>Getting the most out of Lyft's 50 free rides</title><link>/2014/08/03/getting-the-most-out-of-lyfts-50-free-rides/</link><pubDate>Sun, 03 Aug 2014 00:00:00 +0000</pubDate><guid>/2014/08/03/getting-the-most-out-of-lyfts-50-free-rides/</guid><description>&lt;p>In honor of their NYC launch, &lt;a href="https://www.lyft.com/" target="_blank">Lyft&lt;/a> came up with an awesome promotion - 50 free rides, up to $25 each, over the next couple of weeks. This had the desired effect - a bunch of my friends are giving it a shot but since everyone else is doing the same it&amp;rsquo;s difficult to find an available car. And when you do get a car you end up paying the peak demand rate rate.&lt;/p></description></item><item><title>Hiring people on Odesk</title><link>/2014/07/31/hiring-people-on-odesk/</link><pubDate>Thu, 31 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/31/hiring-people-on-odesk/</guid><description>&lt;p>While working on &lt;a href="http://getpressi.com/" target="_blank">Pressi&lt;/a>, we found a niche selling &amp;ldquo;social media mashup pages&amp;rdquo; to colleges and small universities. Once we discovered it we needed a quick way to find these colleges and identify the contact details of their marketing or social media directors. Searching for this information was not the most efficient use of time for our small team so we went looking for other options.&lt;/p>
&lt;p>Two options that stood out were &lt;a href="https://www.mturk.com" target="_blank">Mechanical Turk&lt;/a> and &lt;a href="https://www.odesk.com/" target="_blank">Odesk&lt;/a> but they were designed for quick and simple tasks. Using them for complex tasks would result in poor quality results. One advantage that Odesk had was that it allowed us to work with the same person for many tasks - something we couldn&amp;rsquo;t figure out how to do using Mechanical Turk. This allowed us to come up with a set of potential candidates based on their project interest and skillset. We gave each of them the same set of problems to do and compared the results. Using this approach we discovered someone who was the right balance of cost and quality and we ended up working with her over the next few months to compile this list.&lt;/p></description></item><item><title>Low quality? Start bundling!</title><link>/2014/07/29/low-quality-start-bundling/</link><pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/29/low-quality-start-bundling/</guid><description>&lt;p>In the quest to reduce the amount of stuff I own I&amp;rsquo;ve been going through various cabinets and boxes and trying to list everything on eBay. The most common items are old cables with no corresponding devices (or any ideas what these devices even are) and old DVDs.&lt;/p>
&lt;p>Looking at the historic prices for these items doesn&amp;rsquo;t make me happy - a Lenovo laptop charger is less than $10 while a Raging Bull DVD is a couple of bucks. But this entire process got me thinking about bundling. Bundling makes sense when selling cheap products. It&amp;rsquo;s not worth the time to list these individually and it&amp;rsquo;s likely that there are only a few people interested in each item. Bundling them makes it more likely that various items will appeal to a variety of buyers and increase competition. The Lenovo adapter may appeal to one person while a Game Boy charger may appeal to another. By having them in the same lot they are competing against each other and are willing to bid higher to get what they want.&lt;/p></description></item><item><title>Ephemeral security questions</title><link>/2014/07/27/ephemeral-security-questions/</link><pubDate>Sun, 27 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/27/ephemeral-security-questions/</guid><description>&lt;p>I thought I&amp;rsquo;ve seen every design anti-pattern out there but had the luck to run into a new one a couple of days ago. I was buying domains on &lt;a href="http://neamcheap" target="_blank">Namecheap&lt;/a> and ended up going through checkout without verifying the payment details. Turns out that I had an old credit card on file which led to a declined payment. I was redirected to a page that told me to update my payment methods but instead of doing that I ended up hitting back and refreshed the page which triggered another failed charge attempt. One more and I&amp;rsquo;m locked out of my account.&lt;/p></description></item><item><title>When one door closes, another door opens</title><link>/2014/07/23/when-one-door-closes-another-door-opens/</link><pubDate>Wed, 23 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/23/when-one-door-closes-another-door-opens/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span7">
 &lt;div class="thumbnail">
 &lt;img src="/image/cinnamonsnail.jpg" alt="Cinnamon snail food truck" data-width="1024" data-height="683" data-layout="responsive" />
 &lt;p>Source: &lt;a href="http://ecovegangal.com/eat/restaurant-reviews/item/923-video-review-the-cinnamon-snail-organic-vegan-food-truck-in-nj" target="_blank">
 EcoVeganGal.com
 &lt;/a>&lt;/p>
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>Food trucks have taken over every city I&amp;rsquo;ve been to. A decade ago the best you could find was a taco truck but now food trucks run the gamut from the simple taco up to experimental vegan. Priceonomics has a &lt;a href="http://blog.priceonomics.com/post/45352687467/food-truck-economics" target="_blank">great piece&lt;/a> on the rise of the food truck as well as a fascinating look at the economics of the food truck industry. If you haven&amp;rsquo;t read it yet definitely check it out.&lt;/p></description></item><item><title>A simple way to automate AWS deployments</title><link>/2014/07/16/a-simple-way-to-automate-aws-deployments/</link><pubDate>Wed, 16 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/16/a-simple-way-to-automate-aws-deployments/</guid><description>&lt;p>A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html" target="_blank">other instance information&lt;/a>, including the public hostname, the instance region, and instance type.&lt;/p>
&lt;p>This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles. A lightweight approach would be to use the &lt;a href="https://aws.amazon.com/cli/" target="_blank">AWS CLI&lt;/a> to retrieve a list of all running instances along with their tag names, make a request to the meta-data/instance-id endpoint to get the id of the current instance, and then look up that id in the instance list in order to figure out what the role of this instance should be. Then execute the appropriate set of scripts to configure the instance properly.&lt;/p></description></item><item><title>Set up HTTPS on EC2 running Nginx without ELB</title><link>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</link><pubDate>Tue, 15 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/</guid><description>&lt;p>I recently needed to set up HTTPS for my side project, &lt;a href="https://better404.com/" target="_blank">better404.com&lt;/a>. Amazon makes it easy to &lt;a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/US_UpdatingLoadBalancerSSL.html" target="_blank">set up&lt;/a> by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.&lt;/p></description></item><item><title>Getting low level with HTTP</title><link>/2014/07/14/getting-low-level-with-http/</link><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/14/getting-low-level-with-http/</guid><description>&lt;p>I’m currently working on an application using &lt;a href="http://netty.io/" target="_blank">Netty&lt;/a>, a low level network framework, and it’s given me a wonderful education of the HTTP protocol. Prior to this project, every web application I’ve worked on has leveraged a framework that removed the low level details. They built the HTTP requests from multiple packets, took care of various encoding issues, dealt with keep-alive connections, came with built-in support for sessions and cookies, and in general made it extremely easy to get a web server up and running.&lt;/p></description></item><item><title>Good artists use fewer tools</title><link>/2014/07/13/good-artists-use-fewer-tools/</link><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/13/good-artists-use-fewer-tools/</guid><description>&lt;blockquote>Actually, as the artist gets more into his thing, and as he gets more successful, his number of tools tends to go down. He knows what works for him. Expending mental energy on stuff wastes time.&lt;br/>
&amp;nbsp;&amp;nbsp;- Hugh MacLeod, &lt;a href="http://gapingvoid.com/ie/" target="_blank">Ignore Everybody&lt;/a>
&lt;/blockquote>
&lt;p>This quote refers to art but it can just as easily apply to code. As developers, we’re constantly exposed to new tools and technologies and are curious to try them out. Everything new looks shiny and we imagine it will solve all the problems we’re facing. Yet almost always new tools bring their own set of problems and take time to learn. Instead of constantly chasing something new we should try to master what we’re already using - the value of that will most likely outweigh playing with a new toy. It’s better to rely on a small set of tools that we understand well rather than have a superficial knowledge of dozens of tools and technologies.&lt;/p></description></item><item><title>Amazon courting app developers for the Fire phone</title><link>/2014/07/10/amazon-courting-app-developers-for-the-fire-phone/</link><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/10/amazon-courting-app-developers-for-the-fire-phone/</guid><description>&lt;p>I’m a bit late to the Amazon Fire Phone party but wanted to chime in with a perspective I haven’t seen written about. Amazon is offering a &lt;a href="https://developer.amazon.com/public/community/post/TxA5PWCC1V2HCT/Limited-Time-Offer-Earn-a-Guaranteed-6-CPM-on-Interstitial-Ads-from-the-Amazon-M" target="_blank">$6 CPM to mobile app developers&lt;/a> that launch an Amazon app during Auguster and September. Given that typical CPMs are &lt;a href="http://www.slideshare.net/augustinefou/digital-advertising-benchmarks-2014-by-augustine-fou" target="_blank">less than a dollar&lt;/a> with premium publishers like Facebook and Twitter getting &lt;a href="http://www.nanigans.com/2014/02/05/q4-facebook-global-advertising-trends-biggest-quarter-ever-for-paid-facebook-advertising-in-q4-2013/" target="_blank">close to $6&lt;/a>, this is a very aggressive move by Amazon to build out their ecosystem.&lt;/p></description></item><item><title>On Google Now</title><link>/2014/07/09/on-google-now/</link><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/09/on-google-now/</guid><description>&lt;p>As much as it pains me to admit it I’m really enjoying Google Now. I’m aware of how much information I&amp;rsquo;m sharing with Google to make it helpful but at the moment I find the tradeoff worth it.&lt;/p>
&lt;p>It came in especially useful as I&amp;rsquo;ve been traveling over the past couple of weeks:&lt;/p>
&lt;ul>
&lt;li>Show the official exchange rate when traveling. This may not be perfect, especially in the case of &amp;ldquo;blue markets,&amp;rdquo; but it&amp;rsquo;s nice having a rough idea of how much a US dollar is worth.&lt;/li>
&lt;li>Flight information. Since my flight details get sent to my Gmail account, I can quickly tell whether my flight&amp;rsquo;s delayed and what terminal and gate it&amp;rsquo;s scheduled to depart from and arrive to. This is useful to have when I need to make a transfer since I can quickly see where my next flight departs from.&lt;/li>
&lt;li>Flight boarding passes. In addition to the flight information Google Now also shows the boarding passes for my checked in flights. I didn&amp;rsquo;t have to do anything to board a plane other than activate my home screen and place it against a scanner.&lt;/li>
&lt;li>Hotel information. Similar to flight information, I get a card telling me where my hotel is and how to get there.&lt;/li>
&lt;li>Google calendar integration. This is an obvious one but I run my life through Google calendar. This gives me constant notifications of what I have to do when and as long as I enter an address for my events I also get an estimate for when I should leave.&lt;/li>
&lt;/ul>
&lt;p>A concern is that to actually make it useful I have to integrate more and more of my world with Google and I expect this to get worse as more Google Now cards are developed. The optimist in me hopes that Google Now will be opened up to third party developers in a future version of Android but the cynic suspects it&amp;rsquo;s not going to happen.&lt;/p></description></item><item><title>Retrieving a Twitter user's followers and followees</title><link>/2014/07/07/retrieving-a-twitter-users-followers-and-followees/</link><pubDate>Mon, 07 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/07/retrieving-a-twitter-users-followers-and-followees/</guid><description>&lt;p>After reading Gilad Lotan’s &lt;a href="https://medium.com/i-data/fake-friends-with-real-benefits-eec8c4693bd3" target="_blank">post&lt;/a> where Gilad bought 4,000 Twitter followers in order to analyze them, a &lt;a href="https://twitter.com/geoffgolberg" target="_blank">friend&lt;/a> of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.&lt;/p></description></item><item><title>The future of databases</title><link>/2014/07/05/the-future-of-databases/</link><pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate><guid>/2014/07/05/the-future-of-databases/</guid><description>&lt;p>A couple of weeks ago I attended a talk by &lt;a href="https://en.wikipedia.org/wiki/Michael_Stonebraker" target="_blank">Professor Michael Stonebraker&lt;/a>. For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.&lt;/p></description></item><item><title>On having my Pinterest account hacked</title><link>/2014/06/15/on-having-my-pinterest-account-hacked/</link><pubDate>Sun, 15 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/15/on-having-my-pinterest-account-hacked/</guid><description>&lt;p>Earlier today a I got a message on Twitter letting know that my Twitter account was hacked. Sure enough when I looked at my tweet history I saw a slew of weight loss tweets linking to a Pinterest pin. Turns out that my Pinterest account was compromised and since it was connected to Twitter every time someone pinned a weight loss link it got shared on Twitter.&lt;/p>
&lt;p>The fix was simple - block the Pinterest app from within Twitter, disconnect Twitter from within Pinterest, and reset my Pinterest password. Unfortunately, none of these can be done via the apps nor the mobile sites. Instead, both provide a minimal settings page with no clear way of accessing the complete settings. Since I wasn&amp;rsquo;t near a computer, I had to use the Twitter app to delete the spam tweets that were being posted a few times each hour.&lt;/p></description></item><item><title>An eBay design rant: timezone support</title><link>/2014/06/12/an-ebay-design-rant-timezone-support/</link><pubDate>Thu, 12 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/12/an-ebay-design-rant-timezone-support/</guid><description>&lt;img src="/image/ebay-start-time.png" alt="eBay start time" data-width="556" data-height="93" data-layout="responsive" />
&lt;p>I recently needed to sell something on eBay and encountered an issue I thought they would have taken care of by now. Apparently you can pick the start time for an auction but it has to be in PDT - there’s no way to choose another time zone. The change is trivial and one would think that a $60B company would be able to support multiple time zones in their core product. Someone brought this up in &lt;a href="http://community.ebay.com/t5/My-eBay/HOW-DO-I-CHANGE-THE-TIMEZONE/td-p/2675835" target="_blank">the forums&lt;/a> in 2012 and it turns out that time zone support is only present in the forum to allow users to see posts with a local time.&lt;/p></description></item><item><title>Replacing Evernote</title><link>/2014/06/08/replacing-evernote/</link><pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/08/replacing-evernote/</guid><description>&lt;p>Although there’s been a ton of services launching trying to help people do everything under the sun I’ve been finding myself going back to simple tools. One of these has been replacing Evernote with text files that are synced via Dropbox after getting annoyed with Evernote one too many times. It’s great, I edit files in &lt;a href="http://www.sublimetext.com/" target="_blank">Sublime Text&lt;/a> and came up with my own naming format to make search easier. If that fails, I just use grep and find and almost always find what I’m looking for. Since the files are plain text, every Linux command is a tool. It’s trivial to do bulk search/replace using sed or compose one liners to do various filters and counts. It’s easy to sort files by time or size and being able to do a regex search comes in handy when you only have a vague idea of what words you used when writing a note.&lt;/p></description></item><item><title>Debugging a reverting database update</title><link>/2014/06/07/debugging-a-reverting-database-update/</link><pubDate>Sat, 07 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/07/debugging-a-reverting-database-update/</guid><description>&lt;p>I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For &lt;a href="http://better404.com">Better404&lt;/a>, a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site. To see what was going on, I enabled full query logging in MySQL (SET GLOBAL general_log = &amp;lsquo;ON&amp;rsquo;) and sure enough I saw a lagging query that would update the record to the prior values. Stepping through the code I was able to figure out the cause.&lt;/p></description></item><item><title>Approachable data science</title><link>/2014/06/02/approachable-data-science/</link><pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate><guid>/2014/06/02/approachable-data-science/</guid><description>&lt;p>Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn&amp;rsquo;t have to be this way. The goal of data science is simply to unlock insights and value from data. There&amp;rsquo;s no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.&lt;/p></description></item><item><title>Generate fake SQL data using JavaScript</title><link>/2014/05/29/generate-fake-sql-data-using-javascript/</link><pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/29/generate-fake-sql-data-using-javascript/</guid><description>&lt;p>A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and &lt;a href="https://dangoldin.github.io/js-tools/#tab-sql-data">hacked one together&lt;/a> in JavaScript. It’s currently a part of my js-tools &lt;a href="https://github.com/dangoldin/js-tools" target="_blank">GitHub repo&lt;/a> and suggestions are welcome. One thing I definitely need to add is the ability to specify the range of possible values for each field rather than using a hardcoded distribution.&lt;/p></description></item><item><title>On Fab's latest move</title><link>/2014/05/24/on-fabs-latest-move/</link><pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/24/on-fabs-latest-move/</guid><description>&lt;p>Fab recently &lt;a href="http://techcrunch.com/2014/05/21/fab-lays-off-one-third-of-staff-from-new-york-city-office/" target="_blank">laid off&lt;/a> a third of their staff as they transition from designer flash sales into customized goods and their own private label. The business is tough and reminded me of our experiences building &lt;a href="http://makersalley.com" target="_blank">Makers Alley&lt;/a>. We initially set out to build a place where people can buy customized, personal furniture from local designers. The idea was was consumers would benefit from being able to get items that are custom made and can be customized to fit individual styles while supporting a local business and makers would have a new avenue to sell their products and build their brand.&lt;/p></description></item><item><title>Logging in through your inbox</title><link>/2014/05/20/logging-in-through-your-inbox/</link><pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/20/logging-in-through-your-inbox/</guid><description>&lt;p>MixPanel has a clever way of handling failed login attempts. Instead of locking the user out of the account or forcing a password reset they send an email with two links - one to log in to the account directly and another to reset the password. I don’t recall ever seeing this approach before and wish more sites started doing it. This approach also obviates the need to even have a password - a site can just send a “login link” for an entered email address and the user can login via their inbox. This is similar to the way we login via the various social networks but instead of being sent to a social network for confirmation we are sent to our inbox. The only friction is having to go to your inbox to click on the link but since most people keep their inboxes open all day I don’t see this as a huge problem. The other advantage is security - most people use the same password across multiple sites so if one is compromised the others become vulnerable. Under this approach each site will have its own security controls and it becomes impossible for one site’s shoddy security to affect another’s. This is probably too drastic of a change for most users but I’d love to see sites start embracing this model.&lt;/p></description></item><item><title>Examining ssh login requests</title><link>/2014/05/16/examining-ssh-login-requests/</link><pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/16/examining-ssh-login-requests/</guid><description>&lt;p>I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>grep &lt;span style="color:#e6db74">&amp;#34;Invalid user &amp;#34;&lt;/span> /var/log/auth.log | cut -d&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> -f8 | awk &lt;span style="color:#e6db74">&amp;#39;{a[$0]++}END{for(i in a)print i,a[i]}&amp;#39;&lt;/span> | sort -k &lt;span style="color:#ae81ff">2&lt;/span> -n -r | head -n &lt;span style="color:#ae81ff">100&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>Coding in a VR future</title><link>/2014/05/15/coding-in-a-vr-future/</link><pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/15/coding-in-a-vr-future/</guid><description>&lt;p>The Oculus acquisition got me thinking about the impact it would have on software development. We currently have a slew of editors and IDEs that are making us more productive and I wonder whether there&amp;rsquo;s a place for VR. I don&amp;rsquo;t think it&amp;rsquo;s going to be as extreme as &lt;a href="https://en.wikipedia.org/wiki/Minority_Report_(film)" target="_blank">Minority Report&lt;/a> (at least at first) but I do expect some things to get much easier.&lt;/p>
&lt;p>Typing is currently much quicker than any other form of data entry and I don&amp;rsquo;t imagine VR making this any better. While writing this I took a break and tried looking at the letters making up this sentence on my keyboard and it was slower - not to mention the mistakes that will likely occur during transcription. The only thing that would make data entry faster would be a direct neural connection which isn&amp;rsquo;t going to be happening any time soon.&lt;/p></description></item><item><title>Most commonly used shell commands</title><link>/2014/05/12/most-commonly-used-shell-commands/</link><pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/12/most-commonly-used-shell-commands/</guid><description>&lt;p>I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>history | cut -d&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> -f4 | awk &lt;span style="color:#e6db74">&amp;#39;{a[$0]++}END{for(i in a)print i,a[i]}&amp;#39;&lt;/span> | sort -k &lt;span style="color:#ae81ff">2&lt;/span> -n -r&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>The script is simple - look through my command history, extract the first word, and count the number of times that word appears. I was surprised to see git at the top but it makes sense - I tend to run it as a sequence (git status, git commit, git push) so it leads to an inflated count. The rest make sense - they’re a mix of the standard navigation commands as well as command related to my current projects. Next step is to set up a cron job to track this usage over time and see how it changes.&lt;/p></description></item><item><title>Site down? Fall back to S3</title><link>/2014/05/10/site-down-fall-back-to-s3/</link><pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/10/site-down-fall-back-to-s3/</guid><description>&lt;p>An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.&lt;/p>
&lt;p>Reddit gives logged out users &lt;a href="http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html" target="_blank">cached content&lt;/a> rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site. Content rich sites would benefit significantly from this approach - it would reduce cost and ensure uptime. If it turns out that the site does go down you can flip a switch and serve the cached/static content to everyone while the site is brought back up.&lt;/p></description></item><item><title>Gap fills and cross joins in Excel</title><link>/2014/05/03/gap-fills-and-cross-joins-in-excel/</link><pubDate>Sat, 03 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/03/gap-fills-and-cross-joins-in-excel/</guid><description>&lt;p>During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.&lt;/p>
&lt;p>The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula. The best I could come up with is “If a gap is a value, take the value of the closest non empty cell above it, otherwise keep its value.” We can create a formula in another column that takes this approach and after coming up with the new cell values and pasting them over the originals. In the image below, the formula in cell D2 is &lt;strong>=A1&lt;/strong> and the formula in D3 is &lt;strong>=IF(A3=&amp;quot;&amp;quot;,D2,A3)&lt;/strong> with D4 down being relative copies of D3.&lt;/p></description></item><item><title>Migrating from Linode to Digital Ocean</title><link>/2014/05/02/migrating-from-linode-to-digital-ocean/</link><pubDate>Fri, 02 May 2014 00:00:00 +0000</pubDate><guid>/2014/05/02/migrating-from-linode-to-digital-ocean/</guid><description>&lt;p>Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.&lt;/p>
&lt;p>My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects. Luckily I didn’t run into this issue and most of the effort was spent in trying out my sites and looking at the log files to see which libraries were missing.&lt;/p></description></item><item><title>System knowledge and human creativity</title><link>/2014/04/28/system-knowledge-and-human-creativity/</link><pubDate>Mon, 28 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/28/system-knowledge-and-human-creativity/</guid><description>&lt;p>Last week I read an interesting article about &lt;a href="http://www.bloomberg.com/news/2014-04-06/humans-replacing-robots-herald-toyota-s-vision-of-future.html" target="_blank">humans replacing robots&lt;/a> in Toyota&amp;rsquo;s factory. The thesis being that only humans are creative enough (right now at least) to develop new skills and processes to deal with production inefficiencies. This rings true - in order to improve a manufacturing system you need to understand the entire process, from the raw ingredients up to the way consumers end up using the product. It&amp;rsquo;s more difficult to do these days as products become more complicated with an increasing number of specialized components and I&amp;rsquo;m glad to see companies taking a longer term view and focusing on the value of human creativity rather than short term cost cutting.&lt;/p></description></item><item><title>Craigslist: the web's fertilizer</title><link>/2014/04/25/craigslist-the-webs-fertilizer/</link><pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/25/craigslist-the-webs-fertilizer/</guid><description>&lt;p>Craigslist has become the fertilizer of the web. This realization came to me last week when I needed to get a replacement phone and decided to search for one on Craigslist.Filtering past the obvious scams I thought I find a legitimate offer and reached out. Within a few minutes I received the following response from &amp;ldquo;Kyle&amp;rdquo;:&lt;/p>
&lt;blockquote>Hi a guy bought this from me, but I can tell you where I got it from.
I got 3 of these from http://enetcweb.com/dibzees and I resold them for some extra money.
The trick is to watch for bidding to slow down and then put in a bid. That's what I do and I win most of the time.
&lt;/blockquote>
&lt;p>And this was from a listing that seemed legitimate! The vast majority of listings were clearly fraudulent that promised either amazing deals or was the same posting duplicated a dozen times with slightly different wording. Even beyond the fraud and scams there are probably tons of startups trying to take advantage of the network that Craigslist offers. Some are listing their products and services to validate their market and others are reaching out to owners of various listings trying to sell them on something.&lt;/p></description></item><item><title>Bulk geocoding tool</title><link>/2014/04/21/bulk-geocoding-tool/</link><pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/21/bulk-geocoding-tool/</guid><description>&lt;p>Over the weekend I dug up an &lt;a href="https://github.com/dangoldin/js-tools" target="_blank">old repository&lt;/a> I started to contain a running collection of &lt;a href="https://dangoldin.github.io/js-tools/" target="_blank">JavaScript tools&lt;/a> to make my life easier. Ever since I created it it had two tools - one to convert CSV/TSV text into a bootstrap table and the other to generate a “BCG style” matrix. Earlier today I coded up another script - a quick way to geocode a list of addresses. All you have to do is enter a list of address you want geocoded, one per line, and the script will use the Google Maps API to geocode each one with the resulting latitude/longitude being written to an HTML table. If you have any other suggestions for a quick tool let me know.&lt;/p></description></item><item><title>Vertical integration and web development</title><link>/2014/04/16/vertical-integration-and-web-development/</link><pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/16/vertical-integration-and-web-development/</guid><description>&lt;p>Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.&lt;/p>
&lt;p>Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure. The challenge is figuring out how to break your project down into these services and being sure to revisit that decision as you grow.&lt;/p></description></item><item><title>Some computer memories</title><link>/2014/04/13/some-computer-memories/</link><pubDate>Sun, 13 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/13/some-computer-memories/</guid><description>&lt;p>Inspired by yesterday’s post I decided to compile a list the memories I’ve had growing up with computers - hopefully they spark others. I tried to keep these in chronological order but I made some mistakes.&lt;/p>
&lt;ul>
&lt;li>The increase in the number of colors on a monitor and the various *GAs (CGA, EGA, VGA, XVGA)&lt;/li>
&lt;li>Booting of one floppy disk and then running programs off another&lt;/li>
&lt;li>Having both a 3.5 inch and a 5.25 inch floppy disk drives&lt;/li>
&lt;li>Playing Prince of Persia for the first time&lt;/li>
&lt;li>Using Norton Commander rather than the DOS command line&lt;/li>
&lt;li>Shareware and the tons of paper catalogues selling games by mail&lt;/li>
&lt;li>Upgrading to a 486 DX2&lt;/li>
&lt;li>Installing Windows 3.1 from a ton of floppy disks&lt;/li>
&lt;li>The wonderful blue screen of death&lt;/li>
&lt;li>Getting a second phone line in order to use dial up&lt;/li>
&lt;li>Getting AOL instant messenger and my first screenname&lt;/li>
&lt;li>Upgrading to a 56k modem&lt;/li>
&lt;li>Using NetZero as an ISP&lt;/li>
&lt;li>Hosting my first website at Geocities (wish I knew where it was and could dig it up)&lt;/li>
&lt;li>Lycos and AltaVista&lt;/li>
&lt;li>Finally getting a cable modem&lt;/li>
&lt;li>Learning Pascal and C++ in high school&lt;/li>
&lt;li>Warcraft 2, Starcraft, Diablo, Total Annihilation, Shattered Galaxy and LAN parties&lt;/li>
&lt;li>Using Google for the first time&lt;/li>
&lt;li>Signing up for the Gmail beta&lt;/li>
&lt;li>Getting my first smartphone, a Motorola Droid I&lt;/li>
&lt;/ul>
&lt;p>Since then, I’ve had a ton of experiences but they feel incremental. I guess being steeped in tech for so long gives that perception.&lt;/p></description></item><item><title>My computer experience</title><link>/2014/04/12/my-computer-experience/</link><pubDate>Sat, 12 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/12/my-computer-experience/</guid><description>&lt;p>I’ve been thinking about my history with computers and the impact they’ve had on me. I grew up just as computers were becoming mainstream, the spread of the internet coincided with my teens, saw the rise of “Web 2.0” during college, and got my first smart phone a few years after college. It’s fascinating to think about how much has happened to the world since the rise of computers and the varying experiences everyone’s had.&lt;/p></description></item><item><title>Self hosted services</title><link>/2014/04/07/self-hosted-services/</link><pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/07/self-hosted-services/</guid><description>&lt;p>It never bothered me when apps were acquired and shut down but the &lt;a href="https://readmill.com/epilogue" target="_blank">Readmill news&lt;/a> hit me hard. It was one of the truly “free” ebook readers and never got in my way. It fit my behavior perfectly - I would download my books from wherever, drag them into the Readmill web app, and have them permanently accessible on my iPad after a quick sync.&lt;/p>
&lt;p>My first reaction was wishing that it would be open sourced but that got me thinking about third party services. Numerous people have been saying how dangerous it is to rely on third party services but until Readmill it never really mattered to me. Sure, in the abstract it’s better to have everything hosted on your own but in reality it’s impossible to get to the same level of quality and experience for everything we use. We’re constantly balancing tradeoffs and we’re biased to favor the short term factors, such as ease of use and simplicity of set up, rather than long term ones, such as privacy, control, and data ownership.&lt;/p></description></item><item><title>PostgreSQL Fibonacci</title><link>/2014/04/04/postgresql-fibonacci/</link><pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate><guid>/2014/04/04/postgresql-fibonacci/</guid><description>&lt;p>Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a &lt;a href="http://pgsql.inb4.se/2009/march/fibonacci-sequence-using-with-recursive.html" target="_blank">short PostgreSQL&lt;/a> query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">with&lt;/span> &lt;span style="color:#66d9ef">recursive&lt;/span> f &lt;span style="color:#66d9ef">as&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">select&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">as&lt;/span> a, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">as&lt;/span> b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">union&lt;/span> &lt;span style="color:#66d9ef">all&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">select&lt;/span> b &lt;span style="color:#66d9ef">as&lt;/span> a, a&lt;span style="color:#f92672">+&lt;/span>b &lt;span style="color:#66d9ef">from&lt;/span> f &lt;span style="color:#66d9ef">where&lt;/span> a &lt;span style="color:#f92672">&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">100000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>) &lt;span style="color:#66d9ef">select&lt;/span> a &lt;span style="color:#66d9ef">from&lt;/span> f&lt;/span>&lt;/span>&lt;/code>&lt;/pr</description></item><item><title>AWS is about infrastructure optionality</title><link>/2014/03/30/aws-is-about-infrastructure-optionality/</link><pubDate>Sun, 30 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/30/aws-is-about-infrastructure-optionality/</guid><description>&lt;img src="/image/aws-services.png" alt="AWS services" data-width="1107" data-height="383" data-layout="responsive" />
&lt;p>Every time Amazon announces a price drop there are always people pointing out that it’s still more expensive than other cloud computing services such as Linode or Digital Ocean. The Amazon fans then respond by saying sure AWS is more expensive but the value is the ability to scale quickly when needed.&lt;/p>
&lt;p>For me, the biggest value behind AWS is the ecosystem and the included optionality. When building large scale web services it’s tough to know every issue you will run into and more often than not your needs and implementation will change. AWS provides a ton of available tools that make growing and scaling easier beyond the hardware itself. You may start with using EC2 for your server and S3 for hosting your static assets but over time you may start using Cloudfront as a CDN and Redshift for your analytics and EMR to process your various logs. That’s the biggest value in AWS - not being able to launch new machines quickly but having a set of infrastructure options that can be specialized to fit your needs.&lt;/p></description></item><item><title>Hacker lore</title><link>/2014/03/28/hacker-lore/</link><pubDate>Fri, 28 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/28/hacker-lore/</guid><description>&lt;p>I’ve always been interested in hacker lore and have recently started compiling a list of tech-related stories and anecdotes that I found amusing. My ideal story includes an odd, somewhat ridiculous, situation that required a bit of technical ingenuity to solve while highlighting an arcane corner case and providing some glee.&lt;/p>
&lt;p>So far, I’ve only been able to recall and find two such anecdotes but will add more as I discover them. Depending on how many I gather I may put together a permament list page. If you have any to contribute let me know and I’ll add them to this post.&lt;/p></description></item><item><title>Visualizing my browsing history</title><link>/2014/03/25/visualizing-my-browsing-history/</link><pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/25/visualizing-my-browsing-history/</guid><description>&lt;p>I came across a neat Chrome extension called &lt;a href="http://shan-huang.com/browserdatavis/">Iconic History&lt;/a> that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.&lt;/p></description></item><item><title>Goodbye Gmail</title><link>/2014/03/18/goodbye-gmail/</link><pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/18/goodbye-gmail/</guid><description>&lt;p>&lt;img src="/image/fastmail-load.png" alt="Fastmail load times" data-width="510" data-height="20" data-layout="responsive" /> &lt;br/>&lt;/p>
&lt;p>Over the course of the past year I’ve become more and more pissed off at Gmail. I loved using Gmail when it launched - it made writing and reading email a pleasure. It was simple, clean, and responsive. Now it’s the opposite. All actions feel slow. The initial page load takes a substantial amount of time and then I get to wait for the various page elements to load - including a chat list that I’m almost never signed into and integration with a slew of other Google products. Loading emails or new tabs is noticeably slow and the search is sluggish for a company whose main product is a search engine.&lt;/p></description></item><item><title>Fun with the Oyster books API</title><link>/2014/03/16/fun-with-the-oyster-books-api/</link><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/16/fun-with-the-oyster-books-api/</guid><description>&lt;p>I’m an avid reader and signed up for &lt;a href="http://oysterbooks.com/" target="_blank">Oyster&lt;/a> as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.&lt;/p>
&lt;p>One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.&lt;/p></description></item><item><title>Website load times: NYC vs Beijing</title><link>/2014/03/11/website-load-times-nyc-vs-beijing/</link><pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/11/website-load-times-nyc-vs-beijing/</guid><description>&lt;p>Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and &lt;a href="http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/">compare them&lt;/a> against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from &lt;a href="https://github.com/rahimnathwani" target="_blank">rahimnathwani&lt;/a> who ran the script in Beijing, using home ADSL, and wanted to share his dataset.&lt;/p>
&lt;p>I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.&lt;/p></description></item><item><title>Examining the requests made by the top 100 sites</title><link>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</link><pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/09/examining-the-requests-made-by-the-top-100-sites/</guid><description>&lt;p>Since writing the &lt;a href="http://dangoldin.com/2013/12/01/drowning-in-javascript/">Drowning in JavaScript&lt;/a> post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.&lt;/p></description></item><item><title>Follow the WhatsApp money</title><link>/2014/03/06/follow-the-whatsapp-money/</link><pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/06/follow-the-whatsapp-money/</guid><description>&lt;p>Earlier this week I received the following email from imo.im:&lt;/p>
&lt;img src="/image/imo-announcement.png" alt="Mobile breakdown" data-width="688" data-height="781" data-layout="responsive" />
&lt;p>It’s amazing what $19 billion can do. For years imo.im has supported third party chat clients but within a couple of weeks of the WhatsApp acquisition they’ve abandoned that support to focus on their own network and become the next WhatsApp. For a while now they’ve been building features to support this move - videos in August, stickers in January - and I wonder what would have happened if they focused on their platform earlier. Now they’re just playing catch up to WhatsApp, Kik, Line, and countless others. Timing is critical and I suspect it’s too late for imo.im to be entering the first party messaging fray.&lt;/p></description></item><item><title>Trained for discounts</title><link>/2014/03/02/trained-for-discounts/</link><pubDate>Sun, 02 Mar 2014 00:00:00 +0000</pubDate><guid>/2014/03/02/trained-for-discounts/</guid><description>&lt;p>Nearly every week I receive an $8 off $25 coupon from delivery.com. I’m sure the intent is to generate awareness and develop a habit but it’s having the opposite effect on me: I’m being trained to only order when I have a coupon. Couponing is tough - too little and it will have no impact but too much and you run the risk of training your customers to only react to deals which will result in you needing to have higher prices to maintain your margin.&lt;/p></description></item><item><title>Blog posts are now tagged</title><link>/2014/02/26/blog-posts-are-now-tagged/</link><pubDate>Wed, 26 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/26/blog-posts-are-now-tagged/</guid><description>&lt;p>Turns out that tagging and categorizing blog posts is more difficult than I thought. I start with one set of tags but as I go through my posts I realize that my initial set of tags no longer make sense and I need to restart. The challenge is finding the set of tags that are specific enough to categorize a single post yet general enough that they can be applied to other ones. I haven’t found the perfect set of tags yet but did manage to go through and tag each of &lt;a href="/tags">my posts&lt;/a>. Over time I hope to improve the tag taxonomy and update the existing posts. I’d love to hear suggestions on how to effectively organize my posts and examples of other blogs that are doing this well.&lt;/p></description></item><item><title>More Sierpinski fun</title><link>/2014/02/21/more-sierpinski-fun/</link><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/21/more-sierpinski-fun/</guid><description>&lt;p>As a follow up to my previous &lt;a href="http://dangoldin.com/2014/02/19/sierpinski-triangle-in-d3/">post&lt;/a>, I modified my Sierpinski generation code to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it&amp;rsquo;s difficult to confirm given a fixed screen resolution. I&amp;rsquo;d love to know what&amp;rsquo;s going on here and whether there&amp;rsquo;s a relationship between the number of sides and the distance ratio.&lt;/p></description></item><item><title>Sierpinski triangle in D3</title><link>/2014/02/19/sierpinski-triangle-in-d3/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/19/sierpinski-triangle-in-d3/</guid><description>&lt;p>There&amp;rsquo;s a little known algorithm for constructing a &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_triangle" target="_blank">Sierpinski triangle&lt;/a> that is surprisingly easy to implement.&lt;/p>
&lt;ol>
&lt;li>Start the three vertices that form a triangle&lt;/li>
&lt;li>Pick a random point inside the triangle&lt;/li>
&lt;li>Pick a random vertex&lt;/li>
&lt;li>Go halfway from a the random point to the vertex and mark that point&lt;/li>
&lt;li>Go to step 3 using the result of 4 as the starting point&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;m trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following file. Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the &lt;a href="https://en.wikipedia.org/wiki/Sierpinski_carpet" target="_blank">Sierpinski carpet&lt;/a> can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.&lt;/p></description></item><item><title>Heuristic vs algorithmic approaches</title><link>/2014/02/15/heuristic-vs-algorithmic-approaches/</link><pubDate>Sat, 15 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/15/heuristic-vs-algorithmic-approaches/</guid><description>&lt;p>Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.&lt;/p></description></item><item><title>Foursquare as a contact book</title><link>/2014/02/13/foursquare-as-a-contact-book/</link><pubDate>Thu, 13 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/13/foursquare-as-a-contact-book/</guid><description>&lt;p>A few days ago I discovered a new use case for Foursquare when I was meeting up with a friend. We were catching up and during the course of the conversation I realized I needed to introduce him to someone I had met earlier. Unfortunately, I completely blanked on his name and company. All I recalled was that he frequently checked into his company on Foursquare. Sure enough, when I opened up Foursquare I saw that he had checked in there that morning.&lt;/p></description></item><item><title>Using virtualenv in production</title><link>/2014/02/10/using-virtualenv-in-production/</link><pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/10/using-virtualenv-in-production/</guid><description>&lt;p>One of my favorite things about Python is being able to use &lt;a href="http://www.virtualenv.org/en/latest/index.html" target="_blank">virtualenv&lt;/a> to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.&lt;/p>
&lt;p>The thing that&amp;rsquo;s tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services. I’m sure I could have done it differently but the following worked for me and will hopefully come in handy to others. If you have any questions or I&amp;rsquo;m not being clear enough let me know and I&amp;rsquo;ll updat the post with more information.&lt;/p></description></item><item><title>Travel more</title><link>/2014/02/09/travel-more/</link><pubDate>Sun, 09 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/09/travel-more/</guid><description>&lt;p>Now that I’ve started blogging I realize how important traveling is to creativity. After my trip to India I had a ton of different blog ideas. Some came from comparing the two cultures - for example cab rides and mobile phone business while others just came from realizations, such as the lack of truly global technology products. Many dismiss travel as a luxury but it’s a great way to bring a new perspective and let thoughts settle. In my case, it felt as if these connections formed subconsciously based on what I’ve been thinking about and doing actively for a year. It’s not surprising that our conscious experiences drive these subconscious connections but it’s interesting how stark this realization was. Prior to blogging, I never would have had an idea and immediately think of writing about it but it’s become a consistent thought. Travel encourages this serendipitous thought and companies should be encouraging it. Instead, many black ball employees who take a vacation and make employees feel guilty for taking some time off.&lt;/p></description></item><item><title>Visualizing GPS data in R</title><link>/2014/02/05/visualizing-gps-data-in-r/</link><pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/05/visualizing-gps-data-in-r/</guid><description>&lt;p>Earlier today I read Nathan Yau’s &lt;a href="http://flowingdata.com/2014/02/05/where-people-run/" target="_blank">post&lt;/a> that had a quick &lt;a href="http://projects.flowingdata.com/tut/map-routes.R" target="_blank">R script&lt;/a> to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.&lt;/p></description></item><item><title>Why I manage my own blog</title><link>/2014/02/02/why-i-manage-my-own-blog/</link><pubDate>Sun, 02 Feb 2014 00:00:00 +0000</pubDate><guid>/2014/02/02/why-i-manage-my-own-blog/</guid><description>&lt;p>Given the recent news of Medium &lt;a href="http://recode.net/2014/01/28/medium-evan-williams-post-twitter-media-startup-raises-25-million-round/" target="_blank">raising $25M&lt;/a> and Svbtle &lt;a href="http://blog.svbtle.com/open-for-everyone" target="_blank">opening up&lt;/a> to the public I thought it would be an appropriate time to explain why I’m not using either of them. They’re both simple, clean products that allow writers to concentrate on their writing rather than configuring the dozens of options available in other blogging platforms. They’ve also done a great job with the typography that makes the content enjoyable to read. Compared to the other content websites out there, they’re incredible fast - they have a minimal structure and don’t load a ton of external content - especially when compared to the major publishers out there now such as the news sites and the social networks.&lt;/p></description></item><item><title>Solving coding tests in PostgreSQL</title><link>/2014/01/25/solving-coding-tests-in-postgresql/</link><pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/25/solving-coding-tests-in-postgresql/</guid><description>&lt;p>Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:&lt;/p></description></item><item><title>Developing on a remote instance</title><link>/2014/01/23/developing-on-a-remote-instance/</link><pubDate>Thu, 23 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/23/developing-on-a-remote-instance/</guid><description>&lt;p>One of the first things I was given when joining &lt;a href="http://triplelift.com" target="_blank">TripleLift&lt;/a> was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I&amp;rsquo;m a fan of this approach.&lt;/p></description></item><item><title>Emotional products</title><link>/2014/01/19/emotional-products/</link><pubDate>Sun, 19 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/19/emotional-products/</guid><description>&lt;p>In 2013, I gave myself a goal of running 1000 miles. I used RunKeeper to record my runs and used its goal feature to track my progress and quickly see how much I had left. Two days before the new year, I was able to hit my goal and got a little notification from RunKeeper congratulating me on achieving my goal. This small notification got me thinking about how emotion is built into our products. RunKeeper doesn’t care whether it was a 1 mile or 1000 mile goal - the reaction I get would be the same. Yet if I shared these two achievements with my friends, the reactions I get would be completely different. Sure, an algorithm could be designed to treat accomplishments of various difficulties differently and can even be adapted to take into account that to some people, running one mile is equivalent to others running 1000 miles.&lt;/p></description></item><item><title>Ebook readers</title><link>/2014/01/15/ebook-readers/</link><pubDate>Wed, 15 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/15/ebook-readers/</guid><description>&lt;p>I&amp;rsquo;m an avid reader and have embraced the move to digital. An internet connection gives me access to thousands of books with a device that’s thinner than a single book. What I grapple with are the reading apps - I can’t find one that does everything I want.&lt;/p>
&lt;p>On my iPad, I have iBooks, Readmill, Oyster, Kindle, and ShuBook with each having a separate use case. iBooks and the Kindle app are for books that I purchased from iTunes and Amazon, respectively. Oyster is a great ebook subscription service but I’m limited to the books available in their library. I discovered ShuBook when I wanted to host my own ebook server but have switched to Readmill due to the much nicer reading experience, a web interface to manage my library, and cross-device syncing.&lt;/p></description></item><item><title>Taxi prices around the world</title><link>/2014/01/09/taxi-prices-around-the-world/</link><pubDate>Thu, 09 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/09/taxi-prices-around-the-world/</guid><description>&lt;p>I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on &lt;a href="http://www.worldtaximeter.com" target="_blank" rel="nofollow">worldtaximeter.com&lt;/a> and combining that with the data from &lt;a href="http://www.taxiautofare.com" target="_blank" rel="nofollow">taxiautofare.com&lt;/a> and &lt;a href="http://www.numbeo.com/taxi-fare/" target="_blank">&lt;a href="https://www.numbeo.com">www.numbeo.com&lt;/a>&lt;/a> in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on &lt;a href="https://github.com/dangoldin/taxi-pricing" target="_blank">GitHub&lt;/a> with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.&lt;/p></description></item><item><title>Surge pricing ideas</title><link>/2014/01/05/surge-pricing-ideas/</link><pubDate>Sun, 05 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/05/surge-pricing-ideas/</guid><description>&lt;p>Every time there’s a big event or terrible weather, there’s a slew of complaints about Uber’s surge pricing. By now, you’d think that Uber customers would expect this to happen and yet they’re surprised when a $10 cab ride turns into a $100 Uber ride. I suspect Uber’s already done as much as it can on the messaging side; psychologically it’s just tough for someone to take a $10 ride one day and then a day later pay an order of magnitude more.&lt;/p></description></item><item><title>Visualizing RunKeeper data in R</title><link>/2014/01/04/visualizing-runkeeper-data-in-r/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>/2014/01/04/visualizing-runkeeper-data-in-r/</guid><description>&lt;p>What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to &lt;a href="https://github.com/dangoldin/runkeeper-stats" target="_blank">my GitHub&lt;/a>. If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.&lt;/p></description></item><item><title>2013 blog stats</title><link>/2013/12/30/2013-blog-stats/</link><pubDate>Mon, 30 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/30/2013-blog-stats/</guid><description>&lt;p>Now that I actually have over 100 posts for the year I can actually follow the trend and highlight the most popular ones as well as share some data from my Google Analytics account. This is the first year I’ve seriously committed to blogging and didn’t think I’d enjoy it as much as I did. I will continue to write at least twice a week in 2014 so it will be interesting to see how next year’s data compares against the data from 2013. Thanks for reading and definitely let me know if you have any topics you want me to write about.&lt;/p></description></item><item><title>Taxi pricing in NYC vs Mumbai</title><link>/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/</link><pubDate>Sun, 29 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/</guid><description>&lt;p>Something else that struck me during my trip to India was the difference in taxi fare between &lt;a href="http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml" target="_blank">New York City&lt;/a> and &lt;a href="http://www.taxiautofare.com/taxi-fare-card/Mumbai-Taxi-fare" target="_blank">Mumbai&lt;/a>. I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.&lt;/p></description></item><item><title>Smartphones and literacy</title><link>/2013/12/26/smartphones-and-literacy/</link><pubDate>Thu, 26 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/26/smartphones-and-literacy/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span7">
 &lt;div class="thumbnail">
 &lt;img src="/image/global-smartphone-per-capita.png" alt="Global smartphone ownership per capita" data-width="1200" data-height="900" data-layout="responsive" />
 &lt;p>Source: &lt;a href="http://www.businessinsider.com/smartphone-and-tablet-penetration-2013-10" target="_blank">Business Insider&lt;/a>&lt;/p>
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>In 2012 global smartphone ownership surpassed PC ownership and smartphones are still seeing massive growth. The obvious consequence is that many people who’ve never owned a computer are starting to own smartphones and that’s having a huge impact on the world. Almost everything will be affected - not just technology but also business, politics, and general culture. As these smartphones get more powerful and pervasive we’ll see applications that we can’t even imagine right now.&lt;/p></description></item><item><title>Global products</title><link>/2013/12/25/global-products/</link><pubDate>Wed, 25 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/25/global-products/</guid><description>&lt;p>The trip to India got me thinking about “global products” that work the same wherever they are. It’s surprisingly difficult to find tech products that fit this description. Cell phones will almost always work internationally but roaming charges make it impractical. Having an unlocked phone helps but you still need to get a SIM card which is a &lt;a href="http://dangoldin.com/2013/12/23/getting-a-sim-card-in-india/">hassle&lt;/a> in many countries.&lt;/p>
&lt;p>Even something as standard as a laptop isn’t as easy to use as it should be. Wifi connectivity varies depending where you are with most cities being great fickle elsewhere. Dealing with voltage conversion and plug adapters is something that always comes up. I’ve learned to travel with an adapter kit that includes enough combinations to be able to charge my laptop wherever I go.&lt;/p></description></item><item><title>Getting a SIM card in India</title><link>/2013/12/23/getting-a-sim-card-in-india/</link><pubDate>Mon, 23 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/23/getting-a-sim-card-in-india/</guid><description>&lt;p>I’ve heard about the wonders of an unlocked phone and decided to try it out during my recent trip to India. The idea was to get a cheap unlocked Android phone that I’d be able to use on this and future trips. I was able to get a relatively cheap Samsung phone but it took me a surprisingly long time to get a working SIM card. This post is a description of the steps I took as well as some advice for others trying to do the same.&lt;/p></description></item><item><title>CSV powered products</title><link>/2013/12/21/csv-powered-products/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/21/csv-powered-products/</guid><description>&lt;p>Something that I&amp;rsquo;ve been thinking about ever since I worked as a product manager focused on internal tools is being able to run a product entirely through CSV file uploads. Instead of building a UX designed to handle bulk operations and complicated workflows you build support for file uploads and handle the business logic entirely on the backend. The motivation is that it’s extremely difficult to build a UI that’s going to be as powerful and flexible as a simple CSV file, especially when outside tools, such as Excel, can help generate these files.&lt;/p></description></item><item><title>Why Bank of America, why?</title><link>/2013/12/20/why-bank-of-america-why/</link><pubDate>Fri, 20 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/20/why-bank-of-america-why/</guid><description>&lt;p>Before leaving for a trip to India, I wanted to make sure that I’d be allowed to access the ATM so I decided to contact my bank. Surprisingly, Bank of America was modern enough to allow me to do this online. Unsurprisingly, the UX was lacking.&lt;/p>
&lt;p>Instead of just asking which country I was traveling to using a simple autocomplete or dropdown they have a three step process. First, I get to choose whether I’m traveling domestic or international. If internationally, I get presented with four options that are just the first letter of each country name. After choosing a country range bucket, I can finally pick the actual country.&lt;/p></description></item><item><title>Using the information_schema.columns table</title><link>/2013/12/15/using-the-information_schema.columns-table/</link><pubDate>Sun, 15 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/15/using-the-information_schema.columns-table/</guid><description>&lt;p>Something that’s been really helpful to me in understanding a MySQL database is the built in &lt;a href="http://dev.mysql.com/doc/refman/5.0/en/columns-table.html" target="_blank">information_schema.columns&lt;/a> table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in. Querying the information_schema.columns table for the foreign key helped me quickly figure it out. Below are some sample queries that retrieve data from the information_schema.columns table:&lt;/p></description></item><item><title>Why are there so many cleaning startups?</title><link>/2013/12/09/why-are-there-so-many-cleaning-startups/</link><pubDate>Mon, 09 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/09/why-are-there-so-many-cleaning-startups/</guid><description>&lt;p>The recent rise of marketplace startups is great and benefits all except the incumbent. They provide much needed liquidity and transparency to markets that helpfully reduce costs to the consumer and increase volume to the provider.&lt;/p>
&lt;p>Yet I’m surprised by the number of home cleaning service startups out there. I’m aware of &lt;a href="https://www.homejoy.com/" target="_blank">HomeJoy&lt;/a>, &lt;a href="https://iamexec.com/" target="_blank">Exec&lt;/a>, &lt;a href="http://getmaid.com/" target="_blank">GetMaid&lt;/a>, &lt;a href="https://www.myclean.com/" target="_blank">MyClean&lt;/a>, and &lt;a href="http://www.handybook.com/" target="_blank">HandyBook&lt;/a>, but am sure there are countless other copycats. The Uber approach works because it’s for an immediate service with a one time transaction where the value provided is somewhat of a commodity. This is not the case with home cleaning services. The range of quality among cleaners varies significantly more than the quality among drivers and I’d be willing to have a good cleaner come in at a slightly inconvenient time rather than a poor cleaner at the perfect time. And once I find a cleaner I like I’d want to book them directly rather than go through the company again. This way I can get a lower rate while also giving a cleaner more than they’d otherwise make from using the service. This would violate the company’s terms but I don’t see how they can be enforced.&lt;/p></description></item><item><title>Drowning in JavaScript</title><link>/2013/12/01/drowning-in-javascript/</link><pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate><guid>/2013/12/01/drowning-in-javascript/</guid><description>&lt;p>I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.&lt;/p>
&lt;p>To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.&lt;/p></description></item><item><title>Genetic programming Connect 4</title><link>/2013/11/30/genetic-programming-connect-4/</link><pubDate>Sat, 30 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/30/genetic-programming-connect-4/</guid><description>&lt;p>Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a &lt;a href="https://github.com/dangoldin/connect4bot" target="_blank">Connect 4 bot&lt;/a> that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation. The idea is that over time the resulting strategy will be the most fit.&lt;/p></description></item><item><title>Blog analytics (blogolytics?)</title><link>/2013/11/26/blog-analytics-blogolytics/</link><pubDate>Tue, 26 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/26/blog-analytics-blogolytics/</guid><description>&lt;p>Every time I launch a new website one of the first things I do is add &lt;a href="http://www.google.com/analytics/" target="_blank">Google Analytics&lt;/a> to start gathering data. This blog was no different but I’ve recently been wondering whether Google Analytics is the right way to measure a blog. It’s great for tracking the total number of visitors, where they’re coming from, and how long they’re staying but I wish there was something that was optimized for blogs rather than something that was designed as a general solution.&lt;/p></description></item><item><title>I'm joining TripleLift</title><link>/2013/11/23/im-joining-triplelift/</link><pubDate>Sat, 23 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/23/im-joining-triplelift/</guid><description>&lt;p>Just a quick update on my professional life. I recently joined &lt;a href="http://triplelift.com/" target="_blank">TripleLift&lt;/a>’s engineering team. I met the founders while at &lt;a href="http://eranyc.com/" target="_blank">ERA&lt;/a> and liked the problem they were solving. It was also time for me to move on from my other projects so when I found out they were growing it was a pretty easy decision. Being a startup, it’s hard to pinpoint exactly what everyone’s responsibilities are since everyone becomes a generalist but I’ve been focused on the data side. This entails developing our various data pipelines, leveraging the data we have to improve performance and unlock new opportunities, and doing some light data science to help model and understand the native advertising space.&lt;/p></description></item><item><title>A Twitter flashback</title><link>/2013/11/16/a-twitter-flashback/</link><pubDate>Sat, 16 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/16/a-twitter-flashback/</guid><description>&lt;img src="/image/twitter-ecosystem.png" alt="The Twitter ecosystem" data-width="624" data-height="379" data-layout="responsive" />
&lt;p>I was going through my drafts and stumbled onto one that was going to criticize Twitter’s API changes that they &lt;a href="https://dev.twitter.com/blog/changes-coming-to-twitter-api" target="_blank">announced last August&lt;/a>. In light of last week’s IPO I thought I’d finally finish it up.&lt;/p>
&lt;p>The blog post described the changes Twitter planned on making with the intent of taking control of the developer ecosystem. The changes included being a lot more strict with their API by limiting the number of users a client could authenticate, reducing the volume of API calls, and requiring all Twitter content to be displayed the same way. The post also included a matrix indicating that Twitter did not want anyone developing on the consumer/engagement side but the rest being open.&lt;/p></description></item><item><title>RDS and R</title><link>/2013/11/15/rds-and-r/</link><pubDate>Fri, 15 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/15/rds-and-r/</guid><description>&lt;p>In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better. The one thing that makes it tricky was not being able to connect to RDS directly but having to tunnel through an EC2 instance. Below are the steps to replicate the setup.&lt;/p></description></item><item><title>Is Excel on a Mac intentionally hobbled?</title><link>/2013/11/09/is-excel-on-a-mac-intentionally-hobbled/</link><pubDate>Sat, 09 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/09/is-excel-on-a-mac-intentionally-hobbled/</guid><description>&lt;p>The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.&lt;/p></description></item><item><title>Security in the wake of MongoHQ</title><link>/2013/11/07/security-in-the-wake-of-mongohq/</link><pubDate>Thu, 07 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/07/security-in-the-wake-of-mongohq/</guid><description>&lt;p>Over the past few days my inbox has been filled with security alert emails caused by the &lt;a href="https://www.mongohq.com/home" target="_blank">MongoHQ&lt;/a> database hack. I’m impressed by the number of customers MongoHQ was able to sign up - they spanned the gamut from sites that I don’t even recall signing up for to startups that have been getting significant buzz.&lt;/p>
&lt;p>If a database as a service company is able to get hacked it doesn’t leave me optimistic about the way other companies are securing our data. As much as these “as a service” products make our lives easier they bring an increased risk to our business and more importantly our customers. Sure their security will be better than someone who’s setting up a MongoDB instance for the first time but that has to be balanced against the fact that a hosting site offers a much higher reward for a hacking attempt. Access to the infrastructure provides a lot more information than hacking an individual site.&lt;/p></description></item><item><title>Why I run</title><link>/2013/11/03/why-i-run/</link><pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate><guid>/2013/11/03/why-i-run/</guid><description>&lt;p>In honor of today’s NYC marathon, I finally finished up this post that’s been sitting in my drafts folder the past few weeks.&lt;/p>
&lt;p>I’ve never been into running until the beginning of this year when I decided to run 1000 miles. This led to me to three half marathons and is actually making me consider doing a full one. It’s amazing where a habit and a bit of effort can take you. Initially, I ran just to hit my goal and only signed up for races in order to keep myself motivated and on track. Now, it’s become significantly more than that. There are so many things outside my control yet running is solely about my effort and willpower. If I fail it’s my defeat and if I succeed it’s my victory. I can easily skip a run on a cold, rainy morning and yet I know I’m just deceiving myself and I’ll have to make it up later. Running is one of the simplest things to do and that’s a huge appeal. The human bodies have evolved to run and kids start running as soon as they learn to walk. As our world becomes increasingly complex it’s nice being able to escape with a quick run. Whether it’s running or something else, it’s important to have an outlet that rewards us based on our efforts. The further away this activity is from our day jobs the better.&lt;/p></description></item><item><title>What the SEO? Followup</title><link>/2013/10/30/what-the-seo-followup/</link><pubDate>Wed, 30 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/30/what-the-seo-followup/</guid><description>&lt;p>Last week I &lt;a href="http://dangoldin.com/2013/10/20/what-the-seo/">posted&lt;/a> about a site my mom discovered that had copied the content from her site and has been positioning as their business. I had no clue what the motivation behind it was other than thinking it was just a sketchy way to either blackmail the real business owner or use black hat SEO tactics to increase traffic and monetize using AdSense.&lt;/p>
&lt;p>After sharing my problem and getting a ton of advice, I sent a DMCA request to the host, Colocation America, and received a surprisingly quick reply. Within a few days I was on the phone with a representative from the site claiming that my mom had signed up for a service that was offering free mobile websites and that’s why my mom’s business information and content had been appearing on the other site. My mom doesn’t recall signing up for any site and I believe her - she’s been sending me nearly every offer she receives asking whether it’s legitimate and worth doing and I don’t recall ever seeing this one.&lt;/p></description></item><item><title>Lessons from consulting</title><link>/2013/10/27/lessons-from-consulting/</link><pubDate>Sun, 27 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/27/lessons-from-consulting/</guid><description>&lt;p>To supplement my income while working on a startup, I took on a few consulting projects and wanted to share some lessons learned. It seems that everyone’s consulting experience is different so consider mine experience as just another data point.&lt;/p>
&lt;ul class="bulleted">
 &lt;li>I was able to get more work from my existing network than anything else I tried. As soon as I told people I was looking to take on some consulting projects I was able to get interest and referrals. If I didn’t have that I’m not sure how I would have gotten my first few projects.&lt;/li>
 &lt;li>It took longer than I expected to agree on a project’s scope and get the contract signed. My approach was to do a call or meeting to understand the goals of the project and then break it down into components with an estimated time and cost for each piece. I liked this approach since we were able to discuss the priorities of various pieces and talk about the risks associated with each.&lt;/li>
 &lt;li>It took longer to get paid than I expected. I was confident that I’d get paid but it took a few emails and meetings to get the payments made. The part that helped was getting an initial deposit before starting the work.&lt;/li>
 &lt;li>The biggest benefit was the flexibility to choose when and how to work on the projects. Unfortunately, this flexibility is better in the abstract. I didn’t find the flexibility that valuable since almost everyone I know is working at a full time job which causes me to also follow a pretty standard schedule.&lt;/li>
 &lt;li>Most of the knowledge I gained was on the business/marketing side rather than on the tech side. I wasn’t doing challenging work and for the most part didn’t get a chance to work closely with others. The projects I did were also pretty independent so I had to resort to Google and Stack Overflow to help me deal with various questions that came up.&lt;/li>
 &lt;li>The projects I had were not critical to the company and were mostly “nice to haves.” This had the effect of me not feeling very aligned with the company vision which made the projects less interesting than they should have been. I’m not sure if this was due to the way I positioned myself for consulting work or due to the companies not wanting to outsource their critical projects.&lt;/li>
 &lt;li>A shared GitHub account worked amazingly well. The client was able to track the progress and provide feedback at various stages. This required me to commit well documented, working code but it definitely made communication easier. I also had a staging environment set up for my projects which let the clients see the code in action.&lt;/li>
 &lt;li>I wrote a &lt;a href="http://dangoldin.com/2013/09/28/pricing-small-consulting-projects/">post&lt;/a> last month on pricing smaller consulting projects and wanted to highlight that again. I would come up a time estimate for a project that would be billed at my usual rate. Any work that spilled over would be billed at a discounted rate. This gave clients confidence that my estimate was reasonable and gave them a sense of the total project cost.&lt;/li>
&lt;/ul>
&lt;p>This was my first time doing serious consulting work and it’s a mixed bag. I enjoyed the flexibility but didn’t find it being a huge deal. I was also taking on projects that paid the bills but weren’t the most exciting. My biggest gripe was that I felt I wasn’t learning as much as I would have had I been working as part of a team. This gave the illusion that I was falling behind on my skills and not improving as much as others were.If I were to do it again, I’d want to specialize in a particular field and only do projects that fit in with my passions and interests. I’d also want to get it to the state where I’d be working alongside others rather than being entirely independent.&lt;/p></description></item><item><title>Some more design ranting</title><link>/2013/10/23/some-more-design-ranting/</link><pubDate>Wed, 23 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/23/some-more-design-ranting/</guid><description>&lt;p>I don’t know why, but I’ve become more aware of the UI/UX of various sites and apps that I encounter. Whereas before I might have gotten frustrated about some behavior, I’m now starting to get annoyed whenever I encounter something that’s obviously crummy. Here’s a few of the more recent design anti-patterns I’ve been noticing.&lt;/p>
&lt;ol>
 &lt;li>Submitting a login form with the wrong password removes the entered email address. Especially on mobile, where it both takes longer to type and typos are more common, it’s crappy having to type both my email and password again if I made a simple typo in my password or just don’t know which of my passwords I used. A quick hack I saw that makes this a bit easier is to add a keyboard shortcut to your phone to replace “@@” (or any other character set) with your email address.&lt;/li>
 &lt;li>
 &lt;div class="right10">
 &lt;img src="/image/bad-ui-contact-management.png" alt="Bad UI: Contact management" data-width="640" data-height="960" data-layout="responsive" />
 &lt;/div>
 Confusing placement of sign in and register. I forget which app I saw this in but as you can screen from the screenshot I’m on the sign in screen and yet the button under the form is to register, which causes the app to load the registration screen. The sign in button is up top which is a confusing flow since the user goes down the page first before having to go back to the top. The fact that the app uses a flat UI makes this worse since there’s not a lot of differentiation between the sign in and register actions.
 &lt;/li>
 &lt;li>
 &lt;div class="right10">
 &lt;img src="/image/bad-ui-contact-management.png" alt="Bad UI: Contact management" data-width="640" data-height="960" data-layout="responsive" />
 &lt;/div>
 Create new versus add to existing contact. This is probably the most “first-world” one here but without knowing who is currently in your address book it’s impossible to know whether you want to create a new contact or update an existing and contact. My current approach is to choose add to existing, realize that I actually don’t have that contact in my address book, and then go back a few screens and choose create new. A common database operation is “insert or update” - insert if it doesn’t already exist and update if it does. I’d love to have something like that to manage my address book.
 &lt;/li>
&lt;/ol></description></item><item><title>What the SEO?</title><link>/2013/10/20/what-the-seo/</link><pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/20/what-the-seo/</guid><description>&lt;p>My mom owns a small local business in suburban NJ, &lt;a href="http://www.doremi-nj.com/" target="_blank">The Do Re Mi School&lt;/a>, that’s akin to an after-school program where music, dance, art, language and math is taught. Being surrounded by a family of engineers, we’ve been helping her on the tech side and my brother created the web site she’s been using it for the past couple of years. It’s based on Drupal and allows her to make changes without having to dive into the tech details. This approach has been working well she’s recently started using YouTube, Facebook, and Twitter to help with her marketing and social efforts.&lt;/p></description></item><item><title>But I don't have time</title><link>/2013/10/14/but-i-dont-have-time/</link><pubDate>Mon, 14 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/14/but-i-dont-have-time/</guid><description>&lt;p>I&amp;rsquo;m frustrated by the expression &amp;ldquo;I don&amp;rsquo;t have time&amp;rdquo;. As my friends and I have gotten older, I’ve been hearing it more and more frequently. I’ve even caught myself using when trying to come up with an excuse when coordinating evening or weekend plans.&lt;/p>
&lt;p>The reason I dislike the phrase is that it’s equivalent to saying &amp;ldquo;it&amp;rsquo;s not a priority&amp;rdquo; and yet we phrase it such that we convince ourselves it’s something outside our control rather than due to the choices we make. I could go out until 3 AM if I make that a priority over running 6 miles in the morning before heading to work just like I could go catch a movie instead of working on a side project. If we expressed our choices in terms of priorities rather than time we’d be more likely to deal with them.&lt;/p></description></item><item><title>My running progress</title><link>/2013/10/13/my-running-progress/</link><pubDate>Sun, 13 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/13/my-running-progress/</guid><description>&lt;img src="/image/runkeeper-goal-2013.png" alt="My Running Goal" data-width="669" data-height="316" data-layout="responsive" />
&lt;p>I&amp;rsquo;m not entirely sure why, but I started off 2013 with the goal of running 1000 miles which breaks down into a little more than 19 miles a week. Remarkably, I stuck with it and am somehow at 822 miles for the year and need to average 16/week for the remainder of 2013 to hit the goal. Yet it took me a surprisingly long time to work up to a weekly distance of 19 miles and even longer to consistently run more than 19 miles a week. My first run was less than 1.5 miles and I only started consistently eat into my deficit in June. It took me until the end of August to actually reach the 19 mile cumulative weekly average I need to maintain until the end of the year. It’s been an awesome adventure and I’ve even managed to run three half marathons and improved my time from 1:58:11 on an easy course to 1:56:11 on a challenging one. Here’s a table I put together showcasing my running progress over the course of the year. I’ll update it at the end of the year after hopefully achieving the 1000 mile goal.&lt;/p></description></item><item><title>Decline of cynicism</title><link>/2013/10/12/decline-of-cynicism/</link><pubDate>Sat, 12 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/12/decline-of-cynicism/</guid><description>&lt;p>As I&amp;rsquo;ve gotten older and most likely more mature, I&amp;rsquo;ve become far less cynical. I used to be dismissive of people trying to improve things and believed that they were just wasting their time and nothing would change. Yet as a I&amp;rsquo;ve gotten older I&amp;rsquo;ve come to appreciate this effort even if it doesn&amp;rsquo;t lead to noticeable progress.&lt;/p>
&lt;p>The fact that someone is working for their beliefs should be applauded. The waste is dismissing others’ work while sitting in front of a computer or a TV. We all want to see progress and yet we exert effort belittling others that are actually committed to making things better. If we applied this effort into our own passions we’d be all be much better off.&lt;/p></description></item><item><title>Some JavaScript Tools</title><link>/2013/10/05/some-javascript-tools/</link><pubDate>Sat, 05 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/05/some-javascript-tools/</guid><description>&lt;p>Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.&lt;/p>
&lt;p>To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &amp;lt;td&amp;gt; tag and then wrap each row in a &amp;lt;tr&amp;gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.&lt;/p></description></item><item><title>Why's the iPhone 5C so expensive?</title><link>/2013/10/01/whys-the-iphone-5c-so-expensive/</link><pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate><guid>/2013/10/01/whys-the-iphone-5c-so-expensive/</guid><description>&lt;p>Many people &lt;a href="http://ben-evans.com/benedictevans/2013/9/5/the-price-of-the-5c" target="_blank">expected&lt;/a> the iPhone 5C to be priced low in order to compete with the cheaper Android phones in countries without carrier subsidies. The news that the 5C’s starting price was $549 left many in the tech community &lt;a href="http://www.avc.com/a_vc/2013/09/reactions.html" target="_blank">surprised and concerned&lt;/a> with many believing that the price needed to be lower than $400 in order to compete worldwide.&lt;/p>
&lt;p>I'm definitely speculating but I believe the reason for such a high price for the 5C was to avoid cannibalizing the sales of the 5S while also framing the comparison to be iPhone vs iPhone instead of iPhone vs Android. When people go smartphone shopping they see that the 5S is “only” $100 more than the 5C and pay the difference for the more premium product. If the 5C were significantly cheaper people would be comparing it to a similarly priced Android phone which may encourage them to go with the Android or they'd compare it against the 5S which would get many to purchase the much cheaper 5C instead.&lt;/p></description></item><item><title>Going rate for an email address</title><link>/2013/09/30/going-rate-for-an-email-address/</link><pubDate>Mon, 30 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/30/going-rate-for-an-email-address/</guid><description>&lt;div class="row">
	&lt;div class="span5">
		&lt;p>
A couple of months ago I started noticing popups on various ecommerce sites offering a first purchase discounts in exchange for entering an email address. Every time I noticed this happening I took a screenshot to track the offer and compile a list of the retailers using this approach. I’m still collecting examples and would more of them but so far the going rate seems to be anywhere from 10 to 25% off the first order. The pitch is pretty compelling and I think most people would gladly give up their email for the possibility of a discount. I’d also love to know what impact the magnitude of the discount has on the sign up rates; I suspect it’s minimal but definitely better than gaining entry to a sweepstakes or a lottery.
		&lt;/p></description></item><item><title>Pricing small consulting projects</title><link>/2013/09/28/pricing-small-consulting-projects/</link><pubDate>Sat, 28 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/28/pricing-small-consulting-projects/</guid><description>&lt;p>I&amp;rsquo;ve been doing some consulting work over the few months and wanted to share a pricing model that’s been working well for smaller projects. I’ll sit down with the client to understand the scope of the project and work with them to break it down into smaller, more manageable components. Based on this break down, I’ll estimate the time required for each piece and come up with an estimated total time. I charge my usual hourly rate for the work that falls within the estimated time but will charge a steeply discounted rate for every hour that goes over.&lt;/p></description></item><item><title>In defense of Excel</title><link>/2013/09/20/in-defense-of-excel/</link><pubDate>Fri, 20 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/20/in-defense-of-excel/</guid><description>&lt;div class="right10">
 &lt;img src="/image/excel-logo-2013.png" alt="Excel 2013 logo" data-width="256" data-height="256" data-layout="responsive" />
&lt;/div>
&lt;p>Excel has developed a reputation of being bloated, slow, error prone and used primarily by &amp;ldquo;business people&amp;rdquo; who don&amp;rsquo;t have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.&lt;/p>
&lt;p>The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they&amp;rsquo;re still able to run some neat analyses and derive useful insights.&lt;/p></description></item><item><title>Programming and math</title><link>/2013/09/17/programming-and-math/</link><pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/17/programming-and-math/</guid><description>&lt;p>The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.&lt;/p></description></item><item><title>Offering suggestions</title><link>/2013/09/16/offering-suggestions/</link><pubDate>Mon, 16 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/16/offering-suggestions/</guid><description>&lt;p>Since becoming active in the startup scene, I&amp;rsquo;ve been meeting a ton of founders and am annoyed by how much easier it is to offer suggestions than to apply them to myself. My most common suggestion, in true lean startup fashion, is to advocate a quicker or cheaper way to validate the market before building a product and yet it’s extremely difficult to take my own advice. I’ve been working on &lt;a href="http://better404.com" target="_blank">Better404&lt;/a> on and off for two months now and know I should get it in front of potential customers and yet I keep on making minor tweaks and updates to the product.&lt;/p></description></item><item><title>MoMath visit</title><link>/2013/09/11/momath-visit/</link><pubDate>Wed, 11 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/11/momath-visit/</guid><description>&lt;div class="right10">
 &lt;img src="/image/momath-logo.gif" alt="MoMath logo" data-width="600" data-height="607" data-layout="responsive" />
&lt;/div>
&lt;p>Although I’ve been meaning to visit the &lt;a href="http://momath.org/" target="_blank">Museum of Math&lt;/a> ever since it opened in December, I only got the chance to do it this Labor Day. I wanted to share my thoughts and encourage everyone who can to visit.&lt;/p>
&lt;p>I love the mission. Math should not be taught in a vacuum and having various activities that each showcase different mathematical properties is a great approach to get kids (and adults) engaged while learning some math. Some of the activities that stood out to me were bikes with differently sized square wheels that can only go around a certain diameter track; a &amp;ldquo;helix&amp;rdquo; shape that explains multiplication by lighting up a fiber between the numbers and highlighting the resulting value; and a fractal tree generator that would use your body to create the trunk and branches. I enjoyed these since they had an interactive physical component that provided immediate feedback.&lt;/p></description></item><item><title>Want more sales? Start teaching</title><link>/2013/09/06/want-more-sales-start-teaching/</link><pubDate>Fri, 06 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/06/want-more-sales-start-teaching/</guid><description>&lt;p>A trend I’ve been seeing lately is companies boosting their sales by focusing on customer education. The successful companies don’t just focus on the results their product will deliver but also spend time explaining why those results are important and how the product works and how it can be used.&lt;/p>
&lt;p>This approach seems obvious to me. Borrowing some terminology from &lt;a href="http://en.wikipedia.org/wiki/Crossing_the_Chasm" target="_blank">Crossing the Chasm&lt;/a>, the early adopters will use your product as long as it solves an existing problem but education will help the remaining, slower adopting customer segments discover that they even have a problem and look to you for a solution. In my opinion, the major benefits of customer education are to reduce acquisition costs and improve retention. Acquisition costs will drop as you start relying more on inbound interest rather than on outbound sales. Retention will increase since customers that sign up willingly will stick around longer than customers who needed to be coaxed into it by a sales rep. These “self serve” customers will also be more likely to blame themselves when encountering problems rather than whoever got them to sign up. In addition, by developing original and useful content you’ll help your SEO score which will drive more potential customers to look at your products. Your trustworthiness will also improve since you’ll be offering free and useful knowledge.&lt;/p></description></item><item><title>Design anti pattern: footer under infinite scroll</title><link>/2013/09/03/design-anti-pattern-footer-under-infinite-scroll/</link><pubDate>Tue, 03 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/03/design-anti-pattern-footer-under-infinite-scroll/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span7">
 	&lt;div class="thumbnail">
 		&lt;a href="{{ IMG_PATH }}damn-it-linkedin.png">
 		&lt;img src="/image/damn-it-linkedin.png" alt="LinkedIn infinite scroll and footer" data-width="1482" data-height="801" data-layout="responsive" />
 	&lt;/a>
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>I’m not sure why this needs to be said but if your site offers infinite scroll make sure you don’t have anything clickable in the footer. I’d expect the occasional site to succumb to this but I was surprised to see it happening on LinkedIn. All I wanted to do was read the developer docs but unfortunately the link is located in the footer which provides a nice challenge of clicking the link before new content is loaded. I wasn’t quite able to get it and ended up just searching Google for the LinkedIn documentation link. If your site’s content is only accessible via a Google search you have a problem.&lt;/p></description></item><item><title>Startups aren't black and white</title><link>/2013/09/02/startups-arent-black-and-white/</link><pubDate>Mon, 02 Sep 2013 00:00:00 +0000</pubDate><guid>/2013/09/02/startups-arent-black-and-white/</guid><description>&lt;p>When I was making the leap into the startup world I read every post I came across that talked about people’s experiences and guides in running a startup. The goal was to learn as much as I could form others and apply these hard-fought lessons my own startup. Now that I’ve been working on a startup for almost two years I realize how much startups differ from one another and how black and white these guides tend to be. You can read two posts that will promote contradictory approaches. Should you focus on revenue or growth? Should you raise money or bootstrap? Should you go with a freemium model or paid only? Should you go solo or get a cofounder? Should you focus on consumers or the enterprise?&lt;/p></description></item><item><title>Simplicity vs power in product design</title><link>/2013/08/28/simplicity-vs-power-in-product-design/</link><pubDate>Wed, 28 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/28/simplicity-vs-power-in-product-design/</guid><description>&lt;p>Although I come from a backend background, I’ve been spending more and more time on the UX side of things and have been picking up quite a bit - a combination of using Twitter Bootstrap on my projects, subscribing to the Hack Design lectures, and following a ton of designers on Twitter.&lt;/p>
&lt;p>Something that’s been bothering me is this obsession with trying to make every product as intuitive and approachable as possible. That’s the right approach when focusing on mass market consumer products but if you’re building internal tools or targeting power users a simple, approachable product might be antithetical to what you actually need.&lt;/p></description></item><item><title>Extract info from a web page using JavaScript</title><link>/2013/08/26/extract-info-from-a-web-page-using-javascript/</link><pubDate>Mon, 26 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/26/extract-info-from-a-web-page-using-javascript/</guid><description>&lt;p>How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.&lt;/p>
&lt;p>In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.&lt;/p></description></item><item><title>Splitting an AWS account</title><link>/2013/08/24/splitting-an-aws-account/</link><pubDate>Sat, 24 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/24/splitting-an-aws-account/</guid><description>&lt;p>When we launched &lt;a href="http://getpressi.com" target="_blank">Pressi&lt;/a>, I had it set up under my personal AWS account. Recently, we needed to move it into a separate AWS account and I wanted to share the steps to help others running into the same issue. Unsurprisingly, most of the effort went into planning and figuring out the migration steps and order in which they should be done. We weren’t able to eliminate downtime entirely but we reduced it as much as we could.&lt;/p></description></item><item><title>On Teaching AP Computer Science</title><link>/2013/08/18/on-teaching-ap-computer-science/</link><pubDate>Sun, 18 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/18/on-teaching-ap-computer-science/</guid><description>&lt;p>This year, I started volunteering at a program called &lt;a href="http://tealsk12.org/" target="_blank">TEALS&lt;/a>. The long term goal is to improve computer science education in the United States by having tech professionals volunteer their time to teach computer science classes in schools that want to offer computer science classes but don’t have the necessary teachers. Over time, the goal is to have the in-service teachers in each class learn the material so that they will be able to teach it in the future. Currently, the program exists in 65 high schools across 12 states and offers both Intro to Computer Science and AP Computer Science but I’m looking forward to seeing it expand nationwide and into middle and elementary schools.&lt;/p></description></item><item><title>Netflix profiles, why now?</title><link>/2013/08/14/netflix-profiles-why-now/</link><pubDate>Wed, 14 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/14/netflix-profiles-why-now/</guid><description>&lt;div class="right10">
 &lt;img src="/image/netflix-logo.jpg" alt="Netflix Logo" data-width="200" data-height="200" data-layout="responsive" />
&lt;/div>
&lt;p>
&lt;a href="http://netflix.com/" target="_blank">Netflix&lt;/a> recently reintroduced &lt;a href="http://blog.netflix.com/2013/08/make-netflix-your-own-with-profiles.html" target="_blank">profiles&lt;/a> so now each household member can get their own recommendations, recently watched items, and instant queue rather than being forced to share the same polluted profile. This is an awesome win for Netflix customers but it’s been bugging me that they didn’t do this sooner; it’s such an obvious feature that it should have been built as soon as Netflix realized that multiple family members would be sharing their account.
&lt;/p></description></item><item><title>Rise of flat design</title><link>/2013/08/11/rise-of-flat-design/</link><pubDate>Sun, 11 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/11/rise-of-flat-design/</guid><description>&lt;p>I don’t know whether it’s due to the upcoming version of iOS or Windows 8 but it feels as if flat design is getting more and more common. In the past couple of weeks, I’ve noticed two “mainstream” sites, &lt;a href="http://thesaurus.com/" target="_blank">Thesaurus.com&lt;/a> and &lt;a href="https://www.optimum.net/" target="_blank">Optimum&lt;/a>, adopt a flat design which I suspect is the first design change they’ve made in years. Many companies are updating their iOS apps in time for the fall release and I understand the motivation to want to fit the style but it’s interesting to see websites doing the same. I wonder whether we’ll see more sites adopting this flat design in the next couple of months.&lt;/p></description></item><item><title>Introducing Better 404</title><link>/2013/08/07/introducing-better-404/</link><pubDate>Wed, 07 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/07/introducing-better-404/</guid><description>&lt;p>I don’t understand why websites try to compete on having the cleverest 404 page. The fact that someone ended up on a 404 page is a sign that something is broken but instead of trying to fix the problem they try to distract their visitors by making them laugh. It’s equivalent to getting to a restaurant and seeing an amazing menu only to discover that it’s closed.&lt;/p>
&lt;p>We can’t always control which URLs our visitors will type in or click on but we can control what they see when they get there. Instead of trying to distract them with humor why not offer suggestions for what they may have wanted to see? The majority of 404 visits are the result of typos which could be fixed with a simple spell check and the remainder are due to moved pages which can be solved by notifying the linker or providing a redirect.&lt;/p></description></item><item><title>A brief history of manufacturing</title><link>/2013/08/02/a-brief-history-of-manufacturing/</link><pubDate>Fri, 02 Aug 2013 00:00:00 +0000</pubDate><guid>/2013/08/02/a-brief-history-of-manufacturing/</guid><description>&lt;p>Working on &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a>, I&amp;rsquo;ve spent a fair amount thinking about the evolution of manufacturing and wanted to share an extremely condensed history.&lt;/p>
&lt;p>For most of human history, people either made what they needed on their own or traded with a local craftsman. Over time, this led to a specialization in skills and also the rise of the apprenticeship model. Since trade was mostly local, it was difficult to build a large business and most businesses were family run with parents passing down skills to their children.&lt;/p></description></item><item><title>Run Django under Nginx, Virtualenv and Supervisor</title><link>/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/</link><pubDate>Tue, 30 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/30/run-django-under-nginx-virtualenv-and-supervisor/</guid><description>&lt;p>After yet another attempt to deploy a &lt;a href="https://www.djangoproject.com/" target="_blank">Django&lt;/a> application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.&lt;/p>
&lt;p>In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.&lt;/p></description></item><item><title>The power inbox</title><link>/2013/07/27/the-power-inbox/</link><pubDate>Sat, 27 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/27/the-power-inbox/</guid><description>&lt;p>There are only a few tabs I consistently keep open all day on my computer - Gmail, Google Calendar, Hacker News, and New Relic. Out of these, Gmail is the most important with my entire day running through it. The value of having a presence in the inbox hasn&amp;rsquo;t been lost on companies and there are a ton of third party apps that make Gmail more useful - &lt;a href="http://rapportive.com/" target="_blank">Rapportive&lt;/a>, &lt;a href="http://www.yesware.com/" target="_blank">YesWare&lt;/a>, &lt;a href="http://www1.toutapp.com/" target="_blank">ToutApp&lt;/a>, and &lt;a href="http://www.baydin.com/" target="_blank">Boomerang&lt;/a>. Even Google itself has been providing &amp;ldquo;Lab features&amp;rdquo; to augment the default inbox behavior.&lt;/p></description></item><item><title>Security through monopoly</title><link>/2013/07/22/security-through-monopoly/</link><pubDate>Mon, 22 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/22/security-through-monopoly/</guid><description>&lt;p>A month ago I needed to duplicate a set of keys. In the past, I’d just go to the cheapest looking hardware store and they’d easily replicate my keys for around $2 each. This time, I tried the same approach but was told that they weren’t authorized to handle the keys I had and directed me to another locksmith. That locksmith told me that they wouldn’t be able to duplicate it without approval from my management company and also charged $18 for a duplicate. Amazingly enough, they were only able to duplicate one of the keys and I had to go to yet another locksmith (and get another approval) to get the last key duplicated.&lt;/p></description></item><item><title>Beware the data monopoly</title><link>/2013/07/21/beware-the-data-monopoly/</link><pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/21/beware-the-data-monopoly/</guid><description>&lt;p>I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.&lt;/p>
&lt;p>I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to &lt;a href="http://www.infoworld.com/t/data-management/google-wants-your-phonemes-539" target="_blank">subsidize the cost&lt;/a> of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.&lt;/p></description></item><item><title>Scraping Yahoo fantasy football stats with Scrapy</title><link>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</link><pubDate>Wed, 17 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/</guid><description>&lt;p>Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the &lt;a href="http://scrapy.org/" target="_blank">Scrapy&lt;/a> framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.&lt;/p></description></item><item><title>Marketplaces are hard</title><link>/2013/07/13/marketplaces-are-hard/</link><pubDate>Sat, 13 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/13/marketplaces-are-hard/</guid><description>&lt;p>There are countless posts discussing the business and marketing challenges when building a marketplace but I wanted to discuss the issues on the tech side. While we ran into technical challenges building &lt;a href="http://getpressi.com/" target="_blank">Pressi&lt;/a> they were mostly issues with scaling and dealing with the various social network APIs. With &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a>, we didn&amp;rsquo;t run into scaling or API issues but had to deal with a ton of functionality in order to be seen as a credible marketplace. Individually, the features are simple for an intermediate developer to build but there are a lot of them with varying degrees of nuance and logic that need to be worked out.&lt;/p></description></item><item><title>D3 and Vega</title><link>/2013/07/09/d3-and-vega/</link><pubDate>Tue, 09 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/09/d3-and-vega/</guid><description>&lt;img src="/image/mcsp-star.png" alt="A data visualization I'm working on" data-width="780" data-height="780" data-layout="responsive" />
&lt;p>Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.&lt;/p></description></item><item><title>Meetings: to take or not to take?</title><link>/2013/07/07/meetings-to-take-or-not-to-take/</link><pubDate>Sun, 07 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/07/meetings-to-take-or-not-to-take/</guid><description>&lt;p>I struggle with this one. Some days I feel as if I should take every meeting since it’s impossible to know where it can lead. One meeting can completely change a business, generate some consulting work, or lead to new friendships. At the same time, taking every meeting would eat up a chunk of time and most meetings end up fading from memory.&lt;/p>
&lt;p>I’m still figuring out my approach but do believe that having fewer, more meaningful relationships is more valuable than having many fleeting ones. Unfortunately, it’s not clear what will end up being meaningful before the meeting. Currently, I try to take every first meeting or at least have a phone call but have been scheduling them all on a single day, early in the morning, or late in the evening to avoid disruption. I’m also trying to make every meeting valuable by taking follow up notes in order to reach out later if I come across anything relevant or if I need to send an introduction. Probably the most important thing I’ve learned is that it’s easier to rejuvenate an older relationship than to create a brand new one so I’ve been making an effort to catch up with at least two former acquaintances each month.&lt;/p></description></item><item><title>A design that's too good?</title><link>/2013/07/03/a-design-thats-too-good/</link><pubDate>Wed, 03 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/03/a-design-thats-too-good/</guid><description>&lt;p>This is a bit of a first-world problem but it’s possible for a design to be too good. A great design may lead to an increase in your vanity metrics but that won’t necessarily translate into a successful business. In fact, it’s likely that these low-value users will increase your costs.&lt;/p>
&lt;p>When we redesigned the landing page for Pressi (formerly Glossi) we saw the signup rate from our landing page shoot up to to close to 34% from below 5%. Unfortunately, our retention rates were abysmal and we were stuck supporting thousands of Pressi pages that were not seeing any engagement. This led to a massive increase in our AWS costs that we had to scramble to contain. The solution was to be smarter about the frequency of our data pulling as well as minimizing the amount of data we were storing for our users. In hindsight, we should have solved our retention problem before trying to grow our users but we were too obsessed with our user growth numbers to do the right thing.&lt;/p></description></item><item><title>Externalizing externalities in brick and mortar</title><link>/2013/07/01/externalizing-externalities-in-brick-and-mortar/</link><pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate><guid>/2013/07/01/externalizing-externalities-in-brick-and-mortar/</guid><description>&lt;div class="right10">
 &lt;img src="/image/biergarten.jpg" alt="The scene at the Laguardia Biergarten" data-width="600" data-height="800" data-layout="responsive" />
&lt;/div>
&lt;p>Last week, I had a morning flight out of Laguardia Airport and being into all things tech decided to grab a coffee at a place called Biergarten since they had iPads at every seat. Turned out that the only way to order and pay was by using the provided iPad with the attached credit card reader. I had 30 minutes to kill before my flight and decided to spend it observing the interactions others had with this ordering system.&lt;/p></description></item><item><title>Business frameworks are actually useful</title><link>/2013/06/25/business-frameworks-are-actually-useful/</link><pubDate>Tue, 25 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/25/business-frameworks-are-actually-useful/</guid><description>&lt;p>Tom Tunguz wrote a &lt;a href="http://tomtunguz.com/diagrams" target="_blank">great post&lt;/a> yesterday sharing the frameworks he uses to evaluate and analyze startups. For this post, I’m not interested in the content (which is great for anyone building a company) but I am interested in the concept of business frameworks and their application. When I was younger and came across a “business” framework I would dismiss it as obvious and move on. Now, I’m aware of how valuable a good framework can be. A good framework imposes structure that leads to a clearer though process with better results. At the same time, it needs to be simple to apply but be expressive enough to describe the complexity of a business. Being human, we also don’t want to think about our own fallibility and weaknesses which makes it difficult to critique our businesses. We also want to solve problems on our own rather than share our uncertainties with others. A framework serves as an impartial third party where you go through and fill in the blanks until you discover you aren’t in as good of a shape as you thought. Now you can work on growing your company instead of avoiding self-criticism.&lt;/p></description></item><item><title>Where are you on the sales matrix?</title><link>/2013/06/21/where-are-you-on-the-sales-matrix/</link><pubDate>Fri, 21 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/21/where-are-you-on-the-sales-matrix/</guid><description>&lt;p>Something I’ve been thinking about is the variety of sales approaches. On one extreme, you have pharmaceutical companies sending sales reps to visit doctors offices to try to get them to prescribe their drugs. On the other you have companies such as MixPanel and Dropbox which rely on a self serve approach. And in between you have companies such as NewRelic which offer a self-serve trial and try to upsell you with emails from a sales rep.&lt;/p></description></item><item><title>Pushing moral boundaries</title><link>/2013/06/20/pushing-moral-boundaries/</link><pubDate>Thu, 20 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/20/pushing-moral-boundaries/</guid><description>&lt;p>Startups need to use everything in their arsenal to grow. A big part of it is playing in the grey area between moral and immoral. Do you create fake users and comments to portray an active community? Do you reply to posts on Craigslist trying to get visitors to your site? It’s also much easier to play in this area when you’re a startup - you’re most likely too small to be noticed and even if you are the press won’t spend much time on it. Google already gets a ton of flak every time someone complains about losing business due to a search engine update, imagine what would happen if a Google employee was caught spamming Craigslist.&lt;/p></description></item><item><title>Regulation and the share economy</title><link>/2013/06/16/regulation-and-the-share-economy/</link><pubDate>Sun, 16 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/16/regulation-and-the-share-economy/</guid><description>&lt;p>The past few years have seen the rise of the share economy with companies such as AirBnB, Sidecar, Lyft, and TaskRabbit seeing massive growth. Unfortunately, they’re getting significant opposition from government and the entrenched special interest groups. Most of the pushback is under the guise of consumer safety and that regulations exist to protect the consumer.&lt;/p>
&lt;p>Regulation is necessary when there’s an information asymmetry between a service provider and a consumer. In such cases, regulations help bridge that information gap and make the consumer more comfortable making the transaction. But the internet has been chipping away at this gap by building communities where people can share reviews and experiences. Yelp, Angie’s List, and Google are the largest of these traditional review sites but reviews are starting to appear everywhere that money is changing hands. Ecommerce sites offer reviews and ratings of the products they’re selling. The share economy companies self-regulate by offering communities with well thought out rating systems. Without well functioning communities they wouldn’t survive.&lt;/p></description></item><item><title>Constantly entertained</title><link>/2013/06/12/constantly-entertained/</link><pubDate>Wed, 12 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/12/constantly-entertained/</guid><description>&lt;p>The real time news cycle bothers me. Every time theres some news there are countless reactions on Twitter and quick, shoddy write ups on various &amp;ldquo;news&amp;rdquo; sites. Unfortunately, by the time someone does the research and writes a thoughtful response, we&amp;rsquo;ve moved on to the next piece of news. We&amp;rsquo;re reaching the point where writing something stupid quickly is becoming more valuable than writing something thoughtful but late.&lt;/p>
&lt;p>Twitter’s strength is its weakness. The 140 character limit makes it very easy for anyone to share an opinion but that also leads to everyone sharing an opinion. Of course, its ability to break and spread news is invaluable. I just wish that the more thoughtful, well-researched pieces could get past the noise. This week, I would have preferred to see a few insightful pieces about WWDC rather than the same exact WWDC coverage from dozens of sites.&lt;/p></description></item><item><title>Fun with Prolog: Priceonomics Puzzle</title><link>/2013/06/07/fun-with-prolog-priceonomics-puzzle/</link><pubDate>Fri, 07 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/07/fun-with-prolog-priceonomics-puzzle/</guid><description>&lt;p>The &lt;a href="http://blog.priceonomics.com/" target="_blank">Priceonomics blog&lt;/a> is one of my favorites so when I saw that they had a &lt;a href="http://priceonomics.com/jobs/puzzle/" target="_blank">programming puzzle&lt;/a> up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.&lt;/p></description></item><item><title>Citibike Directions: Second Attempt</title><link>/2013/06/04/citibike-directions-second-attempt/</link><pubDate>Tue, 04 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/04/citibike-directions-second-attempt/</guid><description>&lt;p>To coincide with the launch of Citibike, I wrote a &lt;a href="https://dangoldin.github.io/citibike-station-directions/" target="_blank">simple web app&lt;/a> that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I &lt;a href="https://dangoldin.github.io/citibike-station-directions/" target="_blank">updated the app&lt;/a> to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination. A limitation I ran into is that Google’s &lt;a href="https://developers.google.com/maps/documentation/javascript/directions" target="_blank">Direction Service&lt;/a> doesn’t support different transit methods for multiple waypoints. This, combined with my desire to get it out there, is why the design’s not as good as it should be. I’ll see if I can improve it over the next few weeks. People have also been telling me this needs to be on mobile so I’m going to use this as an excuse to jump into mobile development. I’m excited.&lt;/p></description></item><item><title>Citibike Station to Station Directions</title><link>/2013/06/01/citibike-station-to-station-directions/</link><pubDate>Sat, 01 Jun 2013 00:00:00 +0000</pubDate><guid>/2013/06/01/citibike-station-to-station-directions/</guid><description>&lt;div class="thumbnail">
 &lt;img src="/image/citibikes-nyc.jpg" alt="Snapcode for Jet.com" data-width="1024" data-height="768" data-layout="responsive" />
 &lt;p>Photo by &lt;a href="https://twitter.com/rafat/status/339046904979660800" target="_blank">@rafat&lt;/a>&lt;/p>
&lt;/div>
&lt;p>On Wednesday, I took my first bike ride using New York City&amp;rsquo;s new &lt;a href="http://citibikenyc.com/" target="_blank">Citibike&lt;/a> program. So far it&amp;rsquo;s been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn&amp;rsquo;t take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn&amp;rsquo;t make it easy to find directions from one station to another unless you&amp;rsquo;re already at one of them.&lt;/p></description></item><item><title>On SEO: Authority vs Originality</title><link>/2013/05/29/on-seo-authority-vs-originality/</link><pubDate>Wed, 29 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/29/on-seo-authority-vs-originality/</guid><description>&lt;p>A while ago I read Bruce Schneier’s Liars and Outliers and came across a neat passage:&lt;/p>
&lt;blockquote>There was this kid who came from a poor family. He had no good options in life so he signed up for the military. After a few years he was deployed to a conflict infested, god-forsaken desert outpost. It was the worst tour of duty he could have been assigned. It was going to be hot and dangerous. Everyday he had to live with a hostile populace who hated his presence and the very sight of his uniform. Plus, the place was swarming with insurgents and terrorists.&lt;/blockquote>
&lt;blockquote>Anyhow, one morning the soldier goes to work and finds that he's been assigned that day to a detail that is supposed to oversee the execution of three convicted insurgents. The soldier shakes his head. He didn't sign up for this. His life just totally sucks. "They don't pay me enough," he thinks, "for the shit I have to do."&lt;/blockquote>
&lt;blockquote>He doesn't know he's going to be executing the Son of God that day. He's just going to work, punching the time clock, keeping his head down. He's just trying to stay alive, get through the day, and send some money back home to Rome.&lt;/blockquote>
&lt;p>Bruce mentions that he found this on the internet and &lt;a href="http://experimentaltheology.blogspot.com/2011/06/tales-of-demonic.html" target="_blank">cited it appropriately&lt;/a> in the footnotes. But when I tried Googling for the phrase &amp;ldquo;There was this kid who came from a poor family&amp;rdquo; the &lt;a href="https://www.google.com/search?q=%22there+was+this+kid+who+came+from+a+poor+family%22&amp;amp;aq=f&amp;amp;oq=%22there+was+this+kid+who+came+from+a+poor+family%22&amp;amp;aqs=chrome.0.57j62l3.268j0&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8" target="_blank">top links&lt;/a> were &lt;a href="https://kindle.amazon.com/post/3dC8YohtSmaRMsjNnQcMXA" target="_blank">people citing&lt;/a> Liars and Outliers, including my own &lt;a href="https://readmill.com/dangoldin/reads/liars-and-outliers/highlights/kajlmg" target="_blank">highlight on Readmill&lt;/a>. I even came across a &lt;a href="http://jacksonholechrist.blogspot.com/2012/01/blog-post.html" target="_blank">page&lt;/a> that linked to the &lt;a href="http://experimentaltheology.blogspot.com/2011/06/tales-of-demonic.html" target="_blank">original source&lt;/a> that Bruce cited before I found a link to the original source.&lt;/p></description></item><item><title>Investing in tech stocks</title><link>/2013/05/24/investing-in-tech-stocks/</link><pubDate>Fri, 24 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/24/investing-in-tech-stocks/</guid><description>&lt;p>I initially set out to write a post to complain about how difficult it is for an average investor to “hit it big” these days by investing in a tech company at its IPO but ended up changing my thesis after digging into the data. It’s still possible to get the same returns as it was in the 1980s but it’s not possible by a long-term investment in a single company.&lt;/p></description></item><item><title>Save my reading spot, damn it</title><link>/2013/05/22/save-my-reading-spot-damn-it/</link><pubDate>Wed, 22 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/22/save-my-reading-spot-damn-it/</guid><description>&lt;div class="right10">
 &lt;img src="/image/single-page.png" alt="A typical view as single page option" data-width="400" data-height="128" data-layout="responsive" />
&lt;/div>
&lt;p>When reading a long form piece, I favor the single-page view. Unfortunately, I usually don’t find out that it’s longer than a page until I’ve finished the first page. At that point, I switch to the single page view which causes the entire page to reload and I have to skim the page to find the spot where I stopped reading.&lt;/p></description></item><item><title>Adding attachments to django-postman</title><link>/2013/05/17/adding-attachments-to-django-postman/</link><pubDate>Fri, 17 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/17/adding-attachments-to-django-postman/</guid><description>&lt;p>After doing a round of customer development for &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a>, we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using &lt;a href="https://bitbucket.org/psam/django-postman/overview" target="_blank">django-postman&lt;/a> to handle the user to user communication. Within a few days, we quickly discovered that text messages weren&amp;rsquo;t enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like. Unfortunately, django-postman does not support attachments and we didn’t want to have to incorporate another messaging library. Another constraint was that we were already using the awesome &lt;a href="http://blueimp.github.io/jQuery-File-Upload/" target="_blank">jQuery File Upload&lt;/a> library (in truth, a modified &lt;a href="https://github.com/sigurdga/django-jquery-file-upload" target="_blank">Django version by Sigurd Gartmann&lt;/a>) to allow makers to upload images when managing their storefronts.&lt;/p></description></item><item><title>Google's “free” Adwords credits</title><link>/2013/05/15/googles-free-adwords-credits/</link><pubDate>Wed, 15 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/15/googles-free-adwords-credits/</guid><description>&lt;p>Since we’re using Google Apps for Business for our startup, we’ve been getting a bunch of emails trying to get us to sign up for Adwords. The latest promotion is offering a credit of $300 if we spend $100. It’s a pretty common marketing tactic and tons of companies have similar promotions. What’s special about Google is that they’re running an auction for every single click and by giving some businesses free money, they’re driving the prices up for the entire market.&lt;/p></description></item><item><title>Eighteen Months of Django: Part 2</title><link>/2013/05/10/eighteen-months-of-django-part-2/</link><pubDate>Fri, 10 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/10/eighteen-months-of-django-part-2/</guid><description>&lt;p>On Tuesday, I shared some &lt;a href="http://dangoldin.com/2013/05/07/eighteen-months-of-django/" target="_blank">best practices&lt;/a> I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.&lt;/p>
&lt;ul class="bulleted">
 &lt;li>
 &lt;a href="https://django-registration.readthedocs.org/en/latest/index.html" target="_blank">django-registration&lt;/a> and &lt;a href="http://django-social-auth.readthedocs.org/en/latest/" target="_blank">django-social-auth&lt;/a>: Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter. A small issue that annoyed me is that the signals generated by these two packages occur at different points: django-registration generates signals that includes the request while django-social-auth generates signals that contain the response from the OAuth provider. Depending on your use-case, it may be worth it to use the &lt;a href="https://django-registration.readthedocs.org/en/latest/simple-backend.html" target="_blank">simple backend&lt;/a> for django-registration, it automatically activates and logs-in the newly registered users, making your app a bit easy to get into.&lt;/li>
 &lt;li>&lt;a href="http://django-storages.readthedocs.org/en/latest/" target="_blank">django-storages&lt;/a> and &lt;a href="http://docs.pythonboto.org/en/latest/" target="_blank">boto&lt;/a>: If you plan on using S3 to host static content, definitely take a look at these. They provide backends to make it easy to save and access your static content to S3 without having to deal with the AWS API. I ran into some issues using this along with Cloudfront and django-compressor but I was able to fix them by looking at &lt;a href="http://stackoverflow.com/questions/8688815/django-compressor-how-to-write-to-s3-read-from-cloudfront" target="_blank">Stackoverflow&lt;/a>.&lt;/li>
 &lt;li>&lt;a href="http://django-compressor.readthedocs.org/en/latest/" target="_blank">django-compressor&lt;/a>: This is a neat library that will compress and minify your JS and CSS, check if anything’s been updated, generate an upload the result to static files location, and update the HTML to point to the new location. This makes sure that users never end up with older, cached versions of your static files. One thing to note is that you need to make sure that your Javascript are properly formatted and all end in a semi-colon; otherwise you run the risk of the compression failing. I know that there are other Django compressors &lt;a href="http://django-pipeline.readthedocs.org/en/latest/" target="_blank">out there&lt;/a> but I’ve been happy with django-compressor.&lt;/li>
 &lt;li>&lt;a href="http://sorl-thumbnail.readthedocs.org/en/latest/" target="_blank">sorl-thumbnail&lt;/a> and &lt;a href="http://www.pythonware.com/products/pil/" target="_blank">PIL&lt;/a>: If you allow users to upload images this is a must have. It provides a standard way of resizing the images and caching the result. The library comes built in with support for cropping and a variety of other processing options so you don’t have to worry about it. One thing to note is that if a user is loading a page where none of the images have been generated yet, it will delay the page load until all of the images are generated. As long as you know the required sizes of all images, you can run a task on the &lt;a href="http://sorl-thumbnail.readthedocs.org/en/latest/examples.html#low-level-api-examples" target="_blank">backend to generate&lt;/a> each of the images. You may have trouble installing PIL in a virtualenv but doing some Googling it should be easy to figure out.&lt;/li>
 &lt;li>&lt;a href="http://pythonhosted.org/django-extensions/" target="_blank">django-extensions&lt;/a>: Just a neat library that comes with additional management commands to make developing Django easier.&lt;/li>
 &lt;li>&lt;a href="https://github.com/django-debug-toolbar/django-debug-toolbar" target="_blank">django-debug-toolbar&lt;/a>: This intercepts every Django request and provides some debug information to help you optimize your code. The most useful piece to me is being able to see the SQL queries that are being executed and helps me figure out what needs tweaking/caching.&lt;/li>
 &lt;li>&lt;a href="http://django-crispy-forms.readthedocs.org/en/latest/" target="_blank">django-crispy-forms&lt;/a>: If you’re using Twitter Bootstrap, this is a library that lets you generate Bootstrap forms in Django.&lt;/li>
 &lt;li>&lt;a href="http://docs.celeryproject.org/en/latest/django/" target="_blank">django-celery&lt;/a> and &lt;a href="http://celeryproject.org/" target="_blank">celery&lt;/a>: This is a way to run tasks in the background. With Pressi, we initially started with some management commands behind some cron jobs but we ended up switching to Celery when we wanted to distribute it across multiple machines and have built in support for threading and error handling. One thing to note is that we used RabbitMQ as the backend but it takes a bit of time to setup and I’m still struggling to understand the ways to manage it. A lot of people have been using Redis as the backend successfully and I think I’ll give that a go in future projects.&lt;/li>
 &lt;li>&lt;a href="http://mongoengine.org/" target="_blank">mongoengine&lt;/a> and &lt;a href="http://api.mongodb.org/python/current/" target="_blank">pymongo&lt;/a>: If you’re using Mongo, take a look at mongoengine, which serves as an ORM for Mongo, and is built on top of PyMongo, a Mongo API. Mongoengine makes it very easy to change your models from a relational database to an documented-based one by keeping the field types and model definitions similar. Be aware that document-based databases are significantly different from relational ones and that although cosmetically your models look similar, the interaction with the backend is very different. You shouldn’t switch to MongoDB just because you can - make sure you’re switching for the right reasons. For Pressi, we use a hybrid approach where we use MongoDB to store a user’s social media content with everything else stored in MySQL. Something to be cautious of is that both of these libraries have been evolving pretty quickly and we ran into an issue where we weren't able to consistently connect to a MongoDB instance until we stumbled unto the right versions of the libraries (in our case, 0.6.20 for mongoengine and 2.4.1 for pymongo).&lt;/li>
 &lt;li>&lt;a href="http://haystacksearch.org/" target="_blank">django-haystack&lt;/a>: When you’re ready to graduate from implementing a search using QuerySet filters to an indexing backend, take a look at Haystack. It provides a pretty simple search interface that integrates pretty well with Django and supports a few different backends. We ended up settling on the &lt;a href="http://xapian.org/" target="_blank">Xapian&lt;/a> backend because it was supposedly simpler but ran into some trouble installing it inside a virtualenv until I found &lt;a href="https://gist.github.com/vinilios/199025" target="_blank">this post&lt;/a>. Note that although Haystack supports multiple backends, not all features are supported by every backend so make sure the backend you choose supports everything you need. I believe Solr has the most functionality out of the box but we wanted to keep it simple for Makers Alley.&lt;/li>
 &lt;li>&lt;a href="https://django-postman.readthedocs.org/en/latest/quickstart.html" target="_blank">django-postman&lt;/a>: We just implemented this for Makers Alley but it’s a very simple way of doing user to user messaging. It comes with the standard messaging features (inbox, reply, archive, delete) but one thing I wish it had was a way to include attachments.&lt;/li>
 &lt;li>&lt;a href="http://docs.fabfile.org/en/1.6/" target="_blank">Fabric&lt;/a>: I mentioned this in the previous post but wanted to reiterate it since it makes building and deploying your code easy. It also forces you to think about your environment and you end up with a better structured project as a result.&lt;/li>
 &lt;li>&lt;a href="http://south.aeracode.org/" target="_blank">South&lt;/a>: Another package I mentioned earlier that makes it significantly easier to deal with database migrations in Django. The only time we've run into issues using South is when two of us were making changes to the same model in parallel branches. Even then it's easy to replace the two flawed migrations with a functional one.&lt;/li>
 &lt;li>&lt;a href="https://pypi.python.org/pypi/Unidecode" target="_blank">Unidecode&lt;/a>: This isn’t a Django specific library but we found it useful when cleaning up unicode data. If you ever get random unicode exceptions in your code, Unidecode should be able to help.&lt;/li>
 &lt;li>&lt;a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup&lt;/a> and &lt;a href="http://pythonhosted.org/pyquery/index.html" target="_blank">PyQuery&lt;/a>: If you need to do some HTML scraping in Python, take a look at BeautifulSoup. It turns HTML code into an object that’s easy to navigate and search. After getting more and more familiar with jQuery, I found a python alternative in PyQuery but am still getting comfortable with it. If you come from the jQuery world I’d try using PyQuery first; otherwise I’d try BeautifulSoup.&lt;/li>
 &lt;li>&lt;a href="http://docs.python-requests.org/en/latest/" target="_blank">requests&lt;/a>: Just a nice and simple replacement of urllib and urllib2 that makes it much simpler to make HTTP requests. Your code becomes cleaner, more readable, and more expressive.&lt;/li>
&lt;/ul>
&lt;p>I tried to highlight the libraries that have made developing in Django easier but I’m sure there are tons more. I’d love to hear about them so do share.&lt;/p></description></item><item><title>Eighteen months of Django</title><link>/2013/05/07/eighteen-months-of-django/</link><pubDate>Tue, 07 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/07/eighteen-months-of-django/</guid><description>&lt;p>I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning. Now that I’ve developed and launched (with &lt;a href="http://www.sandylin.com/" target="_blank">Sandy&lt;/a> and &lt;a href="http://marcschaffnergurney.com/" target="_blank">Marc&lt;/a>) two serious Django-based products as well as bunch of smaller ones, I wanted to document some personal best practices I’ve picked up. Obviously, I&amp;rsquo;m still learning and I may be completely wrong with them so let me know if you disagree. If you’re interested in a deeper look at some of the topics let me know and I can write up another post going into detail about a particular topic.&lt;/p></description></item><item><title>Netflix, bundling, and the future of video</title><link>/2013/05/05/netflix-bundling-and-the-future-of-video/</link><pubDate>Sun, 05 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/05/netflix-bundling-and-the-future-of-video/</guid><description>&lt;p>Something that’s been stuck in my head is the relationship between Netflix and bundling. On one hand, we’ve been wishing that cable came unbundled so we’d be able to just pay for the shows we want to watch. On the other hand, we have Netflix which is striving to let us stream every TV show and movie whenever and wherever we want. Why don’t we care that Netflix is actually a bundled product?&lt;/p></description></item><item><title>Follow through on that personal welcome email</title><link>/2013/05/02/follow-through-on-that-personal-welcome-email/</link><pubDate>Thu, 02 May 2013 00:00:00 +0000</pubDate><guid>/2013/05/02/follow-through-on-that-personal-welcome-email/</guid><description>&lt;p>Recently, I’ve been receiving many startups sending out “personal emails” from the CEO or cofounder around 30 minutes after signing up. The idea is to engage the new user by showing them that there’s a real person behind the service that cares and to offer any help that they may need. There’s a great &lt;a href="https://segment.io/academy/email-is-the-easiest-way-to-improve-retention" target="_blank">article&lt;/a> on Segment.io about this tactic as well as a few other emails that can be sent to improve retention. This technique is called “drip marketing” and there are a bunch of companies offering it as a service - the ones I can immediately think of are &lt;a href="http://www.getvero.com/" target="_blank">Vero&lt;/a> and &lt;a href="http://intercom.io" target="_blank">Intercom&lt;/a>; and Mixpanel is moving into this space as well with their &lt;a href="https://mixpanel.com/notifications/" target="_blank">Notifications&lt;/a> product. There are also a variety of open source packages available, I’m familiar with &lt;a href="https://django-drip.readthedocs.org/en/latest/" target="_blank">django-drip&lt;/a> for Django and Dan Shipper’s &lt;a href="https://github.com/dshipper/Faucet" target="_blank">Faucet&lt;/a> for RoR.&lt;/p></description></item><item><title>Some gaming nostalgia</title><link>/2013/04/29/some-gaming-nostalgia/</link><pubDate>Mon, 29 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/29/some-gaming-nostalgia/</guid><description>&lt;p>In the early 90s, being a kid new to the US and new to computers I developed an addiction to computer games. I’d play everything that I got my hands on and remember sharing floppy disks with school friends. Unfortunately, I was plagued by two issues that had pretty clever approaches: age verification and piracy protection.&lt;/p>
&lt;div class="right10">
 &lt;img src="/image/lsl-bar.png" alt="Leisure Suit Larry" data-width="320" data-height="200" data-layout="responsive" />
&lt;/div>
&lt;p>The first manifested itself in &lt;a href="http://en.wikipedia.org/wiki/Leisure_Suit_Larry" target="_blank">Leisure Suit Larry&lt;/a>. I was as giddy as only a kid can be when I got my hands on it. Unfortunately, that went away when I was required to take an “age quiz” as soon as the game loaded. The age quiz consisted of a series of multiple questions that only an adult would be able to answer. These ranged from factual ones such as “Who recorded ‘Let it be’?” to comical ones such as “Do girls really have cooties?” I do have memories of playing it so I must have figured out some way around the verification. I must have either guesses correctly some of the time or took notes of the answers that allowed me to play.&lt;/p></description></item><item><title>Smart software; dumb hardware</title><link>/2013/04/24/smart-software-dumb-hardware/</link><pubDate>Wed, 24 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/24/smart-software-dumb-hardware/</guid><description>&lt;p>Last week, three isolated events gave me a glimpse of how powerful mobile can be. Tech pundits have been saying that for a while now but experiencing it firsthand is definitely more convincing.&lt;/p>
&lt;ul class="bulleted">
 &lt;li>I went for a run with only my phone to keep me company. After my run was done, I wanted to grab a cup of iced coffee and realized that Starbucks gave me a free drink on my birthday. Downloading the app on my phone allowed me to get a drink without having cash or a wallet.&lt;/li>
 &lt;li>While checking out at a grocery store, a friend showed me CardStar which allowed him to store all his loyalty cards on his phone. Since then, I’ve imported all my loyalty cards that have just been sitting in a drawer into my phone.&lt;/li>
 &lt;li>After getting a Raspberry Pi and installing Raspbmc, I was able to use my phone as a remote control just by downloading an app.&lt;/li>
&lt;/ul>
&lt;p>These behaviors are different and yet they’re all converging on the phone. What they have in common is that &lt;a href="http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html" target="_blank">software is replacing hardware&lt;/a>. Hardware doesn’t need to become smarter, it just needs to be able to sync with our phones which can do the heavy lifting. The functionality then becomes limited by software which can be updated more cheaply and quickly than the hardware. It also solves the problem of hardware companies trying to develop software that results in a terrible user experience. Do cars really need the ability to &lt;a href="http://www.cnn.com/2010/TECH/01/07/ford.twitter/index.html" target="_blank">read a Twitter&lt;/a> feed? And if they do, why not just do it via a simple Bluetooth connection and an audio streaming app on a phone?&lt;/p></description></item><item><title>What does getting on the HN front page get you?</title><link>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</link><pubDate>Fri, 19 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/19/what-does-getting-on-the-hn-front-page-get-you/</guid><description>&lt;p>A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.&lt;/p>
&lt;ul>
 &lt;li>The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.
 &lt;img src="/image/ga-blog-post-spike.png" alt="GA Blog Post Spike" data-width="1167" data-height="420" data-layout="responsive" />
 &lt;/li>
 &lt;li>The post ended up being featured in the NY Times &lt;a href="http://bits.blogs.nytimes.com/2013/04/12/todays-scuttlebot-e-mail-cachet-and-cellphone-dial-tones/" target="_blank">Bits blog&lt;/a> which accounted for ~2,900 of the total visits; the &lt;a href="http://gizmodo.com/dial-tones/" target="_blank">Gizmodo network&lt;/a> which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.&lt;/li>
 &lt;li>The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the &lt;a href="http://www.gizmodo.co.uk/2013/04/why-your-mobile-phone-doesnt-have-a-dial-tone/" target="_blank">UK&lt;/a> on Sunday and &lt;a href="http://www.gizmodo.fr/2013/04/16/pourquoi-telephone-portable-tonalite.html" target="_blank">France&lt;/a> on Tuesday. &lt;a href="http://www.cnet.com/8301-13952_1-57579660-81/the-404-1249-where-we-get-the-senior-discount-podcast/" target="_blank">CNET&lt;/a> and the &lt;a href="http://www.guardian.co.uk/technology/blog/2013/apr/15/technology-links-newsbucket" target="_blank">Guardian&lt;/a> both picked it up on Monday.&lt;/li>
 &lt;li>Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.&lt;/li>
 &lt;li>Only 80 people ended up visiting the &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a> site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.&lt;/li>
 &lt;li>I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.&lt;/li>
 &lt;li>It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.&lt;/li>
 &lt;li>If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a &lt;a href="http://nathanmarz.com/blog/you-should-blog-even-if-you-have-no-readers.html" target="_blank">great post&lt;/a> that everyone who's interested in blogging should read.&lt;/li>
 &lt;li>It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.&lt;/li>
&lt;/ul></description></item><item><title>Automatically add a “Follow on HN” link</title><link>/2013/04/15/automatically-add-a-follow-on-hn-link/</link><pubDate>Mon, 15 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/15/automatically-add-a-follow-on-hn-link/</guid><description>&lt;p>This past weekend I wrote a &lt;a href="https://github.com/dangoldin/follow-discussion-hn" target="_blank">small jQuery plugin&lt;/a> that automatically inserts a “Follow this discussion on Hacker News” link on a recently submitted web page. The motivation was to automate the current workflow that consists of first submitting a post to Hacker News, getting the URL of the comment thread, and then updating the original post to link to the thread. I also wanted to see if it could be done entirely in Javascript so that the code could be included on static HTML pages and not require a backend server.&lt;/p></description></item><item><title>Why don't cell phones have a dialtone?</title><link>/2013/04/12/why-dont-cellphones-have-a-dialtone/</link><pubDate>Fri, 12 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/12/why-dont-cellphones-have-a-dialtone/</guid><description>&lt;p>While reading &lt;a href="http://www.amazon.com/The-Idea-Factory-American-Innovation/dp/1594203288" target="_blank">The Idea Factory&lt;/a>, I came across an interesting passage that explained why cell phones don’t have dialtones:&lt;/p>
&lt;blockquote>Meanwhile, Phil Porter, who had worked with [Richard] Frenkiel on the original system, came up with a permanent answer to an interesting question. Should a cellular phone have a dial tone? Porter made a radical suggestion that it shouldn’t. A caller should dial a number and then push “send.” That way, the mobile caller would be less rushed; also, the call would be connected for a shorter time, thus putting less strain on the network. That this idea—dial, then send—would later prove crucial to texting technology was not even considered.
&lt;/blockquote>
&lt;p>It’s amazing that although this suggestion was made in 1971, we’re leveraging it more than 40 years later with text messaging. How many other technologies and businesses are built on top of SMS that wouldn’t have existed without this decision? I’m sure an SMS-like technology would have come along regardless of this decision but it still makes me wonder how significantly past technological decisions influence us in the present.&lt;/p></description></item><item><title>Raspbmc</title><link>/2013/04/10/raspbmc/</link><pubDate>Wed, 10 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/10/raspbmc/</guid><description>&lt;div class="right10">
 &lt;img src="/image/raspbmc-remote.png" alt="Raspbmc Remote" data-width="200" data-height="300" data-layout="responsive" />
&lt;/div>
&lt;p>I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks &lt;a href="http://simongoldin.com/" target="_blank">Simon&lt;/a>!) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.&lt;/p></description></item><item><title>Coke, Pepsi and Passover</title><link>/2013/04/05/coke-pepsi-and-passover/</link><pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/05/coke-pepsi-and-passover/</guid><description>&lt;div class="right10">
 &lt;img src="/image/pepsi-throwback.jpg" alt="Pepsi Throwback" data-width="300" data-height="200" data-layout="responsive" />
&lt;/div>
&lt;p>During Passover, Coke and Pepsi sell sugar based versions of their sodas in order to stay kosher for Passover. These high fructose corn syrup (HFCS) free sodas are extremely popular and people stock up while they can. I don’t know whether this is due to the better taste, the nostalgia, or the limited supply but these sugar based versions are definitely more popular. I wonder what would happen if either Coke or Pepsi decided to go “all in” on sugar and launch a marketing campaign against HFCS based food and drinks. I’d love to look at the margins of sugar vs HFCS based sodas and see what the market share increase would need to be in order to offset the switch to sugar. My gut tells me that pursuing this strategy would be a win but the companies are too entrenched in their current process that it’s just not going to happen. Smaller soda manufacturers, such as Boylan, GuS, and Moxie, are growing by differentiating themselves from the big guys and are emphasizing the healthier ingredients. I’m hopeful that this will pressure Coke and Pepsi to make their soda healthier. Unfortunately, what’s more likely to happen is that they will just acquire the niche manufacturers position them to appeal to the more concious consumer, similar to what’s happening to &lt;a href="http://brookstonbeerbulletin.com/the-big-brewers-brands/" target="_blank">craft breweries&lt;/a>.&lt;/p></description></item><item><title>The new Gmail compose</title><link>/2013/04/02/the-new-gmail-compose/</link><pubDate>Tue, 02 Apr 2013 00:00:00 +0000</pubDate><guid>/2013/04/02/the-new-gmail-compose/</guid><description>&lt;div class="right10">
 &lt;img src="/image/gmail_logo.png" alt="Gmail Logo" data-width="300" data-height="135" data-layout="responsive" />
&lt;/div>
&lt;p>A few days ago, Google made the new compose default on Gmail. It went from a separate page to a popup that’s accessible from anywhere in Gmail. And for the vast majority of the time, it’s better: it’s quicker to get to and makes it easy to reference other emails while writing a new one. Unfortunately, for attaching an image (not embedding it inline) or doing some heavy formatting, it’s a huge step backwards and makes me want the old compose back.&lt;/p></description></item><item><title>Mailbox: Is the server required?</title><link>/2013/03/29/mailbox-is-the-server-required/</link><pubDate>Fri, 29 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/29/mailbox-is-the-server-required/</guid><description>&lt;div class="right10">
 &lt;img src="/image/mailbox1.png" alt="Mailbox" data-width="280" data-height="420" data-layout="responsive" />
&lt;/div>
&lt;p>After a couple of weeks on the waiting list I finally got access to the Mailbox App. It’s a huge improvement over the standard mail app and my mobile email consumption habits have improved significantly. I’m still not at “inbox zero” but am making my way there.&lt;/p>
&lt;p>I don’t know much about iOS development but one thing I’ve been wondering about is whether they could have written it to not use a remote server. Regarding scaling Mailbox, they &lt;a href="http://www.mailboxapp.com/reservations/?p=1#were-ramping-up" target="_blank">wrote&lt;/a>:&lt;/p></description></item><item><title>Welcoming the long, single webpage</title><link>/2013/03/26/welcoming-the-long-single-webpage/</link><pubDate>Tue, 26 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/26/welcoming-the-long-single-webpage/</guid><description>&lt;p>A recent trend in website design I’ve been seeing is the long single page. My first distinct memory of seeing it is from &lt;a href="https://yourkarma.com/" target="_blank">Karma&lt;/a> but I’ve started noticing it everywhere. It runs the gamut from non-profit &lt;a href="http://www.weheartwifi.com/" target="_blank">causes&lt;/a> to video game &lt;a href="http://www.sandboxstrat.com/" target="_blank">PR firms&lt;/a>. In fact, we’re even using this approach for the Makers Alley &lt;a href="https://makersalley.com/" target="_blank">homepage&lt;/a>.&lt;/p>
&lt;p>Surprisingly, it’s starting to make inroads on news sites as well. Whereas before news sites would have an article spread across 20 pages (looking at you Business Insider) in order to increase page views and show more ads, some news outlets are actually improving the user experience. Both &lt;a href="http://apps.npr.org/unfit-for-work/" target="_blank">NPR&lt;/a> and the &lt;a href="http://www.washingtonpost.com/sf/sports/wp/2013/02/27/cyclings-road-forward/" target="_blank">Washington Post&lt;/a> have posted pieces that leverage this approach and it creates a significantly more engaging read. It’s a pleasure reading long form content this way, richer media adds to the experience and minimizing mouse clicks avoids the disruption of a page load.&lt;/p></description></item><item><title>Email addresses are private?</title><link>/2013/03/24/email-addresses-are-private/</link><pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/24/email-addresses-are-private/</guid><description>&lt;p>Last week, Andrew “weev” Aurenheimer, was sentenced to 41 months for going through publicly accessible AT&amp;amp;T URLs which exposed the email address of 114,000 iPad owners. I don’t want to talk get into the absurdity of the sentence or how AT&amp;amp;T should be the one held accountable for this “&lt;a href="http://en.wikipedia.org/wiki/Security_through_obscurity" target="_blank">security&lt;/a>.”&lt;/p>
&lt;p>I’m more interested in the fact that people still consider an email address to be private information (although I do realize that the leak also revealed iPad ownership information). This may have been the case years ago when we arrived on the internet but right now, our email addresses are everywhere. We give it to every new website we sign up for and we display it proudly on our websites. I’m sure my email address appears on dozens of spam lists for sale on the internet. Google already gives 3000 results when I search for my email address.&lt;/p></description></item><item><title>One art, please</title><link>/2013/03/20/one-art-please/</link><pubDate>Wed, 20 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/20/one-art-please/</guid><description>&lt;p>At the beginning of 2013, I set a &lt;a href="http://dangoldin.com/2013/01/02/2013-goals/" target="_blank">goal&lt;/a> to do something with my hands to contrast with the constant life in front of the screen. I finally finished my first “art” project this past weekend and documented the result. I had a stash of old, torn jeans that were just taking up space and instead of throwing them out I decided to have some fun. Here’s the process and end result.&lt;/p></description></item><item><title>Makers Alley v2</title><link>/2013/03/16/makers-alley-v2/</link><pubDate>Sat, 16 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/16/makers-alley-v2/</guid><description>&lt;div class="right10">
 &lt;img src="/image/ma-logo-square.png" alt="Makers Alley" data-width="600" data-height="340" data-layout="responsive" />
&lt;/div>
&lt;p>A brief one today.&lt;/p>
&lt;p>This past week, Sandy and I have been super busy getting a new version of &lt;a href="https://makersalley.com/" target="_blank">Makers Alley&lt;/a> out that allows you to customize and buy furniture. We’re launching with two makers that have items for sale but we’re busy adding more. &lt;a href="https://makersalley.com/maker/withers-and-grain/" target="_blank">Withers &amp;amp; Grain&lt;/a> specialize in using reclaimed wood from the 5 boroughs and do their own wood and metal work. &lt;a href="https://makersalley.com/maker/mark-grattan-design-and-build/" target="_blank">Mark Grattan&lt;/a> is a furniture designer who has designed a furniture collection for Makers Alley in a geometry-inspired style. Take a look at their pages, watch their videos, and customize their pieces. If you have any feedback let me know.&lt;/p></description></item><item><title>Mmmm... pseudo static sites</title><link>/2013/03/12/mmmm-pseudo-static-sites/</link><pubDate>Tue, 12 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/12/mmmm-pseudo-static-sites/</guid><description>&lt;p>Reading &lt;a href="http://blog.apps.npr.org/2013/02/14/app-template-redux.html" target="_blank">Katie Zhu’s post&lt;/a> on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cost:&lt;/strong> S3 is cheaper than EC2.&lt;/li>
&lt;li>&lt;strong>Reliable:&lt;/strong> S3 doesn’t go down near as frequently as EC2.&lt;/li>
&lt;li>&lt;strong>Scalable:&lt;/strong> Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.&lt;/li>
&lt;li>&lt;strong>Simpler:&lt;/strong> There are no weird server issues here. As long as you generate the right content and your rendering is good, you don’t need to worry about a web server acting up.&lt;/li>
&lt;/ul>
&lt;p>I’ve been meaning to write a script that would scrape Hacker News in order to show me the top content I missed while sleeping. I had some time this weekend and decided to give it a go using this “pseudo-static” approach. The result is called Yet Another Hacker News Reader (&lt;a href="http://yahnr.dangoldin.com/" target="_blank">YAHNR&lt;/a>) and you can take a look at the code on &lt;a href="https://github.com/dangoldin/yahnr" target="_blank">GitHub&lt;/a>. Turns out it was pretty simple to write and the most difficult part was thinking differently about the problem. Whereas I’d keep the content in a database I ended up storing them in static JSON files and instead of having the logic to generate the HTML page live on a web server I have it using Mustache templates.&lt;/p></description></item><item><title>Just do the work</title><link>/2013/03/09/just-do-the-work/</link><pubDate>Sat, 09 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/09/just-do-the-work/</guid><description>&lt;p>Entrepreneurs are familiar with the elevator pitch. The idea is to give a pitch in 30 seconds (the duration of an elevator ride) that is compelling enough to an investor that it leads to a follow up meeting where you can go through your pitch deck. An entrepreneur coming up with an elevator pitch is similar to a politician trying to come up with sound bites that are easily digestible, look good on the news, and stick in people’s minds.&lt;/p></description></item><item><title>Discriminatory Pricing in the Post Office</title><link>/2013/03/05/discriminatory-pricing-in-the-post-office/</link><pubDate>Tue, 05 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/05/discriminatory-pricing-in-the-post-office/</guid><description>&lt;p>I recently came across Jeff Jordan’s &lt;a href="http://jeff.a16z.com/2012/10/30/avoiding-financial-armageddon-at-the-post-office/" target="_blank">post&lt;/a> about revamping the post office so it’s no longer losing more than $5 billion a year. Jeff suggests the obvious solution of raising prices but I think a more clever approach would be to start price discriminating. Everyone who needs to mail a letter has to pay 46 cents for a stamp but why not come up with tiered pricing. People who need to send something urgently can pay more than a dollar while others who only care that the letter arrives can pay 20 cents. The postal service would need to ensure their systems are able to track how full or empty each shipment is but this would allow them to ship the cheaper, less urgent mail with the more urgent mail to maximize the shipping space. Another way to price discriminate would be to give a discount for mail that’s picked up at the post office within a few days rather than being delivered to the home.&lt;/p></description></item><item><title>App Store, what took so long?</title><link>/2013/03/01/app-store-what-took-so-long/</link><pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate><guid>/2013/03/01/app-store-what-took-so-long/</guid><description>&lt;p>In old versions of the iOS App Store, every time you downloaded a new app it would close the App Store and navigate to the screen with the now-downloading app. Recent versions of the App Store keep it open and force you explicitly exit. I’m surprised that the App Store didn’t launch with the new behavior - it must have been a conscious decision since the development effort for both seems similar.&lt;/p></description></item><item><title>Book Review: Liars and Outliers</title><link>/2013/02/26/book-review-liars-and-outliers/</link><pubDate>Tue, 26 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/26/book-review-liars-and-outliers/</guid><description>&lt;div class="right10">
 &lt;img src="/image/liars-and-outliers-cover.jpg" alt="Liars and Outliers Book Cover" data-width="600" data-height="900" data-layout="responsive" />
&lt;/div>
&lt;p>I’ve been a fan of Bruce Schneier ever since I read his &lt;a href="http://www.schneier.com/blog/archives/2009/11/beyond_security.html" target="_blank">post about security theater&lt;/a> in the post 9/11 world. As soon as I discovered that he wrote a book, &lt;a href="http://www.amazon.com/Liars-Outliers-Enabling-Society-Thrive/dp/1118143302">Liars and Outliers&lt;/a>, I added it to my to-read list and just finished reading it over the weekend. It’s one of those books that is obvious as you read it but spawns a ton of thoughts. He develops a framework that he uses to analyze security and trust in individuals, organizations, and differently-sized societies.&lt;/p></description></item><item><title>Account management madness</title><link>/2013/02/25/account-management-madness/</link><pubDate>Mon, 25 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/25/account-management-madness/</guid><description>&lt;div class="right10">
 &lt;img src="/image/google-accounts.png" alt="My Google Accounts" data-width="332" data-height="460" data-layout="responsive" />
&lt;/div>
&lt;p>Over the past year, I cofounded two startups and launched a bunch of side projects. Since they all had potential, I wanted to make each as standalone as possible and ended up with dedicated accounts for each. This meant that I had a flood of accounts for each, ranging from the various Google products to Sendgrid and AWS to Freshbooks and Quickbooks. Unsurprisingly, this turned out to be an unmanageable pain in the ass.&lt;/p></description></item><item><title>Vacationing with Foursquare</title><link>/2013/02/22/vacationing-with-foursquare/</link><pubDate>Fri, 22 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/22/vacationing-with-foursquare/</guid><description>&lt;ul class="thumbnails">
 &lt;li class="span6">
 &lt;div class="thumbnail">
 &lt;img src="/image/nola-trip-map.png" alt="My NOLA trip map" data-width="640" data-height="383" data-layout="responsive" />
 &lt;/div>
 &lt;/li>
&lt;/ul>
&lt;p>Last week, my wife and I took a vacation to New Orleans and it was the first time we used Foursquare to plan a trip. I asked friends for suggestions, looked at other Foursquare lists, and did some online research to create &lt;a href="https://foursquare.com/dangoldin/list/nola-trip" target="_blank">my list&lt;/a> of 25 places that I wanted to visit while there. These places ranged from tourist magnets such as Bourbon Street and Cafe Du Monde to the more local places like Cafe Envie and Port of Call. Our typical approach in New Orleans was to go to a neighborhood we wanted to explore and then visit the places that were nearby on our Foursquare lists. Out of the 25 places I had on my list, I ended up visiting 16 which is a bit low but I made up for that by visiting a bunch of local places that I wouldn’t have discovered otherwise.&lt;/p></description></item><item><title>Tesla and privacy</title><link>/2013/02/21/tesla-and-privacy/</link><pubDate>Thu, 21 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/21/tesla-and-privacy/</guid><description>&lt;p>What’s lost in the &lt;a href="http://www.teslamotors.com/blog/most-peculiar-test-drive">Tesla/NY Times discussion&lt;/a> is how much information Tesla is collecting. Tesla collected the location, the speed, and the battery charge throughout the journey and referenced it during the rebuttal. Is Tesla collecting this data for every car sold? Do the drivers know this data is being collected? If John Broder knew Tesla had this data from his drive &lt;a href="http://www.nytimes.com/2013/02/10/automobiles/stalled-on-the-ev-highway.html">his review&lt;/a> would have turned out differently. We’re all in favor of truth and honesty in reporting but should it be this easy to share data? What prevents Elon Musk from digging into the driving data of a politician who proposes some legislation that will adversely impact Tesla and finds likely unethical behavior?&lt;/p></description></item><item><title>An analysis of Lincoln's words</title><link>/2013/02/12/an-analysis-of-lincolns-words/</link><pubDate>Tue, 12 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/12/an-analysis-of-lincolns-words/</guid><description>&lt;p>On Saturday, I finished &lt;a href="http://www.amazon.com/Team-Rivals-Political-Abraham-Lincoln/dp/0743270754">Team of Rivals&lt;/a> and while looking at my calendar noticed that it was also &lt;a href="http://en.wikipedia.org/wiki/Abraham_Lincoln">Lincoln&amp;rsquo;s&lt;/a> birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the &lt;a href="http://www.gutenberg.org/files/3253/3253-h/3253-h.htm">7 volume set&lt;/a> containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.&lt;/p></description></item><item><title>More thoughts on online education</title><link>/2013/02/08/more-thoughts-on-online-education/</link><pubDate>Fri, 08 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/08/more-thoughts-on-online-education/</guid><description>&lt;p>I was excited to read that Coursera can now offer classes for college credit. I’m optimistic that this is a start of a trend that will change higher education. At first, this will be adopted by the motivated student - the one who took AP classes in high school and the price sensitive student - the one who took community college classes before transferring to a university. But over time, this will spread until it’s the dominant approach. It’s simply better. It may be more expensive to get a digital class together but it’s primarily an upfront cost that will be spent on getting the best lecturers, developing engaging lectures, and creating varied course content that’s optimized for different learning methods. This content can be accessed from anywhere and is always available.&lt;/p></description></item><item><title>Identifying duplicate bills across states</title><link>/2013/02/05/identifying-duplicate-bills-across-states/</link><pubDate>Tue, 05 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/05/identifying-duplicate-bills-across-states/</guid><description>&lt;p>This past weekend I participated in the &lt;a href="http://www.bdatafest.computationalreporting.com/">Bicoastal Datafest&lt;/a> hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn&amp;rsquo;t be very easy, especially in two days, but I did meet &lt;a href="https://twitter.com/llewellynhinkes">Llewellyn&lt;/a>, a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.&lt;/p></description></item><item><title>Improving online programming classes</title><link>/2013/02/01/improving-online-programming-classes/</link><pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate><guid>/2013/02/01/improving-online-programming-classes/</guid><description>&lt;p>I’m making an effort to freshen up and improve my data skills so when I found out that two of my friends were going to take an &lt;a href="https://class.coursera.org/compdata-002/class/index">R class&lt;/a> on Coursera, I joined them. The class is pretty typical for an online programming class: each week there are a set of lectures to watch, a quiz to take, and a programming exercise to do. In addition to this, we also have a weekly Google Hangout to discuss the lectures, go over our programs, and share our R questions.&lt;/p></description></item><item><title>The Patent troll troll</title><link>/2013/01/29/the-patent-troll-troll/</link><pubDate>Tue, 29 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/29/the-patent-troll-troll/</guid><description>&lt;div class="right10">
 &lt;img src="/image/patent-troll.jpg" alt="Patent troll" data-width="400" data-height="400" data-layout="responsive" />
 &lt;p class="caption">&lt;a href="http://www.article-3.com/fending-off-trolls-and-avoiding-elephants-99273">A Patent Troll&lt;/a>&lt;/p>
&lt;/div>
&lt;p>&lt;a href="http://arstechnica.com/tech-policy/2013/01/how-newegg-crushed-the-shopping-cart-patent-and-saved-online-retail/">Newegg recently defended&lt;/a> itself against a patent troll that sued them over a shopping cart patent. As a result, the patent was invalidated and Soverain Software will lose $2.5M from this and the $18M they won in 2011 from Victoria’s Secret and Avon. Unfortunately, they’ll still keep the tens of millions of dollars they “earned” in earlier years. Since virtually every ecommerce site has a shopping cart feature you’d think that this patent would have been invalidated sooner.&lt;/p></description></item><item><title>Blog updates since November</title><link>/2013/01/24/blog-updates-since-november/</link><pubDate>Thu, 24 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/24/blog-updates-since-november/</guid><description>&lt;p>In November, I migrated my Tumblr and Wordpress blogs over to GitHub pages and have been making a few tweaks here and there. I started with the awesome &lt;a href="http://jekyllbootstrap.com/">Jekyll-Bootstrap library&lt;/a> but wanted to share the changes I’ve made. It’s all hosted on GitHub so feel free to fork it.&lt;/p>
&lt;ul>
&lt;li>
 &lt;b>Design changes&lt;/b>&lt;br/>
 &lt;ul class="bulleted">
 &lt;li>The version I started with didn’t have the Bootstrap responsiveness library so I added that in&lt;/li>
 &lt;li>Since I’m using it primarily as a blog, I updated the design to emphasize the blog aspect&lt;/li>
 &lt;li>Consolidated the pagination and social sharing widget to fit on one line&lt;/li>
 &lt;li>Incorporated some best practices from &lt;a href="http://www.kaikkonendesign.fi/typography/section/1">Kaikkonen's blog typography guide&lt;/a>&lt;/li>
 &lt;/ul>
&lt;/li>
&lt;li>
 &lt;b>Backend changes&lt;/b>&lt;br/>
 &lt;ul class="bulleted">
 &lt;li>Small improvements to SEO by giving ability to add keywords to each page&lt;/li>
 &lt;li>Added Open Graph meta tags to control what’s displayed when people share the page on Facebook&lt;/li>
 &lt;li>Made a few tweaks to the way the sitemap was being generated&lt;/li>
 &lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Why aren't there more engineers in politics?</title><link>/2013/01/23/why-arent-there-more-engineers-in-politics/</link><pubDate>Wed, 23 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/23/why-arent-there-more-engineers-in-politics/</guid><description>&lt;p>At Aaron Swartz’s memorial service in New York, Doc Searle said something that struck a chord: Aaron was one of the few tech people who would get involved in legal and political issues. It’s true - we hackers aren’t into it. We claim we’d be better off if there were more engineers in charge and yet we’re not making an effort to be those engineers. I’ve heard a variety of unconvincing reasons: it’s just not interesting; there’s too much bullshit; it’s more about selling than creating. I think the real reason is that we’re just too impatient.&lt;/p></description></item><item><title>Making sense of my Twitter archive</title><link>/2013/01/19/making-sense-of-my-twitter-archive/</link><pubDate>Sat, 19 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/19/making-sense-of-my-twitter-archive/</guid><description>&lt;p>I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with &lt;a href="http://matplotlib.org/">matplotlib&lt;/a>. The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one &amp;ldquo;gotcha&amp;rdquo; was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I&amp;rsquo;m more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I&amp;rsquo;d like to explore how my usage of Twitter has evolved and also get to play around with the &lt;a href="http://nltk.org/">NLTK library&lt;/a>.&lt;/p></description></item><item><title>What do Fab and Groupon have in common?</title><link>/2013/01/15/what-do-fab-and-groupon-have-in-common/</link><pubDate>Tue, 15 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/15/what-do-fab-and-groupon-have-in-common/</guid><description>&lt;p>&lt;a href="http://www.groupon.com/">Groupon&lt;/a> has fascinated me since they’ve launched. It popularized an entirely new business model, encouraged the launch of hundreds of competitors, and was able to IPO three years after being founded. This sounds great until you look at the performance after the IPO: the stock is down 80% and it’s consistently missing the quarterly goals.&lt;/p>
&lt;p>The daily deals space isn’t as profitable as it used to be and they’re trying to become a tool platform for small businesses. To grow beyond daily deals, they’ve been on an acquisition spree. Over the past two years they’ve acquired a &lt;a href="http://techcrunch.com/2011/12/07/groupon-debuts-scheduler-to-streamline-online-bookings-for-merchants-consumers/">scheduling startup&lt;/a>, a &lt;a href="http://techcrunch.com/2012/01/20/groupon-buys-social-shopping-platform-mertado-to-bolster-groupon-goods">social shopping startup&lt;/a>, &lt;a href="https://upserve.com/platform/restaurant-pos/">POS system&lt;/a>, and a &lt;a href="http://savored.com/">restaurant reservation system&lt;/a>. I don’t think this will be enough for them to get seen as something bigger than daily deals.&lt;/p></description></item><item><title>Is the pen mightier than the sword in a social world?</title><link>/2013/01/11/is-the-pen-mightier-than-the-sword-in-a-social-world/</link><pubDate>Fri, 11 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/11/is-the-pen-mightier-than-the-sword-in-a-social-world/</guid><description>&lt;p>The recent revolt around Instagram’s TOS changes got me thinking about the revolt against SOPA/PIPA and the impact social media is having on cultural participation. We’re wired to want to improve things and when we come across what we feel is an injustice we want to change it. Unfortunately, social media has made us lazy. Sharing something on Twitter or Facebook gives us the nice, warm feeling that we’re actively contributing to a cause. Instead of going out and demonstrating in public, snail mailing our representatives, or providing financial support, we’re clicking a link and think we’re making a difference. PIPA/SOPA wasn’t stopped due to internet outrage but from people calling their representatives and doing more than just mentioning their opposition. Wikipedia and Reddit didn’t just put a message up saying they oppose PIPA/SOPA but blacked out their site. Would the result be the same if they just had a message stating they opposed it?&lt;/p></description></item><item><title>Web scraping like a pro</title><link>/2013/01/09/web-scraping-like-a-pro/</link><pubDate>Wed, 09 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/09/web-scraping-like-a-pro/</guid><description>&lt;p>I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.&lt;/p>
&lt;p>Hope they’re useful!&lt;/p></description></item><item><title>2013 Goals</title><link>/2013/01/02/2013-goals/</link><pubDate>Wed, 02 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/02/2013-goals/</guid><description>&lt;p>Now’s the time people are making resolutions for 2013 so I’m going to join the club. I’m publishing them publicly since that should help my motivation. I’m also calling them goals since a goal seems harder to abandon than a resolution. I’m hoping that having these goals be specific, however arbitrary, will also help me in achieving them.&lt;/p>
&lt;p>Here goes:&lt;/p>
&lt;ul>
	&lt;li>
	&lt;b>Run 1000 miles in 2013&lt;/b>&lt;br/>
	I’ve gotten out of shape over the past couple of months and that’s a bad place to be in the late 20s. I think it’s important to get good habits now since that will help me maintain my health as I get older. Not to mention that being in better physical health will improve my acuity.
	&lt;/li>
	&lt;li>
	&lt;b>Write at least 2 blog posts a week&lt;/b>&lt;br/>
	The more I work with various people the more I realize the importance of communication. Writing doesn’t come easily to me and I spend the majority of time editing but I’m hoping that it’ll be easier by the end of 2013. And although writing is just one aspect of communication, improving that will lay a solid foundation for the others.
	&lt;/li>
	&lt;li>
	&lt;b>Meet up with 2 old acquaintances each month&lt;/b>&lt;br/>
	Going from a company with hundreds of employees to working with a cofounder reduces the number of people you have contact with. By restoring my old relationships I’ll be able to connect with old friends and strengthen my network. Something I’ve learned over the past year of meeting with various folk in the NYC startup community is to end every meeting with an offer to help and I’m going to adopt that attitude as well.
	&lt;/li>
	&lt;li>
	&lt;b>Develop 6 side projects&lt;/b>&lt;br/>
	This one’s here for a few reasons. One, I want to keep on improving and starting a project from scratch is a great way to work with new technologies and explore different approaches. There are many times that I want to go back to my existing code and rewrite it but why fix something that isn’t broken? Isn’t it better to put that energy into something new? Two, I want to give back to the community and putting these projects on GitHub will hopefully help someone. Three, this will just be a good outlet for when I need a break from the main gig. Four, I want to build my personal brand and having more more of my work publicly available will hopefully help.
	&lt;/li>
	&lt;li>
	&lt;b>Start a hands-on hobby&lt;/b>&lt;br/>
	This one stems from a personal belief that I just need to do something with my hands since I spend so much time in front of the computer. This may end up being drawing, painting or woodworking but the goal is to find something that allows me create something physical and not digital. I’ve already dug up some colored pencils and drawing paper and am in the process of signing up for a woodworking class at a hands-on coworking space in Brooklyn called &lt;a href="http://www.3rdward.com/">3rd Ward&lt;/a>.
	&lt;/li>
&lt;/ul></description></item><item><title>Year in Review 2012</title><link>/2013/01/01/year-in-review-2012/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>/2013/01/01/year-in-review-2012/</guid><description>&lt;p>I&amp;rsquo;m finally in a position to do a &amp;ldquo;Year in Review&amp;rdquo; post that I’m comfortable writing. In March I left my full time job to pursue &lt;a href="http://www.glos.si">Glossi&lt;/a>, our startup, full time. In May, we were accepted into an &lt;a href="http://eranyc.com/">incubator&lt;/a> and had an amazingly productive four months. Unfortunately, none of us were passionate about the direction Glossi was headed and we’ve ended the year pursuing a new venture, &lt;a href="http://makersalley.com/">Makers Alley&lt;/a>.&lt;/p></description></item><item><title>Self hosted Instagram export</title><link>/2012/12/19/self-hosted-instagram-export/</link><pubDate>Wed, 19 Dec 2012 00:00:00 +0000</pubDate><guid>/2012/12/19/self-hosted-instagram-export/</guid><description>&lt;p>I just hacked together a quick app to help download Instagram photos. At first, I tried using &lt;a href="http://instaport.me">Instaport&lt;/a> and &lt;a href="https://openphoto.me/">OpenPhoto&lt;/a> but both of them were backed up with others trying to do the same so I decided to create my own. It&amp;rsquo;s basically a really simple python web app that allows you do a quick authentication with Instagram and then lets you downloads all your images to your hard drive.&lt;/p></description></item><item><title>The joy of upgrading an HD in a Macbook</title><link>/2012/12/16/the-joy-of-upgrading-an-hd-in-a-macbook/</link><pubDate>Sun, 16 Dec 2012 00:00:00 +0000</pubDate><guid>/2012/12/16/the-joy-of-upgrading-an-hd-in-a-macbook/</guid><description>&lt;p>As a gift to myself I decided to upgrade the RAM and HD in my MacBook. The plan was to replace the old HD with the new one and then use the install disc to install Snow Leopard on the new HD before upgrading to Mountain Lion. Unfortunately, it turned out that I had a bad install disc and had to come up with another approach. The general idea was to upgrade to Mountain Lion first in order to create another boot disc.&lt;/p></description></item><item><title>Entrepreneurship is not a job</title><link>/2012/11/29/entrepreneurship-is-not-a-job/</link><pubDate>Thu, 29 Nov 2012 00:00:00 +0000</pubDate><guid>/2012/11/29/entrepreneurship-is-not-a-job/</guid><description>&lt;p>
Let’s start with a joke:
&lt;/p>
&lt;blockquote>
 An American consultant was at a pier in a small coastal Mexican village when a small boat with just one fisherman docked. Inside the small boat were several large tuna. The American complimented the Mexican on the quality of his fish and asked how long it took to catch them.&lt;br/>&lt;br/>
 The Mexican replied "Only a little while." The consultant then asked why didn't he stay out longer and catch more fish? The fisherman said he had enough to support his family's immediate needs. Then the American asked how he spent the rest of his time.&lt;br/>&lt;br/>
 The Mexican fisherman said, "I sleep late, fish a little, play with my children, take a siesta with my wife, Maria, and then stroll into the village each evening where I sip wine and play guitar with my amigos. I have a full and busy life, senor."&lt;br/>&lt;br/>
 The American consultant scoffed, "I am a very successful business consultant and could help you. You should spend more time fishing and, with the proceeds, buy a bigger boat. With the proceeds from the bigger boat, you could buy several boats, and eventually you would have a fleet of fishing boats. Instead of selling your catch to a middleman you would sell directly to the processor, eventually opening your own cannery. You would control the product, processing and distribution. You would need to leave this small coastal fishing village and move to Mexico City, then Los Angeles and eventually New York City where you will run your expanding enterprise."&lt;br/>&lt;br/>
 The Mexican fisherman asked, "But senor, how long will this all take?"&lt;br/>&lt;br/>
 The consultant replied, "Probably 15 to 20 years."&lt;br/>&lt;br/>
 "But what then, senor?" asked the fisherman.&lt;br/>&lt;br/>
 The consultant laughed, and said, "That's the best part! When the time is right, you would announce an IPO and sell your company stock to the public. You'll become very rich, you would make millions!"&lt;br/>&lt;br/>
 "Millions, senor?" replied the Mexican. "Then what?"&lt;br/>&lt;br/>
 The American said, "Then you would retire. Move to a small coastal fishing village where you would sleep late, fish a little, play with your kids, take siestas with your wife, stroll to the village in the evenings where you could sip wine and play your guitar with your amigos."
&lt;/blockquote>
&lt;p>Other than the chuckle, this joke got me thinking about how people view work. The joke suggests that you should only work to support your life outside of work. If you work more than that then the joke’s on you since you’re sacrificing your personal life. This is view that you can’t have if you’re starting a company. Entrepreneurs need to combine their personal and professional lives. If you’re running a startup and aren’t thinking about the market, your product, or your users when you’re in the shower or in bed you’re doing it wrong. More importantly, this should be natural and not forced. If you don’t enjoy thinking about your startup when times are good, how will you be able to do it when times are tough (which they will be)? We need our passion to get over the humps so if you’re not passionate about your startup when you’re starting out, you will abandon it when facing challenges. Your startup will end up consuming you so why not pick something that you care about?&lt;/p></description></item><item><title>Github Migration Notes</title><link>/2012/11/14/github-migration-notes/</link><pubDate>Wed, 14 Nov 2012 00:00:00 +0000</pubDate><guid>/2012/11/14/github-migration-notes/</guid><description>&lt;p>After a few hours of solid work I was able to get my new site up and running on Github pages. I got frustrated with
having too many blogs and decided that I should finally get it together and consolidate everything. Within a few hours
I was able to get it up and running on Github pages up and migrated my old Tumblr and Wordpress posts. Hopefully this
encourages me to write more.&lt;/p></description></item><item><title>Hello, Github!</title><link>/2012/11/13/hello-github/</link><pubDate>Tue, 13 Nov 2012 00:00:00 +0000</pubDate><guid>/2012/11/13/hello-github/</guid><description>&lt;p>I&amp;rsquo;m going to work on migrating my posts over from Wordpress and Tumblr on to here. Let&amp;rsquo;s see how it goes.&lt;/p></description></item><item><title>Eating Yourself - Innovation &amp;amp; Cannibalization</title><link>/2012/08/12/eating-yourself-innovation-amp-cannibalization/</link><pubDate>Sun, 12 Aug 2012 00:00:00 +0000</pubDate><guid>/2012/08/12/eating-yourself-innovation-amp-cannibalization/</guid><description>&lt;img src="/image/snakeself.jpg" alt="traffic efficiency" data-width="550" data-height="370" data-layout="responsive" />
&lt;p>I was rereading the HBR paper on &lt;a href="http://hbr.org/2006/10/strategies-for-two-sided-markets/ar/1" target="_blank">Strategies for Two Sided Markets&lt;/a> and came across a passage describing Apple’s mistake of trying to monetize both sides of their market, the consumers and the developers, rather than focusing on one like Microsoft did by giving away the SDK for free.&lt;/p>
&lt;p>It got me thinking about Apple’s recovery. Many people credit the iPod with revitalizing Apple but I think there’s more than that. I suspect the bigger reason was the decline of desktop software and the ability to be productive on the web. Suddenly the network effects that existed by having software that only worked on Windows no longer existed. Software started migrating to the web and people were more willing to try new operating systems out. In 2006, I switched to Linux without too much trouble. It was also simple to find help online to deal with the various issues I ran into which made the transition easier. In some ways, Google helped Apple recover by speeding up the move to the web with a more accurate search and a good set of productivity apps.&lt;/p></description></item><item><title>In Defense of Yahoo</title><link>/2012/08/05/in-defense-of-yahoo/</link><pubDate>Sun, 05 Aug 2012 00:00:00 +0000</pubDate><guid>/2012/08/05/in-defense-of-yahoo/</guid><description>&lt;img alt="Yahoo logo" src="/image/yahoo-logo.png" />
&lt;p>Reading recent tech coverage makes you think that each newly startup is more valuable than Yahoo. Yahoo is the 4th most visited site in the world with over 300 million users on Yahoo mail. This is a problem every startup should hope to have.&lt;/p>
&lt;p>User acquisition is the most difficult task for a consumer startup. User attrition is an easier problem to solve than user acquisition. Yahoo doesn’t need to build a product that’s 10 times better than the competition, they just need to simplify and improve what they already have. Yahoo also has massive usage among the mass market with millions of people having Yahoo as their home page. These are not the same people that sign up for every startup featured on TechCrunch. Yahoo has challenges but worrying about user acquisition is not one of them. Yahoo will need to develop a vision and relentlessly pursue it. The culture will need to change and vested interests will need to be broken.&lt;/p></description></item><item><title>The Startup Advantage - Details, Details, Details</title><link>/2012/07/28/the-startup-advantage-details-details-details/</link><pubDate>Sat, 28 Jul 2012 00:00:00 +0000</pubDate><guid>/2012/07/28/the-startup-advantage-details-details-details/</guid><description>&lt;div>&lt;img src="https://media.tumblr.com/tumblr_m7vxx4vhKa1qz9esq.png"/>&lt;/div>
&lt;p>A frustration I’ve been experiencing more and more is having to reload a webpage in order to change the date range in the options. If a company expects me to keep a site open for more than a day they should make it easy for me to update the options. The big example is Google Analytics - I open up a page, choose a date range, and get to see my charts. If I keep the tab open and want to want to run the same analysis the next day, I’m forced to reload the page to even be able to include today in the date range. It’s an unnecessary action for the user and it would be easy to correct this behavior with some simple Javascript.&lt;/p></description></item><item><title>Race to 0 - RIM vs IE</title><link>/2012/07/10/race-to-0-rim-vs-ie/</link><pubDate>Tue, 10 Jul 2012 00:00:00 +0000</pubDate><guid>/2012/07/10/race-to-0-rim-vs-ie/</guid><description>&lt;p>I read an &lt;a href="http://www.bloomberg.com/news/2012-07-09/rim-s-customers-working-on-contingency-plans-corporate-canada.html" target="_blank">article&lt;/a> earlier today about how companies are preparing for a possible demise of RIM and couldn’t help but compare RIM’s decline over only a few years compared to how long it’s taking IE to disappear.&lt;/p>
&lt;p>To confirm that there is in fact a difference in behavior, we can compare the RIM share among smartphones and IE share among browsers. Turns out that they are noticeably different: IE is on a linear decline with close to 70% in Q3 2008 but around 36% in Q1 2012 while RIM starts at 16% in Q3 2008, goes up to a high of 21% in 2009 and then drops to 7% in Q1 2012. Plotting their % decline since the data starting point highlights this further. If we calculate the average decline per quarter from their highest levels and try to see how long it will take to hit 0% share, IE will take almost 4 years while RIM will take less than 5 quarters.&lt;/p></description></item><item><title>Photo taken near Times Square</title><link>/2012/07/09/photo-taken-near-times-square/</link><pubDate>Mon, 09 Jul 2012 00:00:00 +0000</pubDate><guid>/2012/07/09/photo-taken-near-times-square/</guid><description>&lt;img src="/image/times-square-poster.jpg" alt="Time square poster" data-width="500" data-height="667" data-layout="responsive" /></description></item><item><title>Achieving browser autocomplete</title><link>/2012/06/07/achieving-browser-autocomplete/</link><pubDate>Thu, 07 Jun 2012 00:00:00 +0000</pubDate><guid>/2012/06/07/achieving-browser-autocomplete/</guid><description>&lt;img src="/image/autocomplete.png" alt="Screenshot of date range tool in action" data-width="500" data-height="89" data-layout="responsive" />
&lt;p>Over the past few days, I’ve been thinking about habits. How do they form? How do they change? And the selfish one - how can you build a product that is habit forming? My cofunder sent me a great Nir &amp;amp; Far &lt;a href="http://www.nirandfar.com/2012/03/how-to-manufacture-desire.html">blog post&lt;/a> that goes into detail about generating desire which is a great read to anyone building a consumer product.&lt;/p></description></item><item><title>Trend of actor vs actress age differences</title><link>/2012/05/23/trend-of-actor-vs-actress-age-differences/</link><pubDate>Wed, 23 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/23/trend-of-actor-vs-actress-age-differences/</guid><description>&lt;p>I recently watched &lt;a href="http://www.missrepresentation.org/" title="Miss Representation" target="_blank">Miss Representation&lt;/a> which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.&lt;br/>&lt;br/>I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via &lt;a href="http://www.imdb.com/chart/1910s">&lt;span>&lt;a href="http://www.imdb.com/chart/1910s">&lt;a href="http://www.imdb.com/chart/1910s">http://www.imdb.com/chart/1910s&lt;/a>&lt;/a>&lt;/span>&lt;/a> - &lt;a href="http://www.imdb.com/chart/2010s)">&lt;span>&lt;a href="http://www.imdb.com/chart/2010s">&lt;a href="http://www.imdb.com/chart/2010s">http://www.imdb.com/chart/2010s&lt;/a>&lt;/a>)&lt;/span>&lt;/a> and then pulling the top 5 cast members for each movie (via &lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast)">&lt;span>&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">&lt;a href="http://www.imdb.com/title/tt1375666/fullcredits#cast">http://www.imdb.com/title/tt1375666/fullcredits#cast&lt;/a>&lt;/a>)&lt;/span>&lt;/a>. I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AqnEN-X663bKdGsxdFV4RTlQM21SdW9QRFBqVEVsaUE" target="_blank">Google Spreadsheets&lt;/a> to clean it up. &lt;br/>&lt;br/>Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.&lt;/p></description></item><item><title>Selling to the enterprise? Target the consumer</title><link>/2012/05/22/selling-to-the-enterprise-target-the-consumer/</link><pubDate>Tue, 22 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/22/selling-to-the-enterprise-target-the-consumer/</guid><description>&lt;p>A trend I’ve been noticing more and more is enterprise sales being done bottoms up. The typical approach is to offer a free trials or have some sort of freemium product. Each sign up is then treated as an inbound lead that is assigned an account manager. Within two weeks of signing up for New Relic I was contacted by an account manager who helped answer my questions and helped me get New Relic set up for Glossi. Working with him, we were able to get a longer trial period and a discounted price for when we’re ready to upgrade. &lt;a href="http://blog.hubspot.com/blog/tabid/6307/bid/31555/Inbound-Leads-Cost-61-Less-Than-Outbound-New-Data.aspx" target="_blank">HubSpot found&lt;/a> that inbound leads cost 61% less than outbound leads. If having a strong SEO and Social Media presence drops acquisition costs that much imagine the drop caused by having a usable product. Although we’re a small, scrappy startup that’s quick to try new products and services, I believe this approach will become the standard way of selling SAAS in the enterprise. It’s much easier to get a person to try something new and if you can turn him into a fan, you’re one step closer to getting the company signed up.&lt;/p></description></item><item><title>Peter Thiel's CS183</title><link>/2012/05/16/peter-thiels-cs183/</link><pubDate>Wed, 16 May 2012 00:00:00 +0000</pubDate><guid>/2012/05/16/peter-thiels-cs183/</guid><description>&lt;p>A great blog I’ve recently started following is &lt;a href="http://blakemasters.tumblr.com/peter-thiels-cs183-startup" title="Peter Thiel's CS 183 by Blake Masters" target="_blank">Blake Master’s notes&lt;/a> from Stanford’s CS183 class being taught by Peter Thiel. Peter provides an insightful view of the tech startup world that is valuable to anyone interested in startups and entrepreneurship.&lt;/p></description></item><item><title>Growth of Consumer Comfort with Technology</title><link>/2012/04/25/growth-of-consumer-comfort-with-technology/</link><pubDate>Wed, 25 Apr 2012 00:00:00 +0000</pubDate><guid>/2012/04/25/growth-of-consumer-comfort-with-technology/</guid><description>&lt;p>In a &lt;a href="http://startupmullings.com/post/20825836395/future-of-startups-small-teams-big-profits" title="Future of Startups: Small Teams, Big Profit" target="_blank">previous post&lt;/a>, I discussed the factors that allow small teams to create products that can be exposed to millions of users within a few months. In this post, I want to take a deeper look into why consumers are so much more comfortable with technology now compared to 20 years ago and try to see where this leads. Since customers are what cause our businesses to grow, we need to be cognizant of what drives their behavior in order to plan for the future. Wayne Gretzky’s father famously said “A good hockey player plays where the puck is. A great hockey player plays where the puck is going to be” and I’m hopeful that we’ll be able to see where the consumer puck is going to be.&lt;br/>&lt;br/>To me, the major driver is &lt;a href="http://en.wikipedia.org/wiki/Moore's_law" title="Moore's Law" target="_blank">Moore’s Law&lt;/a>. We’ve seen computation speeds double every 18 months for the past 50 years. This has obviously led to faster computers but has also led to exponentially reducing costs. This has been a huge economic driver and is allowing computers to be more accessible than ever. Our cellphones are more powerful than what was used to land on the moon. These increases in computation also led to the rise of the modern web. It went from being a military/academic project that dealt with text data to something that’s distributing pictures and videos to whoever is interested.&lt;br/>&lt;br/>More importantly, improvements in computation led to improvements in usability. Even if we had modern browser standards like CSS3 and HTML5 in the 1990s our computers would be too weak to handle them. We would not have any of the modern innovations (AJAX, DOM manipulation) and our web pages would be static without any rich media content. If we never got past the command line, how many people would have computers in their home? How many smartphones would exist? I’d argue that the usability improvements are what led to the massive consumer adoption of tech products. Of course, computation, cost, and usability are all intertwined but computation and cost alone would not have led to the consumer adoption we’ve seen.&lt;br/>&lt;br/>What does this mean for the future? I see usability becoming even more native with us not realizing that we’re even using a computer. We’re already seeing this emerging with Siri and Google Glasses. As long as our computation speeds continue to improve these technologies will become better and better and will recede more and more into the background. Of course, this is all dependent on Moore’s Law holding, with many saying the pace will decrease by 2020. I’m optimistic that we’ll come up with something but even if we don’t, as long as we computing costs keep on dropping, via &lt;a href="http://en.wikipedia.org/wiki/Koomey's_law" title="Koomey's Law" target="_blank">Koomey’s Law&lt;/a>, we should still see the benefits as we move more and more computation to the ever cheaper cloud. It’s difficult to imagine what would happen if our computation speeds stop increasing the way they have been over the past 50 years.&lt;/p></description></item><item><title>Draw Something Zyngafied</title><link>/2012/04/12/draw-something-zyngafied/</link><pubDate>Thu, 12 Apr 2012 00:00:00 +0000</pubDate><guid>/2012/04/12/draw-something-zyngafied/</guid><description>&lt;p>We’re aware of Zynga’s purchase of Draw Something and Zynga’s emphasis on analytics and metrics to drive product features and decisions. I’m a bit late to the party but I tried brainstorming to put together  a Zyngafied version of Draw Something:&lt;strong>&lt;br/>&lt;/strong>&lt;/p>
&lt;ul class="bulleted">
 &lt;li>&lt;span>Favor drawings that require colors that a user does not have to encourage the user to buy new colors.&lt;/span>&lt;/li>
&lt;li>&lt;span>Leverage the priming effect by picking words that will encourage users to spend more. For example using the words “gold”, “coin”, and “rich” would put users in a buying mood.&lt;/span>&lt;/li>
&lt;li>&lt;span>Charge more for the more popular color packs.&lt;/span>&lt;/li>
&lt;li>&lt;span>Include “limited edition” color and word packs.&lt;/span>&lt;/li>
&lt;li>&lt;span>Reward active users with free color packs, bombs, etc.&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>I, for one, am glad that I got to play Draw Something before it turns into this money extraction machine.&lt;/p></description></item><item><title>Future of Startups - Small Teams, Big Profit</title><link>/2012/04/10/future-of-startups-small-teams-big-profit/</link><pubDate>Tue, 10 Apr 2012 00:00:00 +0000</pubDate><guid>/2012/04/10/future-of-startups-small-teams-big-profit/</guid><description>&lt;p>The big news today was that Facebook acquired Instagram for $1B in cash and stock. I don’t want to debate whether that was a good price but I am amazed that Instagram was able to get to over 30 million users with 13 employees, of which 3 are engineers. I see a few factors combining to make this an ideal model for the future tech startup.&lt;/p>
&lt;ul class="bulleted">
 &lt;li>Open source tools and the cloud have made starting easier than ever and a few motivated, talented people can build a marketable product over a weekend.&lt;/li>
&lt;li>Social networks simplify distribution and allow a good product to stand out and succeed without heavy marketing.&lt;/li>
&lt;li>People are comfortable with technology and can start using a product without any dedicated support.&lt;/li>
&lt;/ul>
&lt;p>They are converging to provide a massive increase in leverage. A small team is able to quickly and cheaply build a product that can spread organically to millions of users. The enterprise space will also be impacted as people start expecting their personal tools in their corporate environments. It’s definitely an exciting time to be building a tech startup.&lt;/p></description></item><item><title>On Writing</title><link>/2012/04/04/on-writing/</link><pubDate>Wed, 04 Apr 2012 00:00:00 +0000</pubDate><guid>/2012/04/04/on-writing/</guid><description>&lt;p>&lt;span>I recently made an effort to improve my writing and this blog gives me a great way to practice. I force myself to write at least two posts a week, even if it’s just a paragraph. Writing hasn’t come easy to me and I spent more than 20 years returning the favor. In high school, I rarely edited and a quick spell and grammar check was good enough for me. In college, I avoided the writing-heavy classes and the ones I did take I just followed my high school approach. Something changed when I started working. Although initially driven by my desire to perform, I started seeing writing as a challenging, creative process. I remember spending 30 minutes on a paragraph-long email before being comfortable enough to send it out. Even after only a few weeks, I feel that my writing has gotten better - both in terms of speed and clarity. I still have trouble writing long posts since I tend to go on tangents and lose focus.&lt;/span>&lt;br/>&lt;span>&lt;/span>&lt;br/>&lt;span>An issue I’m currently dealing with is deciding when something is “done.” I could always spend more time editing and rewriting but should I? How much editing is a good use of my time? Jack Kerouac wrote the first draft of On The Road in three weeks and the final draft in 20 days. On the other hand, T. S. Eliot wrote The Waste Land over a few years, with the drafts being almost twice as long. I fall somewhere in between. I realize that I learn better through struggle and forcing myself to edit and rewrite helps me in the long term. At the same time, I realize that I have a ton of other things to do and rewriting the same paragraph a dozen times is not the best use of my time. At the moment, I write and rewrite until I’m proud of what I have and hope that it will get easier in the future. As frustrating as it can get, it’s significantly easier than what people were doing only 20 years ago before computers. This thought helps me focus and slowly work my way up to the fabled 10,000 hours.&lt;/span>&lt;/p></description></item><item><title>Where are the event recommendation startups?</title><link>/2012/03/30/where-are-the-event-recommendation-startups/</link><pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate><guid>/2012/03/30/where-are-the-event-recommendation-startups/</guid><description>&lt;p>A few years ago I worked on a startup with the goal of providing local event recommendations. Unfortunately, we were never able to make it work. We focused too much on building new features, didn’t simplify our product enough, didn’t have a focused vision, and didn’t spend time understanding the market. After reading Mark Hendrickson’s &lt;a href="http://techcrunch.com/2012/01/22/post-mortem-for-plancast/" title="A Post-Mortem for Plancast" target="_blank">Plancast’s postmortem&lt;/a>, I started thinking about the problem again and what a successful approach would look like. As Mark pointed out, it’s difficult to incent people to consistently broadcast their plans. Most people will only plan major events in advance and even fewer will log into a website to note that they’re going to grab beers with a friend in a few hours. One thing people are starting to do is checking in to a venue. Knowing the present is a lot simpler than thinking about the future and smartphones have reinforced this behavior. Foursquare has been riding this wave and apps like GroupMe and Fast Society have also taken advantage.&lt;/p></description></item><item><title>Overcoming the Build Bias</title><link>/2012/03/24/overcoming-the-build-bias/</link><pubDate>Sat, 24 Mar 2012 00:00:00 +0000</pubDate><guid>/2012/03/24/overcoming-the-build-bias/</guid><description>&lt;p>An issue I’ve been trying to overcome is what I like to call the “build bias.” Whenever I’d run into a technical problem, I’d want to solve it on my own - whether it’s by writing some code or by installing and configuring various libraries and packages. I remember the time I needed to collect feedback for a website but instead of just using an off the shelf product like &lt;a href="http://getsatisfaction.com/" title="GetSatisfaction" target="_blank">GetSatisfaction&lt;/a>, I decided to create my own. Although I was able to get it working, it took me longer than expected to get it into a usable state and distracted me from the other improvements I wanted to make.&lt;br/>&lt;br/>As a developer, it’s very easy to convince yourself to build from scratch every time you need something rather than using an existing solution. It’s exciting to work on something new and it’s annoying integrating someone else’s code. It’s even worse when they’re charging a few dollars a month for something that you can build in a few hours.&lt;br/>&lt;br/>More often than not we underestimate the cost of building something of sufficient quality and don’t include the ongoing maintenance cost we’ll most likely be doing. More importantly, we are no longer focusing on the highest leverage activity. As they teach in business schools, you shouldn’t outsource your core competency but everything else is fair game. This is also supported by the lean startup approach which encourages getting your product to market as soon as possible so you can validate your market hypotheses. Why spend time building features when you don’t even know you have a marketable product? If it does turn out that you have a successful product you can always go back and develop your own solution then.&lt;br/>&lt;br/>My new process is to first make sure that the feature is even needed. If it is, I check out the open source alternatives to see if anything can be used. If not, I look at the available paid solutions. For many small projects, it turns out that you can ride the trial/basic version enough to validate your idea. This approach has led &lt;a href="http://www.glos.si" title="Glossi" target="_blank">Glossi&lt;/a> to use &lt;a href="https://mongohq.com/home" title="MongoHQ" target="_blank">MongoHQ&lt;/a> to host my database, &lt;a href="http://sendgrid.com/" title="SendGrid" target="_blank">SendGrid&lt;/a> as my email system, and &lt;a href="http://getsatisfaction.com/" title="GetSatisfaction" target="_blank">GetSatisfaction&lt;/a> as a feedback widget in addition to ton of open source libraries. With every new project, I’m offloading more and more of my auxiliary features to cloud based services and feel much more productive. Makes me wonder how many other services there are out there that can be leveraged.&lt;/p></description></item><item><title>When is it time to leave the full time job?</title><link>/2012/03/21/when-is-it-time-to-leave-the-full-time-job/</link><pubDate>Wed, 21 Mar 2012 00:00:00 +0000</pubDate><guid>/2012/03/21/when-is-it-time-to-leave-the-full-time-job/</guid><description>&lt;p>A common question I get when telling my friends that I’m leaving my full time job to work on Glossi is “Why now? Why not continue working on it nights and weekends?” It’s a fair question - we’ve been working on Glossi for 6 months of nights and weekends and made significant progress. Why not keep doing that and have the best of both worlds?&lt;/p>
&lt;p>Everyone has their own reasons but for me it was more of a gut feeling that it was the right time. I had the following thoughts in mind but I didn’t sit down to make a list of pros and cons:&lt;/p></description></item><item><title>On business models - To collect or to fine?</title><link>/2012/03/05/on-business-models-to-collect-or-to-fine/</link><pubDate>Mon, 05 Mar 2012 00:00:00 +0000</pubDate><guid>/2012/03/05/on-business-models-to-collect-or-to-fine/</guid><description>&lt;p>A few years ago, I was on vacation in Italy and spent a good amount of time on trains. Being from the US, I noticed that my ticket was not checked every single ride. At the same time, not having a ticket and being caught carried a large fine. Having the luxury of time, some assumptions, and some algebra, it’s straightforward to work out how to set the fine to make the two systems have the same expected revenue.&lt;/p></description></item><item><title>Shakespeare and Startups</title><link>/2012/01/29/shakespeare-and-startups/</link><pubDate>Sun, 29 Jan 2012 00:00:00 +0000</pubDate><guid>/2012/01/29/shakespeare-and-startups/</guid><description>&lt;p>I was reading Ben Yagoda&amp;#8217;s book, &lt;a href="http://www.amazon.com/When-You-Catch-Adjective-Kill/dp/0767920775" target="_blank">When You Catch An Adjective; Kill It&lt;/a>, when I came across the following passage: &amp;#8220;In Shakespeare&amp;#8217;s day, there were no fancy props, so the text had to do the work of stage settings.&amp;#8221; Although it was referring to starting sentences with conjunctions, it got me thinking about constraints and the way they foster innovation. Startups operate the same way: you don&amp;#8217;t always have the resources to do what you want and are forced to innovate a way out.&lt;/p></description></item><item><title>Improving the Subway User Experience</title><link>/2012/01/12/improving-the-subway-user-experience/</link><pubDate>Thu, 12 Jan 2012 00:00:00 +0000</pubDate><guid>/2012/01/12/improving-the-subway-user-experience/</guid><description>&lt;img src="/image/nyc-subway.jpg" alt="11.7 GB over 11.7 GB" data-width="537" data-height="357" data-layout="responsive" />
&lt;p class="caption">Source: &lt;a href="https://inhabitat.com/">inhabitat.com&lt;/a>&lt;/p>
&lt;p>The combination of taking the subway every day and reading design books had me thinking of ways to improve the subway user experience, other than the obvious one of making it cleaner.&lt;/p>
&lt;p>One thing that struck me is the feeling you get when you see the train leaving the station. It&amp;#8217;s annoyingly stressful and makes me wonder how long I have to wait until the next train comes. Anything that can avoid this outcome would make waiting for the train a better experience. A way to do this is to limit the sensory feedback provided by seeing and hearing it leave. To avoid seeing the train until the last minute, subway stations can be designed to have stairs that need to be climbed in order to get to the platform. This way, the train and track will be hidden until the platform is reached. Making the train quieter would reduce the noise and prevent you from being aware that a train has left.&lt;/p></description></item><item><title>Screwed if you do, screwed if you don't</title><link>/2011/12/30/screwed-if-you-do-screwed-if-you-dont/</link><pubDate>Fri, 30 Dec 2011 00:00:00 +0000</pubDate><guid>/2011/12/30/screwed-if-you-do-screwed-if-you-dont/</guid><description>&lt;amp-img src="{{ IMG_PATH }}go-daddy-logo.jpg" alt="GoDaddy logo" width="2700" height="886" layout="intrinsic"/>
&lt;p>I'm not entirely sure what to think about the GoDaddy/SOPA situation. On one hand, it's great that the online community was able to get GoDaddy to completely reverse their position on SOPA. On the other, it's disappointing that a web company would support it in the first place.&lt;/p>
&lt;p>Should we still be penalizing GoDaddy for their initial SOPA support or move on the same way they did? What type of example does this set for other companies? That they will be judged based on their original position and nothing after? Why even reverse your position if the community will behave as before?&lt;/p></description></item><item><title>GroupOn Scheduler</title><link>/2011/12/08/groupon-scheduler/</link><pubDate>Thu, 08 Dec 2011 00:00:00 +0000</pubDate><guid>/2011/12/08/groupon-scheduler/</guid><description>&lt;img src="/image/groupon-scheduler.png" alt="iOS security recommendation" data-width="616" data-height="358" data-layout="responsive" />
&lt;p>I'm not surprised that someone came out with an &lt;a href="http://www.groupon.com/scheduler" title="GroupOn Scheduler">online scheduling tool&lt;/a> for SMBs. I am a bit surprised that it was GroupOn though. I suspect many smaller companies have tried doing it but found selling to the SMB much more difficult than they expected. Since GroupOn already has penetration in the SMB space they may find it a lot easier, especially if, as it looks, they will be offering it free to any business that runs a GroupOn promotion.&lt;/p></description></item><item><title>Tech interview question</title><link>/2011/01/08/tech-interview-question/</link><pubDate>Sat, 08 Jan 2011 00:00:00 +0000</pubDate><guid>/2011/01/08/tech-interview-question/</guid><description>&lt;p>When conducting interviews, I&amp;rsquo;ve developed the following criteria for a good interview problem:&lt;/p>
&lt;ul class="bulleted">
	&lt;li>Avoid brain teasers - they tend to be hit/miss and some people don't really do well under this type of problem&lt;/li>
	&lt;li>Challenging - the answer should not be immediately obvious and the should require some creativity&lt;/li>
	&lt;li>Rare - similar to above, the problem should not be a common question in order to get&lt;/li>
	&lt;li>Flexible - the problem has multiple solutions and can be modified on the fly for different skill levels&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;ve found that the following problem satisfies the criteria and gives a pretty good sense of a developer&amp;rsquo;s skill level.&lt;/p></description></item><item><title>Google Giving Employees Free Adwords?</title><link>/2010/08/25/google-giving-employees-free-adwords/</link><pubDate>Wed, 25 Aug 2010 00:00:00 +0000</pubDate><guid>/2010/08/25/google-giving-employees-free-adwords/</guid><description>&lt;p>According to this [http://agtb.wordpress.com/2010/01/06/my-1day-adwords-account/](blog post), Gogole gives their emplo﻿yees $1 a day to advertise on Google. The intent is to give employees the perspective of an Adwords user in order to improve the product. In addition to giving employees exposure to Adwords, this also has potential to increase the competition in Adword auctions by causing bids to increase and leading to more revenue  for Google. Given that Google has more than 10,000 employees worldwide this can have an effect on smaller advertisers.&lt;/p></description></item><item><title>More posts coming soon</title><link>/2010/03/10/more-posts-coming-soon/</link><pubDate>Wed, 10 Mar 2010 00:00:00 +0000</pubDate><guid>/2010/03/10/more-posts-coming-soon/</guid><description>&lt;p>I will try a new policy - at least one post a week. Harass me if I&amp;rsquo;m not doing it.&lt;/p></description></item><item><title>Improving your luck</title><link>/2009/09/28/improving-your-luck/</link><pubDate>Mon, 28 Sep 2009 00:00:00 +0000</pubDate><guid>/2009/09/28/improving-your-luck/</guid><description>&lt;p>&lt;img src="/image/seneca-bust.jpg" alt="Seneca Bust">&lt;/p>
&lt;p>People often blame bad luck for their failures. This absolves them of responsibility and allows them to stop trying. What they should have done is admitted their failure, learned from the experience, and prepared themselves for the next opportunity. Exposing yourself to opportunities is the best way to overcome bad luck. Authors are a great example of this: J K Rowling and John Grisham had their novels rejected numerous times before they succeeded. Yet soon after publishing they became blockbusters. How many authors gave up when trying to have their work published? Imagine if they had the determination that J K Rowling and John Grisham had.&lt;/p></description></item><item><title>Power of Twitter</title><link>/2009/03/08/power-of-twitter/</link><pubDate>Sun, 08 Mar 2009 00:00:00 +0000</pubDate><guid>/2009/03/08/power-of-twitter/</guid><description>&lt;p>I may be a bit late to the party but I was finally able to see the power of &lt;a title="Twitter" rel="homepage" href="http://twitter.com" target="_blank">Twitter&lt;/a> this afternoon.&lt;/p>
&lt;p>I kept on getting an "Authentication failed" message when trying to log in to AIM. A few years ago I would not know what to do except ask my friends if they were having any trouble. Right now, I went to searched for "AIM" on Twitter and discovered that other people were having the same problem. Turns out it was a systematic problem and I wasn't the only one affected. Being able to know more about this problem is a great benefit. There has been a lot of talk of the power of real time search and real time news but this was my first real glimpse into the power of Twitter.&lt;/p></description></item><item><title>Enabling modules in Apache2 under Ubuntu</title><link>/2009/01/23/enabling-modules-in-apache2-under-ubuntu/</link><pubDate>Fri, 23 Jan 2009 00:00:00 +0000</pubDate><guid>/2009/01/23/enabling-modules-in-apache2-under-ubuntu/</guid><description>&lt;p>The Apache enabled modules are found in &lt;span class="gray">"/etc/apache2/mods-enabled"&lt;/span> as a set of .load and .conf files. If the modules you want are in the &lt;span class="gray">/etc/apache2/mods-available&lt;/span> folder but not in &lt;span class="gray">"/etc/apache2/mods-enabled"&lt;/span> folder, just copy the .load and .conf files over (note that the .conf file may not exist).&lt;/p>
&lt;p>If there is no file in the mods-availble folder, you will need to create a new .load file in the mods-available folder to point to a module in &lt;span class="gray">"/usr/lib/apache2/modules"&lt;/span>. To do this, create a .load file containing the line &lt;span class="gray">"LoadModule xxx /usr/lib/apache2/modules/yyy.so"&lt;/span> where xxx is the name of the module and yyy is the file name. After creating this file, you can just copy it over to the mods-enabled folder and restart apache using &lt;span class="gray">"sudo /etc/init.d/apache2 restart"&lt;/span>.&lt;/p></description></item><item><title>What's the easiest way to be elected president?</title><link>/2009/01/21/whats-the-easiest-way-to-be-elected-president/</link><pubDate>Wed, 21 Jan 2009 00:00:00 +0000</pubDate><guid>/2009/01/21/whats-the-easiest-way-to-be-elected-president/</guid><description>&lt;p>Answer: Be elected for a first term, the second term will follow.&lt;/p>
&lt;p>It turns out it&amp;rsquo;s pretty likely that a president will be elected to a second term. If we examine all previous Presidential Elections, we will see 8 presidents who failed to get reelected:&lt;/p>
&lt;table class="table">
&lt;thead>
&lt;tr>
 &lt;th>President&lt;/th>
 &lt;th>Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Benjamin Harrison&lt;/td>
&lt;td>Failed to get reelected in 1892&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>George H. W. Bush&lt;/td>
&lt;td>Failed to get reelected in 1992&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Herbert Hoover&lt;/td>
&lt;td>Failed to get reelected in 1932&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Jimmy Carter&lt;/td>
&lt;td>Failed to get reelected in 1980&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>John Quincy Adams&lt;/td>
&lt;td>Failed to get reelected in 1828&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Theodore Roosevelt&lt;/td>
&lt;td>Failed to get reelected in 1912&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>William Henry Harrison&lt;/td>
&lt;td>Failed to get elected in 1836&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>William Howard Taft&lt;/td>
&lt;td>Failed to get reelected in 1912&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;p>On the other hand, if we look at all presidents with 2 or more terms, we only see a few Presidents who have failed to get elected. Some of these, like Andrew Jackson, failed to get elected initially but were then able to get 2 terms in office. Grover Cleveland had non consecutive terms in office. In total, there were 16 presidents who had a second term.&lt;/p></description></item><item><title>More posts coming up</title><link>/2009/01/20/more-posts-coming-up/</link><pubDate>Tue, 20 Jan 2009 00:00:00 +0000</pubDate><guid>/2009/01/20/more-posts-coming-up/</guid><description>&lt;p>I&amp;rsquo;ve been busy recently but have a bunch of topics that I want to write about so keep on checking.&lt;/p>
&lt;p>Thanks for reading!&lt;/p></description></item><item><title>On weekend voting</title><link>/2008/10/24/on-weekend-voting/</link><pubDate>Fri, 24 Oct 2008 00:00:00 +0000</pubDate><guid>/2008/10/24/on-weekend-voting/</guid><description>&lt;p>I found an &lt;a href="http://www.nytimes.com/2008/10/24/opinion/24ornstein.html?ref=opinion">op-ed&lt;/a> in the NY Times that claimed that the best way to increase voter turnout was by having election day fall on a weekend. They provide a few examples but nothing too detailed. I tried pulling in some data and seeing if I could come to the same conclusion. I used two data sets: &lt;a href="http://en.wikipedia.org/wiki/Voter_turnout ">voter turn out by country&lt;/a> and &lt;a href="http://www.electionguide.org/calendar.php ">election dates by country&lt;/a>.&lt;/p></description></item><item><title>Some thoughts on innovation</title><link>/2008/10/01/some-thoughts-on-innovation/</link><pubDate>Wed, 01 Oct 2008 00:00:00 +0000</pubDate><guid>/2008/10/01/some-thoughts-on-innovation/</guid><description>&lt;p>I stumbled unto an &lt;a href="http://blog.modernmechanix.com/2008/03/24/what-will-life-be-like-in-the-year-2008/">article&lt;/a> written in 1968 that tries to predict what the world of 2008 will be like. Usually, these types of predictions are completely off and tend to predict a future far more advanced than what it actually becomes.&lt;/p>
&lt;img data-title="1895 Benz Velo. Along with its contemporary Duryea Motor Wagon, considered the earliest standardized cars. The decade marking further developments in the history of the automobile." src="https://upload.wikimedia.org/wikipedia/commons/1/1e/Benz-velo.jpg" alt="1895 Benz Velo. Along with its contemporary Duryea Motor Wagon, considered the earliest standardized cars. The decade marking further developments in the history of the automobile." />
&lt;p>As expected, the article had it&amp;rsquo;s exaggerations (automatic cars driving at 250 miles per hour, inter-continental rockets, average work day of 4 hours ) but what struck me the most is how accurate the predictions about computers are:&lt;/p></description></item><item><title>Should we apply the EU model to the US</title><link>/2008/09/15/should-we-apply-the-eu-model-to-the-us/</link><pubDate>Mon, 15 Sep 2008 00:00:00 +0000</pubDate><guid>/2008/09/15/should-we-apply-the-eu-model-to-the-us/</guid><description>&lt;p>I've recently been thinking about whether the US can move to a EU like model with each state having control over it's own policies but sharing a &lt;a title="Single market" rel="wikipedia" href="http://en.wikipedia.org/wiki/Single_market" target="_blank">single market&lt;/a> and monetary union. In addition, competition is well regulated and a shared budget exists. In addition, it looks as if this shared budget is a little over 1% of the &lt;a title="Gross National Income" rel="wikipedia" href="http://en.wikipedia.org/wiki/Gross_National_Income" target="_blank">Gross National Income&lt;/a> of the individual countries (1) - imagine a Federal tax rate of 1%.&lt;/p></description></item><item><title>Voting system proposal</title><link>/2008/09/12/voting-system-proposal/</link><pubDate>Fri, 12 Sep 2008 00:00:00 +0000</pubDate><guid>/2008/09/12/voting-system-proposal/</guid><description>&lt;p>It seems that there is a recent emphasis on "character" in the election. Unfortunately, character can be faked with some acting and campaign management. To understand how the candidates will perform in office we need to look at their past accomplishments and failures. Public records show the &lt;a href="http://www.senate.gov/pagelayout/legislative/a_three_sections_with_teasers/votes.htm" target="_blank">vote history&lt;/a> for the candidates and all it takes is a little bit of research to see how the candidates have voted.&lt;/p></description></item><item><title>On TV commercials</title><link>/2008/09/03/on-tv-commercials/</link><pubDate>Wed, 03 Sep 2008 00:00:00 +0000</pubDate><guid>/2008/09/03/on-tv-commercials/</guid><description>&lt;p>It may be my memory but it seems that TV commericals have been getting longer and longer as compared to a decade ago. It would be very interesting to see a plot of the length of the average commercial break over the past few decades - I think we'll see that the length of the average commercial break has drastically increased.&lt;/p>
&lt;p>In addition, it seems as if there are no commerical breaks between consecutive TV shows anymore. Clearly this is a way to keep us from turning off the TV and doing something productive with our lives.&lt;/p></description></item><item><title>Advice on internet passwords</title><link>/2008/06/19/advice-on-internet-passwords/</link><pubDate>Thu, 19 Jun 2008 00:00:00 +0000</pubDate><guid>/2008/06/19/advice-on-internet-passwords/</guid><description>&lt;p>After my previous post on the lack of privacy, I feel obligated to give some advice regarding internet passwords in order to maintain the privacy that we do have.&lt;/p>
&lt;ol>
 &lt;li>Have at least 3 different passwords:
 &lt;ol>
 	&lt;li>E-mail Account&lt;/li>
 This account controls all your other accounts so protect it as much as you can. All other accounts can be accessed or reset if someone has access to your email.
 	&lt;li>Bank/Financial Accounts&lt;/li>
 These control your money so use a different password for these than for the rest of your accounts. In addition, you may want to keep your credit card account passwords separate from your bank accounts.
 	&lt;li>"Fun" Account&lt;/li>
 These may not be vital to your survival (unless you are a facebook addict) so a password compromise here may not affect you too much. In addition, these sites may not store your password as securely as the bank accounts so you don't want this password being the same as the other accounts.
 &lt;/ol>
 &lt;p>A good way to generate passwords is to contain some sort of "base" and add some prefixes or suffixes to it in order to come up with the password for the various sites. For example, I can have my base password be "orange". For financial sites my password will be "orangeFIN22", for my email it will be "orangeE33", etc. Then you don't have to remember an entirely different set of passwords yet they are distinct enough to avoid compromising all your accounts with a stolen password.&lt;/p></description></item><item><title>Privacy in the digital age</title><link>/2008/06/12/privacy-in-the-digital-age/</link><pubDate>Thu, 12 Jun 2008 00:00:00 +0000</pubDate><guid>/2008/06/12/privacy-in-the-digital-age/</guid><description>&lt;p>With so many people joining social networks like Facebook, MySpace, and LinkedIn, it's becoming harder and harder to protect your personal information. If one of your friends happens to add a host of different facebook apps, those apps will have access to his friends' (your) information. There is nothing you can do to stop this unless you either remove all your friends or create very limited profiles.&lt;/p>
&lt;p>In addition, people have come to expect to be able to add you as a friend after they've met you and rejecting them may be construed as anti-social. Imagine a recruiter not being able to look at your information on LinkedIn or a potential date not being able to look at your interests or photos on facebook - you will be missing out on opportunities.&lt;/p></description></item><item><title>Interesting Perl behavior</title><link>/2008/05/30/interesting-perl-behavior/</link><pubDate>Fri, 30 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/30/interesting-perl-behavior/</guid><description>&lt;p>I ran into this problem a while back and wanted to share it. It was a bit unintuitive but documentd so I guess I shouldn't be surprised by the results. Hopefully this will help someone else avoid this pitfall.&lt;/p>
&lt;p>It looks as if declaring a variable with the "my" statement but then guarded with an "if" statement causes the scope of the variable to be global - note that the "use strict 'vars';" pragma does not give an error in this case.&lt;/p></description></item><item><title>Don't judge words by their author</title><link>/2008/05/28/dont-judge-words-by-their-author/</link><pubDate>Wed, 28 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/28/dont-judge-words-by-their-author/</guid><description>&lt;p>A common idiom is "Don't judge a book by its cover" but I think that in this modern age this needs to rehashed into "Don't judge words by their author."&lt;/p>
&lt;p>How often do we look at the author before we read an article or blog post? And how does this impact the way we absorb it? Studies have been done[1] to show that the same words coming from two different people, one a professor and one an average Joe, are interpreted differently: the professor is trusted while the average Joe is not. This can be expanded to any source of information, anything from a book to a YouTube video. In the past, these sources of information were concentrated - not everyone could write a book, but now anyone can start a blog to spread their thoughts and opinions.&lt;/p></description></item><item><title>Martha's Vineyard Lighthouse</title><link>/2008/05/24/marthas-vineyard-lighthouse/</link><pubDate>Sat, 24 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/24/marthas-vineyard-lighthouse/</guid><description>&lt;img src="/image/marthas-vineyard.jpg" alt="Martha's Vineyard view from the balcony" data-width="700" data-height="524" data-layout="responsive" />
&lt;p>A nice change of scenery for the long weekend. It's amazing how quiet the nights are when you are not in the city.&lt;/p></description></item><item><title>Welcome</title><link>/2008/05/21/welcome/</link><pubDate>Wed, 21 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/21/welcome/</guid><description>&lt;p>So I am porting this blog over from wordpress.com to my own local hosting. Please bear with me and I'll hopefully have more things to read soon.&lt;/p>
&lt;p>Edit: I did a rough job changing the dates in the database so the posts should all have the actual post date now.&lt;/p></description></item><item><title>Brainteasers and interviews</title><link>/2008/05/14/brainteasers-and-interviews/</link><pubDate>Wed, 14 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/14/brainteasers-and-interviews/</guid><description>&lt;p>I've recently been reading some articles opposing the use of brainteasers during interviews on the grounds that they are unfair and some people have difficulty thinking on the spot. You can make the same argument for any part of the interview process and I feel that brainteasers may even attract intelligent employees.&lt;/p>
&lt;p>I can come up with a few good reasons to use brainteasers during an interview. One, you are able to determine how well the interviewee thinks as well as their problem solving ability. In addition, if the interviewee does end up getting a job offer, he or she may be more likely to accept it since it was a challenging interview and getting the job feels like an accomplishment - feels better when you have to earn something than when it falls into your lap. The fact that you even asked a brain teaser shows intelligence on your part and you want to attract people who want to work with other smart people, instead of being the big fish in a small pond.&lt;/p></description></item><item><title>Printers no longer come with cables?</title><link>/2008/05/13/printers-no-longer-come-with-cables/</link><pubDate>Tue, 13 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/13/printers-no-longer-come-with-cables/</guid><description>&lt;p>Since when did printers stop coming with the cables? Does this have anything to do with printers being available in USB form and the manufacturers suddenly assuming that everyone already has USB cables? Or is it some agreement that they have with merchants that requires me to pay $20 for a 6&amp;rsquo; cable. Some quick price look ups do show that the standard printer cables cost around the same as USB cables so if they were able to afford to bundle printer cables before USB, they should be able to bundle USB cables now.&lt;/p></description></item><item><title>Why we may never colonize space</title><link>/2008/05/13/why-we-may-never-colonize-space/</link><pubDate>Tue, 13 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/13/why-we-may-never-colonize-space/</guid><description>&lt;p>Although it&amp;rsquo;s wonderful to think that we will be able to colonize other worlds when we grow too numerous or run out of resources, it may not happen. It seems that given the level of current weapons and state of the world we will more likely try to conquer each other than try to conquer space.&lt;/p>
&lt;p>In order to go into space we would need to have an advanced level of technology which could only be created through innovation. I am just worried that technological advances tend to be used as weapons first, and as humanity benefiting objects second. This may have been fine with the technology of old but we are approaching the level where a weapon can wipe us out. The non warlike use may not come to fruition if there will be no one left to develop it.&lt;/p></description></item><item><title>Pronunciation</title><link>/2008/05/08/pronunciation/</link><pubDate>Thu, 08 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/08/pronunciation/</guid><description>&lt;p>Recently I&amp;rsquo;ve discovered a few words where I know the definition and spelling but I don&amp;rsquo;t know the pronunciation. The problem is that I think I know how to pronounce them so when I use them for the first time in conversation, or hear someone using them, some confusion arises (as well as making me look like a fool).&lt;/p>
&lt;p>I am not sure if this is isolated to me or society as a whole. The world does seem to be getting more and more open so maybe this is a result of that - the spoken dictionary is getting smaller and smaller so many of the words that were commonly used in conversations decades ago are no longer being used.&lt;/p></description></item><item><title>On Microsoft walking away from Yahoo</title><link>/2008/05/04/on-microsoft-walking-away-from-yahoo/</link><pubDate>Sun, 04 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/04/on-microsoft-walking-away-from-yahoo/</guid><description>&lt;p>A little unexpected but I think this was the right decision for Microsoft for a couple of reasons:&lt;/p>
&lt;ol>
&lt;li>Jerry Yang was doing everything in his power to prevent MS from acquiring Yahoo - what type of message does that send and how must the employees of Yahoo feel if their CEO is acting this way.&lt;/li>
&lt;li>In order to compete with Google, the merger must have gotten done quickly and smoothly, this would not have been the case in the case of a hostile takeover.&lt;/li>
&lt;li>There have been rumors that many of Yahoo&amp;rsquo;s employees were just waiting for the merger to happen in order to cash in on their new accelerated vesting and compensation packages, immediately leaving Yahoo there after.&lt;/li>
&lt;li>A lot of the Yahoo employees and MSFT employees and investors did not want the merger to go through. From what I know the largest group that wanted the merger to go through were the Yahoo investors, albeit at a higher price.&lt;/li>
&lt;li>Yahoo will now have to deal with a variety of problems: shareholder lawsuits, talk of Jerry&amp;rsquo;s management ability, sudden price drop. All these may in fact lead to another Microsoft offer in maybe a few months at a much lower price - in which case it will probably be accepted.&lt;/li>
&lt;/ol>
&lt;p>The only downside that just immediately to my mind is that MS and Yahoo are in deep water and do need to do something in order to compete with Google - but I do not think the merger would have necessarily helped them, given the merging difficulties and the corporate culture clash.&lt;/p></description></item><item><title>The homeless in the US</title><link>/2008/05/04/the-homeless-in-the-us/</link><pubDate>Sun, 04 May 2008 00:00:00 +0000</pubDate><guid>/2008/05/04/the-homeless-in-the-us/</guid><description>&lt;p>I find it absurd that the average prisoner costs $25,000 a year to keep in prison ($75,000 for death row) and yet we still have homelessness in the United States. Each one of the homeless may start committing some type of crime in order to get into prison and at least not worry about where their next meal comes from yet they are staying on the streets as free men. Isn&amp;rsquo;t there something we can do to encourage good behavior instead of encouraging bad behavior?&lt;/p></description></item><item><title>Video games as art</title><link>/2008/04/29/video-games-as-art/</link><pubDate>Tue, 29 Apr 2008 00:00:00 +0000</pubDate><guid>/2008/04/29/video-games-as-art/</guid><description>&lt;p>It&amp;rsquo;s about time video games are considered an art. They are creative endeavors that take as much and as long to make as some movies. There is also a huge distinction between the great and the poor games. Games these days can be considered movies and they should be treated in the same way.&lt;/p>
&lt;p>Maybe then will video games no longer be the black sheep of the media and entertainment business. It&amp;rsquo;s ridiculous that some guy throwing feces at a canvas is considered higher than the developers of video games.&lt;/p></description></item><item><title>Why does EA want Take Two?</title><link>/2008/04/29/why-does-ea-want-take-two/</link><pubDate>Tue, 29 Apr 2008 00:00:00 +0000</pubDate><guid>/2008/04/29/why-does-ea-want-take-two/</guid><description>&lt;p>I&amp;rsquo;m not sure why no one is pointing this out but it seems that as soon as EA acquires T2, the T2 folks would just leave to create a new studio. They seem independent and I doubt that they&amp;rsquo;d want to work for EA. And although EA would get the rights to all of the T2 games and may try to develop the series, they might not have the imagination or the guys to do it. In addition, I don&amp;rsquo;t know whether EA would even want to develop such products as GTA and Bully given the violence and public relations ordeal that T2 has been going through the past few years.&lt;/p></description></item><item><title>On Apple buying PA Semiconductor</title><link>/2008/04/25/on-apple-buying-pa-semiconductor/</link><pubDate>Fri, 25 Apr 2008 00:00:00 +0000</pubDate><guid>/2008/04/25/on-apple-buying-pa-semiconductor/</guid><description>&lt;p>This post is a response to &lt;a href="http://www.pbs.org/cringely/pulpit/2008/pulpit_20080425_004775.html" target="_blank">Robert Cringely&amp;rsquo;s PBS Post&lt;/a>. He&amp;rsquo;s giving 2 reasons for the acquisition and I wanted to add to two of his points.&lt;/p>
&lt;ul>
&lt;li>The short term reason is to force Intel to give Apple price cuts for fear that Apple will make their own chips: I do not think that Intel needs to worry about Apple manufacturing their own PC chips as Apple already went through that phase and AMD already provides the necessary pressure on Intel to lower their prices.&lt;/li>
&lt;li>In the future, software and OSes will not be tied down to a specific chip so Apple will start manufacturing their own processors to increase their margins: I think the author is on to something regarding the future of processors but I do not think the PC market will change that drastically. Apple will probably start making their own chips for the iPhone and their new gadgets but I doubt they will do the same for the PC market.&lt;/li>
&lt;/ul>
&lt;p>Edit: Just found out that Apple already makes a server (Thanks Brian). I have to start doing some research from now on.&lt;/p></description></item><item><title>About</title><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>&lt;p>I&amp;rsquo;m currently the founder of &lt;a href="https://www.twingdata.com/">Twing Data&lt;/a> and &lt;a href="https://www.fastpentests.com/">FastPenTests&lt;/a>. Twing Data helps companies get the most out of their data warehouse by analyzing every query that runs. FastPenTests offers a 24 hour penetration testing service for websites and APIs in 24 hours.&lt;/p>
&lt;p>Prior to this I led the engineering team at &lt;a href="https://triplelift.com/">TripleLift&lt;/a> that was acquired by Vista Equity Partners for $1.4B in 2021. Before that I cofounded &lt;a href="http://makersalley.com/">Makers Alley&lt;/a> and &lt;a href="https://pando.com/2012/07/17/glossis-social-megafeed-takes-on-rebelmousevizify/">Glossi&lt;/a>. Before going down the entrepreneurial path I did a stint as a quantitative engineer and product manager at Yodle; a wall street quant at Och Ziff, and a management consultant at ZS Associates.&lt;/p></description></item><item><title>Lore</title><link>/lore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/lore/</guid><description>&lt;p>Whenever I come across some computer lore I post a link to it here.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://nedbatchelder.com/blog/200811/print_this_file_your_printer_will_jam.html">Print this file, your printer will jam&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.ibiblio.org/harris/500milemail.html">The case of the 500-mile email&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cgl.uwaterloo.ca/smann/IceCream/humor.html">Car not starting after buying vanilla ice cream&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://catb.org/esr/jargon/html/magic-story.html">Magical switch on an MIT computer&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://patrickthomson.tumblr.com/post/2499755681/the-best-debugging-story-ive-ever-heard">Print jobs failing after 6 hours&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://beza1e1.tuxen.de/lore/"> Collection of others&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>