<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Removing duplicate files in S3 - Dan Goldin</title><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=referrer content="no-referrer"><meta name=description content="A series of shell commands to identify potentially duplicate fils on S3."><meta property="og:site_name" content="Dan Goldin"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:url" content="/2020/12/18/removing-duplicate-files-in-s3/"><meta property="og:title" content="Removing duplicate files in S3"><meta property="og:image" content="/image/photo.jpg"><meta property="og:description" content="A series of shell commands to identify potentially duplicate fils on S3."><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@dangoldin"><meta name=twitter:creator content="@dangoldin"><meta name=twitter:title content="Removing duplicate files in S3"><meta name=twitter:description content="A series of shell commands to identify potentially duplicate fils on S3."><meta name=twitter:image content="/image/photo.jpg"><link rel=canonical href=/2020/12/18/removing-duplicate-files-in-s3/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.min.css integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin=anonymous><link rel=stylesheet href=https://nanx-assets.netlify.app/fonts.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github-gist.min.css integrity="sha512-od7JLoOTxM8w/HSKGzP9Kexc20K9p/M2zxSWsd7H1e4Ctf+8SQFtCWEZnW5u6ul5ehSECa5QmOk9ju2nQMmlVA==" crossorigin=anonymous><link rel="shortcut icon" href=image/favicon.png><link href=/index.xml rel=alternate type=application/rss+xml title="Dan Goldin"></head><body><div class="my-4 my-lg-5 header"><div class=container><div class=row><div class="col-auto offset-lg-2 d-none d-lg-block"><a href=/><img class="ml-lg-4 logo img-fluid d-block rounded-circle" src=/image/photo.jpg alt=logo></a></div><div class="col-auto align-self-center mr-auto"><a href=/><h1 class=name>Dan Goldin</h1></a><ul class="nav nav-primary"><li class=nav-item><a class="nav-link text-home" href=/>Home</a></li><li class=nav-item><a class="nav-link text-about" href=/about/>About</a></li><li class=nav-item><a class="nav-link text-lore" href=/lore/>Lore</a></li></ul></div></div></div></div><div class=content><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><h1 class="mx-0 mx-md-4 blog-post-title">Removing duplicate files in S3</h1><div class="mb-md-4 meta"><span class="date middot" title="Fri Dec 18 2020 00:00:00 UTC">2020-12-18</span>
<span class="reading-time middot">2 min read</span><div class="d-none d-md-inline tags"><ul class="list-unstyled d-inline"><li class="d-inline middot"><a href=/tags/code>code</a></li><li class="d-inline middot"><a href=/tags/devops>devops</a></li></ul></div><div class="d-none d-md-inline tags"><ul class="list-unstyled d-inline"></ul></div></div><div class="markdown blog-post-content"><p>I&rsquo;m a digital hoarder and whenever I had to switch computers, I was always worried about losing files. These days it&rsquo;s both lower risk since so much is scattered across the cloud but with the ascent of AWS I&rsquo;ve resorted to just backing up my computers onto S3.</p><p>I simply do a recursive copy of my home folder to S3 and call it a day. One problem this exposes is that there are duplicate files scattered all over the place. For example I&rsquo;d have something both in my Downloads folder as well as in a Photos and maybe even a Dropbox folder. Or I would just have the same file duplicated in the same directory. At the end of the day it&rsquo;s not a huge deal but at the same time it feels dirty so I started working on a script to identify these duplicates.</p><p>At this point calling it a script is a bit of an overstatement since it&rsquo;s just a series of shell commands that act as a proof of concept. The end goal is to write a script that will accept a set of destinations to analyze, download the potentially similar files, and give users the interactive ability to choose which of the file(s) to keep.</p><p>In any case, one can get pretty far simply using the following shell commands to get the list of files from S3 and then manipulate them to identify the potential duplicates. At that point the filename likely gives aways the obvious duplicates and the rest you can download and compare.</p><p>Next step is to roll this into an actual script that can take multiple directories, run through the steps at once, and then add that interactive way to fetch, display, and delete the files.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># Dump the filenames, dates, and sizes to a text file.</span>
~ aws s3 ls s3://bucket_path/key_path --profile<span style=color:#f92672>=</span>xyz --recursive &gt; s3_files.txt

<span style=color:#75715e># Sort these so we can then count duplicates</span>
~ sort s3_files.txt -k3 -n | tr -s <span style=color:#e6db74>&#39; &#39;</span> | cut -d<span style=color:#e6db74>&#39; &#39;</span> -f3-10 &gt; sorted_s3_files.txt

<span style=color:#75715e># Extract all the duplicated files</span>
cat sorted_s3_files.txt | cut -d<span style=color:#e6db74>&#39; &#39;</span> -f1 | uniq -d &gt; sorted_s3_files_potential_dups.txt

<span style=color:#75715e># Sort both original and duplicate files as text so we can then use the join command</span>
sort sorted_s3_files.txt &gt; sorted_s3_files_str.txt
sort sorted_s3_files_potential_dups.txt &gt; sorted_s3_files_potential_dups_str.txt

<span style=color:#75715e># Inner join the full set and the duplicates to get the file names of the potential duplicates</span>
join sorted_s3_files_potential_dups_str.txt sorted_s3_files_str.txt | sort -n &gt; sorted_s3_files_potential_dups_full_info.txt</code></pre></div></div><div class=navigation><div class=row><div class="col-12 col-lg-6"><div class="mx-0 mx-md-4 mt-4 text-left"><a href=/2020/12/17/what-do-adtech-and-bgp-have-in-common/>« What do adtech and BGP have in common?</a></div></div><div class="col-12 col-lg-6"><div class="mx-0 mx-md-4 mt-4 text-right"><a href=/2020/12/19/blame-the-algorithm/>Blame the algorithm »</a></div></div></div></div></div></div></div></div><section id=comments><div class="py-3 content"><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><div class=comments><script src=https://utteranc.es/client.js repo=dangoldin/blog.dangoldin.com-comments issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></div></div></div></div></div></section><div class="my-4 footer"><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><hr></div></div><div class="row justify-content-center"><div class="col-sm-12 col-lg-2"><div class="mx-0 mx-md-4 site-copyright">© 2021 Dan Goldin</div></div><div class="col-sm-12 col-lg-6"><div class="mx-0 mx-md-4 site-social"><ul><li><a href=mailto:dan@dangoldin.com>Contact</a></li><li><a href=https://github.com/dangoldin target=_blank>GitHub</a></li><li><a href=https://twitter.com/dangoldin target=_blank>Twitter</a></li><li><a href=https://www.linkedin.com/in/dangoldin target=_blank>LinkedIn</a></li><li><a href=/index.xml class=mr-0>RSS</a></li></ul></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js defer></script><script>window.addEventListener('load',function(){hljs.initHighlighting();},true);</script></body></html>